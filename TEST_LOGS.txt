========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 13 items

kornia/test_metrics.py ...F.....F...                                                                                                                                                             [100%]

=============================================================================================== FAILURES ===============================================================================================
__________________________________________________________________________ test_mean_average_precision[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_mean_average_precision(target_framework, mode, backend_compile):
        trace_args = (
            [torch.tensor([[100, 50, 150, 100.]])],
            [torch.tensor([1.])],
            [torch.tensor([0.7])],
            [torch.tensor([[100, 50, 150, 100.]])],
            [torch.tensor([1.])],
            2,
        )
        trace_kwargs = {}
        test_args = (
            [torch.tensor([[50, 25, 75, 50], [100, 50, 150, 100.]])],
            [torch.tensor([1, 2.])],
            [torch.tensor([0.6, 0.8])],
            [torch.tensor([[50, 25, 75, 50], [100, 50, 150, 100.]])],
            [torch.tensor([1, 2.])],
            3,
        )
        kornia.metrics.mean_average_precision(*trace_args)
        kornia.metrics.mean_average_precision(*test_args)
        test_kwargs = {}
    
        # NOTE: this test fails due to the use of dynamic control flow; skipping
>       _test_function(
            kornia.metrics.mean_average_precision,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
            skip=True,
        )

kornia/test_metrics.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function mean_average_precision at 0x7f42bc908940>
trace_args = ([<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shap....,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>], 2)
trace_kwargs = {}
test_args = ([tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], [tensor([0.6000, 0.8000])], [tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], 3)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = True, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function mean_average_precision at 0x7f42bc908940>
trace_args = ([<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shap....,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>], 2)
trace_kwargs = {}
test_args = ([tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], [tensor([0.6000, 0.8000])], [tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], 3)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pred_boxes = [<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>]
pred_labels = [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>], pred_scores = [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.7], dtype=float32)>]
gt_boxes = [<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>], gt_labels = [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]
n_classes = 2, threshold = 0.5

    def tensorflow_mean_average_precision(
        pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels, n_classes, threshold=0.5
    ):
        from ..core._backend import concatenate
        from ..core._backend import tensor
        from ..core._backend import zeros
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.comparison_ops import tensorflow_sort_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .mean_iou import tensorflow_mean_iou
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_max_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_cumsum_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_tolist_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_arange_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
    
        if (
            not len(pred_boxes)
            == len(pred_labels)
            == len(pred_scores)
            == len(gt_boxes)
            == len(gt_labels)
        ):
            raise AssertionError
        gt_images = []
        for i, labels in enumerate(gt_labels):
            gt_images.extend([i] * tensorflow_size_frnt_(labels, 0))
        _gt_boxes = concatenate(gt_boxes, 0)
        _gt_labels = concatenate(gt_labels, 0)
        _gt_images = tensor(gt_images, device=_gt_boxes.device, dtype=tf.int64)
        if (
            not tensorflow_size_frnt_(_gt_images, 0)
            == tensorflow_size_frnt_(_gt_boxes, 0)
            == tensorflow_size_frnt_(_gt_labels, 0)
        ):
            raise AssertionError
        pred_images = []
        for i, labels in enumerate(pred_labels):
            pred_images.extend([i] * tensorflow_size_frnt_(labels, 0))
        _pred_boxes = concatenate(pred_boxes, 0)
        _pred_labels = concatenate(pred_labels, 0)
        _pred_scores = concatenate(pred_scores, 0)
        _pred_images = tensor(pred_images, device=_pred_boxes.device, dtype=tf.int64)
        if (
            not tensorflow_size_frnt_(_pred_images, 0)
            == tensorflow_size_frnt_(_pred_boxes, 0)
            == tensorflow_size_frnt_(_pred_labels, 0)
            == tensorflow_size_frnt_(_pred_scores, 0)
        ):
            raise AssertionError
        average_precisions = zeros(
            n_classes - 1, device=_pred_boxes.device, dtype=_pred_boxes.dtype
        )
        for c in range(1, n_classes):
            gt_class_images = tensorflow_get_item(_gt_images, _gt_labels == c)
            gt_class_boxes = tensorflow_get_item(_gt_boxes, _gt_labels == c)
            gt_class_boxes_detected = zeros(
                tensorflow_size_frnt_(gt_class_images, 0),
                dtype=tf.uint8,
                device=gt_class_images.device,
            )
            pred_class_images = tensorflow_get_item(_pred_images, _pred_labels == c)
            pred_class_boxes = tensorflow_get_item(_pred_boxes, _pred_labels == c)
            pred_class_scores = tensorflow_get_item(_pred_scores, _pred_labels == c)
            n_class_detections = tensorflow_size_frnt_(pred_class_boxes, 0)
            if n_class_detections == 0:
                continue
            pred_class_scores, sort_ind = tensorflow_sort_frnt(
                pred_class_scores, dim=0, descending=True
            )
            pred_class_images = tensorflow_get_item(pred_class_images, sort_ind)
            pred_class_boxes = tensorflow_get_item(pred_class_boxes, sort_ind)
            gt_positives = zeros(
                (n_class_detections,),
                dtype=pred_class_boxes.dtype,
                device=pred_class_boxes.device,
            )
            false_positives = zeros(
                (n_class_detections,),
                dtype=pred_class_boxes.dtype,
                device=pred_class_boxes.device,
            )
            for d in range(n_class_detections):
                this_detection_box = tensorflow_unsqueeze_frnt_(
                    tensorflow_get_item(pred_class_boxes, d), 0
                )
                this_image = tensorflow_get_item(pred_class_images, d)
                object_boxes = tensorflow_get_item(
                    gt_class_boxes, gt_class_images == this_image
                )
                if tensorflow_size_frnt_(object_boxes, 0) == 0:
                    false_positives = tensorflow_set_item_bknd(false_positives, d, 1)
                    continue
>               overlaps = tensorflow_mean_iou.mean_iou_bbox(
                    this_detection_box, object_boxes
                )
E               AttributeError: 'function' object has no attribute 'mean_iou_bbox'

Translated_Outputs/tensorflow_outputs/kornia/metrics/mean_average_precision.py:131: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.metrics.mean_average_precision.mean_average_precision
___________________________________________________________________________________ test_SSIM[tensorflow-s2s-False] ____________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SSIM(target_framework, mode, backend_compile):
        print("kornia.metrics.SSIM")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSSIM = ivy.transpile(kornia.metrics.SSIM, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(1, 4, 5, 5),
            torch.rand(1, 4, 5, 5),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_ssim = kornia.metrics.SSIM(5)
        transpiled_ssim = TranspiledSSIM(5)
    
        torch_out = torch_ssim(*torch_args)
>       transpiled_out = transpiled_ssim(*transpiled_args)

kornia/test_metrics.py:280: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SSIM()
args = (<tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.82108784, 0.15627497, 0.15522093, 0.49680072, 0.273...42107862, 0.7091143 ],
         [0.02500224, 0.88032764, 0.8708512 , 0.9686694 , 0.4593824 ]]]],
      dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f42b6b99600, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SSIM(), <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.82108784, 0.15627497, 0.15522093...42107862, 0.7091143 ],
         [0.02500224, 0.88032764, 0.8708512 , 0.9686694 , 0.4593824 ]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SSIM(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.82108784, 0.15627497, 0.15522093, 0.49680072, 0.273...42107862, 0.7091143 ],
         [0.02500224, 0.88032764, 0.8708512 , 0.9686694 , 0.4593824 ]]]],
      dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SSIM(), <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.82108784, 0.15627497, 0.15522093...42107862, 0.7091143 ],
         [0.02500224, 0.88032764, 0.8708512 , 0.9686694 , 0.4593824 ]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SSIM(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.82108784, 0.15627497, 0.15522093, 0.49680072, 0.273...42107862, 0.7091143 ],
         [0.02500224, 0.88032764, 0.8708512 , 0.9686694 , 0.4593824 ]]]],
      dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.82108784, 0.15627497, 0.15522093, 0.49680072, 0.2736....18752849, 0.19370598],
         [0.6431256 , 0.35166383, 0.8772307 , 0.6037711 , 0.2267698 ]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (img1, img2)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SSIM(),)
kwargs = {'img1': <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.82108784, 0.15627497, 0.15522093, 0.4968007...42107862, 0.7091143 ],
         [0.02500224, 0.88032764, 0.8708512 , 0.9686694 , 0.4593824 ]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SSIM()
img1 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.82108784, 0.15627497, 0.15522093, 0.49680072, 0.2736....18752849, 0.19370598],
         [0.6431256 , 0.35166383, 0.8772307 , 0.6037711 , 0.2267698 ]]]],
      dtype=float32)>
img2 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.09862727, 0.44072777, 0.28755218, 0.03153324, 0.6999....42107862, 0.7091143 ],
         [0.02500224, 0.88032764, 0.8708512 , 0.9686694 , 0.4593824 ]]]],
      dtype=float32)>

    def call(self, img1, img2):
>       return tensorflow_ssim(
            img1, img2, self.window_size, self.max_val, self.eps, self.padding
        )

Translated_Outputs/tensorflow_outputs/kornia/metrics/ssim.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

img1 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.82108784, 0.15627497, 0.15522093, 0.49680072, 0.2736....18752849, 0.19370598],
         [0.6431256 , 0.35166383, 0.8772307 , 0.6037711 , 0.2267698 ]]]],
      dtype=float32)>
img2 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.09862727, 0.44072777, 0.28755218, 0.03153324, 0.6999....42107862, 0.7091143 ],
         [0.02500224, 0.88032764, 0.8708512 , 0.9686694 , 0.4593824 ]]]],
      dtype=float32)>
window_size = 5, max_val = 1.0, eps = 1e-12, padding = 'same'

    def tensorflow_ssim(img1, img2, window_size, max_val=1.0, eps=1e-12, padding="same"):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..filters.kernels import tensorflow_get_gaussian_kernel1d
        from ..filters.filter import tensorflow_filter2d_separable
        from ..filters.filter import tensorflow__compute_padding
    
        if not isinstance(img1, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input img1 type is not a torch.Tensor. Got {type(img1)}")
        if not isinstance(img2, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input img2 type is not a torch.Tensor. Got {type(img2)}")
        if not isinstance(max_val, (float,)):
            raise TypeError(f"Input max_val type is not a float. Got {type(max_val)}")
        if not len(tensorflow_shape_frnt_(img1)) == 4:
            raise ValueError(
                f"Invalid img1 shape, we expect BxCxHxW. Got: {tensorflow_shape_frnt_(img1)}"
            )
        if not len(tensorflow_shape_frnt_(img2)) == 4:
            raise ValueError(
                f"Invalid img2 shape, we expect BxCxHxW. Got: {tensorflow_shape_frnt_(img2)}"
            )
        if not tensorflow_shape_frnt_(img1) == tensorflow_shape_frnt_(img2):
            raise ValueError(
                f"img1 and img2 shapes must be the same. Got: {tensorflow_shape_frnt_(img1)} and {tensorflow_shape_frnt_(img2)}"
            )
        kernel: typing.Any = tensorflow_get_gaussian_kernel1d(
            window_size, 1.5, device=img1.device, dtype=img1.dtype
        )
        C1: typing.Any = (0.01 * max_val) ** 2
        C2: typing.Any = (0.03 * max_val) ** 2
>       mu1: typing.Any = tensorflow_filter2d_separable(img1, kernel, kernel)

Translated_Outputs/tensorflow_outputs/kornia/metrics/ssim.py:79: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.82108784, 0.15627497, 0.15522093, 0.49680072, 0.2736....18752849, 0.19370598],
         [0.6431256 , 0.35166383, 0.8772307 , 0.6037711 , 0.2267698 ]]]],
      dtype=float32)>
kernel_x = <tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]],
      dtype=float32)>
kernel_y = <tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]],
      dtype=float32)>, border_type = 'reflect', normalized = False
padding = 'same'

    def tensorflow_filter2d_separable(
        input, kernel_x, kernel_y, border_type="reflect", normalized=False, padding="same"
    ):
>       out_x = tensorflow_filter2d(
            input, kernel_x[..., None, :], border_type, normalized, padding
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.15522093, 0.15627497, 0.82108784, 0.15627497, 0.1552...0.6431256 , 0.35166383, 0.8772307 ,
          0.6037711 , 0.2267698 , 0.6037711 , 0.8772307 ]]]],
      dtype=float32)>
kernel = <tf.Tensor: shape=(1, 1, 5), dtype=float32, numpy=
array([[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]],
      dtype=float32)>, border_type = 'reflect', normalized = False
padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.15522093, 0.15627497, 0.82108784, 0.15627497, 0.1552...0.6431256 , 0.35166383, 0.8772307 ,
          0.6037711 , 0.2267698 , 0.6037711 , 0.8772307 ]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 1, 5), dtype=float32, numpy=
array([[[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.1200...88074, 0.12007838]]],


       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 4

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.15522093, 0.15627497, 0.82108784, 0.15627497, 0.1552...0.6431256 , 0.35166383, 0.8772307 ,
          0.6037711 , 0.2267698 , 0.6037711 , 0.8772307 ]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 1, 5), dtype=float32, numpy=
array([[[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.1200...88074, 0.12007838]]],


       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 4

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.15522093, 0.15627497, 0.82108784, 0.15627497, 0.155...

       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f42b699aef0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f42b69cfb50>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.15522093, 0.15627497, 0.82108784, 0.15627497, 0.155...

       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f42b69cfb50>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f42b699ab90>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f42b699aef0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f42b69ccdc0>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 5, 9, 4), dtype=float32, numpy=
array([[[[0.15522093, 0.01155192, 0.11686921, 0.10643464],
     ....8466155 , 0.3958552 , 0.6037711 ],
         [0.37000442, 0.7816157 , 0.6035077 , 0.8772307 ]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(1, 5, 1, 4), dtype=float32, numpy=
array([[[[0.12007838, 0.12007838, 0.12007838, 0.12007838]],

   ...3388074, 0.23388074, 0.23388074]],

        [[0.12007838, 0.12007838, 0.12007838, 0.12007838]]]],
      dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 9, 4), dtype=float32, numpy=
array([[[[0.15522093, 0.01155192, 0.11686921, 0.10643464],
    ...2007838],
         [0.12007838],
         [0.12007838],
         [0.12007838]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f42b699aef0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f42b69cfb50>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 9, 4, 5), dtype=float32, numpy=
array([[[[0.15522093, 0.29031557, 0.5689731 , 0.41936344, 0.3700....03052193, 0.6035077 ],
         [0.10643464, 0.91405225, 0.25654817, 0.48549873, 0.8772307 ]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(1, 5, 4, 1), dtype=float32, numpy=
array([[[[0.12007838],
         [0.12007838],
         [0.120078...8074]],

        [[0.12007838],
         [0.12007838],
         [0.12007838],
         [0.12007838]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SSIM.call().
E       
E       [1m{{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 5 vs 4 [Op:DepthwiseConv2dNative] name: [0m
E       
E       Arguments received by tensorflow_SSIM.call():
E         â€¢ img1=tf.Tensor(shape=(1, 4, 5, 5), dtype=float32)
E         â€¢ img2=tf.Tensor(shape=(1, 4, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.metrics.SSIM
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_metrics.py::test_mean_average_precision[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'mean_iou_bbox'
FAILED kornia/test_metrics.py::test_SSIM[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SSIM.call().
=============================================================================== 2 failed, 11 passed in 222.36s (0:03:42) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 57 items

kornia/geometry/test_transform.py ......................FFF..................FFFF....F..FFF                                                                                                      [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________________ test_build_pyramid[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_build_pyramid(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 8, 8),
            3,
        )
        trace_kwargs = {'border_type': 'reflect', 'align_corners': False}
        test_args = (
            torch.rand(5, 3, 16, 16),
            4,
        )
        test_kwargs = {'border_type': 'reflect', 'align_corners': False}
>       _test_function(
            kornia.geometry.transform.build_pyramid,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_transform.py:566: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function build_pyramid at 0x7f0701c9a170>
trace_args = (tensor([[[[0.0963, 0.4922, 0.8967, 0.9519, 0.1275, 0.6047, 0.5132, 0.3163],
          [0.9017, 0.6016, 0.9002, 0.9509...96, 0.5591, 0.8121, 0.2252, 0.2375],
          [0.8600, 0.1501, 0.7279, 0.5785, 0.4999, 0.1718, 0.4078, 0.5279]]]]), 3)
trace_kwargs = {'align_corners': False, 'border_type': 'reflect'}
test_args = (tensor([[[[5.3149e-01, 9.0878e-01, 6.2021e-01,  ..., 8.9507e-02,
           9.7812e-01, 6.6175e-01],
          [5.234....9847e-01],
          [5.9953e-01, 8.8640e-02, 7.7515e-02,  ..., 8.9824e-01,
           4.4559e-01, 9.5967e-01]]]]), 4)
test_kwargs = {'align_corners': False, 'border_type': 'reflect'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function build_pyramid at 0x7f0701c9a170>
trace_args = (tensor([[[[0.0963, 0.4922, 0.8967, 0.9519, 0.1275, 0.6047, 0.5132, 0.3163],
          [0.9017, 0.6016, 0.9002, 0.9509...96, 0.5591, 0.8121, 0.2252, 0.2375],
          [0.8600, 0.1501, 0.7279, 0.5785, 0.4999, 0.1718, 0.4078, 0.5279]]]]), 3)
trace_kwargs = {'align_corners': False, 'border_type': 'reflect'}
test_args = (tensor([[[[5.3149e-01, 9.0878e-01, 6.2021e-01,  ..., 8.9507e-02,
           9.7812e-01, 6.6175e-01],
          [5.234....9847e-01],
          [5.9953e-01, 8.8640e-02, 7.7515e-02,  ..., 8.9824e-01,
           4.4559e-01, 9.5967e-01]]]]), 4)
test_kwargs = {'align_corners': False, 'border_type': 'reflect'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 8, 8), dtype=float32, numpy=
array([[[[0.09631956, 0.49216604, 0.896742  , 0.9519202 , 0.1275...0824, 0.1501236 , 0.7278852 , 0.5784766 , 0.4998973 ,
          0.17176473, 0.40776652, 0.52786785]]]], dtype=float32)>
max_level = 3, border_type = 'reflect', align_corners = False

    def tensorflow_build_pyramid(
        input, max_level, border_type="reflect", align_corners=False
    ):
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...core.check import tensorflow_KORNIA_CHECK
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK(
            isinstance(max_level, (int,)) or max_level < 0,
            f"Invalid max_level, it must be a positive integer. Got: {max_level}",
        )
        pyramid: typing.Any = []
        pyramid.append(input)
        for _ in range(max_level - 1):
            img_curr: typing.Any = pyramid[-1]
>           img_down: typing.Any = tensorflow_pyrdown(img_curr, border_type, align_corners)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 8, 8), dtype=float32, numpy=
array([[[[0.09631956, 0.49216604, 0.896742  , 0.9519202 , 0.1275...0824, 0.1501236 , 0.7278852 , 0.5784766 , 0.4998973 ,
          0.17176473, 0.40776652, 0.52786785]]]], dtype=float32)>
border_type = 'reflect', align_corners = False, factor = 2.0

    def tensorflow_pyrdown(input, border_type="reflect", align_corners=False, factor=2.0):
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...filters.filter import tensorflow_filter2d
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_interpolate_frnt,
        )
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        kernel: typing.Any = tensorflow__get_pyramid_gaussian_kernel()
        _, _, height, width = tensorflow_shape_frnt_(input)
>       x_blur: typing.Any = tensorflow_filter2d(input, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.9217987 , 0.30579418, 0.95553195, 0.30579418, 0.92...      0.30352247, 0.05798084, 0.06640637, 0.95604944, 0.98680353,
          0.95604944, 0.06640637]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=
array([[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625... 0.0625    , 0.015625  ],
        [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]],
      dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.9217987 , 0.30579418, 0.95553195, 0.30579418, 0.92...      0.30352247, 0.05798084, 0.06640637, 0.95604944, 0.98680353,
          0.95604944, 0.06640637]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.0039....0625    , 0.015625  ],
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.9217987 , 0.30579418, 0.95553195, 0.30579418, 0.92...      0.30352247, 0.05798084, 0.06640637, 0.95604944, 0.98680353,
          0.95604944, 0.06640637]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.0039....0625    , 0.015625  ],
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.9217987 , 0.30579418, 0.95553195, 0.30579418, 0.9...,
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa3d1870>
tensorflow_get_item = <function tensorflow_get_item at 0x7f06fa3cd6c0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.9217987 , 0.30579418, 0.95553195, 0.30579418, 0.9...,
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f06fa3cd6c0>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f06fa3d00d0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa3d1870>
tensorflow_asarray = <function tensorflow_asarray at 0x7f06fa3ceb00>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 12, 12, 3), dtype=float32, numpy=
array([[[[0.9217987 , 0.22600561, 0.2534815 ],
         [0.305...0353],
         [0.6470182 , 0.42483568, 0.95604944],
         [0.49340206, 0.23500079, 0.06640637]]]], dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 1, 3), dtype=float32, numpy=
array([[[[0.00390625, 0.00390625, 0.00390625]],

        [[0.015...]],

        [[0.015625  , 0.015625  , 0.015625  ]],

        [[0.00390625, 0.00390625, 0.00390625]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 12, 12, 3), dtype=float32, numpy=
array([[[[0.9217987 , 0.22600561, 0.2534815 ],
         [0.30...625  ]],

        [[0.00390625],
         [0.00390625],
         [0.00390625]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa3d1870>
tensorflow_get_item = <function tensorflow_get_item at 0x7f06fa3cd6c0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 12, 3, 12), dtype=float32, numpy=
array([[[[0.9217987 , 0.9001815 , 0.896742  , 0.9001815 , 0.92...      0.8710685 , 0.7707055 , 0.06640637, 0.812144  , 0.17176473,
          0.812144  , 0.06640637]]]], dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 3, 1), dtype=float32, numpy=
array([[[[0.00390625],
         [0.00390625],
         [0.003906...25  ],
         [0.015625  ]],

        [[0.00390625],
         [0.00390625],
         [0.00390625]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 12, 3, 12), dtype=float32, numpy=
array([[[[0.9217987 , 0.9001815 , 0.896742  , 0.9001815 , 0.9...0.00390625],
         [0.00390625]]]], dtype=float32)>, [1, 1, 1, 1], [(0, 0), (0, 0), (0, 0), (0, 0)], 'NHWC', [1, 1])
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 12 vs 3 [Op:DepthwiseConv2dNative] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.pyramid.build_pyramid
__________________________________________________________________________ test_build_laplacian_pyramid[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_build_laplacian_pyramid(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 8, 8),
            3,
        )
        trace_kwargs = {'border_type': 'reflect', 'align_corners': False}
        test_args = (
            torch.rand(5, 3, 16, 16),
            4,
        )
        test_kwargs = {'border_type': 'reflect', 'align_corners': False}
>       _test_function(
            kornia.geometry.transform.build_laplacian_pyramid,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_transform.py:590: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function build_laplacian_pyramid at 0x7f0701c9a320>
trace_args = (tensor([[[[0.9851, 0.3279, 0.9483, 0.3455, 0.6548, 0.6760, 0.5540, 0.2177],
          [0.6003, 0.7231, 0.8606, 0.0187...66, 0.1019, 0.3838, 0.0977, 0.7173],
          [0.8555, 0.2511, 0.9494, 0.7662, 0.1841, 0.5240, 0.0021, 0.1587]]]]), 3)
trace_kwargs = {'align_corners': False, 'border_type': 'reflect'}
test_args = (tensor([[[[8.7777e-01, 4.5732e-01, 2.1718e-01,  ..., 7.3053e-01,
           2.4271e-01, 8.2242e-01],
          [5.118....1997e-01],
          [6.1045e-01, 6.8920e-01, 5.2750e-01,  ..., 2.8884e-01,
           4.6389e-01, 7.8474e-01]]]]), 4)
test_kwargs = {'align_corners': False, 'border_type': 'reflect'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function build_laplacian_pyramid at 0x7f0701c9a320>
trace_args = (tensor([[[[0.9851, 0.3279, 0.9483, 0.3455, 0.6548, 0.6760, 0.5540, 0.2177],
          [0.6003, 0.7231, 0.8606, 0.0187...66, 0.1019, 0.3838, 0.0977, 0.7173],
          [0.8555, 0.2511, 0.9494, 0.7662, 0.1841, 0.5240, 0.0021, 0.1587]]]]), 3)
trace_kwargs = {'align_corners': False, 'border_type': 'reflect'}
test_args = (tensor([[[[8.7777e-01, 4.5732e-01, 2.1718e-01,  ..., 7.3053e-01,
           2.4271e-01, 8.2242e-01],
          [5.118....1997e-01],
          [6.1045e-01, 6.8920e-01, 5.2750e-01,  ..., 2.8884e-01,
           4.6389e-01, 7.8474e-01]]]]), 4)
test_kwargs = {'align_corners': False, 'border_type': 'reflect'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 8, 8), dtype=float32, numpy=
array([[[[0.985078  , 0.32791352, 0.9483353 , 0.34548193, 0.6547...398 , 0.2510711 , 0.9494076 , 0.76617074, 0.18409652,
          0.5239864 , 0.00210762, 0.1586641 ]]]], dtype=float32)>
max_level = 3, border_type = 'reflect', align_corners = False

    def tensorflow_build_laplacian_pyramid(
        input, max_level, border_type="reflect", align_corners=False
    ):
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...core.check import tensorflow_KORNIA_CHECK
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...core._backend import pad
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK(
            isinstance(max_level, (int,)) or max_level < 0,
            f"Invalid max_level, it must be a positive integer. Got: {max_level}",
        )
        h = tensorflow_size_frnt_(input)[2]
        w = tensorflow_size_frnt_(input)[3]
        require_padding = not (tensorflow_is_powerof_two(w) or tensorflow_is_powerof_two(h))
        if require_padding:
            padding = (
                0,
                tensorflow_find_next_powerof_two(w) - w,
                0,
                tensorflow_find_next_powerof_two(h) - h,
            )
            input = pad(input, padding, "reflect")
>       gaussian_pyramid: typing.Any = tensorflow_build_pyramid(
            input, max_level, border_type, align_corners
        )

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:146: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 8, 8), dtype=float32, numpy=
array([[[[0.985078  , 0.32791352, 0.9483353 , 0.34548193, 0.6547...398 , 0.2510711 , 0.9494076 , 0.76617074, 0.18409652,
          0.5239864 , 0.00210762, 0.1586641 ]]]], dtype=float32)>
max_level = 3, border_type = 'reflect', align_corners = False

    def tensorflow_build_pyramid(
        input, max_level, border_type="reflect", align_corners=False
    ):
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...core.check import tensorflow_KORNIA_CHECK
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK(
            isinstance(max_level, (int,)) or max_level < 0,
            f"Invalid max_level, it must be a positive integer. Got: {max_level}",
        )
        pyramid: typing.Any = []
        pyramid.append(input)
        for _ in range(max_level - 1):
            img_curr: typing.Any = pyramid[-1]
>           img_down: typing.Any = tensorflow_pyrdown(img_curr, border_type, align_corners)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 8, 8), dtype=float32, numpy=
array([[[[0.985078  , 0.32791352, 0.9483353 , 0.34548193, 0.6547...398 , 0.2510711 , 0.9494076 , 0.76617074, 0.18409652,
          0.5239864 , 0.00210762, 0.1586641 ]]]], dtype=float32)>
border_type = 'reflect', align_corners = False, factor = 2.0

    def tensorflow_pyrdown(input, border_type="reflect", align_corners=False, factor=2.0):
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...filters.filter import tensorflow_filter2d
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_interpolate_frnt,
        )
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        kernel: typing.Any = tensorflow__get_pyramid_gaussian_kernel()
        _, _, height, width = tensorflow_shape_frnt_(input)
>       x_blur: typing.Any = tensorflow_filter2d(input, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.23787397, 0.33987343, 0.10016662, 0.33987343, 0.23...      0.4044357 , 0.76816124, 0.78535664, 0.14778477, 0.65196097,
          0.14778477, 0.78535664]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=
array([[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625... 0.0625    , 0.015625  ],
        [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]],
      dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.23787397, 0.33987343, 0.10016662, 0.33987343, 0.23...      0.4044357 , 0.76816124, 0.78535664, 0.14778477, 0.65196097,
          0.14778477, 0.78535664]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.0039....0625    , 0.015625  ],
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.23787397, 0.33987343, 0.10016662, 0.33987343, 0.23...      0.4044357 , 0.76816124, 0.78535664, 0.14778477, 0.65196097,
          0.14778477, 0.78535664]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.0039....0625    , 0.015625  ],
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.23787397, 0.33987343, 0.10016662, 0.33987343, 0.2...,
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa36d480>
tensorflow_get_item = <function tensorflow_get_item at 0x7f06f9e21d80>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.23787397, 0.33987343, 0.10016662, 0.33987343, 0.2...,
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f06f9e21d80>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f06fa36cdc0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa36d480>
tensorflow_asarray = <function tensorflow_asarray at 0x7f06fa1879a0>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 12, 12, 3), dtype=float32, numpy=
array([[[[0.23787397, 0.9282101 , 0.7600628 ],
         [0.339...6097],
         [0.730789  , 0.6496606 , 0.14778477],
         [0.42923748, 0.29007047, 0.78535664]]]], dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 1, 3), dtype=float32, numpy=
array([[[[0.00390625, 0.00390625, 0.00390625]],

        [[0.015...]],

        [[0.015625  , 0.015625  , 0.015625  ]],

        [[0.00390625, 0.00390625, 0.00390625]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 12, 12, 3), dtype=float32, numpy=
array([[[[0.23787397, 0.9282101 , 0.7600628 ],
         [0.33...625  ]],

        [[0.00390625],
         [0.00390625],
         [0.00390625]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa36d480>
tensorflow_get_item = <function tensorflow_get_item at 0x7f06f9e21d80>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 12, 3, 12), dtype=float32, numpy=
array([[[[0.23787397, 0.86055326, 0.9483353 , 0.86055326, 0.23...      0.8335772 , 0.5013955 , 0.78535664, 0.38377154, 0.5239864 ,
          0.38377154, 0.78535664]]]], dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 3, 1), dtype=float32, numpy=
array([[[[0.00390625],
         [0.00390625],
         [0.003906...25  ],
         [0.015625  ]],

        [[0.00390625],
         [0.00390625],
         [0.00390625]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 12, 3, 12), dtype=float32, numpy=
array([[[[0.23787397, 0.86055326, 0.9483353 , 0.86055326, 0.2...0.00390625],
         [0.00390625]]]], dtype=float32)>, [1, 1, 1, 1], [(0, 0), (0, 0), (0, 0), (0, 0)], 'NHWC', [1, 1])
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 12 vs 3 [Op:DepthwiseConv2dNative] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.pyramid.build_laplacian_pyramid
______________________________________________________________________________ test_upscale_double[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_upscale_double(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 4, 4),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(5, 3, 8, 8),
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.transform.upscale_double,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_transform.py:612: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function upscale_double at 0x7f0701c9a3b0>
trace_args = (tensor([[[[0.0024, 0.3812, 0.4965, 0.8562],
          [0.2573, 0.6570, 0.4834, 0.2010],
          [0.6283, 0.2542, 0...., 0.2186, 0.7960, 0.6434],
          [0.9980, 0.1617, 0.0831, 0.3141],
          [0.8812, 0.3428, 0.7117, 0.9878]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[0.6777, 0.3714, 0.4283, 0.3995, 0.5489, 0.1355, 0.3654, 0.9897],
          [0.0461, 0.8181, 0.1589, 0.4573...4766, 0.8397, 0.0016, 0.3719, 0.3726],
          [0.9897, 0.9497, 0.8788, 0.8917, 0.3516, 0.2255, 0.4734, 0.2542]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function upscale_double at 0x7f0701c9a3b0>
trace_args = (tensor([[[[0.0024, 0.3812, 0.4965, 0.8562],
          [0.2573, 0.6570, 0.4834, 0.2010],
          [0.6283, 0.2542, 0...., 0.2186, 0.7960, 0.6434],
          [0.9980, 0.1617, 0.0831, 0.3141],
          [0.8812, 0.3428, 0.7117, 0.9878]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[0.6777, 0.3714, 0.4283, 0.3995, 0.5489, 0.1355, 0.3654, 0.9897],
          [0.0461, 0.8181, 0.1589, 0.4573...4766, 0.8397, 0.0016, 0.3719, 0.3726],
          [0.9897, 0.9497, 0.8788, 0.8917, 0.3516, 0.2255, 0.4734, 0.2542]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.00239092, 0.3811531 , 0.49645472, 0.85615575],
     ....16174054, 0.08308023, 0.31410474],
         [0.88123053, 0.34277803, 0.7117273 , 0.9877838 ]]]],
      dtype=float32)>

    def tensorflow_upscale_double(x):
        from ...core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...core._backend import zeros
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(x)
        tensorflow_KORNIA_CHECK_SHAPE(x, ["*", "H", "W"])
        double_shape = tensorflow_shape_frnt_(x)[:-2] + (
            tensorflow_shape_frnt_(x)[-2] * 2,
            tensorflow_shape_frnt_(x)[-1] * 2,
        )
        upscaled = zeros(double_shape, device=x.device, dtype=x.dtype)
        upscaled = tensorflow_set_item_bknd(
            upscaled, (..., slice(None, None, 2), slice(None, None, 2)), x
        )
>       upscaled[..., ::2, 1::2] = tensorflow_set_item_bknd(
            upscaled[..., ::2, 1::2],
            (..., slice(None, -1, None)),
            (upscaled[..., ::2, ::2][..., :-1] + upscaled[..., ::2, 2::2]) / 2,
        )
E       TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:177: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.pyramid.upscale_double
___________________________________________________________________________________ test_Shear[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Shear(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Shear")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledShear = ivy.transpile(kornia.geometry.transform.Shear, source="torch", target=target_framework)
    
        x = torch.rand(2, 3, 4, 4)
        shear = torch.tensor([[0.5, 0.0], [0.0, 0.5]])
        torch_out = kornia.geometry.transform.Shear(shear)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_shear = _nest_torch_tensor_to_new_framework(shear, target_framework)
>       transpiled_out = TranspiledShear(transpiled_shear)(transpiled_x)

kornia/geometry/test_transform.py:1156: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear()
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.854269  , 0.13813382, 0.471286  , 0.9512468 ],
    ...5133953, 0.34656453, 0.5843405 ],
         [0.50721896, 0.49510777, 0.438273  , 0.36214155]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x5636f7045e10, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Shear(), <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.854269  , 0.13813382, 0.471286 ...65133953, 0.34656453, 0.5843405 ],
         [0.50721896, 0.49510777, 0.438273  , 0.36214155]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.854269  , 0.13813382, 0.471286  , 0.9512468 ],
    ...5133953, 0.34656453, 0.5843405 ],
         [0.50721896, 0.49510777, 0.438273  , 0.36214155]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Shear(), <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.854269  , 0.13813382, 0.471286 ...65133953, 0.34656453, 0.5843405 ],
         [0.50721896, 0.49510777, 0.438273  , 0.36214155]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.854269  , 0.13813382, 0.471286  , 0.9512468 ],
    ...5133953, 0.34656453, 0.5843405 ],
         [0.50721896, 0.49510777, 0.438273  , 0.36214155]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.854269  , 0.13813382, 0.471286  , 0.9512468 ],
     ....65133953, 0.34656453, 0.5843405 ],
         [0.50721896, 0.49510777, 0.438273  , 0.36214155]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Shear(),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.854269  , 0.13813382, 0.471286  , 0.951246...65133953, 0.34656453, 0.5843405 ],
         [0.50721896, 0.49510777, 0.438273  , 0.36214155]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear()
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.854269  , 0.13813382, 0.471286  , 0.9512468 ],
     ....65133953, 0.34656453, 0.5843405 ],
         [0.50721896, 0.49510777, 0.438273  , 0.36214155]]]],
      dtype=float32)>

    def call(self, input):
>       return shear(
            input, self.shear, self.mode, self.padding_mode, self.align_corners
        )
E       NameError: Exception encountered when calling tensorflow_Shear.call().
E       
E       [1mname 'shear' is not defined[0m
E       
E       Arguments received by tensorflow_Shear.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 4, 4), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/affwarp.py:1527: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Shear
__________________________________________________________________________________ test_PyrDown[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_PyrDown(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.PyrDown")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledPyrDown = ivy.transpile(kornia.geometry.transform.PyrDown, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 4, 4)
        torch_out = kornia.geometry.transform.PyrDown()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledPyrDown()(transpiled_x)

kornia/geometry/test_transform.py:1173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PyrDown()
args = (<tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.23749334, 0.9195851 , 0.78667295, 0.8243506 ],
    ...542882 , 0.4946136 , 0.88684154],
         [0.5021577 , 0.78654814, 0.732664  , 0.4723537 ]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x5636f7b90fb0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_PyrDown(), <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.23749334, 0.9195851 , 0.78667...2542882 , 0.4946136 , 0.88684154],
         [0.5021577 , 0.78654814, 0.732664  , 0.4723537 ]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PyrDown(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.23749334, 0.9195851 , 0.78667295, 0.8243506 ],
    ...542882 , 0.4946136 , 0.88684154],
         [0.5021577 , 0.78654814, 0.732664  , 0.4723537 ]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_PyrDown(), <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.23749334, 0.9195851 , 0.78667...2542882 , 0.4946136 , 0.88684154],
         [0.5021577 , 0.78654814, 0.732664  , 0.4723537 ]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PyrDown(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.23749334, 0.9195851 , 0.78667295, 0.8243506 ],
    ...542882 , 0.4946136 , 0.88684154],
         [0.5021577 , 0.78654814, 0.732664  , 0.4723537 ]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.23749334, 0.9195851 , 0.78667295, 0.8243506 ],
     ....2542882 , 0.4946136 , 0.88684154],
         [0.5021577 , 0.78654814, 0.732664  , 0.4723537 ]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_PyrDown(),)
kwargs = {'input': <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.23749334, 0.9195851 , 0.78667295, 0.824350...2542882 , 0.4946136 , 0.88684154],
         [0.5021577 , 0.78654814, 0.732664  , 0.4723537 ]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PyrDown()
input = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.23749334, 0.9195851 , 0.78667295, 0.8243506 ],
     ....2542882 , 0.4946136 , 0.88684154],
         [0.5021577 , 0.78654814, 0.732664  , 0.4723537 ]]]],
      dtype=float32)>

    def call(self, input):
>       return tensorflow_pyrdown(
            input, self.border_type, self.align_corners, self.factor
        )

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:69: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.23749334, 0.9195851 , 0.78667295, 0.8243506 ],
     ....2542882 , 0.4946136 , 0.88684154],
         [0.5021577 , 0.78654814, 0.732664  , 0.4723537 ]]]],
      dtype=float32)>
border_type = 'reflect', align_corners = False, factor = 2.0

    def tensorflow_pyrdown(input, border_type="reflect", align_corners=False, factor=2.0):
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...filters.filter import tensorflow_filter2d
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_interpolate_frnt,
        )
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        kernel: typing.Any = tensorflow__get_pyramid_gaussian_kernel()
        _, _, height, width = tensorflow_shape_frnt_(input)
>       x_blur: typing.Any = tensorflow_filter2d(input, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:445: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 8, 8), dtype=float32, numpy=
array([[[[0.42505985, 0.70081526, 0.9240479 , 0.70081526, 0.4250...4455, 0.29988402, 0.4021436 , 0.29988402, 0.03214455,
          0.4115013 , 0.03214455, 0.29988402]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=
array([[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625... 0.0625    , 0.015625  ],
        [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]],
      dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 8, 8), dtype=float32, numpy=
array([[[[0.42505985, 0.70081526, 0.9240479 , 0.70081526, 0.4250...4455, 0.29988402, 0.4021436 , 0.29988402, 0.03214455,
          0.4115013 , 0.03214455, 0.29988402]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.0039....0625    , 0.015625  ],
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 8, 8), dtype=float32, numpy=
array([[[[0.42505985, 0.70081526, 0.9240479 , 0.70081526, 0.4250...4455, 0.29988402, 0.4021436 , 0.29988402, 0.03214455,
          0.4115013 , 0.03214455, 0.29988402]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.0039....0625    , 0.015625  ],
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 8, 8), dtype=float32, numpy=
array([[[[0.42505985, 0.70081526, 0.9240479 , 0.70081526, 0.425...,
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa0ae8c0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f06f99c9480>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 8, 8), dtype=float32, numpy=
array([[[[0.42505985, 0.70081526, 0.9240479 , 0.70081526, 0.425...,
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f06f99c9480>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f06fa2d95a0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa0ae8c0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f06fa065090>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 8, 8, 3), dtype=float32, numpy=
array([[[[0.42505985, 0.3296302 , 0.4946136 ],
         [0.70081...013 ],
         [0.5595179 , 0.46795398, 0.03214455],
         [0.28607613, 0.260067  , 0.29988402]]]], dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 1, 3), dtype=float32, numpy=
array([[[[0.00390625, 0.00390625, 0.00390625]],

        [[0.015...]],

        [[0.015625  , 0.015625  , 0.015625  ]],

        [[0.00390625, 0.00390625, 0.00390625]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 8, 8, 3), dtype=float32, numpy=
array([[[[0.42505985, 0.3296302 , 0.4946136 ],
         [0.7008...625  ]],

        [[0.00390625],
         [0.00390625],
         [0.00390625]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa0ae8c0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f06f99c9480>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 8, 3, 8), dtype=float32, numpy=
array([[[[0.42505985, 0.5595179 , 0.78667295, 0.5595179 , 0.4250...882 , 0.29988402, 0.38035285, 0.29988402, 0.2542882 ,
          0.78654814, 0.2542882 , 0.29988402]]]], dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 3, 1), dtype=float32, numpy=
array([[[[0.00390625],
         [0.00390625],
         [0.003906...25  ],
         [0.015625  ]],

        [[0.00390625],
         [0.00390625],
         [0.00390625]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PyrDown.call().
E       
E       [1m{{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 8 vs 3 [Op:DepthwiseConv2dNative] name: [0m
E       
E       Arguments received by tensorflow_PyrDown.call():
E         â€¢ input=tf.Tensor(shape=(1, 3, 4, 4), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.PyrDown
___________________________________________________________________________________ test_PyrUp[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_PyrUp(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.PyrUp")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledPyrUp = ivy.transpile(kornia.geometry.transform.PyrUp, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 4, 4)
        torch_out = kornia.geometry.transform.PyrUp()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledPyrUp()(transpiled_x)

kornia/geometry/test_transform.py:1190: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PyrUp()
args = (<tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.72718817, 0.52424175, 0.0632059 , 0.13500267],
    ...1045326, 0.668847  , 0.11018413],
         [0.22071815, 0.5742024 , 0.93602103, 0.5857533 ]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06fb191600, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_PyrUp(), <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.72718817, 0.52424175, 0.0632059...91045326, 0.668847  , 0.11018413],
         [0.22071815, 0.5742024 , 0.93602103, 0.5857533 ]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PyrUp(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.72718817, 0.52424175, 0.0632059 , 0.13500267],
    ...1045326, 0.668847  , 0.11018413],
         [0.22071815, 0.5742024 , 0.93602103, 0.5857533 ]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_PyrUp(), <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.72718817, 0.52424175, 0.0632059...91045326, 0.668847  , 0.11018413],
         [0.22071815, 0.5742024 , 0.93602103, 0.5857533 ]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PyrUp(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.72718817, 0.52424175, 0.0632059 , 0.13500267],
    ...1045326, 0.668847  , 0.11018413],
         [0.22071815, 0.5742024 , 0.93602103, 0.5857533 ]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.72718817, 0.52424175, 0.0632059 , 0.13500267],
     ....91045326, 0.668847  , 0.11018413],
         [0.22071815, 0.5742024 , 0.93602103, 0.5857533 ]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_PyrUp(),)
kwargs = {'input': <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.72718817, 0.52424175, 0.0632059 , 0.135002...91045326, 0.668847  , 0.11018413],
         [0.22071815, 0.5742024 , 0.93602103, 0.5857533 ]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PyrUp()
input = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.72718817, 0.52424175, 0.0632059 , 0.13500267],
     ....91045326, 0.668847  , 0.11018413],
         [0.22071815, 0.5742024 , 0.93602103, 0.5857533 ]]]],
      dtype=float32)>

    def call(self, input):
>       return tensorflow_pyrup(input, self.border_type, self.align_corners)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:447: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.72718817, 0.52424175, 0.0632059 , 0.13500267],
     ....91045326, 0.668847  , 0.11018413],
         [0.22071815, 0.5742024 , 0.93602103, 0.5857533 ]]]],
      dtype=float32)>
border_type = 'reflect', align_corners = False

    def tensorflow_pyrup(input, border_type="reflect", align_corners=False):
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_interpolate_frnt,
        )
        from ...filters.filter import tensorflow_filter2d
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        kernel: typing.Any = tensorflow__get_pyramid_gaussian_kernel()
        _, _, height, width = tensorflow_shape_frnt_(input)
        x_up: typing.Any = tensorflow_interpolate_frnt(
            input,
            size=(height * 2, width * 2),
            mode="bilinear",
            align_corners=align_corners,
        )
>       x_blur: typing.Any = tensorflow_filter2d(x_up, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:848: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.21242115, 0.337325  , 0.39977694, 0.337325  , 0.21...      0.80370307, 0.758328  , 0.6089995 , 0.35571745, 0.22907643,
          0.35571745, 0.6089995 ]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=
array([[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625... 0.0625    , 0.015625  ],
        [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]],
      dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.21242115, 0.337325  , 0.39977694, 0.337325  , 0.21...      0.80370307, 0.758328  , 0.6089995 , 0.35571745, 0.22907643,
          0.35571745, 0.6089995 ]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.0039....0625    , 0.015625  ],
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.21242115, 0.337325  , 0.39977694, 0.337325  , 0.21...      0.80370307, 0.758328  , 0.6089995 , 0.35571745, 0.22907643,
          0.35571745, 0.6089995 ]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.0039....0625    , 0.015625  ],
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.21242115, 0.337325  , 0.39977694, 0.337325  , 0.2...,
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa1be050>
tensorflow_get_item = <function tensorflow_get_item at 0x7f06fa032dd0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 12, 12), dtype=float32, numpy=
array([[[[0.21242115, 0.337325  , 0.39977694, 0.337325  , 0.2...,
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f06fa032dd0>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f06fa1bca60>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa1be050>
tensorflow_asarray = <function tensorflow_asarray at 0x7f06fa22a3b0>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 12, 12, 3), dtype=float32, numpy=
array([[[[0.21242115, 0.7929367 , 0.6331504 ],
         [0.337...7643],
         [0.53491163, 0.77641916, 0.35571745],
         [0.7609293 , 0.8283411 , 0.6089995 ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 1, 3), dtype=float32, numpy=
array([[[[0.00390625, 0.00390625, 0.00390625]],

        [[0.015...]],

        [[0.015625  , 0.015625  , 0.015625  ]],

        [[0.00390625, 0.00390625, 0.00390625]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 12, 12, 3), dtype=float32, numpy=
array([[[[0.21242115, 0.7929367 , 0.6331504 ],
         [0.33...625  ]],

        [[0.00390625],
         [0.00390625],
         [0.00390625]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa1be050>
tensorflow_get_item = <function tensorflow_get_item at 0x7f06fa032dd0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 12, 3, 12), dtype=float32, numpy=
array([[[[0.21242115, 0.45412594, 0.57497835, 0.45412594, 0.21...      0.32855216, 0.46230492, 0.6089995 , 0.7686359 , 0.8484541 ,
          0.7686359 , 0.6089995 ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 3, 1), dtype=float32, numpy=
array([[[[0.00390625],
         [0.00390625],
         [0.003906...25  ],
         [0.015625  ]],

        [[0.00390625],
         [0.00390625],
         [0.00390625]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PyrUp.call().
E       
E       [1m{{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 12 vs 3 [Op:DepthwiseConv2dNative] name: [0m
E       
E       Arguments received by tensorflow_PyrUp.call():
E         â€¢ input=tf.Tensor(shape=(1, 3, 4, 4), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.PyrUp
_______________________________________________________________________________ test_ScalePyramid[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ScalePyramid(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.ScalePyramid")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledScalePyramid = ivy.transpile(kornia.geometry.transform.ScalePyramid, source="torch", target=target_framework)
    
        x = torch.rand(2, 4, 100, 100)
        torch_out, _, _ = kornia.geometry.transform.ScalePyramid(n_levels=3, min_size=15)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out, _, _ = TranspiledScalePyramid(n_levels=3, min_size=15)(transpiled_x)

kornia/geometry/test_transform.py:1207: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=15, extra_levels=3, border=6, sigma_step=1.2599210498948732, double_image=False)
args = (<tf.Tensor: shape=(2, 4, 100, 100), dtype=float32, numpy=
array([[[[0.01505619, 0.41868228, 0.03797013, ..., 0.934647...
         [0.6360295 , 0.7225543 , 0.7322046 , ..., 0.9943629 ,
          0.5671658 , 0.209871  ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x5636f93a0200, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=15, extra_levels=3, border=6, sigma_step=1.2599210498948...,
         [0.6360295 , 0.7225543 , 0.7322046 , ..., 0.9943629 ,
          0.5671658 , 0.209871  ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=15, extra_levels=3, border=6, sigma_step=1.2599210498948732, double_image=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 4, 100, 100), dtype=float32, numpy=
array([[[[0.01505619, 0.41868228, 0.03797013, ..., 0.934647...
         [0.6360295 , 0.7225543 , 0.7322046 , ..., 0.9943629 ,
          0.5671658 , 0.209871  ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=15, extra_levels=3, border=6, sigma_step=1.2599210498948...,
         [0.6360295 , 0.7225543 , 0.7322046 , ..., 0.9943629 ,
          0.5671658 , 0.209871  ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=15, extra_levels=3, border=6, sigma_step=1.2599210498948732, double_image=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 4, 100, 100), dtype=float32, numpy=
array([[[[0.01505619, 0.41868228, 0.03797013, ..., 0.934647...
         [0.6360295 , 0.7225543 , 0.7322046 , ..., 0.9943629 ,
          0.5671658 , 0.209871  ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 4, 100, 100), dtype=float32, numpy=
array([[[[0.01505619, 0.41868228, 0.03797013, ..., 0.9346477...],
         [0.6360295 , 0.7225543 , 0.7322046 , ..., 0.9943629 ,
          0.5671658 , 0.209871  ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=15, extra_levels=3, border=6, sigma_step=1.2599210498948732, double_image=False),)
kwargs = {'x': <tf.Tensor: shape=(2, 4, 100, 100), dtype=float32, numpy=
array([[[[0.01505619, 0.41868228, 0.03797013, ..., 0.9...,
         [0.6360295 , 0.7225543 , 0.7322046 , ..., 0.9943629 ,
          0.5671658 , 0.209871  ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=15, extra_levels=3, border=6, sigma_step=1.2599210498948732, double_image=False)
x = <tf.Tensor: shape=(2, 4, 100, 100), dtype=float32, numpy=
array([[[[0.01505619, 0.41868228, 0.03797013, ..., 0.9346477...],
         [0.6360295 , 0.7225543 , 0.7322046 , ..., 0.9943629 ,
          0.5671658 , 0.209871  ]]]], dtype=float32)>

    def call(self, x):
        from ...filters.gaussian import tensorflow_gaussian_blur2d
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from ...core._backend import ones
        from ...core._backend import stack
    
        bs, _, _, _ = tensorflow_size_frnt_(x)
>       cur_level, cur_sigma, pixel_distance = self.get_first_level(x)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:869: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=15, extra_levels=3, border=6, sigma_step=1.2599210498948732, double_image=False)
input = <tf.Tensor: shape=(2, 4, 100, 100), dtype=float32, numpy=
array([[[[0.01505619, 0.41868228, 0.03797013, ..., 0.9346477...],
         [0.6360295 , 0.7225543 , 0.7322046 , ..., 0.9943629 ,
          0.5671658 , 0.209871  ]]]], dtype=float32)>

    def get_first_level(self, input):
        from ...filters.gaussian import tensorflow_gaussian_blur2d
    
        pixel_distance = 1.0
        cur_sigma = 0.5
        if self.double_image:
            x = tensorflow_upscale_double(input)
            pixel_distance = 0.5
            cur_sigma = cur_sigma * 2.0
        else:
            x = input
        if self.init_sigma > cur_sigma:
            sigma = max(math.sqrt(self.init_sigma**2 - cur_sigma**2), 0.01)
            ksize = self.get_kernel_size(sigma)
>           cur_level = tensorflow_gaussian_blur2d(x, (ksize, ksize), (sigma, sigma))

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:852: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 100, 100), dtype=float32, numpy=
array([[[[0.01505619, 0.41868228, 0.03797013, ..., 0.9346477...],
         [0.6360295 , 0.7225543 , 0.7322046 , ..., 0.9943629 ,
          0.5671658 , 0.209871  ]]]], dtype=float32)>
kernel_size = (13, 13), sigma = <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1.5198684, 1.5198684]], dtype=float32)>, border_type = 'reflect', separable = True

    def tensorflow_gaussian_blur2d(
        input, kernel_size, sigma, border_type="reflect", separable=True
    ):
        from ..core._backend import tensor
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .kernels import tensorflow__unpack_2d_ks
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .kernels import tensorflow_get_gaussian_kernel1d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from .filter import tensorflow_filter2d_separable
        from .kernels import tensorflow_get_gaussian_kernel2d
        from .filter import tensorflow_filter2d
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        if isinstance(sigma, (tuple,)):
            sigma = tensor([sigma], device=input.device, dtype=input.dtype)
        else:
            tensorflow_KORNIA_CHECK_IS_TENSOR(sigma)
            sigma = tensorflow_to_frnt_(sigma, device=input.device, dtype=input.dtype)
        if separable:
            ky, kx = tensorflow__unpack_2d_ks(kernel_size)
            bs = tensorflow_shape_frnt_(sigma)[0]
            kernel_x = tensorflow_get_gaussian_kernel1d(
                kx, tensorflow_view_frnt_(sigma[:, 1], bs, 1)
            )
            kernel_y = tensorflow_get_gaussian_kernel1d(
                ky, tensorflow_view_frnt_(sigma[:, 0], bs, 1)
            )
>           out = tensorflow_filter2d_separable(input, kernel_x, kernel_y, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/gaussian.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 100, 100), dtype=float32, numpy=
array([[[[0.01505619, 0.41868228, 0.03797013, ..., 0.9346477...],
         [0.6360295 , 0.7225543 , 0.7322046 , ..., 0.9943629 ,
          0.5671658 , 0.209871  ]]]], dtype=float32)>
kernel_x = <tf.Tensor: shape=(1, 13), dtype=float32, numpy=
array([[1.08391927e-04, 1.17228064e-03, 8.22355878e-03, 3.74180935e-0...e-01,
        1.10432625e-01, 3.74180935e-02, 8.22355878e-03, 1.17228064e-03,
        1.08391920e-04]], dtype=float32)>
kernel_y = <tf.Tensor: shape=(1, 13), dtype=float32, numpy=
array([[1.08391927e-04, 1.17228064e-03, 8.22355878e-03, 3.74180935e-0...e-01,
        1.10432625e-01, 3.74180935e-02, 8.22355878e-03, 1.17228064e-03,
        1.08391920e-04]], dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same'

    def tensorflow_filter2d_separable(
        input, kernel_x, kernel_y, border_type="reflect", normalized=False, padding="same"
    ):
>       out_x = tensorflow_filter2d(
            input, kernel_x[..., None, :], border_type, normalized, padding
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 100, 112), dtype=float32, numpy=
array([[[[0.298191  , 0.63084054, 0.01518518, ..., 0.0902700...],
         [0.4610278 , 0.3321241 , 0.4802441 , ..., 0.4597671 ,
          0.79292953, 0.26062298]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 1, 13), dtype=float32, numpy=
array([[[1.08391927e-04, 1.17228064e-03, 8.22355878e-03, 3.7418093...1,
         1.10432625e-01, 3.74180935e-02, 8.22355878e-03, 1.17228064e-03,
         1.08391920e-04]]], dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 100, 112), dtype=float32, numpy=
array([[[[0.298191  , 0.63084054, 0.01518518, ..., 0.0902700...],
         [0.4610278 , 0.3321241 , 0.4802441 , ..., 0.4597671 ,
          0.79292953, 0.26062298]]]], dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 1, 13), dtype=float32, numpy=
array([[[[1.08391927e-04, 1.17228064e-03, 8.22355878e-03,
     ...1.10432625e-01,
          3.74180935e-02, 8.22355878e-03, 1.17228064e-03,
          1.08391920e-04]]]], dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 4

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 100, 112), dtype=float32, numpy=
array([[[[0.298191  , 0.63084054, 0.01518518, ..., 0.0902700...],
         [0.4610278 , 0.3321241 , 0.4802441 , ..., 0.4597671 ,
          0.79292953, 0.26062298]]]], dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 1, 13), dtype=float32, numpy=
array([[[[1.08391927e-04, 1.17228064e-03, 8.22355878e-03,
     ...1.10432625e-01,
          3.74180935e-02, 8.22355878e-03, 1.17228064e-03,
          1.08391920e-04]]]], dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 4

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 4, 100, 112), dtype=float32, numpy=
array([[[[0.298191  , 0.63084054, 0.01518518, ..., 0.090270...    3.74180935e-02, 8.22355878e-03, 1.17228064e-03,
          1.08391920e-04]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa7d4160>
tensorflow_get_item = <function tensorflow_get_item at 0x7f06f9ae7400>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(2, 4, 100, 112), dtype=float32, numpy=
array([[[[0.298191  , 0.63084054, 0.01518518, ..., 0.090270...    3.74180935e-02, 8.22355878e-03, 1.17228064e-03,
          1.08391920e-04]]]], dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f06f9ae7400>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f06fa7d5d80>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa7d4160>
tensorflow_asarray = <function tensorflow_asarray at 0x7f06fa1ef9a0>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 100, 112, 4), dtype=float32, numpy=
array([[[[0.298191  , 0.3841189 , 0.85745364, 0.30191308],
 ....29249126, 0.27707988, 0.79292953],
         [0.60881346, 0.08064252, 0.8165731 , 0.26062298]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(1, 13, 1, 4), dtype=float32, numpy=
array([[[[1.08391927e-04, 1.08391927e-04, 1.08391927e-04,
     ...7228064e-03]],

        [[1.08391920e-04, 1.08391920e-04, 1.08391920e-04,
          1.08391920e-04]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 100, 112, 4), dtype=float32, numpy=
array([[[[0.298191  , 0.3841189 , 0.85745364, 0.30191308],
...       [1.08391920e-04],
         [1.08391920e-04],
         [1.08391920e-04]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f06fa7d4160>
tensorflow_get_item = <function tensorflow_get_item at 0x7f06f9ae7400>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 112, 4, 100), dtype=float32, numpy=
array([[[[0.298191  , 0.08967203, 0.1109544 , ..., 0.8910476...],
         [0.0273788 , 0.36283565, 0.28001976, ..., 0.85314834,
          0.04254436, 0.26062298]]]], dtype=float32)>
filters = <tf.Tensor: shape=(1, 13, 4, 1), dtype=float32, numpy=
array([[[[1.08391927e-04],
         [1.08391927e-04],
         ... [[1.08391920e-04],
         [1.08391920e-04],
         [1.08391920e-04],
         [1.08391920e-04]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_ScalePyramid.call().
E       
E       [1m{{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 100 vs 4 [Op:DepthwiseConv2dNative] name: [0m
E       
E       Arguments received by tensorflow_ScalePyramid.call():
E         â€¢ x=tf.Tensor(shape=(2, 4, 100, 100), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.ScalePyramid
__________________________________________________________________________________ test_Rescale[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Rescale(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Rescale")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledRescale = ivy.transpile(kornia.geometry.transform.Rescale, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 4, 4)
        torch_out = kornia.geometry.transform.Rescale((2.0, 3.0))(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_out = TranspiledRescale((2.0, 3.0))(transpiled_x)
    
>       _to_numpy_and_allclose(torch_out, transpiled_out)

kornia/geometry/test_transform.py:1294: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0.4170, 0.5128, 0.6086, 0.7044, 0.7346, 0.6336, 0.5326, 0.4316,
           0.4941, 0.6384, 0.7827, 0.9271],...        [0.3646, 0.4885, 0.6124, 0.7363, 0.8178, 0.8146, 0.8114, 0.8083,
           0.7981, 0.7846, 0.7710, 0.7574]]]])
transpiled_x = <tf.Tensor: shape=(1, 3, 8, 12), dtype=float32, numpy=
array([[[[0.41701418, 0.41701418, 0.53409964, 0.65118515, 0.768...      0.81497633, 0.8110845 , 0.80719274, 0.7905928 , 0.7739929 ,
          0.75739294, 0.75739294]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0.41701418, 0.51281136, 0.6086086 , 0.7044058 , 0.7346013 ,
          0.63359374, 0.5325861 , 0.4315785 , 0....       0.8146225 , 0.8114383 , 0.8082542 , 0.79813826, 0.78455645,
          0.7709747 , 0.75739294]]]], dtype=float32)
y = array([[[[0.41701418, 0.41701418, 0.53409964, 0.65118515, 0.76827055,
          0.6448168 , 0.521363  , 0.39790934, 0....       0.81497633, 0.8110845 , 0.80719274, 0.7905928 , 0.7739929 ,
          0.75739294, 0.75739294]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Rescale
________________________________________________________________________________ test_Homography[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Homography(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Homography")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHomography = ivy.transpile(
            kornia.geometry.transform.image_registrator.Homography, source="torch", target=target_framework
        )
    
        torch_out = kornia.geometry.transform.image_registrator.Homography()()
>       transpiled_out = TranspiledHomography()()

kornia/geometry/test_transform.py:1354: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>), args = (), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x5636f6c419f0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>),), kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>), v = None, buffers = None
args = (), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>),)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>), v = None, buffers = None
args = (), kwargs = {}, first_arr = None, replace_v = False, replace_buffers = False, call_signature = <Signature ()>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>),), kwargs = {}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <keras.src.layers.layer.CallSpec object at 0x7f06f99bf9a0>, signature = <Signature ()>, args = (), kwargs = {}

    def __init__(self, signature, args, kwargs):
        # `training` and `mask` are special kwargs that are always available in
        # a layer, if user specifies them in their call without adding to spec,
        # we remove them to be able to bind variables. User is not using
        # `training` anyway so we can ignore.
        # TODO: If necessary use workaround for `mask`
        if "training" in kwargs and "training" not in signature.parameters:
            kwargs.pop("training")
            bound_args = signature.bind(*args, **kwargs)
        else:
            bound_args = signature.bind(*args, **kwargs)
        self.user_arguments_dict = {
            k: v for k, v in bound_args.arguments.items()
        }
        bound_args.apply_defaults()
        arg_dict = {}
        arg_names = []
        tensor_arg_dict = {}
        tensor_args = []
        tensor_arg_names = []
        nested_tensor_arg_names = []
        for name, value in bound_args.arguments.items():
            arg_dict[name] = value
            arg_names.append(name)
            if is_backend_tensor_or_symbolic(value):
                tensor_args.append(value)
                tensor_arg_names.append(name)
                tensor_arg_dict[name] = value
            elif tree.is_nested(value) and len(value) > 0:
                flat_values = tree.flatten(value)
                if all(
                    is_backend_tensor_or_symbolic(x, allow_none=True)
                    for x in flat_values
                ):
                    tensor_args.append(value)
                    tensor_arg_names.append(name)
                    tensor_arg_dict[name] = value
                    nested_tensor_arg_names.append(name)
                elif any(is_backend_tensor_or_symbolic(x) for x in flat_values):
                    raise ValueError(
                        "In a nested call() argument, "
                        "you cannot mix tensors and non-tensors. "
                        "Received invalid mixed argument: "
                        f"{name}={value}"
                    )
        self.arguments_dict = arg_dict
        self.argument_names = arg_names
        self.tensor_arguments_dict = tensor_arg_dict
        self.tensor_arguments_names = tensor_arg_names
        self.nested_tensor_argument_names = nested_tensor_arg_names
>       self.first_arg = arg_dict[arg_names[0]]
E       IndexError: list index out of range

/opt/fw/tensorflow/keras/src/layers/layer.py:1614: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Homography
_____________________________________________________________________________ test_ImageRegistrator[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ImageRegistrator(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.ImageRegistrator")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledImageRegistrator = ivy.transpile(
            kornia.geometry.transform.ImageRegistrator, source="torch", target=target_framework
        )

kornia/geometry/test_transform.py:1365: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.transform.image_registrator.ImageRegistrator'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geometry.transform.image_registrator: name 'Args' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.ImageRegistrator
________________________________________________________________________________ test_Similarity[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Similarity(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Similarity")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledSimilarity = ivy.transpile(
            kornia.geometry.transform.image_registrator.Similarity, source="torch", target=target_framework
        )

kornia/geometry/test_transform.py:1386: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.transform.image_registrator.Similarity'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geometry.transform.image_registrator: name 'Args' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Similarity
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_transform.py::test_build_pyramid[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_d...
FAILED kornia/geometry/test_transform.py::test_build_laplacian_pyramid[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv...
FAILED kornia/geometry/test_transform.py::test_upscale_double[tensorflow-s2s-False] - TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment
FAILED kornia/geometry/test_transform.py::test_Shear[tensorflow-s2s-False] - NameError: Exception encountered when calling tensorflow_Shear.call().
FAILED kornia/geometry/test_transform.py::test_PyrDown[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PyrDown.ca...
FAILED kornia/geometry/test_transform.py::test_PyrUp[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PyrUp.call().
FAILED kornia/geometry/test_transform.py::test_ScalePyramid[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_Scale...
FAILED kornia/geometry/test_transform.py::test_Rescale[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_transform.py::test_Homography[tensorflow-s2s-False] - IndexError: list index out of range
FAILED kornia/geometry/test_transform.py::test_ImageRegistrator[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geom...
FAILED kornia/geometry/test_transform.py::test_Similarity[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geometry.t...
============================================================================== 11 failed, 46 passed in 937.99s (0:15:37) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 39 items

kornia/test_enhance.py ..............F........................                                                                                                                                   [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_equalize_clahe[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_equalize_clahe(target_framework, mode, backend_compile):
        trace_args = (torch.rand(1, 10, 20),)
        trace_kwargs = {
            "clip_limit": 40.0,
            "grid_size": (8, 8),
            "slow_and_differentiable": False,
        }
        test_args = (torch.rand(2, 3, 10, 20),)
        test_kwargs = {
            "clip_limit": 20.0,
            "grid_size": (4, 4),
            "slow_and_differentiable": False,
        }
>       _test_function(
            kornia.enhance.equalize_clahe,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
            skip=True,
        )

kornia/test_enhance.py:363: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function equalize_clahe at 0x7fd4b5879360>
trace_args = (tensor([[[0.4071, 0.0131, 0.4199, 0.0394, 0.8440, 0.4874, 0.9673, 0.4980,
          0.4539, 0.3268, 0.5325, 0.4390, 0...         0.9276, 0.6115, 0.9699, 0.7680, 0.6201, 0.0071, 0.9300, 0.7735,
          0.4967, 0.7382, 0.8898, 0.3728]]]),)
trace_kwargs = {'clip_limit': 40.0, 'grid_size': (8, 8), 'slow_and_differentiable': False}
test_args = (tensor([[[[0.9183, 0.3143, 0.7014,  ..., 0.0168, 0.2721, 0.4540],
          [0.8719, 0.7247, 0.4122,  ..., 0.6531, 0...., 0.5673, 0.3459,  ..., 0.4072, 0.7513, 0.1199],
          [0.6338, 0.1536, 0.6771,  ..., 0.6534, 0.1148, 0.2772]]]]),)
test_kwargs = {'clip_limit': 20.0, 'grid_size': (4, 4), 'slow_and_differentiable': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = True
deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function equalize_clahe at 0x7fd4b5879360>
trace_args = (tensor([[[0.4071, 0.0131, 0.4199, 0.0394, 0.8440, 0.4874, 0.9673, 0.4980,
          0.4539, 0.3268, 0.5325, 0.4390, 0...         0.9276, 0.6115, 0.9699, 0.7680, 0.6201, 0.0071, 0.9300, 0.7735,
          0.4967, 0.7382, 0.8898, 0.3728]]]),)
trace_kwargs = {'clip_limit': 40.0, 'grid_size': (8, 8), 'slow_and_differentiable': False}
test_args = (tensor([[[[0.9183, 0.3143, 0.7014,  ..., 0.0168, 0.2721, 0.4540],
          [0.8719, 0.7247, 0.4122,  ..., 0.6531, 0...., 0.5673, 0.3459,  ..., 0.4072, 0.7513, 0.1199],
          [0.6338, 0.1536, 0.6771,  ..., 0.6534, 0.1148, 0.2772]]]]),)
test_kwargs = {'clip_limit': 20.0, 'grid_size': (4, 4), 'slow_and_differentiable': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 10, 20), dtype=float32, numpy=
array([[[[0.4071384 , 0.01313633, 0.4198817 , 0.03938174, 0.84...0.00708038, 0.929951  ,
          0.77353674, 0.4966703 , 0.73815143, 0.88984203, 0.3728438 ]]]],
      dtype=float32)>
args = (), kwargs = {'clip_limit': 40.0, 'grid_size': (8, 8), 'slow_and_differentiable': False}, tensorflow_numel_frnt_ = <function tensorflow_numel_frnt_ at 0x7fd4ae12a050>
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7fd4ae1283a0>, tensorflow_view_frnt_ = <function tensorflow_view_frnt_ at 0x7fd4ae12a200>
input_shape = ivy.frontends.torch.Size([1, 10, 20])

    @wraps(f)
    def _wrapper(input, *args, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_numel_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
        if tensorflow_numel_frnt_(input) == 0:
            raise ValueError("Invalid input tensor, it is empty.")
        input_shape = tensorflow_shape_frnt_(input)
        input = tensorflow__to_bchw(input)
>       output = f(input, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/utils/image.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 10, 20), dtype=float32, numpy=
array([[[[0.4071384 , 0.01313633, 0.4198817 , 0.03938174, 0.84...0.00708038, 0.929951  ,
          0.77353674, 0.4966703 , 0.73815143, 0.88984203, 0.3728438 ]]]],
      dtype=float32)>
clip_limit = 40.0, grid_size = (8, 8), slow_and_differentiable = False

    @tensorflow_perform_keep_shape_image
    def tensorflow_equalize_clahe(
        input, clip_limit=40.0, grid_size=(8, 8), slow_and_differentiable=False
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_as_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
    
        if not isinstance(clip_limit, (float,)):
            raise TypeError(f"Input clip_limit type is not float. Got {type(clip_limit)}")
        if not isinstance(grid_size, (tuple,)):
            raise TypeError(f"Input grid_size type is not Tuple. Got {type(grid_size)}")
        if len(grid_size) != 2:
            raise TypeError(
                f"Input grid_size is not a Tuple with 2 elements. Got {len(grid_size)}"
            )
        if isinstance(grid_size[0], (float,)) or isinstance(grid_size[1], (float,)):
            raise TypeError("Input grid_size type is not valid, must be a Tuple[int, int].")
        if grid_size[0] <= 0 or grid_size[1] <= 0:
            raise ValueError(f"Input grid_size elements must be positive. Got {grid_size}")
        imgs: typing.Any = input
>       hist_tiles, img_padded = tensorflow__compute_tiles(imgs, grid_size, True)

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:513: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

imgs = <tf.Tensor: shape=(1, 1, 10, 20), dtype=float32, numpy=
array([[[[0.4071384 , 0.01313633, 0.4198817 , 0.03938174, 0.84...0.00708038, 0.929951  ,
          0.77353674, 0.4966703 , 0.73815143, 0.88984203, 0.3728438 ]]]],
      dtype=float32)>
grid_size = (8, 8), even_tile_size = True

    def tensorflow__compute_tiles(imgs, grid_size, even_tile_size=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_pad_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unfold_frnt_
    
        batch: typing.Any = imgs
        h, w = tensorflow_shape_frnt_(batch)[-2:][0], tensorflow_shape_frnt_(batch)[-2:][1]
        kernel_vert: typing.Any = math.ceil(h / grid_size[0])
        kernel_horz: typing.Any = math.ceil(w / grid_size[1])
        if even_tile_size:
            kernel_vert = kernel_vert + (1 if kernel_vert % 2 else 0)
            kernel_horz = kernel_horz + (1 if kernel_horz % 2 else 0)
        pad_vert = kernel_vert * grid_size[0] - h
        pad_horz = kernel_horz * grid_size[1] - w
        if (
            pad_vert > tensorflow_shape_frnt_(batch)[-2]
            or pad_horz > tensorflow_shape_frnt_(batch)[-1]
        ):
            raise ValueError(
                "Cannot compute tiles on the image according to the given grid size"
            )
        if pad_vert > 0 or pad_horz > 0:
            batch = tensorflow_pad_frnt(batch, [0, pad_horz, 0, pad_vert], mode="reflect")
        c: typing.Any = tensorflow_shape_frnt_(batch)[-3]
        tiles: typing.Any = tensorflow_contiguous_frnt_(
            tensorflow_squeeze_frnt_(
                tensorflow_unfold_frnt_(
>                   tensorflow_unfold_frnt_(
                        tensorflow_unfold_frnt_(batch, 1, c, c), 2, kernel_vert, kernel_vert
                    ),
                    3,
                    kernel_horz,
                    kernel_horz,
                ),
                1,
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 1, 32, 16), dtype=float32, numpy=
array([[[[[0.4071384 , 0.24614036, 0.6445247 , 0.13787931,...      0.81983453, 0.17995566, 0.47128445, 0.17264783, 0.69813806,
           0.89503074]]]]], dtype=float32)>, 2, 2, 2)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fd4ae17d630>
array_like = <tf.Tensor: shape=(1, 1, 1, 32, 16), dtype=float32, numpy=
array([[[[[0.4071384 , 0.24614036, 0.6445247 , 0.13787931, ...735,
           0.81983453, 0.17995566, 0.47128445, 0.17264783, 0.69813806,
           0.89503074]]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 1, 1, 32, 16), dtype=float32, numpy=
array([[[[[0.4071384 , 0.24614036, 0.6445247 , 0.13787931, ...735,
           0.81983453, 0.17995566, 0.47128445, 0.17264783, 0.69813806,
           0.89503074]]]]], dtype=float32)>
dimension = 2, size = 2, step = 2

    @tensorflow_handle_methods
    def tensorflow_unfold_frnt_(tensor, dimension, size, step):
        from ...backends.tensorflow.general import tensorflow_get_item
        from ...ivy.general import tensorflow_set_item_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_stack_frnt
    
        slices = []
        self_shape = tuple(tensorflow_shape_frnt_(tensor))
        for i in range(0, tensorflow_get_item(self_shape, dimension) - size + 1, step):
            slicing = [slice(None)] * len(tensorflow_shape_frnt_(tensor))
            slicing = tensorflow_set_item_bknd(slicing, dimension, slice(i, i + size))
            slices.append(tensorflow_get_item(tensor, tuple(slicing)))
>       stacked = tensorflow_stack_frnt(slices, dim=dimension)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:423: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = [], dim = 2

    def tensorflow_stack_frnt(tensors, dim=0, *, out=None):
        from ...backends.tensorflow.manipulation import tensorflow_stack
    
>       return tensorflow_stack(tensors, axis=dim, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ([],), kwargs = {'axis': 2, 'out': None}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fd4ae17d630>, array_like = []

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
>           array_like = array_like[0]
E           IndexError: list index out of range

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:46: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.enhance.equalization.equalize_clahe
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_enhance.py::test_equalize_clahe[tensorflow-s2s-False] - IndexError: list index out of range
=============================================================================== 1 failed, 38 passed in 612.45s (0:10:12) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 5 items

kornia/geometry/test_solvers.py .....                                                                                                                                                            [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 5 passed in 117.56s (0:01:57) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 26 items

kornia/geometry/test_epipolar.py ...FF.....................                                                                                                                                      [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________ test_decompose_essential_matrix[tensorflow-s2s-False] _________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_decompose_essential_matrix(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(3, 3),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(3, 3),
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.epipolar.decompose_essential_matrix,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
            # NOTE: numerical instability in svd()/lu() leads to logits not being allclose
            deterministic=False,
        )

kornia/geometry/test_epipolar.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function decompose_essential_matrix at 0x7fa80fe17c70>, trace_args = (tensor([[0.0221, 0.5806, 0.0633],
        [0.8233, 0.2666, 0.0685],
        [0.5698, 0.6555, 0.2915]]),), trace_kwargs = {}
test_args = (tensor([[0.2231, 0.7613, 0.9559],
        [0.3536, 0.8213, 0.8006],
        [0.2057, 0.1868, 0.8254]]),), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, mode = 's2s', skip = False, deterministic = False

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function decompose_essential_matrix at 0x7fa80fe17c70>, trace_args = (tensor([[0.0221, 0.5806, 0.0633],
        [0.8233, 0.2666, 0.0685],
        [0.5698, 0.6555, 0.2915]]),), trace_kwargs = {}
test_args = (tensor([[0.2231, 0.7613, 0.9559],
        [0.3536, 0.8213, 0.8006],
        [0.2057, 0.1868, 0.8254]]),), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, deterministic = False

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
>           _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:261: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[-0.8028, -0.3849,  0.4553],
         [ 0.0080,  0.7567,  0.6537],
         [-0.5962,  0.5285, -0.6044]]]), ... -0.9702,  0.2297],
         [ 0.6973, -0.2173, -0.6831]]]), tensor([[-0.5926],
        [-0.4557],
        [ 0.6642]]))
transpiled_x = (<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[ 0.7126198 ,  0.1073238 ,  0.6932928 ],
        [-0.07735...shape=(3, 1), dtype=float32, numpy=
array([[ 0.59255415],
       [ 0.4557489 ],
       [-0.6642082 ]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_shape_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_shape_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[-0.8028333 , -0.3849017 ,  0.45531237],
        [ 0.00796852,  0.7566906 ,  0.65372455],
        [-0.5961501...633, -0.6830694 ]]], dtype=float32), array([[-0.5925542 ],
       [-0.45574898],
       [ 0.66420805]], dtype=float32))
y = (array([[[ 0.7126198 ,  0.1073238 ,  0.6932928 ],
        [-0.07735424, -0.97018313,  0.22969781],
        [ 0.697273 ...985, -0.60442954]]], dtype=float32), array([[ 0.59255415],
       [ 0.4557489 ],
       [-0.6642082 ]], dtype=float32))
tolerance = 0.001

    def _check_shape_allclose(x, y, tolerance=1e-3):
        """
        Checks that all array shapes are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x.shape, y.shape, atol=tolerance), "numpy array shapes are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7fa80b8ff4c0>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[-0.8028333 , -0.3849017 ,  0.45531237],
        [ 0.00796852,  0.7566906 ,  0.65372455],
        [-0.59615016,  0.5284599 , -0.6044294 ]]], dtype=float32)
y = array([[[ 0.7126198 ,  0.1073238 ,  0.6932928 ],
        [-0.07735424, -0.97018313,  0.22969781],
        [ 0.697273  , -0.21731618, -0.68306965]]], dtype=float32), tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.epipolar.essential.decompose_essential_matrix
___________________________________________________________________________ test_motion_from_essential[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_motion_from_essential(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(3, 3),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(3, 3),
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.epipolar.motion_from_essential,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_epipolar.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function motion_from_essential at 0x7fa80fe17e20>, trace_args = (tensor([[0.0488, 0.0095, 0.3701],
        [0.6068, 0.9354, 0.0508],
        [0.0576, 0.8053, 0.4030]]),), trace_kwargs = {}
test_args = (tensor([[0.5561, 0.2208, 0.5904],
        [0.2330, 0.7442, 0.0742],
        [0.1333, 0.2863, 0.7602]]),), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function motion_from_essential at 0x7fa80fe17e20>, trace_args = (tensor([[0.0488, 0.0095, 0.3701],
        [0.6068, 0.9354, 0.0508],
        [0.0576, 0.8053, 0.4030]]),), trace_kwargs = {}
test_args = (tensor([[0.5561, 0.2208, 0.5904],
        [0.2330, 0.7442, 0.0742],
        [0.1333, 0.2863, 0.7602]]),), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
            _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)
    
        # test it works with the test_args as input
        orig_out = fn(*test_args, **test_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(test_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(test_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[[ 0.7679,  0.4684, -0.4369],
          [-0.4469, -0.0968, -0.8893],
          [-0.4588,  0.8782,  0.1350]],...   [[ 0.7138],
         [-0.0618],
         [-0.6976]],

        [[-0.7138],
         [ 0.0618],
         [ 0.6976]]]))
transpiled_x = (<tf.Tensor: shape=(1, 4, 3, 3), dtype=float32, numpy=
array([[[[ 0.51103944, -0.8571441 , -0.06436092],
         [ 0....5563],
        [ 0.69759107]],

       [[ 0.7138297 ],
        [-0.06175563],
        [-0.69759107]]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[ 0.7679406 ,  0.4683721 , -0.43691546],
         [-0.44692034, -0.09682216, -0.8893185 ],
         [-0.4588...75564],
        [-0.69759107]],

       [[-0.71382976],
        [ 0.06175564],
        [ 0.69759107]]], dtype=float32))
y = (array([[[[ 0.51103944, -0.8571441 , -0.06436092],
         [ 0.33627173,  0.13045608,  0.93268555],
         [-0.7910...75563],
        [ 0.69759107]],

       [[ 0.7138297 ],
        [-0.06175563],
        [-0.69759107]]], dtype=float32))
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7fa808df7880>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[ 0.7679406 ,  0.4683721 , -0.43691546],
         [-0.44692034, -0.09682216, -0.8893185 ],
         [-0.45883...,
         [ 0.33627176,  0.13045608,  0.93268555],
         [-0.7910499 , -0.49828178,  0.35490152]]]], dtype=float32)
y = array([[[[ 0.51103944, -0.8571441 , -0.06436092],
         [ 0.33627173,  0.13045608,  0.93268555],
         [-0.79104...,
         [-0.4469203 , -0.09682216, -0.8893185 ],
         [-0.45883498,  0.8782102 ,  0.1349713 ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.epipolar.essential.motion_from_essential
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_epipolar.py::test_decompose_essential_matrix[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_epipolar.py::test_motion_from_essential[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
=============================================================================== 2 failed, 24 passed in 386.06s (0:06:26) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 16 items

kornia/test_utils.py .F....F...FFFF..                                                                                                                                                            [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_draw_rectangle[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_draw_rectangle(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(2, 3, 10, 12),
            torch.tensor([[[0, 0, 4, 4]], [[4, 4, 10, 10]]]),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(3, 3, 10, 12),
            torch.tensor([[[0, 0, 4, 4]], [[4, 4, 10, 10]], [[2, 2, 6, 6]]]),
        )
        test_kwargs = {}
>       _test_function(
            kornia.utils.draw_rectangle,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_utils.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function draw_rectangle at 0x7f69c64d9a20>
trace_args = (tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9774, 0.3022, 0.3377,
           0.6863, 0.6495, 0.5740, 0.2691]...000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.4747]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]]]))
trace_kwargs = {}
test_args = (tensor([[[[0.1788, 0.6587, 0.4418,  ..., 0.9058, 0.4804, 0.5366],
          [0.5742, 0.8136, 0.7553,  ..., 0.6792, 0....., 0.7772, 0.8777, 0.0369]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]],

        [[ 2,  2,  6,  6]]]))
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function draw_rectangle at 0x7f69c64d9a20>
trace_args = (tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9774, 0.3022, 0.3377,
           0.6863, 0.6495, 0.5740, 0.2691]...000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.4747]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]]]))
trace_kwargs = {}
test_args = (tensor([[[[0.1788, 0.6587, 0.4418,  ..., 0.9058, 0.4804, 0.5366],
          [0.5742, 0.8136, 0.7553,  ..., 0.6792, 0....., 0.7772, 0.8777, 0.0369]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]],

        [[ 2,  2,  6,  6]]]))
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.  ...      0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.47465265]]]], dtype=float32)>
rectangle = <tf.Tensor: shape=(2, 1, 4), dtype=int64, numpy=
array([[[ 0,  0,  4,  4]],

       [[ 4,  4, 10,  9]]])>
color = <tf.Tensor: shape=(2, 1, 3), dtype=float32, numpy=
array([[[0., 0., 0.]],

       [[0., 0., 0.]]], dtype=float32)>, fill = False

    def tensorflow_draw_rectangle(image, rectangle, color=None, fill=None):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_long_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_clamp_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_tensor_frnt
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
        batch, c, h, w = tensorflow_shape_frnt_(image)
        batch_rect, num_rectangle, num_points = tensorflow_shape_frnt_(rectangle)
        if batch != batch_rect:
            raise AssertionError("Image batch and rectangle batch must be equal")
        if num_points != 4:
            raise AssertionError("Number of points in rectangle must be 4")
        rectangle = tensorflow_clone_frnt_(tensorflow_long_frnt_(rectangle))
        rectangle = tensorflow_set_item_bknd(
            rectangle,
            (slice(None, None, None), slice(None, None, None), slice(1, None, 2)),
            tensorflow_clamp_frnt(rectangle[:, :, 1::2], 0, h - 1),
        )
        rectangle = tensorflow_set_item_bknd(
            rectangle,
            (slice(None, None, None), slice(None, None, None), slice(None, None, 2)),
            tensorflow_clamp_frnt(rectangle[:, :, ::2], 0, w - 1),
        )
        if color is None:
            color = tensorflow_expand_frnt_(
                tensorflow_tensor_frnt([0.0] * c), batch, num_rectangle, c
            )
        if fill is None:
            fill = False
        if len(tensorflow_shape_frnt_(color)) == 1:
            color = tensorflow_expand_frnt_(color, batch, num_rectangle, c)
        b, n, color_channels = tensorflow_shape_frnt_(color)
        if color_channels == 1 and c == 3:
            color = tensorflow_expand_frnt_(color, batch, num_rectangle, c)
        for b in range(batch):
            for n in range(num_rectangle):
                if fill:
                    image = tensorflow_set_item_bknd(
                        image,
                        (
                            b,
                            slice(None, None, None),
                            slice(
                                int(rectangle[b, n, 1]), int(rectangle[b, n, 3] + 1), None
                            ),
                            slice(
                                int(rectangle[b, n, 0]), int(rectangle[b, n, 2] + 1), None
                            ),
                        ),
                        tensorflow_get_item(
                            color, (b, n, slice(None, None, None), None, None)
                        ),
                    )
                else:
                    image = tensorflow_set_item_bknd(
                        image,
                        (
                            b,
                            slice(None, None, None),
                            slice(
                                int(rectangle[b, n, 1]), int(rectangle[b, n, 3] + 1), None
                            ),
                            rectangle[b, n, 0],
                        ),
                        tensorflow_get_item(color, (b, n, slice(None, None, None), None)),
                    )
>                   image = tensorflow_set_item_bknd(
                        image,
                        (
                            b,
                            slice(None, None, None),
                            slice(
                                int(rectangle[b, n, 1]), int(rectangle[b, n, 3] + 1), None
                            ),
                            rectangle[b, n, 2],
                        ),
                        tensorflow_get_item(color, (b, n, slice(None, None, None), None)),
                    )

Translated_Outputs/tensorflow_outputs/kornia/utils/draw.py:275: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inp = <tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.  ...      0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.47465265]]]], dtype=float32)>
query = (0, slice(None, None, None), slice(0, 5, None), <tf.Tensor: shape=(), dtype=int64, numpy=4>)
val = <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.],
       [0.],
       [0.]], dtype=float32)>, kwargs = {}

    @functools.wraps(fn)
    def wrapper(inp, query, val, **kwargs):
        try:
            inp.__setitem__(query, val)
            res = inp
        except IndexError:
            raise
        except Exception:
>           res = fn(inp, query, val, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0. ...64, numpy=4>), <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.],
       [0.],
       [0.]], dtype=float32)>)
kwargs = {}, handle_mixed_in_backend = True

    @functools.wraps(fn)
    def _handle_partial_mixed_function(*args, **kwargs):
        handle_mixed_in_backend = False
        if not hasattr(fn, "partial_mixed_handler"):
            handle_mixed_in_backend = True
        else:
            compos = getattr(fn, "compos")
            condition = getattr(fn, "partial_mixed_handler")
        if handle_mixed_in_backend or condition(*args, **kwargs):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.  ...      0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.47465265]]]], dtype=float32)>
query = (0, slice(None, None, None), slice(0, 5, None), <tf.Tensor: shape=(), dtype=int64, numpy=4>)
val = <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.],
       [0.],
       [0.]], dtype=float32)>

    @tensorflow_handle_set_item
    @tensorflow_handle_partial_mixed_function
    def tensorflow_set_item_bknd(
        x: Union[tensorflow.Tensor, tf.Tensor],
        query: Union[tensorflow.Tensor, tf.Tensor, Tuple],
        val: Union[tensorflow.Tensor, tf.Tensor],
        /,
        *,
        copy: Optional[bool] = False,
    ):
        from ..backends.tensorflow.general import tensorflow_shape
        from ..backends.tensorflow.creation import tensorflow_copy_array
        from ..backends.tensorflow.creation import tensorflow_asarray
        from .data_type import tensorflow_is_bool_dtype_bknd
        from ..backends.tensorflow.manipulation import tensorflow_tile
        from ..backends.tensorflow.searching import tensorflow_nonzero
        from ...data_classes.array.data_type import tensorflow_astype_bknd_
        from ..backends.tensorflow.general import tensorflow_scatter_nd
    
        if copy:
            x = tensorflow_copy_array(x)
        if not tensorflow_is_array_bknd(val):
            val = tensorflow_asarray(val)
        if 0 in x.shape or 0 in val.shape:
            return x
        if tensorflow_is_array_bknd(query) and tensorflow_is_bool_dtype_bknd(query):
            if not len(query.shape):
                query = tensorflow_tile(query, (x.shape[0],))
            indices = tensorflow_nonzero(query, as_tuple=False)
        else:
            indices, target_shape, _ = tensorflow__parse_query_bknd(
                query, tensorflow_shape(x, as_array=True), scatter=True
            )
            if indices is None:
                return x
        val = tensorflow_astype_bknd_(val, x.dtype)
>       ret = tensorflow_scatter_nd(indices, val, reduction="replace", out=x)

Translated_Outputs/tensorflow_outputs/ivy/functional/ivy/general.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

indices = <tf.Tensor: shape=(3, 5, 4), dtype=int64, numpy=
array([[[0, 4, 0, 0],
        [0, 4, 0, 1],
        [0, 4, 0, 2],
   ... 4]],

       [[0, 4, 2, 0],
        [0, 4, 2, 1],
        [0, 4, 2, 2],
        [0, 4, 2, 3],
        [0, 4, 2, 4]]])>
updates = <tf.Tensor: shape=(3, 5), dtype=float32, numpy=
array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)>, shape = None

    def tensorflow_scatter_nd(
        indices: Union[tensorflow.Tensor, tensorflow.Variable],
        updates: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Optional[Union[tf.TensorShape, Sequence[int]]] = None,
        *,
        reduction: str = "sum",
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.general import tensorflow_exists_bknd
        from ...ivy.data_type import tensorflow_promote_types_bknd
        from .data_type import tensorflow_as_native_dtype
        from ...ivy.general import tensorflow__broadcast_to_bknd
        from ....utils.assertions import tensorflow_check_equal
        from .elementwise import tensorflow_multiply
    
        updates_dtype = updates.dtype
        if tensorflow_exists_bknd(out):
            dtype = tensorflow_promote_types_bknd(out.dtype, updates_dtype)
        updates = tensorflow.cast(
            updates,
            tensorflow_as_native_dtype(dtype)
            if tensorflow_exists_bknd(out)
            else updates_dtype,
        )
        expected_shape = (
            list(tensorflow.shape(indices)[:-1])
            + list(out.shape[tensorflow.shape(indices)[-1] :])
            if tensorflow_exists_bknd(out)
            else list(tensorflow.shape(indices)[:-1])
            + list(shape[tensorflow.shape(indices)[-1] :])
        )
        updates = tensorflow__broadcast_to_bknd(updates, expected_shape)
        if len(updates.shape) == 0:
            indices = tensorflow.expand_dims(indices, 0)
            updates = tensorflow.expand_dims(updates, 0)
        target = out
        target_given = tensorflow_exists_bknd(target)
        if tensorflow_exists_bknd(shape) and target_given:
            tensorflow_check_equal(tuple(target.shape), tuple(shape), as_array=False)
        if not target_given:
            shape = list(shape) if tensorflow_exists_bknd(shape) else list(out.shape)
            target = tensorflow.zeros(shape, dtype=updates.dtype)
        if reduction == "sum":
            res = tensorflow.tensor_scatter_nd_add(target, indices, updates)
        elif reduction == "min":
            res = tensorflow.tensor_scatter_nd_min(target, indices, updates)
        elif reduction == "max":
            res = tensorflow.tensor_scatter_nd_max(target, indices, updates)
        elif reduction == "mul":
            updates = tensorflow_multiply(tensorflow_gather_nd(target, indices), updates)
            res = tensorflow.tensor_scatter_nd_update(target, indices, updates)
        elif reduction == "replace":
>           res = tensorflow.tensor_scatter_nd_update(target, indices, updates)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/general.py:274: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0. ...loat32, numpy=
array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)>)
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__TensorScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,0] = [0, 4, 0, 0] does not index into shape [2,3,10,12] [Op:TensorScatterUpdate] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.draw.draw_rectangle
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-13 13:35:46.038453: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at scatter_nd_op.cc:218 : INVALID_ARGUMENT: indices[0,0] = [0, 4, 0, 0] does not index into shape [2,3,10,12]
______________________________________________________________________________ test_tensor_to_image[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_tensor_to_image(target_framework, mode, backend_compile):
        print("kornia.utils.tensor_to_image")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_func = ivy.transpile(kornia.utils.tensor_to_image, source="torch", target=target_framework)
    
        tensor = torch.ones(1, 3, 3)
        transpiled_tensor = _nest_torch_tensor_to_new_framework(tensor, target_framework)
        torch_image = kornia.utils.tensor_to_image(tensor)
>       transpiled_image = transpiled_func(transpiled_tensor)

kornia/test_utils.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]]], dtype=float32)>, keepdim = False, force_contiguous = False

    def tensorflow_tensor_to_image(tensor, keepdim=False, force_contiguous=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_detach_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_numpy_frnt_
    
        if not isinstance(tensor, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(tensor)}")
        if (
            len(tensorflow_shape_frnt_(tensor)) > 4
            or len(tensorflow_shape_frnt_(tensor)) < 2
        ):
            raise ValueError("Input size must be a two, three or four dimensional tensor")
        input_shape = tensorflow_shape_frnt_(tensor)
        image = tensorflow_detach_frnt_(tensor.cpu())
        if len(input_shape) == 2:
            pass
        elif len(input_shape) == 3:
            if input_shape[0] == 1:
                image = tensorflow_squeeze_frnt_(image)
            else:
                image = tensorflow_permute_frnt_(image, 1, 2, 0)
        elif len(input_shape) == 4:
            image = tensorflow_permute_frnt_(image, 0, 2, 3, 1)
            if input_shape[0] == 1 and not keepdim:
                image = tensorflow_squeeze_frnt_(image, 0)
            if input_shape[1] == 1:
                image = tensorflow_squeeze_frnt_(image, -1)
        else:
            raise ValueError(f"Cannot process tensor with shape {input_shape}")
        if force_contiguous:
            image = tensorflow_contiguous_frnt_(image)
>       return tensorflow_numpy_frnt_(image)

Translated_Outputs/tensorflow_outputs/kornia/utils/image.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>,), kwargs = {}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f69beef5090>
array_like = <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>

    @tensorflow_handle_methods
    def tensorflow_numpy_frnt_(tensor):
>       return np_frontend_array(tensor)
E       NameError: name 'np_frontend_array' is not defined

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:381: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.tensor_to_image
____________________________________________________________________________ test_save_pointcloud_ply[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_save_pointcloud_ply(target_framework, mode, backend_compile):
        print("kornia.utils.save_pointcloud_ply")
    
        if backend_compile:
            pytest.skip()
    
        pointcloud = torch.rand(100, 3)
        transpiled_pointcloud = _nest_torch_tensor_to_new_framework(pointcloud, target_framework)
    
        with tempfile.NamedTemporaryFile(suffix=".ply") as temp_file:
            filename = temp_file.name
    
            transpiled_save_pointcloud = ivy.transpile(kornia.utils.save_pointcloud_ply, source="torch", target=target_framework)
            transpiled_load_pointcloud = ivy.transpile(kornia.utils.load_pointcloud_ply, source="torch", target=target_framework)
    
            # Save and load pointcloud to ensure both steps work correctly
            transpiled_save_pointcloud(filename, transpiled_pointcloud)
>           loaded_pointcloud = transpiled_load_pointcloud(filename)

kornia/test_utils.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

filename = '/tmp/tmpd6glqmtu.ply', header_size = 8

    def tensorflow_load_pointcloud_ply(filename, header_size=8):
        from ...genericpath import tensorflow_isfile
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_split_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_tensor_frnt
    
        if not isinstance(filename, (str,)) and filename[-3:] == ".ply":
            raise TypeError(
                f"Input filename must be a string in with the .ply  extension. Got {filename}"
            )
>       if not tensorflow_isfile(filename):

Translated_Outputs/tensorflow_outputs/kornia/utils/pointcloud_io.py:91: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

path = '/tmp/tmpd6glqmtu.ply'

    def tensorflow_isfile(path):
        try:
            st = os.stat(path)
        except (OSError, ValueError):
            return False
>       return stat.S_ISREG(st.st_mode)
E       NameError: name 'stat' is not defined

Translated_Outputs/tensorflow_outputs/genericpath.py:35: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.save_pointcloud_ply
_______________________________________________________________________ test_get_cuda_device_if_available[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_cuda_device_if_available(target_framework, mode, backend_compile):
        print("kornia.utils.get_cuda_device_if_available")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_get_cuda_device_if_available = ivy.transpile(kornia.utils.get_cuda_device_if_available, source="torch", target=target_framework)
    
        torch_device = kornia.utils.get_cuda_device_if_available()
>       transpiled_device = transpiled_get_cuda_device_if_available()

kornia/test_utils.py:283: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

index = 0

    def tensorflow_get_cuda_device_if_available(index=0):
        from ...ivy.functional.frontends.torch.__init__ import tensorflow_device_frnt
    
>       if cuda.is_available():
E       NameError: name 'cuda' is not defined

Translated_Outputs/tensorflow_outputs/kornia/utils/helpers.py:31: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.get_cuda_device_if_available
________________________________________________________________________ test_get_mps_device_if_available[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_mps_device_if_available(target_framework, mode, backend_compile):
        print("kornia.utils.get_mps_device_if_available")
    
        if backend_compile:
            pytest.skip()
    
>       transpiled_get_mps_device_if_available = ivy.transpile(kornia.utils.get_mps_device_if_available, source="torch", target=target_framework)

kornia/test_utils.py:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <function get_mps_device_if_available at 0x7f69c64da440>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Unsupported type: None

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.get_mps_device_if_available
____________________________________________________________________ test_get_cuda_or_mps_device_if_available[tensorflow-s2s-False] ____________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_cuda_or_mps_device_if_available(target_framework, mode, backend_compile):
        print("kornia.utils.get_cuda_or_mps_device_if_available")
    
        if backend_compile:
            pytest.skip()
    
>       transpiled_get_cuda_or_mps_device_if_available = ivy.transpile(kornia.utils.get_cuda_or_mps_device_if_available, source="torch", target=target_framework)

kornia/test_utils.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <function get_cuda_or_mps_device_if_available at 0x7f69c64da4d0>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Unsupported type: None

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.get_cuda_or_mps_device_if_available
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_utils.py::test_draw_rectangle[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__TensorScatterUpdate_device_/job:loc...
FAILED kornia/test_utils.py::test_tensor_to_image[tensorflow-s2s-False] - NameError: name 'np_frontend_array' is not defined
FAILED kornia/test_utils.py::test_save_pointcloud_ply[tensorflow-s2s-False] - NameError: name 'stat' is not defined
FAILED kornia/test_utils.py::test_get_cuda_device_if_available[tensorflow-s2s-False] - NameError: name 'cuda' is not defined
FAILED kornia/test_utils.py::test_get_mps_device_if_available[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Unsupported type: None
FAILED kornia/test_utils.py::test_get_cuda_or_mps_device_if_available[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Unsupported type: None
=============================================================================== 6 failed, 10 passed in 276.41s (0:04:36) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

kornia/geometry/test_quaternion.py .                                                                                                                                                             [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 1 passed in 118.59s (0:01:58) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 4 items

kornia/test_sensors.py ....                                                                                                                                                                      [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
===================================================================================== 4 passed in 93.94s (0:01:33) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/geometry/test_boxes.py FF                                                                                                                                                                 [100%]

=============================================================================================== FAILURES ===============================================================================================
___________________________________________________________________________________ test_Boxes[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Boxes(target_framework, mode, backend_compile):
        print("kornia.geometry.boxes.Boxes")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledBoxes = ivy.transpile(kornia.geometry.boxes.Boxes, source="torch", target=target_framework)
    
        torch_args = (
            torch.as_tensor([[0, 3, 1, 4], [5, 1, 8, 4]]),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        # test .from_tensor
        torch_boxes = kornia.geometry.boxes.Boxes.from_tensor(*torch_args, mode="xyxy")
        transpiled_boxes = TranspiledBoxes.from_tensor(*transpiled_args, mode="xyxy")
        _check_boxes_same(torch_boxes, transpiled_boxes)
    
        # test .compute_area
        torch_area = torch_boxes.compute_area()
        transpiled_area = transpiled_boxes.compute_area()
        _to_numpy_and_allclose(torch_area, transpiled_area)
    
        # test .get_boxes_shape
        torch_heights, torch_widths = torch_boxes.get_boxes_shape()
        transpiled_heights, transpiled_widths = transpiled_boxes.get_boxes_shape()
        _to_numpy_and_allclose(torch_heights, transpiled_heights)
        _to_numpy_and_allclose(torch_widths, transpiled_widths)
    
        # test .merge
        torch_x = torch.as_tensor([[6, 6, 10, 10], [6, 6, 10, 10]])
        transpiled_x = _nest_torch_tensor_to_new_framework(torch_x, target_framework)
        merge_boxes = kornia.geometry.boxes.Boxes.from_tensor(torch_x, mode="xyxy")
        transpiled_merge_boxes = TranspiledBoxes.from_tensor(transpiled_x, mode="xyxy")
        torch_merged_boxes = torch_boxes.merge(merge_boxes)
        transpiled_merged_boxes = transpiled_boxes.merge(transpiled_merge_boxes)
        _check_boxes_same(torch_merged_boxes, transpiled_merged_boxes)
    
        # test .to_mask
        height, width = 10, 10
        torch_mask = torch_boxes.to_mask(height, width)
        transpiled_mask = transpiled_boxes.to_mask(height, width)
>       _to_numpy_and_allclose(torch_mask, transpiled_mask)

kornia/geometry/test_boxes.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0....., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])
transpiled_x = <tf.Tensor: shape=(2, 10, 10), dtype=float32, numpy=
array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0....,
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0...],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)
y = array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0...],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.boxes.Boxes
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/ivy/ivy/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.
  warnings.warn(
__________________________________________________________________________________ test_Boxes3D[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Boxes3D(target_framework, mode, backend_compile):
        print("kornia.geometry.boxes.Boxes3D")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledBoxes3D = ivy.transpile(kornia.geometry.boxes.Boxes3D, source="torch", target=target_framework)
    
        torch_args = (
            torch.as_tensor([[0, 3, 6, 1, 4, 8], [5, 1, 3, 8, 4, 9]]),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        # test .from_tensor
        torch_boxes3d = kornia.geometry.boxes.Boxes3D.from_tensor(*torch_args, mode="xyzxyz")
        transpiled_boxes3d = TranspiledBoxes3D.from_tensor(*transpiled_args, mode="xyzxyz")
        _check_boxes_same(torch_boxes3d, transpiled_boxes3d)
    
        # test .get_boxes_shape
        torch_depths, torch_heights, torch_widths = torch_boxes3d.get_boxes_shape()
        transpiled_depths, transpiled_heights, transpiled_widths = transpiled_boxes3d.get_boxes_shape()
        _to_numpy_and_allclose(torch_depths, transpiled_depths)
        _to_numpy_and_allclose(torch_heights, transpiled_heights)
        _to_numpy_and_allclose(torch_widths, transpiled_widths)
    
        # test .to_mask
        depth, height, width = 10, 10, 10
        torch_mask = torch_boxes3d.to_mask(depth, height, width)
        transpiled_mask = transpiled_boxes3d.to_mask(depth, height, width)
>       _to_numpy_and_allclose(torch_mask, transpiled_mask)

kornia/geometry/test_boxes.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0... [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
transpiled_x = <tf.Tensor: shape=(2, 10, 10, 10), dtype=float32, numpy=
array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0.,...., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]...0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)
y = array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]...0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.boxes.Boxes3D
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_boxes.py::test_Boxes[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_boxes.py::test_Boxes3D[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
==================================================================================== 2 failed in 170.79s (0:02:50) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/test_image.py ....FFFF                                                                                                                                                                    [100%]

=============================================================================================== FAILURES ===============================================================================================
___________________________________________________________________________________ test_Image[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image(target_framework, mode, backend_compile):
        print("kornia.image.Image")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImageSize = ivy.transpile(kornia.image.ImageSize, source="torch", target=target_framework)
        TranspiledPixelFormat = ivy.transpile(kornia.image.PixelFormat, source="torch", target=target_framework)
        TranspiledChannelsOrder = ivy.transpile(kornia.image.ChannelsOrder, source="torch", target=target_framework)
        TranspiledImageLayout = ivy.transpile(kornia.image.ImageLayout, source="torch", target=target_framework)
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        torch_data = torch.randint(0, 255, (3, 4, 5), dtype=torch.uint8)
        transpiled_data = _array_to_new_backend(torch_data, target_framework)
    
        # torch
        pixel_format = kornia.image.PixelFormat(
            color_space=ColorSpace.rgb,
            bit_depth=8,
        )
        layout = kornia.image.ImageLayout(
            image_size=kornia.image.ImageSize(4, 5),
            channels=3,
            channels_order=kornia.image.ChannelsOrder.CHANNELS_FIRST,
        )
        torch_img = kornia.image.Image(torch_data, pixel_format, layout)
    
        # transpiled
        pixel_format = TranspiledPixelFormat(
            color_space=ColorSpace.rgb,
            bit_depth=8,
        )
        layout = TranspiledImageLayout(
            image_size=TranspiledImageSize(4, 5),
            channels=3,
            channels_order=TranspiledChannelsOrder.CHANNELS_FIRST,
        )
>       transpiled_img = TranspiledImage(transpiled_data, pixel_format, layout)

kornia/test_image.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_Image' object has no attribute '_data'") raised in repr()] tensorflow_Image object at 0x7f8f5e0b1db0>
data = <tf.Tensor: shape=(3, 4, 5), dtype=uint8, numpy=
array([[[172, 149, 233,  26,  91],
        [  4,  74, 202,  89, 165],...       [231, 240,  45,  99,  17],
        [  1, 239,  30, 135, 204],
        [105,  78,  23, 239,  98]]], dtype=uint8)>
pixel_format = tensorflow_PixelFormat(color_space=<ColorSpace.rgb: 0>, bit_depth=8)
layout = tensorflow_ImageLayout(image_size=tensorflow_ImageSize(height=4, width=5), channels=3, channels_order=<tensorflow_ChannelsOrder.CHANNELS_FIRST: 0>)

    def __init__(self, data, pixel_format, layout):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
    
        if layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            shape = [
                str(layout.channels),
                str(layout.image_size.height),
                str(layout.image_size.width),
            ]
        elif layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            shape = [
                str(layout.image_size.height),
                str(layout.image_size.width),
                str(layout.channels),
            ]
        else:
            raise NotImplementedError(
                f"Layout {layout.channels_order} not implemented."
            )
        tensorflow_KORNIA_CHECK_SHAPE(data, shape)
        tensorflow_KORNIA_CHECK(
>           data.element_size() == pixel_format.bit_depth // 8, "Invalid bit depth."
        )

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Tensor: shape=(3, 4, 5), dtype=uint8, numpy=
array([[[172, 149, 233,  26,  91],
        [  4,  74, 202,  89, 165],...       [231, 240,  45,  99,  17],
        [  1, 239,  30, 135, 204],
        [105,  78,  23, 239,  98]]], dtype=uint8)>
name = 'element_size'

    def __getattr__(self, name):
      if name in {"T", "astype", "ravel", "transpose", "reshape", "clip", "size",
                  "tolist", "data"}:
        # TODO(wangpeng): Export the enable_numpy_behavior knob
        raise AttributeError(
            f"{type(self).__name__} object has no attribute '{name}'. " + """
          If you are looking for numpy-related methods, please run the following:
          tf.experimental.numpy.experimental_enable_numpy_behavior()
        """)
>     self.__getattribute__(name)
E     AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'

/opt/fw/tensorflow/tensorflow/python/framework/tensor.py:260: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image
_____________________________________________________________________________ test_Image_from_numpy[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image_from_numpy(target_framework, mode, backend_compile):
        print("kornia.image.Image.from_numpy")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        data = np.ones((4, 5, 3), dtype=np.uint8)
>       img = TranspiledImage.from_numpy(data, color_space=ColorSpace.rgb)

kornia/test_image.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'Translated_Outputs.tensorflow_outputs.kornia.image.image.tensorflow_Image'>
data = array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]],

       [[1, 1, 1],
  ... 1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)
color_space = <ColorSpace.rgb: 0>, channels_order = <tensorflow_ChannelsOrder.CHANNELS_LAST: 1>

    @classmethod
    def from_numpy(
        cls,
        data,
        color_space=tensorflow_ColorSpace.RGB,
        channels_order=tensorflow_ChannelsOrder.CHANNELS_LAST,
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .base import tensorflow_ImageSize
        from .base import tensorflow_PixelFormat
        from ...ivy.data_classes.array.array import tensorflow_itemsize_bknd_
        from .base import tensorflow_ImageLayout
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_from_numpy_frnt,
        )
    
        if channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[0],
                width=tensorflow_shape_frnt_(data)[1],
            )
            channels = tensorflow_shape_frnt_(data)[2]
        elif channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[1],
                width=tensorflow_shape_frnt_(data)[2],
            )
            channels = tensorflow_shape_frnt_(data)[0]
        else:
            raise ValueError(
                "channels_order must be either `CHANNELS_LAST` or `CHANNELS_FIRST`"
            )
        pixel_format = tensorflow_PixelFormat(
            color_space=color_space, bit_depth=tensorflow_itemsize_bknd_(data) * 8
        )
        layout = tensorflow_ImageLayout(
            image_size=image_size, channels=channels, channels_order=channels_order
        )
>       return cls(tensorflow_from_numpy_frnt(data), pixel_format, layout)

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_Image' object has no attribute '_data'") raised in repr()] tensorflow_Image object at 0x7f8f5f5b3bb0>
data = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
pixel_format = tensorflow_PixelFormat(color_space=<ColorSpace.rgb: 0>, bit_depth=8)
layout = tensorflow_ImageLayout(image_size=tensorflow_ImageSize(height=4, width=5), channels=3, channels_order=<tensorflow_ChannelsOrder.CHANNELS_LAST: 1>)

    def __init__(self, data, pixel_format, layout):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
    
        if layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            shape = [
                str(layout.channels),
                str(layout.image_size.height),
                str(layout.image_size.width),
            ]
        elif layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            shape = [
                str(layout.image_size.height),
                str(layout.image_size.width),
                str(layout.channels),
            ]
        else:
            raise NotImplementedError(
                f"Layout {layout.channels_order} not implemented."
            )
        tensorflow_KORNIA_CHECK_SHAPE(data, shape)
        tensorflow_KORNIA_CHECK(
>           data.element_size() == pixel_format.bit_depth // 8, "Invalid bit depth."
        )

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
name = 'element_size'

    def __getattr__(self, name):
      if name in {"T", "astype", "ravel", "transpose", "reshape", "clip", "size",
                  "tolist", "data"}:
        # TODO(wangpeng): Export the enable_numpy_behavior knob
        raise AttributeError(
            f"{type(self).__name__} object has no attribute '{name}'. " + """
          If you are looking for numpy-related methods, please run the following:
          tf.experimental.numpy.experimental_enable_numpy_behavior()
        """)
>     self.__getattribute__(name)
E     AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'

/opt/fw/tensorflow/tensorflow/python/framework/tensor.py:260: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image.from_numpy
_____________________________________________________________________________ test_Image_from_dlpack[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image_from_dlpack(target_framework, mode, backend_compile):
        print("kornia.image.Image.from_dlpack")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        x = np.ones((4, 5, 3))
>       img = TranspiledImage.from_dlpack(x.__dlpack__())

kornia/test_image.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'Translated_Outputs.tensorflow_outputs.kornia.image.image.tensorflow_Image'>, data = <capsule object "dltensor" at 0x7f8f5e914240>

    @classmethod
    def from_dlpack(cls, data):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .base import tensorflow_ImageSize
        from .base import tensorflow_PixelFormat
        from .base import tensorflow_ImageLayout
        from ...ivy.data_classes.array.creation import tensorflow_from_dlpack_bknd_
    
>       _data: typing.Any = tensorflow_from_dlpack_bknd_(torch.utils.dlpack, data)
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:188: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image.from_dlpack
______________________________________________________________________________ test_Image_to_numpy[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image_to_numpy(target_framework, mode, backend_compile):
        print("kornia.image.Image.to_numpy")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        data = np.ones((4, 5, 3), dtype=np.uint8)
>       img = TranspiledImage.from_numpy(data, color_space=ColorSpace.rgb)

kornia/test_image.py:160: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'Translated_Outputs.tensorflow_outputs.kornia.image.image.tensorflow_Image'>
data = array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]],

       [[1, 1, 1],
  ... 1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)
color_space = <ColorSpace.rgb: 0>, channels_order = <tensorflow_ChannelsOrder.CHANNELS_LAST: 1>

    @classmethod
    def from_numpy(
        cls,
        data,
        color_space=tensorflow_ColorSpace.RGB,
        channels_order=tensorflow_ChannelsOrder.CHANNELS_LAST,
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .base import tensorflow_ImageSize
        from .base import tensorflow_PixelFormat
        from ...ivy.data_classes.array.array import tensorflow_itemsize_bknd_
        from .base import tensorflow_ImageLayout
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_from_numpy_frnt,
        )
    
        if channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[0],
                width=tensorflow_shape_frnt_(data)[1],
            )
            channels = tensorflow_shape_frnt_(data)[2]
        elif channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[1],
                width=tensorflow_shape_frnt_(data)[2],
            )
            channels = tensorflow_shape_frnt_(data)[0]
        else:
            raise ValueError(
                "channels_order must be either `CHANNELS_LAST` or `CHANNELS_FIRST`"
            )
        pixel_format = tensorflow_PixelFormat(
            color_space=color_space, bit_depth=tensorflow_itemsize_bknd_(data) * 8
        )
        layout = tensorflow_ImageLayout(
            image_size=image_size, channels=channels, channels_order=channels_order
        )
>       return cls(tensorflow_from_numpy_frnt(data), pixel_format, layout)

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_Image' object has no attribute '_data'") raised in repr()] tensorflow_Image object at 0x7f8f5e9472b0>
data = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
pixel_format = tensorflow_PixelFormat(color_space=<ColorSpace.rgb: 0>, bit_depth=8)
layout = tensorflow_ImageLayout(image_size=tensorflow_ImageSize(height=4, width=5), channels=3, channels_order=<tensorflow_ChannelsOrder.CHANNELS_LAST: 1>)

    def __init__(self, data, pixel_format, layout):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
    
        if layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            shape = [
                str(layout.channels),
                str(layout.image_size.height),
                str(layout.image_size.width),
            ]
        elif layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            shape = [
                str(layout.image_size.height),
                str(layout.image_size.width),
                str(layout.channels),
            ]
        else:
            raise NotImplementedError(
                f"Layout {layout.channels_order} not implemented."
            )
        tensorflow_KORNIA_CHECK_SHAPE(data, shape)
        tensorflow_KORNIA_CHECK(
>           data.element_size() == pixel_format.bit_depth // 8, "Invalid bit depth."
        )

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
name = 'element_size'

    def __getattr__(self, name):
      if name in {"T", "astype", "ravel", "transpose", "reshape", "clip", "size",
                  "tolist", "data"}:
        # TODO(wangpeng): Export the enable_numpy_behavior knob
        raise AttributeError(
            f"{type(self).__name__} object has no attribute '{name}'. " + """
          If you are looking for numpy-related methods, please run the following:
          tf.experimental.numpy.experimental_enable_numpy_behavior()
        """)
>     self.__getattribute__(name)
E     AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'

/opt/fw/tensorflow/tensorflow/python/framework/tensor.py:260: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image.to_numpy
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_image.py::test_Image[tensorflow-s2s-False] - AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'
FAILED kornia/test_image.py::test_Image_from_numpy[tensorflow-s2s-False] - AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'
FAILED kornia/test_image.py::test_Image_from_dlpack[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_image.py::test_Image_to_numpy[tensorflow-s2s-False] - AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'
=============================================================================== 4 failed, 4 passed in 195.01s (0:03:15) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/geometry/test_vector.py ..                                                                                                                                                                [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
===================================================================================== 2 passed in 84.82s (0:01:24) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 5 items

kornia/test_x.py sssss                                                                                                                                                                           [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 5 skipped in 113.95s (0:01:53) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 3 items

kornia/augmentation/test_auto.py FFF                                                                                                                                                             [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________________ test_AutoAugment[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_AutoAugment(target_framework, mode, backend_compile):
        print("kornia.augmentation.auto.AutoAugment")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledAutoAugment = ivy.transpile(
            kornia.augmentation.auto.AutoAugment,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_auto.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.auto.autoaugment.autoaugment.AutoAugment'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.auto.AutoAugment
________________________________________________________________________________ test_RandAugment[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandAugment(target_framework, mode, backend_compile):
        print("kornia.augmentation.auto.RandAugment")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledRandAugment = ivy.transpile(
            kornia.augmentation.auto.RandAugment,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_auto.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.auto.rand_augment.rand_augment.RandAugment'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.auto.RandAugment
______________________________________________________________________________ test_TrivialAugment[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_TrivialAugment(target_framework, mode, backend_compile):
        print("kornia.augmentation.auto.TrivialAugment")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledTrivialAugment = ivy.transpile(
            kornia.augmentation.auto.TrivialAugment,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_auto.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.auto.trivial_augment.trivial_augment.TrivialAugment'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.auto.TrivialAugment
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/augmentation/test_auto.py::test_AutoAugment[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix....
FAILED kornia/augmentation/test_auto.py::test_RandAugment[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix....
FAILED kornia/augmentation/test_auto.py::test_TrivialAugment[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.m...
==================================================================================== 3 failed in 322.87s (0:05:22) =====================================================================================


========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

transformers/test_vision.py .                                                                                                                                                                    [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 1 passed in 1019.44s (0:16:59) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

kornia/test_tracking.py F                                                                                                                                                                        [100%]

=============================================================================================== FAILURES ===============================================================================================
_____________________________________________________________________________ test_HomographyTracker[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HomographyTracker(target_framework, mode, backend_compile):
        print("kornia.tracking.HomographyTracker")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledHomographyTracker = ivy.transpile(kornia.tracking.HomographyTracker, source="torch", target=target_framework)

kornia/test_tracking.py:30: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.tracking.planar_tracker.HomographyTracker'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, line 77)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.tracking.HomographyTracker
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/ivy/ivy/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.
  warnings.warn(
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_tracking.py::test_HomographyTracker[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>...
==================================================================================== 1 failed in 701.48s (0:11:41) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 38 items

kornia/geometry/test_conversions.py ......................................                                                                                                                       [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 38 passed in 297.16s (0:04:57) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/test_nerf.py FF.FFFFF                                                                                                                                                                     [100%]

=============================================================================================== FAILURES ===============================================================================================
_________________________________________________________________________________ test_NerfModel[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_NerfModel(target_framework, mode, backend_compile):
        print("kornia.nerf.nerf_model.NerfModel")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledNerfModel = ivy.transpile(nerf_model.NerfModel, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(5, 3),
            torch.rand(5, 3),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_nerf = nerf_model.NerfModel(num_ray_points=32)
>       transpiled_nerf = TranspiledNerfModel(num_ray_points=32)

kornia/test_nerf.py:34: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_NerfModel(
  (_renderer): tensorflow_IrregularRenderer()
  (_pos_encoder): tensorflow_PositionalEncoder()
 ...ense()
  (_fc2): tensorflow_Sequential(
    (0): KerasDense()
    (1): tensorflow_ReLU()
  )
  (_sigma): KerasDense()
)
num_ray_points = 32, irregular_ray_sampling = True, num_pos_freqs = 10, num_dir_freqs = 4, num_units = 2, num_unit_layers = 4, num_hidden = 128, log_space_encoding = True

    def __init__(
        self,
        num_ray_points,
        irregular_ray_sampling=True,
        num_pos_freqs=10,
        num_dir_freqs=4,
        num_units=2,
        num_unit_layers=4,
        num_hidden=128,
        log_space_encoding=True,
    ):
        from .volume_renderer import tensorflow_IrregularRenderer
        from .volume_renderer import tensorflow_RegularRenderer
        from .positional_encoder import tensorflow_PositionalEncoder
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.init import tensorflow_xavier_uniform_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_data_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
        from ...torch.nn.modules.activation import tensorflow_Sigmoid
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            num_ray_points,
            irregular_ray_sampling=irregular_ray_sampling,
            num_pos_freqs=num_pos_freqs,
            num_dir_freqs=num_dir_freqs,
            num_units=num_units,
            num_unit_layers=num_unit_layers,
            num_hidden=num_hidden,
            log_space_encoding=log_space_encoding,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self._num_ray_points = num_ray_points
        self._irregular_ray_sampling = irregular_ray_sampling
        self._renderer = (
            tensorflow_IrregularRenderer()
            if self._irregular_ray_sampling
            else tensorflow_RegularRenderer()
        )
        self._pos_encoder = tensorflow_PositionalEncoder(
            3, num_pos_freqs, log_space=log_space_encoding
        )
        self._dir_encoder = tensorflow_PositionalEncoder(
            3, num_dir_freqs, log_space=log_space_encoding
        )
        self._mlp = tensorflow_MLP(
            self._pos_encoder.num_encoded_dims, num_units, num_unit_layers, num_hidden
        )
        self._fc1 = KerasDense(in_features=num_hidden, units=num_hidden, use_bias=True)
        self._fc2 = tensorflow_Sequential(
            KerasDense(
                in_features=num_hidden + self._dir_encoder.num_encoded_dims,
                units=num_hidden // 2,
                use_bias=True,
            ),
            tensorflow_ReLU(),
        )
        self._sigma = KerasDense(in_features=num_hidden, units=1, use_bias=True)
        self._sigma.weight = tensorflow_xavier_uniform_(
            tensorflow_data_frnt_(self._sigma.weight)
        )
>       self._sigma.bias.data = tensorflow_float_frnt_(tensorflow_tensor_frnt([0.1]))
E       AttributeError: can't set attribute

Translated_Outputs/tensorflow_outputs/kornia/nerf/nerf_model.py:507: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.nerf_model.NerfModel
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/ivy/ivy/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.
  warnings.warn(
_____________________________________________________________________________ test_NerfModelRenderer[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_NerfModelRenderer(target_framework, mode, backend_compile):
        print("kornia.nerf.nerf_model.NerfModelRenderer")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)
        TranspiledNerfModel = ivy.transpile(nerf_model.NerfModel, source="torch", target=target_framework)
        TranspiledNerfModelRenderer = ivy.transpile(nerf_model.NerfModelRenderer, source="torch", target=target_framework)
    
        torch_nerf_model = nerf_model.NerfModel(num_ray_points=32)
>       transpiled_nerf_model = TranspiledNerfModel(num_ray_points=32)

kornia/test_nerf.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_NerfModel(
  (_renderer): tensorflow_IrregularRenderer()
  (_pos_encoder): tensorflow_PositionalEncoder()
 ...ense()
  (_fc2): tensorflow_Sequential(
    (0): KerasDense()
    (1): tensorflow_ReLU()
  )
  (_sigma): KerasDense()
)
num_ray_points = 32, irregular_ray_sampling = True, num_pos_freqs = 10, num_dir_freqs = 4, num_units = 2, num_unit_layers = 4, num_hidden = 128, log_space_encoding = True

    def __init__(
        self,
        num_ray_points,
        irregular_ray_sampling=True,
        num_pos_freqs=10,
        num_dir_freqs=4,
        num_units=2,
        num_unit_layers=4,
        num_hidden=128,
        log_space_encoding=True,
    ):
        from .volume_renderer import tensorflow_IrregularRenderer
        from .volume_renderer import tensorflow_RegularRenderer
        from .positional_encoder import tensorflow_PositionalEncoder
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.init import tensorflow_xavier_uniform_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_data_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
        from ...torch.nn.modules.activation import tensorflow_Sigmoid
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            num_ray_points,
            irregular_ray_sampling=irregular_ray_sampling,
            num_pos_freqs=num_pos_freqs,
            num_dir_freqs=num_dir_freqs,
            num_units=num_units,
            num_unit_layers=num_unit_layers,
            num_hidden=num_hidden,
            log_space_encoding=log_space_encoding,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self._num_ray_points = num_ray_points
        self._irregular_ray_sampling = irregular_ray_sampling
        self._renderer = (
            tensorflow_IrregularRenderer()
            if self._irregular_ray_sampling
            else tensorflow_RegularRenderer()
        )
        self._pos_encoder = tensorflow_PositionalEncoder(
            3, num_pos_freqs, log_space=log_space_encoding
        )
        self._dir_encoder = tensorflow_PositionalEncoder(
            3, num_dir_freqs, log_space=log_space_encoding
        )
        self._mlp = tensorflow_MLP(
            self._pos_encoder.num_encoded_dims, num_units, num_unit_layers, num_hidden
        )
        self._fc1 = KerasDense(in_features=num_hidden, units=num_hidden, use_bias=True)
        self._fc2 = tensorflow_Sequential(
            KerasDense(
                in_features=num_hidden + self._dir_encoder.num_encoded_dims,
                units=num_hidden // 2,
                use_bias=True,
            ),
            tensorflow_ReLU(),
        )
        self._sigma = KerasDense(in_features=num_hidden, units=1, use_bias=True)
        self._sigma.weight = tensorflow_xavier_uniform_(
            tensorflow_data_frnt_(self._sigma.weight)
        )
>       self._sigma.bias.data = tensorflow_float_frnt_(tensorflow_tensor_frnt([0.1]))
E       AttributeError: can't set attribute

Translated_Outputs/tensorflow_outputs/kornia/nerf/nerf_model.py:507: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.nerf_model.NerfModelRenderer
________________________________________________________________________________ test_NerfSolver[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_NerfSolver(target_framework, mode, backend_compile):
        print("kornia.nerf.nerf_solver.NerfSolver")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)
>       TranspiledNerfSolver = ivy.transpile(nerf_solver.NerfSolver, source="torch", target=target_framework)

kornia/test_nerf.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.nerf.nerf_solver.NerfSolver'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.nerf_solver.NerfSolver
_____________________________________________________________________________ test_IrregularRenderer[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_IrregularRenderer(target_framework, mode, backend_compile):
        print("kornia.nerf.volume_renderer.IrregularRenderer")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledIrregularRenderer = ivy.transpile(volume_renderer.IrregularRenderer, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(5, 32, 3),
            torch.rand(5, 32, 1),
            torch.rand(5, 32, 3),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_renderer = volume_renderer.IrregularRenderer(shift=1)
        transpiled_renderer = TranspiledIrregularRenderer(shift=1)
    
        torch_out = torch_renderer(*torch_args)
>       transpiled_out = transpiled_renderer(*transpiled_args)

kornia/test_nerf.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer()
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.25694865, 0.3312056 , 0.2909763 ],
        [0.60641694...9236e-01, 5.77796102e-02, 9.60468590e-01],
        [5.89281082e-01, 5.68811297e-01, 2.65814662e-02]]], dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7fe62a8c69f0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_IrregularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.25694865, 0.3312056 , ...9236e-01, 5.77796102e-02, 9.60468590e-01],
        [5.89281082e-01, 5.68811297e-01, 2.65814662e-02]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.25694865, 0.3312056 , 0.2909763 ],
        [0.60641694...9236e-01, 5.77796102e-02, 9.60468590e-01],
        [5.89281082e-01, 5.68811297e-01, 2.65814662e-02]]], dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_IrregularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.25694865, 0.3312056 , ...9236e-01, 5.77796102e-02, 9.60468590e-01],
        [5.89281082e-01, 5.68811297e-01, 2.65814662e-02]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.25694865, 0.3312056 , 0.2909763 ],
        [0.60641694...9236e-01, 5.77796102e-02, 9.60468590e-01],
        [5.89281082e-01, 5.68811297e-01, 2.65814662e-02]]], dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.25694865, 0.3312056 , 0.2909763 ],
        [0.60641694,...711609 ],
        [0.18313968, 0.23497307, 0.59894544],
        [0.8494517 , 0.46341133, 0.647323  ]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (rgbs, densities, points_3d)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_IrregularRenderer(),)
kwargs = {'densities': <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[0.3264414 ],
        [0.3377455 ],
        ...11609 ],
        [0.18313968, 0.23497307, 0.59894544],
        [0.8494517 , 0.46341133, 0.647323  ]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer()
rgbs = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.25694865, 0.3312056 , 0.2909763 ],
        [0.60641694,...711609 ],
        [0.18313968, 0.23497307, 0.59894544],
        [0.8494517 , 0.46341133, 0.647323  ]]], dtype=float32)>
densities = <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[0.3264414 ],
        [0.3377455 ],
        [0.8896066 ],
...[0.88628185],
        [0.582233  ],
        [0.82348454],
        [0.08796358],
        [0.05141932]]], dtype=float32)>
points_3d = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[9.35559809e-01, 2.50467896e-01, 4.21657920e-01],
        ...69236e-01, 5.77796102e-02, 9.60468590e-01],
        [5.89281082e-01, 5.68811297e-01, 2.65814662e-02]]], dtype=float32)>

    def call(self, rgbs, densities, points_3d):
        from .samplers import tensorflow_calc_ray_t_vals
        from ...ivy.functional.frontends.torch.tensor import tensorflow_fill__frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_empty_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_exp_frnt
    
>       t_vals = tensorflow_calc_ray_t_vals(points_3d)

Translated_Outputs/tensorflow_outputs/kornia/nerf/volume_renderer.py:443: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

points_3d = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[9.35559809e-01, 2.50467896e-01, 4.21657920e-01],
        ...69236e-01, 5.77796102e-02, 9.60468590e-01],
        [5.89281082e-01, 5.68811297e-01, 2.65814662e-02]]], dtype=float32)>

    def tensorflow_calc_ray_t_vals(points_3d):
        from ...ivy.functional.frontends.torch.linalg import tensorflow_norm_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
    
>       t_vals = tensorflow_norm_frnt(
            points_3d - tensorflow_unsqueeze_frnt_(points_3d[..., 0, :], -2), dim=-1
        )

Translated_Outputs/tensorflow_outputs/kornia/nerf/samplers.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[ 0.        ,  0.        ,  0.        ],
        [ 0.06174...8],
        [-0.63161904, -0.5573975 ,  0.57567513],
        [-0.3781072 , -0.0463658 , -0.358212  ]]], dtype=float32)>
ord = None, dim = -1, keepdim = False

    def tensorflow_norm_frnt(
        input, ord=None, dim=None, keepdim=False, *, dtype=None, out=None
    ):
        from .tensor import tensorflow_ndim_frnt_
        from ...backends.tensorflow.linear_algebra import tensorflow_vector_norm
        from ...backends.tensorflow.linear_algebra import tensorflow_matrix_norm
        from ...backends.tensorflow.manipulation import tensorflow_flatten
        from ...backends.tensorflow.data_type import tensorflow_astype
    
        if dim is None and ord is not None:
            if tensorflow_ndim_frnt_(input) == 1:
                ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)
            else:
                ret = tensorflow_matrix_norm(input, keepdims=keepdim, ord=ord)
        elif dim is None and ord is None:
            input = tensorflow_flatten(input)
            ret = tensorflow_vector_norm(input, axis=0, keepdims=keepdim, ord=2)
        elif isinstance(dim, (int,)):
>           ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/linalg.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[ 0.        ,  0.        ,  0.        ],
        [ 0.0617...],
        [-0.63161904, -0.5573975 ,  0.57567513],
        [-0.3781072 , -0.0463658 , -0.358212  ]]], dtype=float32)>]
kwargs = {'axis': -1, 'keepdims': False, 'ord': None}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fe628fc6ef0>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7fe628fc75b0>, tensorflow_asarray = <function tensorflow_asarray at 0x7fe628fd65f0>
tensorflow_get_item = <function tensorflow_get_item at 0x7fe628fc7eb0>, num_args = 1
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'axis', 'keepdims', 'ord', 'dtype', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[int, ...es.DType], typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 0

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[ 0.        ,  0.        ,  0.        ],
        [ 0.06174...8],
        [-0.63161904, -0.5573975 ,  0.57567513],
        [-0.3781072 , -0.0463658 , -0.358212  ]]], dtype=float32)>

    @tensorflow_handle_array_like_without_promotion
    def tensorflow_vector_norm(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        axis: Optional[Union[int, Sequence[int]]] = None,
        keepdims: bool = False,
        ord: Union[int, float, Literal[inf, -inf]] = 2,
        dtype: Optional[tf.DType] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.constants import inf
    
        if dtype and x.dtype != dtype:
            x = tensorflow.cast(x, dtype)
        abs_x = tensorflow.abs(x)
        if ord == 0:
            return tensorflow.reduce_sum(
                tensorflow.cast(x != 0, abs_x.dtype), axis=axis, keepdims=keepdims
            )
        elif ord == inf:
            return tensorflow.reduce_max(abs_x, axis=axis, keepdims=keepdims)
        elif ord == -inf:
            return tensorflow.reduce_min(abs_x, axis=axis, keepdims=keepdims)
        else:
>           return tensorflow.reduce_sum(abs_x**ord, axis=axis, keepdims=keepdims) ** (
                1.0 / ord
            )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/linear_algebra.py:200: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.        , 0.        , 0.        ],
        [0.06174141...],
        [0.63161904, 0.5573975 , 0.57567513],
        [0.3781072 , 0.0463658 , 0.358212  ]]], dtype=float32)>, None)
kwargs = {}, arg = None

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       ValueError: Exception encountered when calling tensorflow_IrregularRenderer.call().
E       
E       [1mAttempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.[0m
E       
E       Arguments received by tensorflow_IrregularRenderer.call():
E         â€¢ rgbs=tf.Tensor(shape=(5, 32, 3), dtype=float32)
E         â€¢ densities=tf.Tensor(shape=(5, 32, 1), dtype=float32)
E         â€¢ points_3d=tf.Tensor(shape=(5, 32, 3), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.volume_renderer.IrregularRenderer
______________________________________________________________________________ test_RegularRenderer[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RegularRenderer(target_framework, mode, backend_compile):
        print("kornia.nerf.volume_renderer.RegularRenderer")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledRegularRenderer = ivy.transpile(volume_renderer.RegularRenderer, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(5, 32, 3),
            torch.rand(5, 32, 1),
            torch.rand(5, 32, 3),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_renderer = volume_renderer.RegularRenderer(shift=1)
        transpiled_renderer = TranspiledRegularRenderer(shift=1)
    
        torch_out = torch_renderer(*torch_args)
>       transpiled_out = transpiled_renderer(*transpiled_args)

kornia/test_nerf.py:179: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer()
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9191566 , 0.3535124 , 0.18022013],
        [0.59526986...4395e-01, 1.67984545e-01, 9.61483717e-01],
        [9.06689048e-01, 4.17925358e-01, 5.80098510e-01]]], dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x56201fb91080, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RegularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9191566 , 0.3535124 , 0....4395e-01, 1.67984545e-01, 9.61483717e-01],
        [9.06689048e-01, 4.17925358e-01, 5.80098510e-01]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9191566 , 0.3535124 , 0.18022013],
        [0.59526986...4395e-01, 1.67984545e-01, 9.61483717e-01],
        [9.06689048e-01, 4.17925358e-01, 5.80098510e-01]]], dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RegularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9191566 , 0.3535124 , 0....4395e-01, 1.67984545e-01, 9.61483717e-01],
        [9.06689048e-01, 4.17925358e-01, 5.80098510e-01]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9191566 , 0.3535124 , 0.18022013],
        [0.59526986...4395e-01, 1.67984545e-01, 9.61483717e-01],
        [9.06689048e-01, 4.17925358e-01, 5.80098510e-01]]], dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9191566 , 0.3535124 , 0.18022013],
        [0.59526986,...1230316],
        [0.09804356, 0.59145945, 0.3762328 ],
        [0.13163841, 0.2625916 , 0.30211627]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (rgbs, densities, points_3d)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RegularRenderer(),)
kwargs = {'densities': <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[0.5198681 ],
        [0.95575553],
        ...230316],
        [0.09804356, 0.59145945, 0.3762328 ],
        [0.13163841, 0.2625916 , 0.30211627]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer()
rgbs = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9191566 , 0.3535124 , 0.18022013],
        [0.59526986,...1230316],
        [0.09804356, 0.59145945, 0.3762328 ],
        [0.13163841, 0.2625916 , 0.30211627]]], dtype=float32)>
densities = <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[0.5198681 ],
        [0.95575553],
        [0.9838201 ],
...[0.5736588 ],
        [0.6051605 ],
        [0.71908104],
        [0.7588723 ],
        [0.91822475]]], dtype=float32)>
points_3d = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[1.49414718e-01, 4.99727130e-02, 5.85150719e-02],
        ...24395e-01, 1.67984545e-01, 9.61483717e-01],
        [9.06689048e-01, 4.17925358e-01, 5.80098510e-01]]], dtype=float32)>

    def call(self, rgbs, densities, points_3d):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.linalg import tensorflow_norm_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_exp_frnt
    
        tensorflow_KORNIA_CHECK_SHAPE(rgbs, ["*", "N", "3"])
        tensorflow_KORNIA_CHECK_SHAPE(densities, ["*", "N"])
        tensorflow_KORNIA_CHECK_SHAPE(points_3d, ["*", "N", "3"])
        num_ray_points: typing.Any = tensorflow_shape_frnt_(points_3d)[-2]
        points_3d = tensorflow_reshape_frnt_(points_3d, -1, num_ray_points, 3)
        delta_3d = points_3d[0, 1, :] - points_3d[0, 0, :]
>       delta = tensorflow_norm_frnt(delta_3d, dim=-1)

Translated_Outputs/tensorflow_outputs/kornia/nerf/volume_renderer.py:472: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.3908943, -0.0024966,  0.8964195], dtype=float32)>, ord = None, dim = -1, keepdim = False

    def tensorflow_norm_frnt(
        input, ord=None, dim=None, keepdim=False, *, dtype=None, out=None
    ):
        from .tensor import tensorflow_ndim_frnt_
        from ...backends.tensorflow.linear_algebra import tensorflow_vector_norm
        from ...backends.tensorflow.linear_algebra import tensorflow_matrix_norm
        from ...backends.tensorflow.manipulation import tensorflow_flatten
        from ...backends.tensorflow.data_type import tensorflow_astype
    
        if dim is None and ord is not None:
            if tensorflow_ndim_frnt_(input) == 1:
                ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)
            else:
                ret = tensorflow_matrix_norm(input, keepdims=keepdim, ord=ord)
        elif dim is None and ord is None:
            input = tensorflow_flatten(input)
            ret = tensorflow_vector_norm(input, axis=0, keepdims=keepdim, ord=2)
        elif isinstance(dim, (int,)):
>           ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/linalg.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.3908943, -0.0024966,  0.8964195], dtype=float32)>], kwargs = {'axis': -1, 'keepdims': False, 'ord': None}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fe628fc6ef0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7fe628fc75b0>
tensorflow_asarray = <function tensorflow_asarray at 0x7fe628fd65f0>, tensorflow_get_item = <function tensorflow_get_item at 0x7fe628fc7eb0>, num_args = 1
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'axis', 'keepdims', 'ord', 'dtype', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[int, ...es.DType], typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 0

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.3908943, -0.0024966,  0.8964195], dtype=float32)>

    @tensorflow_handle_array_like_without_promotion
    def tensorflow_vector_norm(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        axis: Optional[Union[int, Sequence[int]]] = None,
        keepdims: bool = False,
        ord: Union[int, float, Literal[inf, -inf]] = 2,
        dtype: Optional[tf.DType] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.constants import inf
    
        if dtype and x.dtype != dtype:
            x = tensorflow.cast(x, dtype)
        abs_x = tensorflow.abs(x)
        if ord == 0:
            return tensorflow.reduce_sum(
                tensorflow.cast(x != 0, abs_x.dtype), axis=axis, keepdims=keepdims
            )
        elif ord == inf:
            return tensorflow.reduce_max(abs_x, axis=axis, keepdims=keepdims)
        elif ord == -inf:
            return tensorflow.reduce_min(abs_x, axis=axis, keepdims=keepdims)
        else:
>           return tensorflow.reduce_sum(abs_x**ord, axis=axis, keepdims=keepdims) ** (
                1.0 / ord
            )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/linear_algebra.py:200: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.3908943, 0.0024966, 0.8964195], dtype=float32)>, None), kwargs = {}, arg = None

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       ValueError: Exception encountered when calling tensorflow_RegularRenderer.call().
E       
E       [1mAttempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.[0m
E       
E       Arguments received by tensorflow_RegularRenderer.call():
E         â€¢ rgbs=tf.Tensor(shape=(5, 32, 3), dtype=float32)
E         â€¢ densities=tf.Tensor(shape=(5, 32, 1), dtype=float32)
E         â€¢ points_3d=tf.Tensor(shape=(5, 32, 3), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.volume_renderer.RegularRenderer
________________________________________________________________________________ test_RaySampler[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RaySampler(target_framework, mode, backend_compile):
        print("kornia.nerf.samplers.RaySampler")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)

kornia/test_nerf.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.camera.pinhole.PinholeCamera'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.samplers.RaySampler
_____________________________________________________________________________ test_RandomRaySampler[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRaySampler(target_framework, mode, backend_compile):
        print("kornia.nerf.samplers.RandomRaySampler")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)

kornia/test_nerf.py:224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.camera.pinhole.PinholeCamera'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.samplers.RandomRaySampler
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_nerf.py::test_NerfModel[tensorflow-s2s-False] - AttributeError: can't set attribute
FAILED kornia/test_nerf.py::test_NerfModelRenderer[tensorflow-s2s-False] - AttributeError: can't set attribute
FAILED kornia/test_nerf.py::test_NerfSolver[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor'...
FAILED kornia/test_nerf.py::test_IrregularRenderer[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_IrregularRenderer.call().
FAILED kornia/test_nerf.py::test_RegularRenderer[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_RegularRenderer.call().
FAILED kornia/test_nerf.py::test_RaySampler[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor'...
FAILED kornia/test_nerf.py::test_RandomRaySampler[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'T...
=============================================================================== 7 failed, 1 passed in 255.09s (0:04:15) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 6 items

kornia/augmentation/test_container.py FFFFFF                                                                                                                                                     [100%]

=============================================================================================== FAILURES ===============================================================================================
__________________________________________________________________________ test_AugmentationSequential[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_AugmentationSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.AugmentationSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledAugmentationSequential = ivy.transpile(
            kornia.augmentation.container.AugmentationSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.augment.AugmentationSequential'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.AugmentationSequential
______________________________________________________________________ test_ManyToManyAugmentationDispather[tensorflow-s2s-False] ______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ManyToManyAugmentationDispather(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.ManyToManyAugmentationDispather")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledManyToManyAugmentationDispather = ivy.transpile(
            kornia.augmentation.container.ManyToManyAugmentationDispather,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:81: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.dispatcher.ManyToManyAugmentationDispather'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.ManyToManyAugmentationDispather
______________________________________________________________________ test_ManyToOneAugmentationDispather[tensorflow-s2s-False] _______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ManyToOneAugmentationDispather(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.ManyToOneAugmentationDispather")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledManyToOneAugmentationDispather = ivy.transpile(
            kornia.augmentation.container.ManyToOneAugmentationDispather,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.dispatcher.ManyToOneAugmentationDispather'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.ManyToOneAugmentationDispather
______________________________________________________________________________ test_ImageSequential[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ImageSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.ImageSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledImageSequential = ivy.transpile(
            kornia.augmentation.container.ImageSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.image.ImageSequential'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.ImageSequential
______________________________________________________________________________ test_PatchSequential[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_PatchSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.PatchSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledPatchSequential = ivy.transpile(
            kornia.augmentation.container.PatchSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.patch.PatchSequential'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.PatchSequential
______________________________________________________________________________ test_VideoSequential[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_VideoSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.VideoSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledVideoSequential = ivy.transpile(
            kornia.augmentation.container.VideoSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:376: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.video.VideoSequential'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.VideoSequential
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/augmentation/test_container.py::test_AugmentationSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augme...
FAILED kornia/augmentation/test_container.py::test_ManyToManyAugmentationDispather[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kor...
FAILED kornia/augmentation/test_container.py::test_ManyToOneAugmentationDispather[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.korn...
FAILED kornia/augmentation/test_container.py::test_ImageSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation...
FAILED kornia/augmentation/test_container.py::test_PatchSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation...
FAILED kornia/augmentation/test_container.py::test_VideoSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation...
==================================================================================== 6 failed in 324.30s (0:05:24) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/test_io.py ..                                                                                                                                                                             [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
===================================================================================== 2 passed in 91.02s (0:01:31) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 5 items

kornia/geometry/test_calibration.py ....F                                                                                                                                                        [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________________ test_solve_pnp_dlt[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_solve_pnp_dlt(target_framework, mode, backend_compile):
        trace_args = (
            torch.tensor([[
                [5.0, -5.0, 0.0], [0.0, 0.0, 1.5],
                [2.5, 3.0, 6.0], [9.0, -2.0, 3.0],
                [-4.0, 5.0, 2.0], [-5.0, 5.0, 1.0]
            ]], dtype=torch.float64),
            torch.tensor([[
                [1409.1504, -800.936], [407.0207, -182.1229],
                [392.7021, 177.9428], [1016.838, -2.9416],
                [-63.1116, 142.9204], [-219.3874, 99.666]
            ]], dtype=torch.float64),
            torch.tensor([[
                [500.0, 0.0, 250.0],
                [0.0, 500.0, 250.0],
                [0.0, 0.0, 1.0]
            ]], dtype=torch.float64),
        )
        trace_kwargs = {'svd_eps': 1e-3}
        test_args = (
            torch.tensor([[
                [10.0, -10.0, 0.0], [0.0, 0.0, 3.0],
                [5.0, 6.0, 12.0], [18.0, -4.0, 6.0],
                [-8.0, 10.0, 4.0], [-10.0, 10.0, 2.0]
            ]], dtype=torch.float64),
            torch.tensor([[
                [2818.3008, -1601.872], [814.0414, -364.2458],
                [785.4042, 355.8856], [2033.676, -5.8832],
                [-126.2232, 285.8408], [-438.7748, 199.332]
            ]], dtype=torch.float64),
            torch.tensor([[
                [1000.0, 0.0, 500.0],
                [0.0, 1000.0, 500.0],
                [0.0, 0.0, 1.0]
            ]], dtype=torch.float64),
        )
        test_kwargs = {'svd_eps': 1e-3}
>       _test_function(
            kornia.geometry.calibration.solve_pnp_dlt,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_calibration.py:148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function solve_pnp_dlt at 0x7fb1711ae8c0>
trace_args = (tensor([[[ 5.0000, -5.0000,  0.0000],
         [ 0.0000,  0.0000,  1.5000],
         [ 2.5000,  3.0000,  6.0000],
   ...loat64), tensor([[[500.,   0., 250.],
         [  0., 500., 250.],
         [  0.,   0.,   1.]]], dtype=torch.float64))
trace_kwargs = {'svd_eps': 0.001}
test_args = (tensor([[[ 10., -10.,   0.],
         [  0.,   0.,   3.],
         [  5.,   6.,  12.],
         [ 18.,  -4.,   6.],
 ...tensor([[[1000.,    0.,  500.],
         [   0., 1000.,  500.],
         [   0.,    0.,    1.]]], dtype=torch.float64))
test_kwargs = {'svd_eps': 0.001}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function solve_pnp_dlt at 0x7fb1711ae8c0>
trace_args = (tensor([[[ 5.0000, -5.0000,  0.0000],
         [ 0.0000,  0.0000,  1.5000],
         [ 2.5000,  3.0000,  6.0000],
   ...loat64), tensor([[[500.,   0., 250.],
         [  0., 500., 250.],
         [  0.,   0.,   1.]]], dtype=torch.float64))
trace_kwargs = {'svd_eps': 0.001}
test_args = (tensor([[[ 10., -10.,   0.],
         [  0.,   0.,   3.],
         [  5.,   6.,  12.],
         [ 18.,  -4.,   6.],
 ...tensor([[[1000.,    0.,  500.],
         [   0., 1000.,  500.],
         [   0.,    0.,    1.]]], dtype=torch.float64))
test_kwargs = {'svd_eps': 0.001}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

world_points = <tf.Tensor: shape=(1, 6, 3), dtype=float64, numpy=
array([[[ 5. , -5. ,  0. ],
        [ 0. ,  0. ,  1.5],
        [ 2.5,  3. ,  6. ],
        [ 9. , -2. ,  3. ],
        [-4. ,  5. ,  2. ],
        [-5. ,  5. ,  1. ]]])>
img_points = <tf.Tensor: shape=(1, 6, 2), dtype=float64, numpy=
array([[[1409.1504, -800.936 ],
        [ 407.0207, -182.1229],
   ...92.7021,  177.9428],
        [1016.838 ,   -2.9416],
        [ -63.1116,  142.9204],
        [-219.3874,   99.666 ]]])>
intrinsics = <tf.Tensor: shape=(1, 3, 3), dtype=float64, numpy=
array([[[500.,   0., 250.],
        [  0., 500., 250.],
        [  0.,   0.,   1.]]])>, weights = None, svd_eps = 0.001

    def tensorflow_solve_pnp_dlt(
        world_points, img_points, intrinsics, weights=None, svd_eps=0.0001
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...utils.helpers import tensorflow__torch_linalg_svdvals
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_any_frnt
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_inverse_frnt,
        )
        from ..conversions import tensorflow_convert_points_to_homogeneous
        from ..linalg import tensorflow_transform_points
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_svd_frnt_base_count_1_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...utils.misc import tensorflow_eye_like
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_bmm_frnt,
        )
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_det_frnt,
        )
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_norm_frnt
        from ....ivy.functional.frontends.torch.linalg import tensorflow_qr_frnt
        from ....ivy.functional.frontends.torch.pointwise_ops import tensorflow_sign_frnt
        from ....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...core._backend import zeros
        from ...core._backend import ones_like
        from ...core._backend import where
    
        if not isinstance(world_points, (tensorflow.Tensor, tensorflow.Variable)):
            raise AssertionError(
                f"world_points is not an instance of torch.Tensor. Type of world_points is {type(world_points)}"
            )
        if not isinstance(img_points, (tensorflow.Tensor, tensorflow.Variable)):
            raise AssertionError(
                f"img_points is not an instance of torch.Tensor. Type of img_points is {type(img_points)}"
            )
        if not isinstance(intrinsics, (tensorflow.Tensor, tensorflow.Variable)):
            raise AssertionError(
                f"intrinsics is not an instance of torch.Tensor. Type of intrinsics is {type(intrinsics)}"
            )
        if weights is not None and not isinstance(
            weights, (tensorflow.Tensor, tensorflow.Variable)
        ):
            raise AssertionError(
                f"If weights is not None, then weights should be an instance of torch.Tensor. Type of weights is {type(weights)}"
            )
        if not isinstance(svd_eps, (float,)):
            raise AssertionError(f"Type of svd_eps is not float. Got {type(svd_eps)}")
        accepted_dtypes = tf.float32, tf.float64
        if world_points.dtype not in accepted_dtypes:
            raise AssertionError(
                f"world_points must have one of the following dtypes {accepted_dtypes}. Currently it has {world_points.dtype}."
            )
        if img_points.dtype not in accepted_dtypes:
            raise AssertionError(
                f"img_points must have one of the following dtypes {accepted_dtypes}. Currently it has {img_points.dtype}."
            )
        if intrinsics.dtype not in accepted_dtypes:
            raise AssertionError(
                f"intrinsics must have one of the following dtypes {accepted_dtypes}. Currently it has {intrinsics.dtype}."
            )
        if (
            len(tensorflow_shape_frnt_(world_points)) != 3
            or tensorflow_shape_frnt_(world_points)[2] != 3
        ):
            raise AssertionError(
                f"world_points must be of shape (B, N, 3). Got shape {tensorflow_shape_frnt_(world_points)}."
            )
        if (
            len(tensorflow_shape_frnt_(img_points)) != 3
            or tensorflow_shape_frnt_(img_points)[2] != 2
        ):
            raise AssertionError(
                f"img_points must be of shape (B, N, 2). Got shape {tensorflow_shape_frnt_(img_points)}."
            )
        if len(tensorflow_shape_frnt_(intrinsics)) != 3 or tensorflow_shape_frnt_(
            intrinsics
        )[1:] != (3, 3):
            raise AssertionError(
                f"intrinsics must be of shape (B, 3, 3). Got shape {tensorflow_shape_frnt_(intrinsics)}."
            )
        if tensorflow_shape_frnt_(world_points)[1] != tensorflow_shape_frnt_(img_points)[1]:
            raise AssertionError(
                "world_points and img_points must have equal number of points."
            )
        if (
            tensorflow_shape_frnt_(world_points)[0] != tensorflow_shape_frnt_(img_points)[0]
            or tensorflow_shape_frnt_(world_points)[0]
            != tensorflow_shape_frnt_(intrinsics)[0]
        ):
            raise AssertionError(
                "world_points, img_points and intrinsics must have the same batch size."
            )
        if tensorflow_shape_frnt_(world_points)[1] < 6:
            raise AssertionError(
                f"At least 6 points are required to use this function. Got {tensorflow_shape_frnt_(world_points)[1]} points."
            )
        B, N = (
            tensorflow_shape_frnt_(world_points)[:2][0],
            tensorflow_shape_frnt_(world_points)[:2][1],
        )
        world_points_norm, world_transform_norm = (
            tensorflow__mean_isotropic_scale_normalize(world_points)
        )
        s = tensorflow__torch_linalg_svdvals(world_points_norm)
        if tensorflow_any_frnt(s[:, -1] < svd_eps):
            raise AssertionError(
                f"The last singular value of one/more of the elements of the batch is smaller than {svd_eps}. This function cannot be used if all world_points (of any element of the batch) lie on a line or if all world_points (of any element of the batch) lie on a plane."
            )
        intrinsics_inv = tensorflow_inverse_frnt(intrinsics)
        world_points_norm_h = tensorflow_convert_points_to_homogeneous(world_points_norm)
        img_points_inv = tensorflow_transform_points(intrinsics_inv, img_points)
        img_points_norm, img_transform_norm = tensorflow__mean_isotropic_scale_normalize(
            img_points_inv
        )
        inv_img_transform_norm = tensorflow_inverse_frnt(img_transform_norm)
        system = zeros((B, 2 * N, 12), dtype=world_points.dtype, device=world_points.device)
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(0, None, 2), slice(0, 4, None)),
            world_points_norm_h,
        )
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(1, None, 2), slice(4, 8, None)),
            world_points_norm_h,
        )
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(0, None, 2), slice(8, 12, None)),
            world_points_norm_h * -1 * img_points_norm[..., 0:1],
        )
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(1, None, 2), slice(8, 12, None)),
            world_points_norm_h * -1 * img_points_norm[..., 1:2],
        )
        _, _, v = tensorflow_svd_frnt_base_count_1_frnt(system)
        solution = v[..., -1]
        solution = tensorflow_reshape_frnt_(solution, B, 3, 4)
        solution_4x4 = tensorflow_eye_like(4, solution)
        solution_4x4 = tensorflow_set_item_bknd(
            solution_4x4,
            (slice(None, None, None), slice(None, 3, None), slice(None, None, None)),
            solution,
        )
        intermediate = tensorflow_bmm_frnt(solution_4x4, world_transform_norm)
        solution = tensorflow_bmm_frnt(inv_img_transform_norm, intermediate[:, :3, :])
        det = tensorflow_det_frnt(solution[:, :3, :3])
        ones = ones_like(det)
        sign_fix = where(det < 0, ones * -1, ones)
        solution = solution * sign_fix[:, None, None]
>       norm_col = tensorflow_norm_frnt(input=solution[:, :3, 0], p=2, dim=1)

Translated_Outputs/tensorflow_outputs/kornia/geometry/calibration/pnp.py:237: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (), kwargs = {'dim': 1, 'input': <tf.Tensor: shape=(1, 3), dtype=float64, numpy=array([[-0.02017233,  0.02388227,  0.05749283]])>, 'p': 2}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fb169589750>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
>       array_like = args[0]
E       IndexError: tuple index out of range

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:44: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.calibration.pnp.solve_pnp_dlt
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_calibration.py::test_solve_pnp_dlt[tensorflow-s2s-False] - IndexError: tuple index out of range
=============================================================================== 1 failed, 4 passed in 174.32s (0:02:54) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 15 items

kornia/geometry/test_subpix.py ..F.........F..                                                                                                                                                   [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________ test_conv_quad_interp3d[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_conv_quad_interp3d(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(20, 16, 3, 50, 32),
        )
        trace_kwargs = {
            'strict_maxima_bonus': 10.0,
            'eps': 1e-7,
        }
        test_args = (
            torch.rand(10, 16, 5, 50, 32),
        )
        test_kwargs = {
            'strict_maxima_bonus': 5.0,
            'eps': 1e-7,
        }
>       _test_function(
            kornia.geometry.subpix.conv_quad_interp3d,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_subpix.py:109: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function conv_quad_interp3d at 0x7f2e29148b80>
trace_args = (tensor([[[[[2.1344e-01, 2.4266e-01, 8.9304e-01,  ..., 7.6393e-01,
            2.4135e-01, 2.6333e-01],
           [8....5744e-01],
           [5.7684e-01, 1.1449e-01, 7.7635e-01,  ..., 9.7283e-01,
            6.3385e-01, 8.1853e-01]]]]]),)
trace_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 10.0}
test_args = (tensor([[[[[3.7661e-01, 8.5611e-01, 4.5557e-01,  ..., 2.5623e-02,
            1.1773e-01, 6.8805e-01],
           [3....5034e-01],
           [9.1190e-02, 5.9425e-01, 2.4594e-01,  ..., 9.4183e-01,
            5.7162e-01, 3.2857e-01]]]]]),)
test_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 5.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function conv_quad_interp3d at 0x7f2e29148b80>
trace_args = (tensor([[[[[2.1344e-01, 2.4266e-01, 8.9304e-01,  ..., 7.6393e-01,
            2.4135e-01, 2.6333e-01],
           [8....5744e-01],
           [5.7684e-01, 1.1449e-01, 7.7635e-01,  ..., 9.7283e-01,
            6.3385e-01, 8.1853e-01]]]]]),)
trace_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 10.0}
test_args = (tensor([[[[[3.7661e-01, 8.5611e-01, 4.5557e-01,  ..., 2.5623e-02,
            1.1773e-01, 6.8805e-01],
           [3....5034e-01],
           [9.1190e-02, 5.9425e-01, 2.4594e-01,  ..., 9.4183e-01,
            5.7162e-01, 3.2857e-01]]]]]),)
test_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 5.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(20, 16, 3, 50, 32), dtype=float32, numpy=
array([[[[[2.13435233e-01, 2.42664039e-01, 8.93042088e-01...4491045e-01, 7.76345670e-01, ...,
           9.72833991e-01, 6.33847177e-01, 8.18525136e-01]]]]],
      dtype=float32)>
strict_maxima_bonus = 10.0, eps = 1e-07

    def tensorflow_conv_quad_interp3d(input, strict_maxima_bonus=10.0, eps=1e-07):
        from ....ivy.functional.frontends.torch.tensor_functions import (
            tensorflow_is_tensor_frnt_,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...utils.grid import tensorflow_create_meshgrid3d
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...filters.sobel import tensorflow_sobel
        from ....ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...utils._compat import tensorflow_torch_version_ge
        from ....ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from .nms import tensorflow_nms3d
        from ...utils.helpers import tensorflow_safe_solve_with_mask
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import (
            tensorflow_masked_scatter_frnt_,
        )
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_masked_fill__frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_expand_as_frnt_
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_bmm_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...core._backend import stack
        from ...core._backend import rand
        from ...core._backend import zeros_like
        from ...core._backend import where
    
        if not tensorflow_is_tensor_frnt_(input):
            raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
        if not len(tensorflow_shape_frnt_(input)) == 5:
            raise ValueError(
                f"Invalid input shape, we expect BxCxDxHxW. Got: {tensorflow_shape_frnt_(input)}"
            )
        B, CH, D, H, W = tensorflow_shape_frnt_(input)
        grid_global: typing.Any = tensorflow_permute_frnt_(
            tensorflow_create_meshgrid3d(D, H, W, False, device=input.device), 0, 4, 1, 2, 3
        )
        grid_global = tensorflow_to_frnt_(grid_global, input.dtype)
>       b: typing.Any = tensorflow_sobel.spatial_gradient3d(input, order=1, mode="diff")
E       AttributeError: 'function' object has no attribute 'spatial_gradient3d'

Translated_Outputs/tensorflow_outputs/kornia/geometry/subpix/spatial_soft_argmax.py:434: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.subpix.spatial_soft_argmax.conv_quad_interp3d
_____________________________________________________________________________ test_ConvQuadInterp3d[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ConvQuadInterp3d(target_framework, mode, backend_compile):
        print("kornia.geometry.subpix.ConvQuadInterp3d")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledConvQuadInterp3d = ivy.transpile(
            kornia.geometry.subpix.ConvQuadInterp3d, source="torch", target=target_framework
        )
    
        conv_quad_interp3d = kornia.geometry.subpix.ConvQuadInterp3d()
        transpiled_conv_quad_interp3d = TranspiledConvQuadInterp3d()
    
        heatmap = torch.randn(1, 1, 3, 5, 5, requires_grad=True)
        transpiled_heatmap = _nest_torch_tensor_to_new_framework(heatmap, target_framework)
    
        torch_output = conv_quad_interp3d(heatmap)
>       transpiled_output = transpiled_conv_quad_interp3d(transpiled_heatmap)

kornia/geometry/test_subpix.py:371: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0)
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[ 0.2946328 , -0.46178487,  0.5047988 , -0.2983038...0613034],
          [-1.505291  ,  1.3744872 ,  0.20118436, -1.027042  ,
            1.5752511 ]]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f2e2188a650, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array...50613034],
          [-1.505291  ,  1.3744872 ,  0.20118436, -1.027042  ,
            1.5752511 ]]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[ 0.2946328 , -0.46178487,  0.5047988 , -0.2983038...0613034],
          [-1.505291  ,  1.3744872 ,  0.20118436, -1.027042  ,
            1.5752511 ]]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array...50613034],
          [-1.505291  ,  1.3744872 ,  0.20118436, -1.027042  ,
            1.5752511 ]]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[ 0.2946328 , -0.46178487,  0.5047988 , -0.2983038...0613034],
          [-1.505291  ,  1.3744872 ,  0.20118436, -1.027042  ,
            1.5752511 ]]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[ 0.2946328 , -0.46178487,  0.5047988 , -0.29830384....50613034],
          [-1.505291  ,  1.3744872 ,  0.20118436, -1.027042  ,
            1.5752511 ]]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0),)
kwargs = {'x': <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[ 0.2946328 , -0.46178487,  0.5047988 , -0.29...50613034],
          [-1.505291  ,  1.3744872 ,  0.20118436, -1.027042  ,
            1.5752511 ]]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0)
x = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[ 0.2946328 , -0.46178487,  0.5047988 , -0.29830384....50613034],
          [-1.505291  ,  1.3744872 ,  0.20118436, -1.027042  ,
            1.5752511 ]]]]], dtype=float32)>

    def call(self, x):
>       return tensorflow_conv_quad_interp3d(x, self.strict_maxima_bonus, self.eps)

Translated_Outputs/tensorflow_outputs/kornia/geometry/subpix/spatial_soft_argmax.py:1667: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[ 0.2946328 , -0.46178487,  0.5047988 , -0.29830384....50613034],
          [-1.505291  ,  1.3744872 ,  0.20118436, -1.027042  ,
            1.5752511 ]]]]], dtype=float32)>
strict_maxima_bonus = 10.0, eps = 1e-07

    def tensorflow_conv_quad_interp3d(input, strict_maxima_bonus=10.0, eps=1e-07):
        from ....ivy.functional.frontends.torch.tensor_functions import (
            tensorflow_is_tensor_frnt_,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...utils.grid import tensorflow_create_meshgrid3d
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...filters.sobel import tensorflow_sobel
        from ....ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...utils._compat import tensorflow_torch_version_ge
        from ....ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from .nms import tensorflow_nms3d
        from ...utils.helpers import tensorflow_safe_solve_with_mask
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import (
            tensorflow_masked_scatter_frnt_,
        )
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_masked_fill__frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_expand_as_frnt_
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_bmm_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...core._backend import stack
        from ...core._backend import rand
        from ...core._backend import zeros_like
        from ...core._backend import where
    
        if not tensorflow_is_tensor_frnt_(input):
            raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
        if not len(tensorflow_shape_frnt_(input)) == 5:
            raise ValueError(
                f"Invalid input shape, we expect BxCxDxHxW. Got: {tensorflow_shape_frnt_(input)}"
            )
        B, CH, D, H, W = tensorflow_shape_frnt_(input)
        grid_global: typing.Any = tensorflow_permute_frnt_(
            tensorflow_create_meshgrid3d(D, H, W, False, device=input.device), 0, 4, 1, 2, 3
        )
        grid_global = tensorflow_to_frnt_(grid_global, input.dtype)
>       b: typing.Any = tensorflow_sobel.spatial_gradient3d(input, order=1, mode="diff")
E       AttributeError: Exception encountered when calling tensorflow_ConvQuadInterp3d.call().
E       
E       [1m'function' object has no attribute 'spatial_gradient3d'[0m
E       
E       Arguments received by tensorflow_ConvQuadInterp3d.call():
E         â€¢ x=tf.Tensor(shape=(1, 1, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/geometry/subpix/spatial_soft_argmax.py:1584: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.subpix.ConvQuadInterp3d
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_subpix.py::test_conv_quad_interp3d[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'spatial_gradient3d'
FAILED kornia/geometry/test_subpix.py::test_ConvQuadInterp3d[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_ConvQuadInterp3d.call().
=============================================================================== 2 failed, 13 passed in 318.79s (0:05:18) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/geometry/test_homography.py .F.....F                                                                                                                                                      [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________ test_find_homography_dlt_iterated[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_find_homography_dlt_iterated(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 4, 2),
            torch.rand(1, 4, 2),
            torch.rand(1, 4),
        )
        trace_kwargs = {'soft_inl_th': 3.0, 'n_iter': 5}
        test_args = (
            torch.rand(5, 4, 2),
            torch.rand(5, 4, 2),
            torch.rand(5, 4),
        )
        test_kwargs = {'soft_inl_th': 4.0, 'n_iter': 5}
>       _test_function(
            kornia.geometry.homography.find_homography_dlt_iterated,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=5e-2,
            mode=mode,
            # NOTE: numerical instability in svd()/lu() leads to logits not being allclose
            deterministic=False,
        )

kornia/geometry/test_homography.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function find_homography_dlt_iterated at 0x7f0e77049090>
trace_args = (tensor([[[0.1932, 0.0160],
         [0.0267, 0.2142],
         [0.3849, 0.2080],
         [0.4105, 0.6990]]]), tensor... [0.0958, 0.4303],
         [0.2350, 0.9944],
         [0.3229, 0.4894]]]), tensor([[0.8325, 0.2338, 0.6232, 0.3090]]))
trace_kwargs = {'n_iter': 5, 'soft_inl_th': 3.0}
test_args = (tensor([[[0.3422, 0.1370],
         [0.2511, 0.2982],
         [0.2252, 0.6060],
         [0.4621, 0.7932]],

       ...[0.5628, 0.6434, 0.7791, 0.5113],
        [0.1677, 0.8040, 0.0437, 0.9708],
        [0.3088, 0.5041, 0.6984, 0.6617]]))
test_kwargs = {'n_iter': 5, 'soft_inl_th': 4.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.05, mode = 's2s', skip = False, deterministic = False

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function find_homography_dlt_iterated at 0x7f0e77049090>
trace_args = (tensor([[[0.1932, 0.0160],
         [0.0267, 0.2142],
         [0.3849, 0.2080],
         [0.4105, 0.6990]]]), tensor... [0.0958, 0.4303],
         [0.2350, 0.9944],
         [0.3229, 0.4894]]]), tensor([[0.8325, 0.2338, 0.6232, 0.3090]]))
trace_kwargs = {'n_iter': 5, 'soft_inl_th': 3.0}
test_args = (tensor([[[0.3422, 0.1370],
         [0.2511, 0.2982],
         [0.2252, 0.6060],
         [0.4621, 0.7932]],

       ...[0.5628, 0.6434, 0.7791, 0.5113],
        [0.1677, 0.8040, 0.0437, 0.9708],
        [0.3088, 0.5041, 0.6984, 0.6617]]))
test_kwargs = {'n_iter': 5, 'soft_inl_th': 4.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.05, deterministic = False

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

points1 = <tf.Tensor: shape=(1, 4, 2), dtype=float32, numpy=
array([[[0.19324225, 0.01601708],
        [0.0267393 , 0.21421713],
        [0.3848673 , 0.20800734],
        [0.41048998, 0.6989988 ]]], dtype=float32)>
points2 = <tf.Tensor: shape=(1, 4, 2), dtype=float32, numpy=
array([[[0.63656944, 0.64560294],
        [0.09582055, 0.4302581 ],
        [0.23500699, 0.99440664],
        [0.32287204, 0.48942083]]], dtype=float32)>
weights = <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.8325267 , 0.23375976, 0.62316865, 0.30896795]], dtype=float32)>, soft_inl_th = 3.0, n_iter = 5

    def tensorflow_find_homography_dlt_iterated(
        points1, points2, weights, soft_inl_th=3.0, n_iter=5
    ):
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_exp_frnt
    
        H: typing.Any = tensorflow_find_homography_dlt(points1, points2, weights)
        for _ in range(n_iter - 1):
            errors: typing.Any = tensorflow_symmetric_transfer_error(
                points1, points2, H, False
            )
            weights_new: typing.Any = tensorflow_exp_frnt(-errors / (2.0 * soft_inl_th**2))
>           H = tensorflow_find_homography_dlt(points1, points2, weights_new)

Translated_Outputs/tensorflow_outputs/kornia/geometry/homography.py:203: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

points1 = <tf.Tensor: shape=(1, 4, 2), dtype=float32, numpy=
array([[[0.19324225, 0.01601708],
        [0.0267393 , 0.21421713],
        [0.3848673 , 0.20800734],
        [0.41048998, 0.6989988 ]]], dtype=float32)>
points2 = <tf.Tensor: shape=(1, 4, 2), dtype=float32, numpy=
array([[[0.63656944, 0.64560294],
        [0.09582055, 0.4302581 ],
        [0.23500699, 0.99440664],
        [0.32287204, 0.48942083]]], dtype=float32)>
weights = <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.99999446, 0.99999446, 0.99999446, 0.99999446]], dtype=float32)>, solver = 'lu'

    def tensorflow_find_homography_dlt(points1, points2, weights=None, solver="lu"):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..utils.helpers import tensorflow__extract_device_dtype
        from .epipolar.fundamental import tensorflow_normalize_points
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_chunk_frnt,
        )
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_ones_like_v_0p4p0_and_above_frnt,
        )
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_zeros_like_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_transpose_frnt_
        from ...ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_diag_embed_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ..utils.helpers import tensorflow__torch_svd_cast
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_empty_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from ..utils.helpers import tensorflow_safe_solve_with_mask
        from ..utils.helpers import tensorflow_safe_inverse_with_mask
    
        if tensorflow_shape_frnt_(points1) != tensorflow_shape_frnt_(points2):
            raise AssertionError(tensorflow_shape_frnt_(points1))
        if tensorflow_shape_frnt_(points1)[1] < 4:
            raise AssertionError(tensorflow_shape_frnt_(points1))
        tensorflow_KORNIA_CHECK_SHAPE(points1, ["B", "N", "2"])
        tensorflow_KORNIA_CHECK_SHAPE(points2, ["B", "N", "2"])
        device, dtype = tensorflow__extract_device_dtype([points1, points2])
        eps: typing.Any = 1e-08
        points1_norm, transform1 = tensorflow_normalize_points(points1)
        points2_norm, transform2 = tensorflow_normalize_points(points2)
        x1, y1 = tensorflow_chunk_frnt(points1_norm, dim=-1, chunks=2)
        x2, y2 = tensorflow_chunk_frnt(points2_norm, dim=-1, chunks=2)
        ones, zeros = (
            tensorflow_ones_like_v_0p4p0_and_above_frnt(x1),
            tensorflow_zeros_like_frnt(x1),
        )
        ax = tensorflow_cat_frnt(
            [zeros, zeros, zeros, -x1, -y1, -ones, y2 * x1, y2 * y1, y2], dim=-1
        )
        ay = tensorflow_cat_frnt(
            [x1, y1, ones, zeros, zeros, zeros, -x2 * x1, -x2 * y1, -x2], dim=-1
        )
        A = tensorflow_reshape_frnt_(
            tensorflow_cat_frnt((ax, ay), dim=-1),
            tensorflow_shape_frnt_(ax)[0],
            -1,
            tensorflow_shape_frnt_(ax)[-1],
        )
        if weights is None:
            A = tensorflow_transpose_frnt_(A, -2, -1) @ A
        else:
            if not (
                len(tensorflow_shape_frnt_(weights)) == 2
                and tensorflow_shape_frnt_(weights) == tensorflow_shape_frnt_(points1)[:2]
            ):
                raise AssertionError(tensorflow_shape_frnt_(weights))
            w_diag = tensorflow_diag_embed_frnt(
                tensorflow_reshape_frnt_(
                    tensorflow_repeat_frnt_(
                        tensorflow_unsqueeze_frnt_(weights, dim=-1), 1, 1, 2
                    ),
                    tensorflow_shape_frnt_(weights)[0],
                    -1,
                )
            )
            A = tensorflow_transpose_frnt_(A, -2, -1) @ w_diag @ A
        if solver == "svd":
            try:
                _, _, V = tensorflow__torch_svd_cast(A)
            except RuntimeError:
                warnings.warn("SVD did not converge", RuntimeWarning)
                return tensorflow_empty_frnt(
                    (tensorflow_size_frnt_(points1_norm, 0), 3, 3),
                    device=device,
                    dtype=dtype,
                )
            H = tensorflow_view_frnt_(V[..., -1], -1, 3, 3)
        elif solver == "lu":
            B = tensorflow_ones_frnt(
                tensorflow_shape_frnt_(A)[0],
                tensorflow_shape_frnt_(A)[1],
                device=device,
                dtype=dtype,
            )
>           sol, _, _ = tensorflow_safe_solve_with_mask(B, A)

Translated_Outputs/tensorflow_outputs/kornia/geometry/homography.py:183: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

B = <tf.Tensor: shape=(1, 9), dtype=float32, numpy=array([[1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)>
A = <tf.Tensor: shape=(1, 9, 9), dtype=float32, numpy=
array([[[ 2.5288212e+00,  2.2728243e+00, -1.1920929e-07,  0.0000000...0,
          1.9358095e+00,  2.3841858e-07, -8.3421493e-01, -4.2774668e+00,
          8.6399202e+00]]], dtype=float32)>

    def tensorflow_safe_solve_with_mask(B, A):
        from ._compat import tensorflow_torch_version_ge
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from ...ivy.functional.frontends.torch.linalg import tensorflow_lu_factor_ex_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.linalg import tensorflow_lu_solve_frnt
        from ...ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_lu_solve_frnt_base_count_1_frnt,
        )
        from ...typing import TYPE_CHECKING
        from ...ivy.functional.frontends.torch.linalg import lu
    
        if not tensorflow_torch_version_ge(1, 10):
            sol = tensorflow__torch_solve_cast(A, B)
            warnings.warn(
                "PyTorch version < 1.10, solve validness mask maybe not correct",
                RuntimeWarning,
            )
            return sol, sol, tensorflow_ones_frnt(len(A), dtype=tf.bool, device=A.device)
        if not isinstance(B, (tensorflow.Tensor, tensorflow.Variable)):
            raise AssertionError(f"B must be Tensor. Got: {type(B)}.")
        dtype: typing.Any = B.dtype
        if dtype not in (tf.float32, tf.float64):
            dtype = tf.float32
        if TYPE_CHECKING:
            A_LU: typing.Any
            pivots: typing.Any
            info: typing.Any
        elif tensorflow_torch_version_ge(1, 13):
>           A_LU, pivots, info = tensorflow_lu_factor_ex_frnt(tensorflow_to_frnt_(A, dtype))

Translated_Outputs/tensorflow_outputs/kornia/utils/helpers.py:123: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 9, 9), dtype=float32, numpy=
array([[[ 2.5288212e+00,  2.2728243e+00, -1.1920929e-07,  0.0000000...0,
          1.9358095e+00,  2.3841858e-07, -8.3421493e-01, -4.2774668e+00,
          8.6399202e+00]]], dtype=float32)>

    def tensorflow_lu_factor_ex_frnt(A, *, pivot=True, check_errors=False, out=None):
        from ...backends.tensorflow.experimental.linear_algebra import tensorflow_lu_factor
        from ...backends.tensorflow.creation import tensorflow_zeros
        from .tensor import tensorflow_shape_frnt_
        from ...backends.tensorflow.creation import tensorflow_full_like
        from ...backends.tensorflow.creation import tensorflow_ones
    
        try:
>           LU, pivots = tensorflow_lu_factor(A, pivot=pivot, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/linalg.py:68: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 9, 9), dtype=float32, numpy=
array([[[ 2.5288212e+00,  2.2728243e+00, -1.1920929e-07,  0.000000...,
          1.9358095e+00,  2.3841858e-07, -8.3421493e-01, -4.2774668e+00,
          8.6399202e+00]]], dtype=float32)>]
kwargs = {'out': None, 'pivot': True}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f0e70847e20>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f0e708281f0>, tensorflow_asarray = <function tensorflow_asarray at 0x7f0e70829f30>
tensorflow_get_item = <function tensorflow_get_item at 0x7f0e6b7c4e50>, num_args = 1
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'pivot', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Optional[bool], typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 0

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 9, 9), dtype=float32, numpy=
array([[[ 2.5288212e+00,  2.2728243e+00, -1.1920929e-07,  0.0000000...0,
          1.9358095e+00,  2.3841858e-07, -8.3421493e-01, -4.2774668e+00,
          8.6399202e+00]]], dtype=float32)>

    @tensorflow_handle_array_like_without_promotion
    def tensorflow_lu_factor(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        pivot: Optional[bool] = True,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
>       ret = tensorflow.linalg.lu(x)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/experimental/linear_algebra.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 9, 9), dtype=float32, numpy=
array([[[ 2.5288212e+00,  2.2728243e+00, -1.1920929e-07,  0.0000000...0,
          1.9358095e+00,  2.3841858e-07, -8.3421493e-01, -4.2774668e+00,
          8.6399202e+00]]], dtype=float32)>
output_idx_type = tf.int32, name = None

    @_dispatch.add_fallback_dispatch_list
    @_dispatch.add_type_based_api_dispatcher
    @tf_export('linalg.lu')
    def lu(input: Annotated[Any, TV_Lu_T], output_idx_type:TV_Lu_output_idx_type=_dtypes.int32, name=None):
      r"""Computes the LU decomposition of one or more square matrices.
    
      The input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions
      form square matrices.
    
      The input has to be invertible.
    
      The output consists of two tensors LU and P containing the LU decomposition
      of all input submatrices `[..., :, :]`. LU encodes the lower triangular and
      upper triangular factors.
    
      For each input submatrix of shape `[M, M]`, L is a lower triangular matrix of
      shape `[M, M]` with unit diagonal whose entries correspond to the strictly lower
      triangular part of LU. U is a upper triangular matrix of shape `[M, M]` whose
      entries correspond to the upper triangular part, including the diagonal, of LU.
    
      P represents a permutation matrix encoded as a list of indices each between `0`
      and `M-1`, inclusive. If P_mat denotes the permutation matrix corresponding to
      P, then the L, U and P satisfies P_mat * input = L * U.
    
      Args:
        input: A `Tensor`. Must be one of the following types: `float64`, `float32`, `half`, `complex64`, `complex128`.
          A tensor of shape `[..., M, M]` whose inner-most 2 dimensions form matrices of
          size `[M, M]`.
        output_idx_type: An optional `tf.DType` from: `tf.int32, tf.int64`. Defaults to `tf.int32`.
        name: A name for the operation (optional).
    
      Returns:
        A tuple of `Tensor` objects (lu, p).
    
        lu: A `Tensor`. Has the same type as `input`.
        p: A `Tensor` of type `output_idx_type`.
      """
      _ctx = _context._context or _context.context()
      tld = _ctx._thread_local_data
      if tld.is_eager:
        try:
          _result = pywrap_tfe.TFE_Py_FastPathExecute(
            _ctx, "Lu", name, input, "output_idx_type", output_idx_type)
          _result = _LuOutput._make(_result)
          return _result
        except _core._NotOkStatusException as e:
>         _ops.raise_from_not_ok_status(e, name)

/opt/fw/tensorflow/tensorflow/python/ops/gen_linalg_ops.py:1296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Lu_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input is not invertible. [Op:Lu] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.homography.find_homography_dlt_iterated
_________________________________________________________________________ test_symmetric_transfer_error[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_symmetric_transfer_error(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 4, 2),
            torch.rand(1, 4, 2),
            torch.rand(1, 3, 3),
        )
        trace_kwargs = {'squared': True, 'eps': 1e-8}
        test_args = (
            torch.rand(5, 4, 2),
            torch.rand(5, 4, 2),
            torch.rand(5, 3, 3),
        )
        test_kwargs = {'squared': True, 'eps': 1e-7}
>       _test_function(
            kornia.geometry.homography.symmetric_transfer_error,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_homography.py:206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function symmetric_transfer_error at 0x7f0e77048e50>
trace_args = (tensor([[[0.0649, 0.6129],
         [0.1046, 0.6496],
         [0.2919, 0.8941],
         [0.5698, 0.0015]]]), tensor...0.9336]]]), tensor([[[0.8460, 0.1910, 0.4743],
         [0.0984, 0.6190, 0.2057],
         [0.0529, 0.9985, 0.0125]]]))
trace_kwargs = {'eps': 1e-08, 'squared': True}
test_args = (tensor([[[0.3081, 0.2745],
         [0.6457, 0.1722],
         [0.7576, 0.8664],
         [0.6194, 0.7891]],

       ... 0.3235]],

        [[0.0878, 0.5430, 0.5010],
         [0.2115, 0.6412, 0.0152],
         [0.7423, 0.3377, 0.4771]]]))
test_kwargs = {'eps': 1e-07, 'squared': True}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function symmetric_transfer_error at 0x7f0e77048e50>
trace_args = (tensor([[[0.0649, 0.6129],
         [0.1046, 0.6496],
         [0.2919, 0.8941],
         [0.5698, 0.0015]]]), tensor...0.9336]]]), tensor([[[0.8460, 0.1910, 0.4743],
         [0.0984, 0.6190, 0.2057],
         [0.0529, 0.9985, 0.0125]]]))
trace_kwargs = {'eps': 1e-08, 'squared': True}
test_args = (tensor([[[0.3081, 0.2745],
         [0.6457, 0.1722],
         [0.7576, 0.8664],
         [0.6194, 0.7891]],

       ... 0.3235]],

        [[0.0878, 0.5430, 0.5010],
         [0.2115, 0.6412, 0.0152],
         [0.7423, 0.3377, 0.4771]]]))
test_kwargs = {'eps': 1e-07, 'squared': True}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
            _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)
    
        # test it works with the test_args as input
        orig_out = fn(*test_args, **test_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(test_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(test_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[1.6195e+00, 1.5198e+00, 2.5534e+00, 6.1362e-01],
        [4.2673e+01, 5.4472e+00, 5.4446e+01, 1.6351e+05],
  ...],
        [4.0232e+01, 6.7921e+01, 1.9729e+01, 2.3894e+01],
        [3.2337e+01, 1.3143e+00, 1.0285e+01, 1.4933e+00]])
transpiled_x = <tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[1.6195397e+00, 1.5198004e+00, 2.5534208e+00, 6.1361963e-01],
 ...729362e+01, 2.3893681e+01],
       [3.2337395e+01, 1.3142598e+00, 1.0284955e+01, 1.4932551e+00]],
      dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[1.6195397e+00, 1.5198002e+00, 2.5534205e+00, 6.1361957e-01],
       [4.2672729e+01, 5.4472122e+00, 5.4446060e+...9729362e+01, 2.3893684e+01],
       [3.2337395e+01, 1.3142593e+00, 1.0284954e+01, 1.4932551e+00]],
      dtype=float32)
y = array([[1.6195397e+00, 1.5198004e+00, 2.5534208e+00, 6.1361963e-01],
       [4.2672729e+01, 5.4472122e+00, 5.4446175e+...9729362e+01, 2.3893681e+01],
       [3.2337395e+01, 1.3142598e+00, 1.0284955e+01, 1.4932551e+00]],
      dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.homography.symmetric_transfer_error
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_homography.py::test_find_homography_dlt_iterated[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__Lu_devi...
FAILED kornia/geometry/test_homography.py::test_symmetric_transfer_error[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
=============================================================================== 2 failed, 6 passed in 170.72s (0:02:50) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/geometry/test_bbox.py ..F.....                                                                                                                                                            [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________________ test_bbox_to_mask[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_bbox_to_mask(target_framework, mode, backend_compile):
        trace_args = (
            torch.tensor([[[1., 1.], [3., 1.], [3., 2.], [1., 2.]]]),
            5,
            5,
        )
        trace_kwargs = {}
        test_args = (
            torch.tensor([[[2., 2.], [4., 2.], [4., 3.], [2., 3.]]]),
            6,
            6,
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.bbox.bbox_to_mask,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_bbox.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function bbox_to_mask at 0x7f2878871ea0>, trace_args = (tensor([[[1., 1.],
         [3., 1.],
         [3., 2.],
         [1., 2.]]]), 5, 5), trace_kwargs = {}
test_args = (tensor([[[2., 2.],
         [4., 2.],
         [4., 3.],
         [2., 3.]]]), 6, 6), test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s'
skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function bbox_to_mask at 0x7f2878871ea0>, trace_args = (tensor([[[1., 1.],
         [3., 1.],
         [3., 2.],
         [1., 2.]]]), 5, 5), trace_kwargs = {}
test_args = (tensor([[[2., 2.],
         [4., 2.],
         [4., 3.],
         [2., 3.]]]), 6, 6), test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001
deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[0., 0., 0., 0., 0.],
         [0., 1., 1., 1., 0.],
         [0., 1., 1., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
transpiled_x = <tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=
array([[[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[0., 0., 0., 0., 0.],
        [0., 1., 1., 1., 0.],
        [0., 1., 1., 1., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]]], dtype=float32)
y = array([[[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]]], dtype=float32), tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.bbox.bbox_to_mask
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_bbox.py::test_bbox_to_mask[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
=============================================================================== 1 failed, 7 passed in 150.29s (0:02:30) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 44 items

kornia/test_filters.py .FFFF.F.FFFF...............F.F..F..F..FF.FF.                                                                                                                              [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________________ test_blur_pool2d[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_blur_pool2d(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 5, 5),
            3,
        )
        trace_kwargs = {'stride': 2}
        test_args = (
            torch.rand(5, 3, 8, 8),
            3,  # NOTE: changing this kernel size fails the test; also true for some of the other tests in this file
        )
        test_kwargs = {'stride': 2}
>       _test_function(
            kornia.filters.blur_pool2d,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function blur_pool2d at 0x7f542c1e40d0>
trace_args = (tensor([[[[0.9291, 0.8900, 0.7875, 0.5114, 0.5462],
          [0.6578, 0.2247, 0.9878, 0.4505, 0.1537],
          [0....0.7051],
          [0.3518, 0.0705, 0.0643, 0.5792, 0.5483],
          [0.6091, 0.1983, 0.7545, 0.8050, 0.3536]]]]), 3)
trace_kwargs = {'stride': 2}
test_args = (tensor([[[[5.2363e-01, 9.2389e-01, 2.4679e-01, 3.8499e-02, 9.7308e-01,
           2.8456e-01, 9.0936e-01, 8.0053e-01]...    [6.9560e-01, 6.9003e-01, 3.2603e-01, 8.7061e-01, 4.5706e-01,
           7.7505e-01, 2.5122e-01, 2.9382e-01]]]]), 3)
test_kwargs = {'stride': 2}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function blur_pool2d at 0x7f542c1e40d0>
trace_args = (tensor([[[[0.9291, 0.8900, 0.7875, 0.5114, 0.5462],
          [0.6578, 0.2247, 0.9878, 0.4505, 0.1537],
          [0....0.7051],
          [0.3518, 0.0705, 0.0643, 0.5792, 0.5483],
          [0.6091, 0.1983, 0.7545, 0.8050, 0.3536]]]]), 3)
trace_kwargs = {'stride': 2}
test_args = (tensor([[[[5.2363e-01, 9.2389e-01, 2.4679e-01, 3.8499e-02, 9.7308e-01,
           2.8456e-01, 9.0936e-01, 8.0053e-01]...    [6.9560e-01, 6.9003e-01, 3.2603e-01, 8.7061e-01, 4.5706e-01,
           7.7505e-01, 2.5122e-01, 2.9382e-01]]]]), 3)
test_kwargs = {'stride': 2}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.929136  , 0.8900118 , 0.7875193 , 0.5114402 , 0.5462....5792407 , 0.54832375],
         [0.6090976 , 0.19825864, 0.7545041 , 0.8049975 , 0.35361755]]]],
      dtype=float32)>
kernel_size = 3, stride = 2

    def tensorflow_blur_pool2d(input, kernel_size, stride=2):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from .kernels import tensorflow_get_pascal_kernel_2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
    
        kernel = tensorflow_repeat_frnt_(
            tensorflow_get_pascal_kernel_2d(
                kernel_size, norm=True, device=input.device, dtype=input.dtype
            ),
            (tensorflow_size_frnt_(input, 1), 1, 1, 1),
        )
>       return tensorflow__blur_pool_by_kernel2d(input, kernel, stride)

Translated_Outputs/tensorflow_outputs/kornia/filters/blur_pool.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.929136  , 0.8900118 , 0.7875193 , 0.5114402 , 0.5462....5792407 , 0.54832375],
         [0.6090976 , 0.19825864, 0.7545041 , 0.8049975 , 0.35361755]]]],
      dtype=float32)>
kernel = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.1...   [[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.125 ],
         [0.0625, 0.125 , 0.0625]]]], dtype=float32)>
stride = 2

    def tensorflow__blur_pool_by_kernel2d(input, kernel, stride):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .median import tensorflow__compute_zero_padding
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
    
        tensorflow_KORNIA_CHECK(
            len(tensorflow_shape_frnt_(kernel)) == 4
            and tensorflow_shape_frnt_(kernel)[-2] == tensorflow_shape_frnt_(kernel)[-1],
            f"Invalid kernel shape. Expect CxC_(out, None)xNxN, Got {tensorflow_shape_frnt_(kernel)}",
        )
        padding = tensorflow__compute_zero_padding(
            (tensorflow_shape_frnt_(kernel)[-2], tensorflow_shape_frnt_(kernel)[-1])
        )
>       return tensorflow_conv2d_frnt(
            input,
            kernel,
            padding=padding,
            stride=stride,
            groups=tensorflow_shape_frnt_(input)[1],
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/blur_pool.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.929136  , 0.8900118 , 0.7875193 , 0.5114402 , 0.5462....5792407 , 0.54832375],
         [0.6090976 , 0.19825864, 0.7545041 , 0.8049975 , 0.35361755]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.1...   [[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.125 ],
         [0.0625, 0.125 , 0.0625]]]], dtype=float32)>
bias = None, stride = 2, padding = (1, 1), dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.929136  , 0.8900118 , 0.7875193 , 0.5114402 , 0.5462....5792407 , 0.54832375],
         [0.6090976 , 0.19825864, 0.7545041 , 0.8049975 , 0.35361755]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.1...   [[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.125 ],
         [0.0625, 0.125 , 0.0625]]]], dtype=float32)>
bias = None, stride = 2, padding = [(1, 1), (1, 1)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.929136  , 0.8900118 , 0.7875193 , 0.5114402 , 0.546...0.0625],
         [0.125 , 0.25  , 0.125 ],
         [0.0625, 0.125 , 0.0625]]]], dtype=float32)>, 2, [(1, 1), (1, 1)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f542535b7f0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5424cd8040>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.929136  , 0.8900118 , 0.7875193 , 0.5114402 , 0.546...0.0625],
         [0.125 , 0.25  , 0.125 ],
         [0.0625, 0.125 , 0.0625]]]], dtype=float32)>, 2, [(1, 1), (1, 1)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f5424cd8040>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f542535b490>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f542535b7f0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f542539d6c0>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 5, 5, 3), dtype=float32, numpy=
array([[[[0.929136  , 0.5712011 , 0.96029323],
         [0.89001...041 ],
         [0.87460715, 0.5968554 , 0.8049975 ],
         [0.07851398, 0.8524238 , 0.35361755]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 1, 3), dtype=float32, numpy=
array([[[[0.0625, 0.0625, 0.0625]],

        [[0.125 , 0.125 , 0...[[0.0625, 0.0625, 0.0625]],

        [[0.125 , 0.125 , 0.125 ]],

        [[0.0625, 0.0625, 0.0625]]]], dtype=float32)>
strides = 2, padding = [(1, 1), (1, 1)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 5, 3), dtype=float32, numpy=
array([[[[0.929136  , 0.5712011 , 0.96029323],
         [0.8900...
         [0.125 ]],

        [[0.0625],
         [0.0625],
         [0.0625]]]], dtype=float32)>, 2, [(1, 1), (1, 1)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f542535b7f0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5424cd8040>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 5, 3, 5), dtype=float32, numpy=
array([[[[0.929136  , 0.6577893 , 0.31934333, 0.34121543, 0.3382....12537724, 0.8524238 ],
         [0.9367869 , 0.26557183, 0.7051273 , 0.54832375, 0.35361755]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 3, 1), dtype=float32, numpy=
array([[[[0.0625],
         [0.0625],
         [0.0625]],

     ... ],
         [0.125 ],
         [0.125 ]],

        [[0.0625],
         [0.0625],
         [0.0625]]]], dtype=float32)>
strides = [1, 2, 2, 1], padding = [(0, 0), (1, 1), (1, 1), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 3, 5), dtype=float32, numpy=
array([[[[0.929136  , 0.6577893 , 0.31934333, 0.34121543, 0.338...       [0.0625],
         [0.0625]]]], dtype=float32)>, [1, 2, 2, 1], [(0, 0), (1, 1), (1, 1), (0, 0)], 'NHWC', [1, 1])
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 5 vs 3 [Op:DepthwiseConv2dNative] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.blur_pool.blur_pool2d
_________________________________________________________________________________ test_box_blur[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_box_blur(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 5, 5),
            (3, 3),
        )
        trace_kwargs = {'border_type': 'reflect', 'separable': False}
        test_args = (
            torch.rand(5, 3, 5, 5),
            (3, 3),
        )
        test_kwargs = {'border_type': 'reflect', 'separable': False}
>       _test_function(
            kornia.filters.box_blur,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:80: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function box_blur at 0x7f542c1d7760>
trace_args = (tensor([[[[0.0858, 0.2182, 0.7278, 0.6681, 0.6195],
          [0.8023, 0.4946, 0.6312, 0.7703, 0.2790],
          [0....9],
          [0.7830, 0.2069, 0.4531, 0.7668, 0.1590],
          [0.1069, 0.8430, 0.6500, 0.1603, 0.8550]]]]), (3, 3))
trace_kwargs = {'border_type': 'reflect', 'separable': False}
test_args = (tensor([[[[0.7776, 0.1759, 0.1905, 0.6990, 0.5557],
          [0.0424, 0.7780, 0.2364, 0.3273, 0.0350],
          [0....8],
          [0.8166, 0.8446, 0.8922, 0.3217, 0.5808],
          [0.8886, 0.8343, 0.5581, 0.4227, 0.9834]]]]), (3, 3))
test_kwargs = {'border_type': 'reflect', 'separable': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function box_blur at 0x7f542c1d7760>
trace_args = (tensor([[[[0.0858, 0.2182, 0.7278, 0.6681, 0.6195],
          [0.8023, 0.4946, 0.6312, 0.7703, 0.2790],
          [0....9],
          [0.7830, 0.2069, 0.4531, 0.7668, 0.1590],
          [0.1069, 0.8430, 0.6500, 0.1603, 0.8550]]]]), (3, 3))
trace_kwargs = {'border_type': 'reflect', 'separable': False}
test_args = (tensor([[[[0.7776, 0.1759, 0.1905, 0.6990, 0.5557],
          [0.0424, 0.7780, 0.2364, 0.3273, 0.0350],
          [0....8],
          [0.8166, 0.8446, 0.8922, 0.3217, 0.5808],
          [0.8886, 0.8343, 0.5581, 0.4227, 0.9834]]]]), (3, 3))
test_kwargs = {'border_type': 'reflect', 'separable': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08579034, 0.21819067, 0.7277769 , 0.6681195 , 0.6194....76676065, 0.15896243],
         [0.10685575, 0.84298193, 0.6499676 , 0.16029513, 0.8550218 ]]]],
      dtype=float32)>
kernel_size = (3, 3), border_type = 'reflect', separable = False

    def tensorflow_box_blur(input, kernel_size, border_type="reflect", separable=False):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from .kernels import tensorflow__unpack_2d_ks
        from .kernels import tensorflow_get_box_kernel1d
        from .filter import tensorflow_filter2d_separable
        from .kernels import tensorflow_get_box_kernel2d
        from .filter import tensorflow_filter2d
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        if separable:
            ky, kx = tensorflow__unpack_2d_ks(kernel_size)
            kernel_y = tensorflow_get_box_kernel1d(
                ky, device=input.device, dtype=input.dtype
            )
            kernel_x = tensorflow_get_box_kernel1d(
                kx, device=input.device, dtype=input.dtype
            )
            out = tensorflow_filter2d_separable(input, kernel_x, kernel_y, border_type)
        else:
            kernel = tensorflow_get_box_kernel2d(
                kernel_size, device=input.device, dtype=input.dtype
            )
>           out = tensorflow_filter2d(input, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/blur.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 7, 7), dtype=float32, numpy=
array([[[[0.4946046 , 0.8023194 , 0.4946046 , 0.63123214, 0.7702...     [0.20694971, 0.78298146, 0.20694971, 0.45306933, 0.76676065,
          0.15896243, 0.76676065]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0.11111111, 0.11111111, 0.11111111],
        [0.11111111, 0.11111111, 0.11111111],
        [0.11111111, 0.11111111, 0.11111111]]], dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 7, 7), dtype=float32, numpy=
array([[[[0.4946046 , 0.8023194 , 0.4946046 , 0.63123214, 0.7702...     [0.20694971, 0.78298146, 0.20694971, 0.45306933, 0.76676065,
          0.15896243, 0.76676065]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111],
         [0.11111...1111],
         [0.11111111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 7, 7), dtype=float32, numpy=
array([[[[0.4946046 , 0.8023194 , 0.4946046 , 0.63123214, 0.7702...     [0.20694971, 0.78298146, 0.20694971, 0.45306933, 0.76676065,
          0.15896243, 0.76676065]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111],
         [0.11111...1111],
         [0.11111111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 7, 7), dtype=float32, numpy=
array([[[[0.4946046 , 0.8023194 , 0.4946046 , 0.63123214, 0.770...11111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f54254f93f0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5425121bd0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 7, 7), dtype=float32, numpy=
array([[[[0.4946046 , 0.8023194 , 0.4946046 , 0.63123214, 0.770...11111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f5425121bd0>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f54254f9090>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f54254f93f0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f54254fb250>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 7, 7, 3), dtype=float32, numpy=
array([[[[0.4946046 , 0.1728375 , 0.1491195 ],
         [0.80231...6065],
         [0.15437144, 0.6684504 , 0.15896243],
         [0.7156613 , 0.43291414, 0.76676065]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 1, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111]],

        [[0.111...]],

        [[0.11111111, 0.11111111, 0.11111111]],

        [[0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 7, 7, 3), dtype=float32, numpy=
array([[[[0.4946046 , 0.1728375 , 0.1491195 ],
         [0.8023...11111]],

        [[0.11111111],
         [0.11111111],
         [0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f54254f93f0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5425121bd0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 7, 3, 7), dtype=float32, numpy=
array([[[[0.4946046 , 0.21819067, 0.4946046 , 0.575104  , 0.2681...     [0.5801285 , 0.3607487 , 0.5801285 , 0.60025394, 0.76676065,
          0.16029513, 0.76676065]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 3, 1), dtype=float32, numpy=
array([[[[0.11111111],
         [0.11111111],
         [0.111111...1111],
         [0.11111111]],

        [[0.11111111],
         [0.11111111],
         [0.11111111]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 7, 3, 7), dtype=float32, numpy=
array([[[[0.4946046 , 0.21819067, 0.4946046 , 0.575104  , 0.268...0.11111111],
         [0.11111111]]]], dtype=float32)>, [1, 1, 1, 1], [(0, 0), (0, 0), (0, 0), (0, 0)], 'NHWC', [1, 1])
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 7 vs 3 [Op:DepthwiseConv2dNative] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.blur.box_blur
______________________________________________________________________________ test_gaussian_blur2d[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_gaussian_blur2d(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 5, 5),
            (3, 3),
            (1.5, 1.5),
        )
        trace_kwargs = {'border_type': 'reflect', 'separable': True}
        test_args = (
            torch.rand(5, 3, 5, 5),
            (3, 3),
            (1.5, 1.5),
        )
        test_kwargs = {'border_type': 'reflect', 'separable': True}
>       _test_function(
            kornia.filters.gaussian_blur2d,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function gaussian_blur2d at 0x7f542c1e4b80>
trace_args = (tensor([[[[0.6471, 0.0168, 0.0566, 0.2443, 0.2322],
          [0.5313, 0.1510, 0.4057, 0.5741, 0.9436],
          [0....  [0.6285, 0.8895, 0.7653, 0.8074, 0.9969],
          [0.9406, 0.6325, 0.5498, 0.6900, 0.5258]]]]), (3, 3), (1.5, 1.5))
trace_kwargs = {'border_type': 'reflect', 'separable': True}
test_args = (tensor([[[[0.1858, 0.5789, 0.5138, 0.4606, 0.9673],
          [0.7761, 0.6780, 0.6210, 0.1222, 0.5388],
          [0....  [0.9814, 0.5327, 0.8517, 0.2372, 0.7411],
          [0.4171, 0.0922, 0.6920, 0.6413, 0.2819]]]]), (3, 3), (1.5, 1.5))
test_kwargs = {'border_type': 'reflect', 'separable': True}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function gaussian_blur2d at 0x7f542c1e4b80>
trace_args = (tensor([[[[0.6471, 0.0168, 0.0566, 0.2443, 0.2322],
          [0.5313, 0.1510, 0.4057, 0.5741, 0.9436],
          [0....  [0.6285, 0.8895, 0.7653, 0.8074, 0.9969],
          [0.9406, 0.6325, 0.5498, 0.6900, 0.5258]]]]), (3, 3), (1.5, 1.5))
trace_kwargs = {'border_type': 'reflect', 'separable': True}
test_args = (tensor([[[[0.1858, 0.5789, 0.5138, 0.4606, 0.9673],
          [0.7761, 0.6780, 0.6210, 0.1222, 0.5388],
          [0....  [0.9814, 0.5327, 0.8517, 0.2372, 0.7411],
          [0.4171, 0.0922, 0.6920, 0.6413, 0.2819]]]]), (3, 3), (1.5, 1.5))
test_kwargs = {'border_type': 'reflect', 'separable': True}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.64706063, 0.01680648, 0.05663651, 0.24427831, 0.2321....8074453 , 0.99690443],
         [0.9406272 , 0.6324708 , 0.5497613 , 0.68996716, 0.5258079 ]]]],
      dtype=float32)>
kernel_size = (3, 3), sigma = <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1.5, 1.5]], dtype=float32)>, border_type = 'reflect', separable = True

    def tensorflow_gaussian_blur2d(
        input, kernel_size, sigma, border_type="reflect", separable=True
    ):
        from ..core._backend import tensor
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .kernels import tensorflow__unpack_2d_ks
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .kernels import tensorflow_get_gaussian_kernel1d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from .filter import tensorflow_filter2d_separable
        from .kernels import tensorflow_get_gaussian_kernel2d
        from .filter import tensorflow_filter2d
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        if isinstance(sigma, (tuple,)):
            sigma = tensor([sigma], device=input.device, dtype=input.dtype)
        else:
            tensorflow_KORNIA_CHECK_IS_TENSOR(sigma)
            sigma = tensorflow_to_frnt_(sigma, device=input.device, dtype=input.dtype)
        if separable:
            ky, kx = tensorflow__unpack_2d_ks(kernel_size)
            bs = tensorflow_shape_frnt_(sigma)[0]
            kernel_x = tensorflow_get_gaussian_kernel1d(
                kx, tensorflow_view_frnt_(sigma[:, 1], bs, 1)
            )
            kernel_y = tensorflow_get_gaussian_kernel1d(
                ky, tensorflow_view_frnt_(sigma[:, 0], bs, 1)
            )
>           out = tensorflow_filter2d_separable(input, kernel_x, kernel_y, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/gaussian.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.64706063, 0.01680648, 0.05663651, 0.24427831, 0.2321....8074453 , 0.99690443],
         [0.9406272 , 0.6324708 , 0.5497613 , 0.68996716, 0.5258079 ]]]],
      dtype=float32)>
kernel_x = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.30780134, 0.38439736, 0.30780134]], dtype=float32)>
kernel_y = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.30780134, 0.38439736, 0.30780134]], dtype=float32)>, border_type = 'reflect', normalized = False, padding = 'same'

    def tensorflow_filter2d_separable(
        input, kernel_x, kernel_y, border_type="reflect", normalized=False, padding="same"
    ):
>       out_x = tensorflow_filter2d(
            input, kernel_x[..., None, :], border_type, normalized, padding
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 7), dtype=float32, numpy=
array([[[[0.01680648, 0.64706063, 0.01680648, 0.05663651, 0.2442...     [0.6324708 , 0.9406272 , 0.6324708 , 0.5497613 , 0.68996716,
          0.5258079 , 0.68996716]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 1, 3), dtype=float32, numpy=array([[[0.30780134, 0.38439736, 0.30780134]]], dtype=float32)>, border_type = 'reflect', normalized = False, padding = 'same'
behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 7), dtype=float32, numpy=
array([[[[0.01680648, 0.64706063, 0.01680648, 0.05663651, 0.2442...     [0.6324708 , 0.9406272 , 0.6324708 , 0.5497613 , 0.68996716,
          0.5258079 , 0.68996716]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 1, 3), dtype=float32, numpy=
array([[[[0.30780134, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]]], dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 7), dtype=float32, numpy=
array([[[[0.01680648, 0.64706063, 0.01680648, 0.05663651, 0.2442...     [0.6324708 , 0.9406272 , 0.6324708 , 0.5497613 , 0.68996716,
          0.5258079 , 0.68996716]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 1, 3), dtype=float32, numpy=
array([[[[0.30780134, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]]], dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 5, 7), dtype=float32, numpy=
array([[[[0.01680648, 0.64706063, 0.01680648, 0.05663651, 0.244...4, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5425851090>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5424a71870>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 5, 7), dtype=float32, numpy=
array([[[[0.01680648, 0.64706063, 0.01680648, 0.05663651, 0.244...4, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]]], dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f5424a71870>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f5425850d30>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5425851090>
tensorflow_asarray = <function tensorflow_asarray at 0x7f5425852ef0>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 5, 7, 3), dtype=float32, numpy=
array([[[[0.01680648, 0.97462326, 0.5206791 ],
         [0.64706...6716],
         [0.6638427 , 0.31937432, 0.5258079 ],
         [0.7728576 , 0.16450858, 0.68996716]]]], dtype=float32)>
filters = <tf.Tensor: shape=(1, 3, 1, 3), dtype=float32, numpy=
array([[[[0.30780134, 0.30780134, 0.30780134]],

        [[0.38439736, 0.38439736, 0.38439736]],

        [[0.30780134, 0.30780134, 0.30780134]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 7, 3), dtype=float32, numpy=
array([[[[0.01680648, 0.97462326, 0.5206791 ],
         [0.6470...39736]],

        [[0.30780134],
         [0.30780134],
         [0.30780134]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5425851090>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5424a71870>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 7, 3, 5), dtype=float32, numpy=
array([[[[0.01680648, 0.15100694, 0.6879726 , 0.2657405 , 0.5749....98065275, 0.16450858],
         [0.4954375 , 0.59629154, 0.6512276 , 0.8074453 , 0.68996716]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
array([[[[0.30780134],
         [0.30780134],
         [0.307801...9736],
         [0.38439736]],

        [[0.30780134],
         [0.30780134],
         [0.30780134]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 7, 3, 5), dtype=float32, numpy=
array([[[[0.01680648, 0.15100694, 0.6879726 , 0.2657405 , 0.574...0.30780134],
         [0.30780134]]]], dtype=float32)>, [1, 1, 1, 1], [(0, 0), (0, 0), (0, 0), (0, 0)], 'NHWC', [1, 1])
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 5 vs 3 [Op:DepthwiseConv2dNative] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.gaussian.gaussian_blur2d
________________________________________________________________________________ test_guided_blur[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_guided_blur(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 5, 5),
            torch.rand(1, 3, 5, 5),
            (3, 3),
            0.1,
        )
        trace_kwargs = {'border_type': 'reflect', 'subsample': 1}
        test_args = (
            torch.rand(5, 3, 5, 5),
            torch.rand(5, 3, 5, 5),
            (3, 3),
            0.1,
        )
        test_kwargs = {'border_type': 'reflect', 'subsample': 1}
>       _test_function(
            kornia.filters.guided_blur,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function guided_blur at 0x7f542c1e6830>
trace_args = (tensor([[[[0.9573, 0.8420, 0.2639, 0.3554, 0.0807],
          [0.0788, 0.5283, 0.8384, 0.5101, 0.4157],
          [0....         [0.3270, 0.7031, 0.5365, 0.8374, 0.0260],
          [0.4362, 0.9134, 0.8814, 0.5949, 0.1128]]]]), (3, 3), 0.1)
trace_kwargs = {'border_type': 'reflect', 'subsample': 1}
test_args = (tensor([[[[0.5948, 0.0867, 0.9947, 0.2889, 0.7347],
          [0.4045, 0.7396, 0.8035, 0.2758, 0.3919],
          [0....         [0.3625, 0.8336, 0.5587, 0.7362, 0.7346],
          [0.0775, 0.3358, 0.8306, 0.8184, 0.5979]]]]), (3, 3), 0.1)
test_kwargs = {'border_type': 'reflect', 'subsample': 1}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function guided_blur at 0x7f542c1e6830>
trace_args = (tensor([[[[0.9573, 0.8420, 0.2639, 0.3554, 0.0807],
          [0.0788, 0.5283, 0.8384, 0.5101, 0.4157],
          [0....         [0.3270, 0.7031, 0.5365, 0.8374, 0.0260],
          [0.4362, 0.9134, 0.8814, 0.5949, 0.1128]]]]), (3, 3), 0.1)
trace_kwargs = {'border_type': 'reflect', 'subsample': 1}
test_args = (tensor([[[[0.5948, 0.0867, 0.9947, 0.2889, 0.7347],
          [0.4045, 0.7396, 0.8035, 0.2758, 0.3919],
          [0....         [0.3625, 0.8336, 0.5587, 0.7362, 0.7346],
          [0.0775, 0.3358, 0.8306, 0.8184, 0.5979]]]]), (3, 3), 0.1)
test_kwargs = {'border_type': 'reflect', 'subsample': 1}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

guidance = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.9573299 , 0.84198475, 0.26385492, 0.3553655 , 0.0807....32038677, 0.6341752 ],
         [0.7030945 , 0.1739214 , 0.1091693 , 0.5338436 , 0.37474626]]]],
      dtype=float32)>
input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.7734203 , 0.44854176, 0.8788753 , 0.9215261 , 0.9038....8374343 , 0.02595085],
         [0.4362412 , 0.91337   , 0.8813968 , 0.5948517 , 0.11279947]]]],
      dtype=float32)>
kernel_size = (3, 3), eps = 0.1, border_type = 'reflect', subsample = 1

    def tensorflow_guided_blur(
        guidance, input, kernel_size, eps, border_type="reflect", subsample=1
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(guidance)
        tensorflow_KORNIA_CHECK_SHAPE(guidance, ["B", "C", "H", "W"])
        if input is not guidance:
            tensorflow_KORNIA_CHECK_IS_TENSOR(input)
            tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
            tensorflow_KORNIA_CHECK(
                tensorflow_shape_frnt_(guidance)[0] == tensorflow_shape_frnt_(input)[0]
                and tensorflow_shape_frnt_(guidance)[-2:]
                == tensorflow_shape_frnt_(input)[-2:],
                "guidance and input should have the same batch size and spatial dimensions",
            )
        if tensorflow_shape_frnt_(guidance)[1] == 1:
            return tensorflow__guided_blur_grayscale_guidance(
                guidance, input, kernel_size, eps, border_type, subsample
            )
        else:
>           return tensorflow__guided_blur_multichannel_guidance(
                guidance, input, kernel_size, eps, border_type, subsample
            )

Translated_Outputs/tensorflow_outputs/kornia/filters/guided.py:224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

guidance = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.9573299 , 0.84198475, 0.26385492, 0.3553655 , 0.0807....32038677, 0.6341752 ],
         [0.7030945 , 0.1739214 , 0.1091693 , 0.5338436 , 0.37474626]]]],
      dtype=float32)>
input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.7734203 , 0.44854176, 0.8788753 , 0.9215261 , 0.9038....8374343 , 0.02595085],
         [0.4362412 , 0.91337   , 0.8813968 , 0.5948517 , 0.11279947]]]],
      dtype=float32)>
kernel_size = (3, 3), eps = 0.1, border_type = 'reflect', subsample = 1

    def tensorflow__guided_blur_multichannel_guidance(
        guidance, input, kernel_size, eps, border_type="reflect", subsample=1
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from .blur import tensorflow_box_blur
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flatten_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_eye_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_diag_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_new_full_frnt_
        from ...ivy.functional.frontends.torch.linalg import tensorflow_solve_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_interpolate_frnt,
        )
        from ...ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_einsum_frnt,
        )
    
        guidance_sub, input_sub, kernel_size = tensorflow__preprocess_fast_guided_blur(
            guidance, input, kernel_size, subsample
        )
        B, C, H, W = tensorflow_shape_frnt_(guidance_sub)
        mean_I = tensorflow_permute_frnt_(
>           tensorflow_box_blur(guidance_sub, kernel_size, border_type), 0, 2, 3, 1
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/guided.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.9573299 , 0.84198475, 0.26385492, 0.3553655 , 0.0807....32038677, 0.6341752 ],
         [0.7030945 , 0.1739214 , 0.1091693 , 0.5338436 , 0.37474626]]]],
      dtype=float32)>
kernel_size = (3, 3), border_type = 'reflect', separable = False

    def tensorflow_box_blur(input, kernel_size, border_type="reflect", separable=False):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from .kernels import tensorflow__unpack_2d_ks
        from .kernels import tensorflow_get_box_kernel1d
        from .filter import tensorflow_filter2d_separable
        from .kernels import tensorflow_get_box_kernel2d
        from .filter import tensorflow_filter2d
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        if separable:
            ky, kx = tensorflow__unpack_2d_ks(kernel_size)
            kernel_y = tensorflow_get_box_kernel1d(
                ky, device=input.device, dtype=input.dtype
            )
            kernel_x = tensorflow_get_box_kernel1d(
                kx, device=input.device, dtype=input.dtype
            )
            out = tensorflow_filter2d_separable(input, kernel_x, kernel_y, border_type)
        else:
            kernel = tensorflow_get_box_kernel2d(
                kernel_size, device=input.device, dtype=input.dtype
            )
>           out = tensorflow_filter2d(input, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/blur.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 7, 7), dtype=float32, numpy=
array([[[[0.5283396 , 0.07878065, 0.5283396 , 0.83837897, 0.5101...     [0.3157146 , 0.61314166, 0.3157146 , 0.74526757, 0.32038677,
          0.6341752 , 0.32038677]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0.11111111, 0.11111111, 0.11111111],
        [0.11111111, 0.11111111, 0.11111111],
        [0.11111111, 0.11111111, 0.11111111]]], dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 7, 7), dtype=float32, numpy=
array([[[[0.5283396 , 0.07878065, 0.5283396 , 0.83837897, 0.5101...     [0.3157146 , 0.61314166, 0.3157146 , 0.74526757, 0.32038677,
          0.6341752 , 0.32038677]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111],
         [0.11111...1111],
         [0.11111111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 7, 7), dtype=float32, numpy=
array([[[[0.5283396 , 0.07878065, 0.5283396 , 0.83837897, 0.5101...     [0.3157146 , 0.61314166, 0.3157146 , 0.74526757, 0.32038677,
          0.6341752 , 0.32038677]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111],
         [0.11111...1111],
         [0.11111111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 7, 7), dtype=float32, numpy=
array([[[[0.5283396 , 0.07878065, 0.5283396 , 0.83837897, 0.510...11111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5424de89d0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f542509e8c0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 7, 7), dtype=float32, numpy=
array([[[[0.5283396 , 0.07878065, 0.5283396 , 0.83837897, 0.510...11111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f542509e8c0>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f5424de8670>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5424de89d0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f5424deb760>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 7, 7, 3), dtype=float32, numpy=
array([[[[0.5283396 , 0.57661927, 0.7131278 ],
         [0.07878...8677],
         [0.47680765, 0.19127434, 0.6341752 ],
         [0.3993011 , 0.50505114, 0.32038677]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 1, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111]],

        [[0.111...]],

        [[0.11111111, 0.11111111, 0.11111111]],

        [[0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 7, 7, 3), dtype=float32, numpy=
array([[[[0.5283396 , 0.57661927, 0.7131278 ],
         [0.0787...11111]],

        [[0.11111111],
         [0.11111111],
         [0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5424de89d0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f542509e8c0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 7, 3, 7), dtype=float32, numpy=
array([[[[0.5283396 , 0.84198475, 0.5283396 , 0.83894736, 0.1983...     [0.5557589 , 0.27688694, 0.5557589 , 0.04114205, 0.32038677,
          0.5338436 , 0.32038677]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 3, 1), dtype=float32, numpy=
array([[[[0.11111111],
         [0.11111111],
         [0.111111...1111],
         [0.11111111]],

        [[0.11111111],
         [0.11111111],
         [0.11111111]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 7, 3, 7), dtype=float32, numpy=
array([[[[0.5283396 , 0.84198475, 0.5283396 , 0.83894736, 0.198...0.11111111],
         [0.11111111]]]], dtype=float32)>, [1, 1, 1, 1], [(0, 0), (0, 0), (0, 0), (0, 0)], 'NHWC', [1, 1])
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 7 vs 3 [Op:DepthwiseConv2dNative] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.guided.guided_blur
______________________________________________________________________________ test_max_blur_pool2d[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_max_blur_pool2d(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 5, 5),
            3,
        )
        trace_kwargs = {'stride': 2, 'max_pool_size': 2, 'ceil_mode': False}
        test_args = (
            torch.rand(5, 3, 8, 8),
            3,
        )
        test_kwargs = {'stride': 2, 'max_pool_size': 2, 'ceil_mode': False}
>       _test_function(
            kornia.filters.max_blur_pool2d,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:188: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function max_blur_pool2d at 0x7f542c1e44c0>
trace_args = (tensor([[[[0.8991, 0.9106, 0.1147, 0.1211, 0.8338],
          [0.1667, 0.5205, 0.1566, 0.8875, 0.2251],
          [0....0.2011],
          [0.1035, 0.5315, 0.7284, 0.3669, 0.3607],
          [0.9190, 0.8475, 0.4150, 0.7582, 0.1259]]]]), 3)
trace_kwargs = {'ceil_mode': False, 'max_pool_size': 2, 'stride': 2}
test_args = (tensor([[[[7.0957e-01, 5.2305e-01, 5.8130e-01, 4.7959e-01, 5.7516e-01,
           6.5325e-02, 4.6389e-01, 8.9867e-01]...    [2.1784e-01, 6.7599e-01, 5.0942e-01, 8.2906e-01, 9.9456e-01,
           1.2441e-01, 4.1779e-01, 6.6869e-01]]]]), 3)
test_kwargs = {'ceil_mode': False, 'max_pool_size': 2, 'stride': 2}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function max_blur_pool2d at 0x7f542c1e44c0>
trace_args = (tensor([[[[0.8991, 0.9106, 0.1147, 0.1211, 0.8338],
          [0.1667, 0.5205, 0.1566, 0.8875, 0.2251],
          [0....0.2011],
          [0.1035, 0.5315, 0.7284, 0.3669, 0.3607],
          [0.9190, 0.8475, 0.4150, 0.7582, 0.1259]]]]), 3)
trace_kwargs = {'ceil_mode': False, 'max_pool_size': 2, 'stride': 2}
test_args = (tensor([[[[7.0957e-01, 5.2305e-01, 5.8130e-01, 4.7959e-01, 5.7516e-01,
           6.5325e-02, 4.6389e-01, 8.9867e-01]...    [2.1784e-01, 6.7599e-01, 5.0942e-01, 8.2906e-01, 9.9456e-01,
           1.2441e-01, 4.1779e-01, 6.6869e-01]]]]), 3)
test_kwargs = {'ceil_mode': False, 'max_pool_size': 2, 'stride': 2}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.89909333, 0.9105778 , 0.11474454, 0.12112355, 0.8337....3668931 , 0.3606758 ],
         [0.9190074 , 0.84750515, 0.41503006, 0.7582096 , 0.12594515]]]],
      dtype=float32)>
kernel_size = 3, stride = 2, max_pool_size = 2, ceil_mode = False

    def tensorflow_max_blur_pool2d(
        input, kernel_size, stride=2, max_pool_size=2, ceil_mode=False
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from .kernels import tensorflow_get_pascal_kernel_2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        kernel = tensorflow_repeat_frnt_(
            tensorflow_get_pascal_kernel_2d(
                kernel_size, norm=True, device=input.device, dtype=input.dtype
            ),
            (tensorflow_shape_frnt_(input)[1], 1, 1, 1),
        )
>       return tensorflow__max_blur_pool_by_kernel2d(
            input, kernel, stride, max_pool_size, ceil_mode
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/blur_pool.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9105778 , 0.9105778 , 0.8874947 , 0.8874947 ],
     ....728355  , 0.8710942 , 0.8710942 ],
         [0.9190074 , 0.84750515, 0.7582096 , 0.7582096 ]]]],
      dtype=float32)>
kernel = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.1...   [[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.125 ],
         [0.0625, 0.125 , 0.0625]]]], dtype=float32)>
stride = 2, max_pool_size = 2, ceil_mode = False

    def tensorflow__max_blur_pool_by_kernel2d(
        input, kernel, stride, max_pool_size, ceil_mode
    ):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.pooling_functions import (
            tensorflow_max_pool2d_frnt,
        )
        from .median import tensorflow__compute_zero_padding
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
    
        tensorflow_KORNIA_CHECK(
            len(tensorflow_shape_frnt_(kernel)) == 4
            and tensorflow_shape_frnt_(kernel)[-2] == tensorflow_shape_frnt_(kernel)[-1],
            f"Invalid kernel shape. Expect CxC_outxNxN, Got {tensorflow_shape_frnt_(kernel)}",
        )
        input = tensorflow_max_pool2d_frnt(
            input, kernel_size=max_pool_size, padding=0, stride=1, ceil_mode=ceil_mode
        )
        padding = tensorflow__compute_zero_padding(
            (tensorflow_shape_frnt_(kernel)[-2], tensorflow_shape_frnt_(kernel)[-1])
        )
>       return tensorflow_conv2d_frnt(
            input,
            kernel,
            padding=padding,
            stride=stride,
            groups=tensorflow_size_frnt_(input, 1),
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/blur_pool.py:112: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9105778 , 0.9105778 , 0.8874947 , 0.8874947 ],
     ....728355  , 0.8710942 , 0.8710942 ],
         [0.9190074 , 0.84750515, 0.7582096 , 0.7582096 ]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.1...   [[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.125 ],
         [0.0625, 0.125 , 0.0625]]]], dtype=float32)>
bias = None, stride = 2, padding = (1, 1), dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9105778 , 0.9105778 , 0.8874947 , 0.8874947 ],
     ....728355  , 0.8710942 , 0.8710942 ],
         [0.9190074 , 0.84750515, 0.7582096 , 0.7582096 ]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.1...   [[[0.0625, 0.125 , 0.0625],
         [0.125 , 0.25  , 0.125 ],
         [0.0625, 0.125 , 0.0625]]]], dtype=float32)>
bias = None, stride = 2, padding = [(1, 1), (1, 1)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9105778 , 0.9105778 , 0.8874947 , 0.8874947 ],
    ...0.0625],
         [0.125 , 0.25  , 0.125 ],
         [0.0625, 0.125 , 0.0625]]]], dtype=float32)>, 2, [(1, 1), (1, 1)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5425139a20>
tensorflow_get_item = <function tensorflow_get_item at 0x7f542479be20>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9105778 , 0.9105778 , 0.8874947 , 0.8874947 ],
    ...0.0625],
         [0.125 , 0.25  , 0.125 ],
         [0.0625, 0.125 , 0.0625]]]], dtype=float32)>, 2, [(1, 1), (1, 1)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f542479be20>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f54251396c0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5425139a20>
tensorflow_asarray = <function tensorflow_asarray at 0x7f5424798d30>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 4, 4, 3), dtype=float32, numpy=
array([[[[0.9105778 , 0.95440114, 0.81506056],
         [0.91057...0515],
         [0.75872076, 0.6287413 , 0.7582096 ],
         [0.49897933, 0.75138813, 0.7582096 ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 1, 3), dtype=float32, numpy=
array([[[[0.0625, 0.0625, 0.0625]],

        [[0.125 , 0.125 , 0...[[0.0625, 0.0625, 0.0625]],

        [[0.125 , 0.125 , 0.125 ]],

        [[0.0625, 0.0625, 0.0625]]]], dtype=float32)>
strides = 2, padding = [(1, 1), (1, 1)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 4, 4, 3), dtype=float32, numpy=
array([[[[0.9105778 , 0.95440114, 0.81506056],
         [0.9105...
         [0.125 ]],

        [[0.0625],
         [0.0625],
         [0.0625]]]], dtype=float32)>, 2, [(1, 1), (1, 1)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5425139a20>
tensorflow_get_item = <function tensorflow_get_item at 0x7f542479be20>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 4, 3, 4), dtype=float32, numpy=
array([[[[0.9105778 , 0.5205177 , 0.9173586 , 0.9173586 ],
     ....86234736, 0.86234736, 0.75138813],
         [0.81145763, 0.8710942 , 0.8710942 , 0.7582096 ]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 3, 1), dtype=float32, numpy=
array([[[[0.0625],
         [0.0625],
         [0.0625]],

     ... ],
         [0.125 ],
         [0.125 ]],

        [[0.0625],
         [0.0625],
         [0.0625]]]], dtype=float32)>
strides = [1, 2, 2, 1], padding = [(0, 0), (1, 1), (1, 1), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 4, 3, 4), dtype=float32, numpy=
array([[[[0.9105778 , 0.5205177 , 0.9173586 , 0.9173586 ],
    ...       [0.0625],
         [0.0625]]]], dtype=float32)>, [1, 2, 2, 1], [(0, 0), (1, 1), (1, 1), (0, 0)], 'NHWC', [1, 1])
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 4 vs 3 [Op:DepthwiseConv2dNative] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.blur_pool.max_blur_pool2d
________________________________________________________________________________ test_motion_blur[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_motion_blur(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 5, 5),
            5,
            45.0,
            1.0,
        )
        trace_kwargs = {'border_type': 'constant', 'mode': 'nearest'}
        test_args = (
            torch.rand(5, 3, 5, 5),
            5,
            90.0,
            0.5,
        )
        test_kwargs = {'border_type': 'constant', 'mode': 'nearest'}
>       _test_function(
            kornia.filters.motion_blur,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:240: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function motion_blur at 0x7f542c0d97e0>
trace_args = (tensor([[[[0.0840, 0.2511, 0.2358, 0.7717, 0.1620],
          [0.9424, 0.4071, 0.5672, 0.2211, 0.1998],
          [0....        [0.0793, 0.2111, 0.2221, 0.1005, 0.5521],
          [0.8716, 0.1811, 0.9448, 0.5835, 0.3594]]]]), 5, 45.0, 1.0)
trace_kwargs = {'border_type': 'constant', 'mode': 'nearest'}
test_args = (tensor([[[[0.8935, 0.9298, 0.7817, 0.5431, 0.3411],
          [0.1381, 0.3496, 0.8550, 0.4754, 0.0517],
          [0....        [0.5029, 0.7336, 0.0173, 0.3639, 0.4773],
          [0.5878, 0.2666, 0.4793, 0.3182, 0.9447]]]]), 5, 90.0, 0.5)
test_kwargs = {'border_type': 'constant', 'mode': 'nearest'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function motion_blur at 0x7f542c0d97e0>
trace_args = (tensor([[[[0.0840, 0.2511, 0.2358, 0.7717, 0.1620],
          [0.9424, 0.4071, 0.5672, 0.2211, 0.1998],
          [0....        [0.0793, 0.2111, 0.2221, 0.1005, 0.5521],
          [0.8716, 0.1811, 0.9448, 0.5835, 0.3594]]]]), 5, 45.0, 1.0)
trace_kwargs = {'border_type': 'constant', 'mode': 'nearest'}
test_args = (tensor([[[[0.8935, 0.9298, 0.7817, 0.5431, 0.3411],
          [0.1381, 0.3496, 0.8550, 0.4754, 0.0517],
          [0....        [0.5029, 0.7336, 0.0173, 0.3639, 0.4773],
          [0.5878, 0.2666, 0.4793, 0.3182, 0.9447]]]]), 5, 90.0, 0.5)
test_kwargs = {'border_type': 'constant', 'mode': 'nearest'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08403188, 0.25105125, 0.23579055, 0.7716586 , 0.1620....10047251, 0.5520936 ],
         [0.8716304 , 0.18106854, 0.94480085, 0.5834675 , 0.35941368]]]],
      dtype=float32)>
kernel_size = 5, angle = 45.0, direction = 1.0, border_type = 'constant', mode = 'nearest'

    def tensorflow_motion_blur(
        input, kernel_size, angle, direction, border_type="constant", mode="nearest"
    ):
        from .kernels_geometry import tensorflow_get_motion_kernel2d
        from .filter import tensorflow_filter2d
    
        kernel = tensorflow_get_motion_kernel2d(kernel_size, angle, direction, mode)
>       return tensorflow_filter2d(input, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/motion.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 9, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.    ...0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ]]]],
      dtype=float32)>
kernel = <tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=
array([[[0.        , 0.        , 0.        , 0.        , 0.        ... 0.        , 0.        ],
        [0.        , 0.        , 0.        , 0.        , 0.        ]]],
      dtype=float32)>
border_type = 'constant', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 9, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.    ...0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.    ....        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ]]]],
      dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 9, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.    ...0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.    ....        , 0.        ],
         [0.        , 0.        , 0.        , 0.        , 0.        ]]]],
      dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 9, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.   ...,
         [0.        , 0.        , 0.        , 0.        , 0.        ]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f542520cee0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f542529b880>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 9, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.   ...,
         [0.        , 0.        , 0.        , 0.        , 0.        ]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f542529b880>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f542520cb80>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f542520cee0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f5425298280>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 9, 9, 3), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        ],
         [0.     ...    ],
         [0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 1, 3), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        ]],

        [[0.   ...]],

        [[0.        , 0.        , 0.        ]],

        [[0.        , 0.        , 0.        ]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 9, 9, 3), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        ],
         [0.    ...     ]],

        [[0.        ],
         [0.        ],
         [0.        ]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f542520cee0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f542529b880>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 9, 3, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.    ...0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 3, 1), dtype=float32, numpy=
array([[[[0.        ],
         [0.        ],
         [0.      ...    ],
         [0.        ]],

        [[0.        ],
         [0.        ],
         [0.        ]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 9, 3, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.   ...0.        ],
         [0.        ]]]], dtype=float32)>, [1, 1, 1, 1], [(0, 0), (0, 0), (0, 0), (0, 0)], 'NHWC', [1, 1])
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 9 vs 3 [Op:DepthwiseConv2dNative] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.motion.motion_blur
_______________________________________________________________________________ test_unsharp_mask[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_unsharp_mask(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 5, 5),
            (3, 3),
            (1.5, 1.5),
        )
        trace_kwargs = {'border_type': 'reflect'}
        test_args = (
            torch.rand(5, 3, 5, 5),
            (5, 5),
            (2.0, 2.0),
        )
        test_kwargs = {'border_type': 'reflect'}
>       _test_function(
            kornia.filters.unsharp_mask,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unsharp_mask at 0x7f542c0d9d80>
trace_args = (tensor([[[[0.1953, 0.7564, 0.7690, 0.2295, 0.2215],
          [0.9210, 0.7547, 0.4224, 0.4930, 0.3939],
          [0....  [0.3332, 0.6251, 0.2610, 0.7268, 0.4175],
          [0.3843, 0.2879, 0.3709, 0.1222, 0.9472]]]]), (3, 3), (1.5, 1.5))
trace_kwargs = {'border_type': 'reflect'}
test_args = (tensor([[[[8.0897e-01, 4.8720e-01, 7.4594e-01, 5.1534e-01, 6.9858e-01],
          [7.9727e-01, 9.7922e-01, 3.4370e-01....6870e-01, 5.1652e-01],
          [8.1023e-01, 4.2043e-01, 2.3960e-01, 2.6375e-01, 6.7047e-01]]]]), (5, 5), (2.0, 2.0))
test_kwargs = {'border_type': 'reflect'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unsharp_mask at 0x7f542c0d9d80>
trace_args = (tensor([[[[0.1953, 0.7564, 0.7690, 0.2295, 0.2215],
          [0.9210, 0.7547, 0.4224, 0.4930, 0.3939],
          [0....  [0.3332, 0.6251, 0.2610, 0.7268, 0.4175],
          [0.3843, 0.2879, 0.3709, 0.1222, 0.9472]]]]), (3, 3), (1.5, 1.5))
trace_kwargs = {'border_type': 'reflect'}
test_args = (tensor([[[[8.0897e-01, 4.8720e-01, 7.4594e-01, 5.1534e-01, 6.9858e-01],
          [7.9727e-01, 9.7922e-01, 3.4370e-01....6870e-01, 5.1652e-01],
          [8.1023e-01, 4.2043e-01, 2.3960e-01, 2.6375e-01, 6.7047e-01]]]]), (5, 5), (2.0, 2.0))
test_kwargs = {'border_type': 'reflect'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.19525665, 0.7563527 , 0.76896363, 0.2294758 , 0.2215....7268085 , 0.41752744],
         [0.38433576, 0.28791708, 0.37090904, 0.12224853, 0.9471908 ]]]],
      dtype=float32)>
kernel_size = (3, 3), sigma = (1.5, 1.5), border_type = 'reflect'

    def tensorflow_unsharp_mask(input, kernel_size, sigma, border_type="reflect"):
        from .kernels import tensorflow_gaussian
    
>       data_blur: typing.Any = tensorflow_gaussian.gaussian_blur2d(
            input, kernel_size, sigma, border_type
        )
E       AttributeError: 'function' object has no attribute 'gaussian_blur2d'

Translated_Outputs/tensorflow_outputs/kornia/filters/unsharp.py:33: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.unsharp.unsharp_mask
___________________________________________________________________________________ test_canny[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_canny(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 4, 4),
        )
        trace_kwargs = {
            'low_threshold': 0.1,
            'high_threshold': 0.2,
            'kernel_size': (5, 5),
            'sigma': (1, 1),
            'hysteresis': True,
            'eps': 1e-6,
        }
        test_args = (
            torch.rand(5, 3, 4, 4),
        )
        test_kwargs = {
            'low_threshold': 0.2,
            'high_threshold': 0.3,
            'kernel_size': (5, 5),
            'sigma': (1, 1),
            'hysteresis': True,
            'eps': 1e-6,
        }
>       _test_function(
            kornia.filters.canny,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:302: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function canny at 0x7f542c1e48b0>
trace_args = (tensor([[[[0.1219, 0.3379, 0.7409, 0.0088],
          [0.6263, 0.7972, 0.6380, 0.1743],
          [0.7682, 0.8967, 0...., 0.5181, 0.2346, 0.6228],
          [0.9514, 0.9233, 0.1942, 0.9835],
          [0.4900, 0.0243, 0.2014, 0.9687]]]]),)
trace_kwargs = {'eps': 1e-06, 'high_threshold': 0.2, 'hysteresis': True, 'kernel_size': (5, 5), ...}
test_args = (tensor([[[[0.6643, 0.6257, 0.2110, 0.9891],
          [0.0420, 0.5813, 0.1522, 0.7869],
          [0.6842, 0.1770, 0...., 0.4519, 0.9473, 0.6143],
          [0.6105, 0.9827, 0.9024, 0.8600],
          [0.9203, 0.4892, 0.9292, 0.9360]]]]),)
test_kwargs = {'eps': 1e-06, 'high_threshold': 0.3, 'hysteresis': True, 'kernel_size': (5, 5), ...}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False
deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function canny at 0x7f542c1e48b0>
trace_args = (tensor([[[[0.1219, 0.3379, 0.7409, 0.0088],
          [0.6263, 0.7972, 0.6380, 0.1743],
          [0.7682, 0.8967, 0...., 0.5181, 0.2346, 0.6228],
          [0.9514, 0.9233, 0.1942, 0.9835],
          [0.4900, 0.0243, 0.2014, 0.9687]]]]),)
trace_kwargs = {'eps': 1e-06, 'high_threshold': 0.2, 'hysteresis': True, 'kernel_size': (5, 5), ...}
test_args = (tensor([[[[0.6643, 0.6257, 0.2110, 0.9891],
          [0.0420, 0.5813, 0.1522, 0.7869],
          [0.6842, 0.1770, 0...., 0.4519, 0.9473, 0.6143],
          [0.6105, 0.9827, 0.9024, 0.8600],
          [0.9203, 0.4892, 0.9292, 0.9360]]]]),)
test_kwargs = {'eps': 1e-06, 'high_threshold': 0.3, 'hysteresis': True, 'kernel_size': (5, 5), ...}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[0.3746352 , 0.19419625, 0.43167832, 0.2879394 ],
     ....40364268, 0.70785576, 0.42520112],
         [0.76583534, 0.60807645, 0.43920404, 0.34611833]]]],
      dtype=float32)>
low_threshold = 0.1, high_threshold = 0.2, kernel_size = (5, 5), sigma = (1, 1), hysteresis = True, eps = 1e-06

    def tensorflow_canny(
        input,
        low_threshold=0.1,
        high_threshold=0.2,
        kernel_size=(5, 5),
        sigma=(1, 1),
        hysteresis=True,
        eps=1e-06,
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..color.gray import tensorflow_rgb_to_grayscale
        from .kernels import tensorflow_gaussian
        from .sobel import tensorflow_sobel
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_atan2_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_round_frnt_
        from .kernels import tensorflow_get_canny_nms_kernel
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_long_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.non_linear_activation_functions import (
            tensorflow_threshold_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from .kernels import tensorflow_get_hysteresis_kernel
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK(
            low_threshold <= high_threshold,
            f"Invalid input thresholds. low_threshold should be smaller than the high_threshold. Got: {low_threshold}>{high_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < low_threshold < 1,
            f"Invalid low threshold. Should be in range (0, 1). Got: {low_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < high_threshold < 1,
            f"Invalid high threshold. Should be in range (0, 1). Got: {high_threshold}",
        )
        device = input.device
        dtype = input.dtype
        if tensorflow_shape_frnt_(input)[1] == 3:
            input = tensorflow_rgb_to_grayscale(input)
>       blurred: typing.Any = tensorflow_gaussian.gaussian_blur2d(input, kernel_size, sigma)
E       AttributeError: 'function' object has no attribute 'gaussian_blur2d'

Translated_Outputs/tensorflow_outputs/kornia/filters/canny.py:91: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.canny.canny
_________________________________________________________________________________ test_laplacian[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_laplacian(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(2, 4, 5, 5),
            3,
        )
        trace_kwargs = {
            'border_type': 'reflect',
            'normalized': True,
        }
        test_args = (
            torch.rand(5, 4, 5, 5),
            3,
        )
        test_kwargs = {
            'border_type': 'reflect',
            'normalized': True,
        }
>       _test_function(
            kornia.filters.laplacian,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:332: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function laplacian at 0x7f542c2bb760>
trace_args = (tensor([[[[0.6779, 0.2533, 0.0171, 0.3992, 0.1538],
          [0.3733, 0.6409, 0.5017, 0.8952, 0.8524],
          [0....0.4654],
          [0.9057, 0.4188, 0.5512, 0.6367, 0.6721],
          [0.9921, 0.2768, 0.8611, 0.2322, 0.4558]]]]), 3)
trace_kwargs = {'border_type': 'reflect', 'normalized': True}
test_args = (tensor([[[[3.2061e-01, 4.2987e-01, 1.6778e-01, 8.3240e-01, 4.0326e-01],
          [4.8368e-01, 2.0353e-01, 4.1453e-01...01, 5.4646e-01, 5.7223e-01, 3.7435e-01],
          [1.9377e-01, 3.3212e-01, 5.5848e-01, 9.6642e-01, 7.0495e-01]]]]), 3)
test_kwargs = {'border_type': 'reflect', 'normalized': True}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function laplacian at 0x7f542c2bb760>
trace_args = (tensor([[[[0.6779, 0.2533, 0.0171, 0.3992, 0.1538],
          [0.3733, 0.6409, 0.5017, 0.8952, 0.8524],
          [0....0.4654],
          [0.9057, 0.4188, 0.5512, 0.6367, 0.6721],
          [0.9921, 0.2768, 0.8611, 0.2322, 0.4558]]]]), 3)
trace_kwargs = {'border_type': 'reflect', 'normalized': True}
test_args = (tensor([[[[3.2061e-01, 4.2987e-01, 1.6778e-01, 8.3240e-01, 4.0326e-01],
          [4.8368e-01, 2.0353e-01, 4.1453e-01...01, 5.4646e-01, 5.7223e-01, 3.7435e-01],
          [1.9377e-01, 3.3212e-01, 5.5848e-01, 9.6642e-01, 7.0495e-01]]]]), 3)
test_kwargs = {'border_type': 'reflect', 'normalized': True}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 5, 5), dtype=float32, numpy=
array([[[[0.6778863 , 0.25326926, 0.01705509, 0.39923066, 0.1537....6367093 , 0.67207986],
         [0.9920996 , 0.2767784 , 0.8611468 , 0.23222339, 0.45577443]]]],
      dtype=float32)>
kernel_size = 3, border_type = 'reflect', normalized = True

    def tensorflow_laplacian(input, kernel_size, border_type="reflect", normalized=True):
        from .kernels import tensorflow_get_laplacian_kernel2d
        from .kernels import tensorflow_normalize_kernel2d
        from .filter import tensorflow_filter2d
    
        kernel = tensorflow_get_laplacian_kernel2d(
            kernel_size, device=input.device, dtype=input.dtype
        )[None, ...]
        if normalized:
            kernel = tensorflow_normalize_kernel2d(kernel)
>       return tensorflow_filter2d(input, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/laplacian.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 7, 7), dtype=float32, numpy=
array([[[[0.6409442 , 0.3732896 , 0.6409442 , 0.50172687, 0.8951...     [0.41876084, 0.90573937, 0.41876084, 0.55120313, 0.6367093 ,
          0.67207986, 0.6367093 ]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[ 0.0625,  0.0625,  0.0625],
        [ 0.0625, -0.5   ,  0.0625],
        [ 0.0625,  0.0625,  0.0625]]], dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 7, 7), dtype=float32, numpy=
array([[[[0.6409442 , 0.3732896 , 0.6409442 , 0.50172687, 0.8951...     [0.41876084, 0.90573937, 0.41876084, 0.55120313, 0.6367093 ,
          0.67207986, 0.6367093 ]]]], dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 3, 3), dtype=float32, numpy=
array([[[[ 0.0625,  0.0625,  0.0625],
         [ 0.0625, -0.5   ...0625,  0.0625,  0.0625],
         [ 0.0625, -0.5   ,  0.0625],
         [ 0.0625,  0.0625,  0.0625]]]], dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 4

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 7, 7), dtype=float32, numpy=
array([[[[0.6409442 , 0.3732896 , 0.6409442 , 0.50172687, 0.8951...     [0.41876084, 0.90573937, 0.41876084, 0.55120313, 0.6367093 ,
          0.67207986, 0.6367093 ]]]], dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 3, 3), dtype=float32, numpy=
array([[[[ 0.0625,  0.0625,  0.0625],
         [ 0.0625, -0.5   ...0625,  0.0625,  0.0625],
         [ 0.0625, -0.5   ,  0.0625],
         [ 0.0625,  0.0625,  0.0625]]]], dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 4

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 4, 7, 7), dtype=float32, numpy=
array([[[[0.6409442 , 0.3732896 , 0.6409442 , 0.50172687, 0.895...],
         [ 0.0625, -0.5   ,  0.0625],
         [ 0.0625,  0.0625,  0.0625]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f54251c6440>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5424f55120>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(2, 4, 7, 7), dtype=float32, numpy=
array([[[[0.6409442 , 0.3732896 , 0.6409442 , 0.50172687, 0.895...],
         [ 0.0625, -0.5   ,  0.0625],
         [ 0.0625,  0.0625,  0.0625]]]], dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f5424f55120>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f54251c60e0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f54251c6440>
tensorflow_asarray = <function tensorflow_asarray at 0x7f54250ad7e0>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 7, 7, 4), dtype=float32, numpy=
array([[[[0.6409442 , 0.35031986, 0.8389677 , 0.6736076 ],
     ....78520113, 0.46218443, 0.67207986],
         [0.8404825 , 0.32898062, 0.81242603, 0.6367093 ]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 1, 4), dtype=float32, numpy=
array([[[[ 0.0625,  0.0625,  0.0625,  0.0625]],

        [[ 0.06...]],

        [[ 0.0625,  0.0625,  0.0625,  0.0625]],

        [[ 0.0625,  0.0625,  0.0625,  0.0625]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 7, 7, 4), dtype=float32, numpy=
array([[[[0.6409442 , 0.35031986, 0.8389677 , 0.6736076 ],
    ...       [[ 0.0625],
         [ 0.0625],
         [ 0.0625],
         [ 0.0625]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f54251c6440>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5424f55120>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 7, 4, 7), dtype=float32, numpy=
array([[[[0.6409442 , 0.25326926, 0.6409442 , 0.62628037, 0.1855...     [0.06372517, 0.60720307, 0.06372517, 0.05461836, 0.6367093 ,
          0.23222339, 0.6367093 ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 4, 1), dtype=float32, numpy=
array([[[[ 0.0625],
         [ 0.0625],
         [ 0.0625],
    ...        [ 0.0625]],

        [[ 0.0625],
         [ 0.0625],
         [ 0.0625],
         [ 0.0625]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 7, 4, 7), dtype=float32, numpy=
array([[[[0.6409442 , 0.25326926, 0.6409442 , 0.62628037, 0.185...     [ 0.0625],
         [ 0.0625]]]], dtype=float32)>, [1, 1, 1, 1], [(0, 0), (0, 0), (0, 0), (0, 0)], 'NHWC', [1, 1])
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 7 vs 4 [Op:DepthwiseConv2dNative] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.laplacian.laplacian
_________________________________________________________________________________ test_Laplacian[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Laplacian(target_framework, mode, backend_compile):
        print("kornia.filters.Laplacian")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLaplacian = ivy.transpile(kornia.filters.Laplacian, source="torch", target=target_framework)
    
        x = torch.rand(2, 4, 5, 5)
        torch_out = kornia.filters.Laplacian(kernel_size=3)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledLaplacian(kernel_size=3)(transpiled_x)

kornia/test_filters.py:679: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Laplacian(kernel_size=3, normalized=True, border_type=reflect)
args = (<tf.Tensor: shape=(2, 4, 5, 5), dtype=float32, numpy=
array([[[[6.17340922e-01, 4.76614833e-02, 1.69402063e-01,
     ...       [5.71914792e-01, 9.56445277e-01, 8.80818188e-01,
          6.10247612e-01, 4.59000230e-01]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f5424554610, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Laplacian(kernel_size=3, normalized=True, border_type=reflect), <tf.Tensor: shape=(2, 4, 5, 5), dtype=floa...        [5.71914792e-01, 9.56445277e-01, 8.80818188e-01,
          6.10247612e-01, 4.59000230e-01]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Laplacian(kernel_size=3, normalized=True, border_type=reflect), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 4, 5, 5), dtype=float32, numpy=
array([[[[6.17340922e-01, 4.76614833e-02, 1.69402063e-01,
     ...       [5.71914792e-01, 9.56445277e-01, 8.80818188e-01,
          6.10247612e-01, 4.59000230e-01]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Laplacian(kernel_size=3, normalized=True, border_type=reflect), <tf.Tensor: shape=(2, 4, 5, 5), dtype=floa...        [5.71914792e-01, 9.56445277e-01, 8.80818188e-01,
          6.10247612e-01, 4.59000230e-01]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Laplacian(kernel_size=3, normalized=True, border_type=reflect), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 4, 5, 5), dtype=float32, numpy=
array([[[[6.17340922e-01, 4.76614833e-02, 1.69402063e-01,
     ...       [5.71914792e-01, 9.56445277e-01, 8.80818188e-01,
          6.10247612e-01, 4.59000230e-01]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 4, 5, 5), dtype=float32, numpy=
array([[[[6.17340922e-01, 4.76614833e-02, 1.69402063e-01,
      ...         [5.71914792e-01, 9.56445277e-01, 8.80818188e-01,
          6.10247612e-01, 4.59000230e-01]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Laplacian(kernel_size=3, normalized=True, border_type=reflect),)
kwargs = {'input': <tf.Tensor: shape=(2, 4, 5, 5), dtype=float32, numpy=
array([[[[6.17340922e-01, 4.76614833e-02, 1.69402063e-...        [5.71914792e-01, 9.56445277e-01, 8.80818188e-01,
          6.10247612e-01, 4.59000230e-01]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Laplacian(kernel_size=3, normalized=True, border_type=reflect)
input = <tf.Tensor: shape=(2, 4, 5, 5), dtype=float32, numpy=
array([[[[6.17340922e-01, 4.76614833e-02, 1.69402063e-01,
      ...         [5.71914792e-01, 9.56445277e-01, 8.80818188e-01,
          6.10247612e-01, 4.59000230e-01]]]], dtype=float32)>

    def call(self, input):
>       return tensorflow_laplacian(
            input, self.kernel_size, self.border_type, self.normalized
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/laplacian.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 5, 5), dtype=float32, numpy=
array([[[[6.17340922e-01, 4.76614833e-02, 1.69402063e-01,
      ...         [5.71914792e-01, 9.56445277e-01, 8.80818188e-01,
          6.10247612e-01, 4.59000230e-01]]]], dtype=float32)>
kernel_size = 3, border_type = 'reflect', normalized = True

    def tensorflow_laplacian(input, kernel_size, border_type="reflect", normalized=True):
        from .kernels import tensorflow_get_laplacian_kernel2d
        from .kernels import tensorflow_normalize_kernel2d
        from .filter import tensorflow_filter2d
    
        kernel = tensorflow_get_laplacian_kernel2d(
            kernel_size, device=input.device, dtype=input.dtype
        )[None, ...]
        if normalized:
            kernel = tensorflow_normalize_kernel2d(kernel)
>       return tensorflow_filter2d(input, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/laplacian.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 7, 7), dtype=float32, numpy=
array([[[[2.33435631e-02, 6.53117359e-01, 2.33435631e-02,
      ...5.24883270e-02,
          1.24423563e-01, 3.30980659e-01, 1.60339892e-01,
          3.30980659e-01]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[ 0.0625,  0.0625,  0.0625],
        [ 0.0625, -0.5   ,  0.0625],
        [ 0.0625,  0.0625,  0.0625]]], dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 7, 7), dtype=float32, numpy=
array([[[[2.33435631e-02, 6.53117359e-01, 2.33435631e-02,
      ...5.24883270e-02,
          1.24423563e-01, 3.30980659e-01, 1.60339892e-01,
          3.30980659e-01]]]], dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 3, 3), dtype=float32, numpy=
array([[[[ 0.0625,  0.0625,  0.0625],
         [ 0.0625, -0.5   ...0625,  0.0625,  0.0625],
         [ 0.0625, -0.5   ,  0.0625],
         [ 0.0625,  0.0625,  0.0625]]]], dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 4

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 7, 7), dtype=float32, numpy=
array([[[[2.33435631e-02, 6.53117359e-01, 2.33435631e-02,
      ...5.24883270e-02,
          1.24423563e-01, 3.30980659e-01, 1.60339892e-01,
          3.30980659e-01]]]], dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 3, 3), dtype=float32, numpy=
array([[[[ 0.0625,  0.0625,  0.0625],
         [ 0.0625, -0.5   ...0625,  0.0625,  0.0625],
         [ 0.0625, -0.5   ,  0.0625],
         [ 0.0625,  0.0625,  0.0625]]]], dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 4

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 4, 7, 7), dtype=float32, numpy=
array([[[[2.33435631e-02, 6.53117359e-01, 2.33435631e-02,
     ...],
         [ 0.0625, -0.5   ,  0.0625],
         [ 0.0625,  0.0625,  0.0625]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5424574940>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5424803880>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(2, 4, 7, 7), dtype=float32, numpy=
array([[[[2.33435631e-02, 6.53117359e-01, 2.33435631e-02,
     ...],
         [ 0.0625, -0.5   ,  0.0625],
         [ 0.0625,  0.0625,  0.0625]]]], dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f5424803880>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f54245745e0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5424574940>
tensorflow_asarray = <function tensorflow_asarray at 0x7f5424577c70>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 7, 7, 4), dtype=float32, numpy=
array([[[[2.33435631e-02, 3.67499352e-01, 6.87410176e-01,
      ....60339892e-01],
         [2.96323717e-01, 3.29309702e-02, 9.56654489e-01,
          3.30980659e-01]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 1, 4), dtype=float32, numpy=
array([[[[ 0.0625,  0.0625,  0.0625,  0.0625]],

        [[ 0.06...]],

        [[ 0.0625,  0.0625,  0.0625,  0.0625]],

        [[ 0.0625,  0.0625,  0.0625,  0.0625]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 7, 7, 4), dtype=float32, numpy=
array([[[[2.33435631e-02, 3.67499352e-01, 6.87410176e-01,
     ...       [[ 0.0625],
         [ 0.0625],
         [ 0.0625],
         [ 0.0625]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5424574940>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5424803880>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 7, 4, 7), dtype=float32, numpy=
array([[[[2.33435631e-02, 4.76614833e-02, 2.33435631e-02,
      ...4.54454839e-01,
          5.15075147e-01, 3.30980659e-01, 6.10247612e-01,
          3.30980659e-01]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 4, 1), dtype=float32, numpy=
array([[[[ 0.0625],
         [ 0.0625],
         [ 0.0625],
    ...        [ 0.0625]],

        [[ 0.0625],
         [ 0.0625],
         [ 0.0625],
         [ 0.0625]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_Laplacian.call().
E       
E       [1m{{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 7 vs 4 [Op:DepthwiseConv2dNative] name: [0m
E       
E       Arguments received by tensorflow_Laplacian.call():
E         â€¢ input=tf.Tensor(shape=(2, 4, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.Laplacian
___________________________________________________________________________________ test_Canny[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Canny(target_framework, mode, backend_compile):
        print("kornia.filters.Canny")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledCanny = ivy.transpile(kornia.filters.Canny, source="torch", target=target_framework)
    
        x = torch.rand(5, 3, 4, 4)
        torch_out_magnitude, torch_out_edges = kornia.filters.Canny()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out_magnitude, transpiled_out_edges = TranspiledCanny()(transpiled_x)

kornia/test_filters.py:713: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f5424d91870>, built=True,...ze at 0x7f54247ff0a0>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
args = (<tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.333193  , 0.19917738, 0.2529149 , 0.87814194],
    ...1314034, 0.9897526 , 0.86384124],
         [0.27907383, 0.8156676 , 0.5228969 , 0.58243734]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f5424564b20, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f5424d91870>, built=True...11314034, 0.9897526 , 0.86384124],
         [0.27907383, 0.8156676 , 0.5228969 , 0.58243734]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f5424d91870>, built=True,...ze at 0x7f54247ff0a0>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.333193  , 0.19917738, 0.2529149 , 0.87814194],
    ...1314034, 0.9897526 , 0.86384124],
         [0.27907383, 0.8156676 , 0.5228969 , 0.58243734]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f5424d91870>, built=True...11314034, 0.9897526 , 0.86384124],
         [0.27907383, 0.8156676 , 0.5228969 , 0.58243734]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f5424d91870>, built=True,...ze at 0x7f54247ff0a0>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.333193  , 0.19917738, 0.2529149 , 0.87814194],
    ...1314034, 0.9897526 , 0.86384124],
         [0.27907383, 0.8156676 , 0.5228969 , 0.58243734]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.333193  , 0.19917738, 0.2529149 , 0.87814194],
     ....11314034, 0.9897526 , 0.86384124],
         [0.27907383, 0.8156676 , 0.5228969 , 0.58243734]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f5424d91870>, built=True... at 0x7f54247ff0a0>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None),)
kwargs = {'input': <tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.333193  , 0.19917738, 0.2529149 , 0.878141...11314034, 0.9897526 , 0.86384124],
         [0.27907383, 0.8156676 , 0.5228969 , 0.58243734]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f5424d91870>, built=True,...ze at 0x7f54247ff0a0>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
input = <tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.333193  , 0.19917738, 0.2529149 , 0.87814194],
     ....11314034, 0.9897526 , 0.86384124],
         [0.27907383, 0.8156676 , 0.5228969 , 0.58243734]]]],
      dtype=float32)>

    def call(self, input):
>       return tensorflow_canny(
            input,
            self.low_threshold,
            self.high_threshold,
            self.kernel_size,
            self.sigma,
            self.hysteresis,
            self.eps,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/canny.py:205: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(5, 1, 4, 4), dtype=float32, numpy=
array([[[[0.44661522, 0.5034691 , 0.7526912 , 0.42511955],
     ....61718756, 0.59180695, 0.19126515],
         [0.34155682, 0.59603506, 0.7429683 , 0.6915516 ]]]],
      dtype=float32)>
low_threshold = 0.1, high_threshold = 0.2, kernel_size = (5, 5), sigma = (1, 1), hysteresis = True, eps = 1e-06

    def tensorflow_canny(
        input,
        low_threshold=0.1,
        high_threshold=0.2,
        kernel_size=(5, 5),
        sigma=(1, 1),
        hysteresis=True,
        eps=1e-06,
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..color.gray import tensorflow_rgb_to_grayscale
        from .kernels import tensorflow_gaussian
        from .sobel import tensorflow_sobel
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_atan2_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_round_frnt_
        from .kernels import tensorflow_get_canny_nms_kernel
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_long_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.non_linear_activation_functions import (
            tensorflow_threshold_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from .kernels import tensorflow_get_hysteresis_kernel
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK(
            low_threshold <= high_threshold,
            f"Invalid input thresholds. low_threshold should be smaller than the high_threshold. Got: {low_threshold}>{high_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < low_threshold < 1,
            f"Invalid low threshold. Should be in range (0, 1). Got: {low_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < high_threshold < 1,
            f"Invalid high threshold. Should be in range (0, 1). Got: {high_threshold}",
        )
        device = input.device
        dtype = input.dtype
        if tensorflow_shape_frnt_(input)[1] == 3:
            input = tensorflow_rgb_to_grayscale(input)
>       blurred: typing.Any = tensorflow_gaussian.gaussian_blur2d(input, kernel_size, sigma)
E       AttributeError: Exception encountered when calling tensorflow_Canny.call().
E       
E       [1m'function' object has no attribute 'gaussian_blur2d'[0m
E       
E       Arguments received by tensorflow_Canny.call():
E         â€¢ input=tf.Tensor(shape=(5, 3, 4, 4), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/filters/canny.py:95: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.Canny
__________________________________________________________________________________ test_DexiNed[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DexiNed(target_framework, mode, backend_compile):
        print("kornia.filters.DexiNed")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDexiNed = ivy.transpile(kornia.filters.DexiNed, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 320, 320)
        torch_out = kornia.filters.DexiNed(pretrained=False)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDexiNed(pretrained=False)(transpiled_x)

kornia/test_filters.py:765: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
args = (<tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.7896293 , 0.42071182, 0.84663516, ..., 0.059923...
         [0.742179  , 0.37276274, 0.15765464, ..., 0.42932588,
          0.42186213, 0.5382319 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x56221d0c8d00, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...,
         [0.742179  , 0.37276274, 0.15765464, ..., 0.42932588,
          0.42186213, 0.5382319 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.7896293 , 0.42071182, 0.84663516, ..., 0.059923...
         [0.742179  , 0.37276274, 0.15765464, ..., 0.42932588,
          0.42186213, 0.5382319 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...,
         [0.742179  , 0.37276274, 0.15765464, ..., 0.42932588,
          0.42186213, 0.5382319 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.7896293 , 0.42071182, 0.84663516, ..., 0.059923...
         [0.742179  , 0.37276274, 0.15765464, ..., 0.42932588,
          0.42186213, 0.5382319 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
),)
kwargs = {'x': <tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.7896293 , 0.42071182, 0.84663516, ..., 0.0...,
         [0.742179  , 0.37276274, 0.15765464, ..., 0.42932588,
          0.42186213, 0.5382319 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
x = <tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.7896293 , 0.42071182, 0.84663516, ..., 0.0599238...],
         [0.742179  , 0.37276274, 0.15765464, ..., 0.42932588,
          0.42186213, 0.5382319 ]]]], dtype=float32)>

    def call(self, x):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..core._backend import concatenate
    
        block_1 = self.block_1(x)
        block_1_side = self.side_1(block_1)
        block_2 = self.block_2(block_1)
        block_2_down = self.maxpool(block_2)
>       block_2_add = block_2_down + block_1_side

Translated_Outputs/tensorflow_outputs/kornia/filters/dexined.py:1117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 64, 160, 80), dtype=float32, numpy=
array([[[[ 0.01707065,  0.05597848,  0.03492394, ...,  0.03...3795e-02, -6.03503213e-05, ...,
           8.70139003e-02,  6.98981732e-02,  8.02099183e-02]]]],
      dtype=float32)>)
kwargs = {}
arg = <tf.Tensor: shape=(1, 128, 80, 80), dtype=float32, numpy=
array([[[[-3.70801473e-03, -1.61816403e-02, -3.01603936e-02,...83795e-02, -6.03503213e-05, ...,
           8.70139003e-02,  6.98981732e-02,  8.02099183e-02]]]],
      dtype=float32)>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_DexiNed.call().
E       
E       [1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1,64,160,80] vs. [1,128,80,80] [Op:AddV2] name: [0m
E       
E       Arguments received by tensorflow_DexiNed.call():
E         â€¢ x=tf.Tensor(shape=(1, 3, 320, 320), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.DexiNed
__________________________________________________________________________________ test_BoxBlur[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_BoxBlur(target_framework, mode, backend_compile):
        print("kornia.filters.BoxBlur")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledBoxBlur = ivy.transpile(kornia.filters.BoxBlur, source="torch", target=target_framework)
    
        x = torch.rand(2, 3, 5, 5)
        torch_out = kornia.filters.BoxBlur(kernel_size=(3, 3))(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledBoxBlur(kernel_size=(3, 3))(transpiled_x)

kornia/test_filters.py:816: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_BoxBlur(kernel_size=(3, 3), border_type=reflect, separable=False)
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.7829118 , 0.77937526, 0.5617248 , 0.72171205, 0.092...226349 , 0.7782743 ],
         [0.5319567 , 0.21630287, 0.88760173, 0.9104591 , 0.3423612 ]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x56221d02f3d0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_BoxBlur(kernel_size=(3, 3), border_type=reflect, separable=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=f...6226349 , 0.7782743 ],
         [0.5319567 , 0.21630287, 0.88760173, 0.9104591 , 0.3423612 ]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_BoxBlur(kernel_size=(3, 3), border_type=reflect, separable=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.7829118 , 0.77937526, 0.5617248 , 0.72171205, 0.092...226349 , 0.7782743 ],
         [0.5319567 , 0.21630287, 0.88760173, 0.9104591 , 0.3423612 ]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_BoxBlur(kernel_size=(3, 3), border_type=reflect, separable=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=f...6226349 , 0.7782743 ],
         [0.5319567 , 0.21630287, 0.88760173, 0.9104591 , 0.3423612 ]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_BoxBlur(kernel_size=(3, 3), border_type=reflect, separable=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.7829118 , 0.77937526, 0.5617248 , 0.72171205, 0.092...226349 , 0.7782743 ],
         [0.5319567 , 0.21630287, 0.88760173, 0.9104591 , 0.3423612 ]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.7829118 , 0.77937526, 0.5617248 , 0.72171205, 0.0920....6226349 , 0.7782743 ],
         [0.5319567 , 0.21630287, 0.88760173, 0.9104591 , 0.3423612 ]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_BoxBlur(kernel_size=(3, 3), border_type=reflect, separable=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.7829118 , 0.77937526, 0.5617248 , 0.721712...6226349 , 0.7782743 ],
         [0.5319567 , 0.21630287, 0.88760173, 0.9104591 , 0.3423612 ]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_BoxBlur(kernel_size=(3, 3), border_type=reflect, separable=False)
input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.7829118 , 0.77937526, 0.5617248 , 0.72171205, 0.0920....6226349 , 0.7782743 ],
         [0.5319567 , 0.21630287, 0.88760173, 0.9104591 , 0.3423612 ]]]],
      dtype=float32)>

    def call(self, input):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from .filter import tensorflow_filter2d_separable
        from .filter import tensorflow_filter2d
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        if self.separable:
            return tensorflow_filter2d_separable(
                input, self.kernel_x, self.kernel_y, self.border_type
            )
>       return tensorflow_filter2d(input, self.kernel, self.border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/blur.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 7, 7), dtype=float32, numpy=
array([[[[0.6416794 , 0.20272946, 0.6416794 , 0.5111331 , 0.7778...     [0.11775273, 0.5218188 , 0.11775273, 0.6756073 , 0.6226349 ,
          0.7782743 , 0.6226349 ]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0.11111111, 0.11111111, 0.11111111],
        [0.11111111, 0.11111111, 0.11111111],
        [0.11111111, 0.11111111, 0.11111111]]], dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 7, 7), dtype=float32, numpy=
array([[[[0.6416794 , 0.20272946, 0.6416794 , 0.5111331 , 0.7778...     [0.11775273, 0.5218188 , 0.11775273, 0.6756073 , 0.6226349 ,
          0.7782743 , 0.6226349 ]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111],
         [0.11111...1111],
         [0.11111111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 7, 7), dtype=float32, numpy=
array([[[[0.6416794 , 0.20272946, 0.6416794 , 0.5111331 , 0.7778...     [0.11775273, 0.5218188 , 0.11775273, 0.6756073 , 0.6226349 ,
          0.7782743 , 0.6226349 ]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111],
         [0.11111...1111],
         [0.11111111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 3, 7, 7), dtype=float32, numpy=
array([[[[0.6416794 , 0.20272946, 0.6416794 , 0.5111331 , 0.777...11111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5400a477f0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f541415b1c0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(2, 3, 7, 7), dtype=float32, numpy=
array([[[[0.6416794 , 0.20272946, 0.6416794 , 0.5111331 , 0.777...11111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f541415b1c0>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f5400a47490>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5400a477f0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f5400a82ef0>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 7, 7, 3), dtype=float32, numpy=
array([[[[0.6416794 , 0.381427  , 0.27004576],
         [0.20272...349 ],
         [0.5737787 , 0.53354794, 0.7782743 ],
         [0.7865272 , 0.25435007, 0.6226349 ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 1, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111]],

        [[0.111...]],

        [[0.11111111, 0.11111111, 0.11111111]],

        [[0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:379: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 7, 7, 3), dtype=float32, numpy=
array([[[[0.6416794 , 0.381427  , 0.27004576],
         [0.2027...11111]],

        [[0.11111111],
         [0.11111111],
         [0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5400a477f0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f541415b1c0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 7, 3, 7), dtype=float32, numpy=
array([[[[0.6416794 , 0.77937526, 0.6416794 , 0.7782293 , 0.2087...     [0.68230736, 0.06638777, 0.68230736, 0.19317436, 0.6226349 ,
          0.9104591 , 0.6226349 ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 3, 1), dtype=float32, numpy=
array([[[[0.11111111],
         [0.11111111],
         [0.111111...1111],
         [0.11111111]],

        [[0.11111111],
         [0.11111111],
         [0.11111111]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_BoxBlur.call().
E       
E       [1m{{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 7 vs 3 [Op:DepthwiseConv2dNative] name: [0m
E       
E       Arguments received by tensorflow_BoxBlur.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:268: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.BoxBlur
______________________________________________________________________________ test_GaussianBlur2d[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_GaussianBlur2d(target_framework, mode, backend_compile):
        print("kornia.filters.GaussianBlur2d")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledGaussianBlur2d = ivy.transpile(kornia.filters.GaussianBlur2d, source="torch", target=target_framework)
    
        x = torch.rand(2, 3, 5, 5)
        torch_out = kornia.filters.GaussianBlur2d((3, 3), (1.5, 1.5))(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledGaussianBlur2d((3, 3), (1.5, 1.5))(transpiled_x)

kornia/test_filters.py:867: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_GaussianBlur2d(kernel_size=(3, 3), sigma=(1.5, 1.5), border_type=reflect, separable=True)
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[4.64812219e-01, 5.37721515e-01, 8.10274482e-02,
     ...       [6.66405261e-01, 9.28466141e-01, 5.09453893e-01,
          4.05834317e-02, 8.42636287e-01]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f5426071240, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_GaussianBlur2d(kernel_size=(3, 3), sigma=(1.5, 1.5), border_type=reflect, separable=True), <tf.Tensor: sha...        [6.66405261e-01, 9.28466141e-01, 5.09453893e-01,
          4.05834317e-02, 8.42636287e-01]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_GaussianBlur2d(kernel_size=(3, 3), sigma=(1.5, 1.5), border_type=reflect, separable=True), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[4.64812219e-01, 5.37721515e-01, 8.10274482e-02,
     ...       [6.66405261e-01, 9.28466141e-01, 5.09453893e-01,
          4.05834317e-02, 8.42636287e-01]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_GaussianBlur2d(kernel_size=(3, 3), sigma=(1.5, 1.5), border_type=reflect, separable=True), <tf.Tensor: sha...        [6.66405261e-01, 9.28466141e-01, 5.09453893e-01,
          4.05834317e-02, 8.42636287e-01]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_GaussianBlur2d(kernel_size=(3, 3), sigma=(1.5, 1.5), border_type=reflect, separable=True), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[4.64812219e-01, 5.37721515e-01, 8.10274482e-02,
     ...       [6.66405261e-01, 9.28466141e-01, 5.09453893e-01,
          4.05834317e-02, 8.42636287e-01]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[4.64812219e-01, 5.37721515e-01, 8.10274482e-02,
      ...         [6.66405261e-01, 9.28466141e-01, 5.09453893e-01,
          4.05834317e-02, 8.42636287e-01]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_GaussianBlur2d(kernel_size=(3, 3), sigma=(1.5, 1.5), border_type=reflect, separable=True),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[4.64812219e-01, 5.37721515e-01, 8.10274482e-...        [6.66405261e-01, 9.28466141e-01, 5.09453893e-01,
          4.05834317e-02, 8.42636287e-01]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_GaussianBlur2d(kernel_size=(3, 3), sigma=(1.5, 1.5), border_type=reflect, separable=True)
input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[4.64812219e-01, 5.37721515e-01, 8.10274482e-02,
      ...         [6.66405261e-01, 9.28466141e-01, 5.09453893e-01,
          4.05834317e-02, 8.42636287e-01]]]], dtype=float32)>

    def call(self, input):
>       return tensorflow_gaussian_blur2d(
            input, self.kernel_size, self.sigma, self.border_type, self.separable
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/gaussian.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[4.64812219e-01, 5.37721515e-01, 8.10274482e-02,
      ...         [6.66405261e-01, 9.28466141e-01, 5.09453893e-01,
          4.05834317e-02, 8.42636287e-01]]]], dtype=float32)>
kernel_size = (3, 3), sigma = <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1.5, 1.5]], dtype=float32)>, border_type = 'reflect', separable = True

    def tensorflow_gaussian_blur2d(
        input, kernel_size, sigma, border_type="reflect", separable=True
    ):
        from ..core._backend import tensor
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .kernels import tensorflow__unpack_2d_ks
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .kernels import tensorflow_get_gaussian_kernel1d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from .filter import tensorflow_filter2d_separable
        from .kernels import tensorflow_get_gaussian_kernel2d
        from .filter import tensorflow_filter2d
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        if isinstance(sigma, (tuple,)):
            sigma = tensor([sigma], device=input.device, dtype=input.dtype)
        else:
            tensorflow_KORNIA_CHECK_IS_TENSOR(sigma)
            sigma = tensorflow_to_frnt_(sigma, device=input.device, dtype=input.dtype)
        if separable:
            ky, kx = tensorflow__unpack_2d_ks(kernel_size)
            bs = tensorflow_shape_frnt_(sigma)[0]
            kernel_x = tensorflow_get_gaussian_kernel1d(
                kx, tensorflow_view_frnt_(sigma[:, 1], bs, 1)
            )
            kernel_y = tensorflow_get_gaussian_kernel1d(
                ky, tensorflow_view_frnt_(sigma[:, 0], bs, 1)
            )
>           out = tensorflow_filter2d_separable(input, kernel_x, kernel_y, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/gaussian.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[4.64812219e-01, 5.37721515e-01, 8.10274482e-02,
      ...         [6.66405261e-01, 9.28466141e-01, 5.09453893e-01,
          4.05834317e-02, 8.42636287e-01]]]], dtype=float32)>
kernel_x = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.30780134, 0.38439736, 0.30780134]], dtype=float32)>
kernel_y = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.30780134, 0.38439736, 0.30780134]], dtype=float32)>, border_type = 'reflect', normalized = False, padding = 'same'

    def tensorflow_filter2d_separable(
        input, kernel_x, kernel_y, border_type="reflect", normalized=False, padding="same"
    ):
>       out_x = tensorflow_filter2d(
            input, kernel_x[..., None, :], border_type, normalized, padding
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[5.37721515e-01, 4.64812219e-01, 5.37721515e-01,
      ...9.28466141e-01,
          5.09453893e-01, 4.05834317e-02, 8.42636287e-01,
          4.05834317e-02]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 1, 3), dtype=float32, numpy=array([[[0.30780134, 0.38439736, 0.30780134]]], dtype=float32)>, border_type = 'reflect', normalized = False, padding = 'same'
behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[5.37721515e-01, 4.64812219e-01, 5.37721515e-01,
      ...9.28466141e-01,
          5.09453893e-01, 4.05834317e-02, 8.42636287e-01,
          4.05834317e-02]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 1, 3), dtype=float32, numpy=
array([[[[0.30780134, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]]], dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[5.37721515e-01, 4.64812219e-01, 5.37721515e-01,
      ...9.28466141e-01,
          5.09453893e-01, 4.05834317e-02, 8.42636287e-01,
          4.05834317e-02]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 1, 3), dtype=float32, numpy=
array([[[[0.30780134, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]]], dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[5.37721515e-01, 4.64812219e-01, 5.37721515e-01,
     ...4, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5400a80430>
tensorflow_get_item = <function tensorflow_get_item at 0x7f54240cc550>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[5.37721515e-01, 4.64812219e-01, 5.37721515e-01,
     ...4, 0.38439736, 0.30780134]]],


       [[[0.30780134, 0.38439736, 0.30780134]]]], dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f54240cc550>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f5400a800d0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5400a80430>
tensorflow_asarray = <function tensorflow_asarray at 0x7f5400a90700>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 5, 7, 3), dtype=float32, numpy=
array([[[[5.37721515e-01, 5.79065084e-03, 6.52878165e-01],
     ...1, 6.43048048e-01, 8.42636287e-01],
         [1.29031599e-01, 1.38247073e-01, 4.05834317e-02]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(1, 3, 1, 3), dtype=float32, numpy=
array([[[[0.30780134, 0.30780134, 0.30780134]],

        [[0.38439736, 0.38439736, 0.38439736]],

        [[0.30780134, 0.30780134, 0.30780134]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:379: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 5, 7, 3), dtype=float32, numpy=
array([[[[5.37721515e-01, 5.79065084e-03, 6.52878165e-01],
    ...39736]],

        [[0.30780134],
         [0.30780134],
         [0.30780134]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5400a80430>
tensorflow_get_item = <function tensorflow_get_item at 0x7f54240cc550>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 7, 3, 5), dtype=float32, numpy=
array([[[[5.37721515e-01, 8.51485252e-01, 7.13287175e-01,
      ...         [1.54583633e-01, 1.96781635e-01, 7.09853172e-02,
          6.75137401e-01, 4.05834317e-02]]]], dtype=float32)>
filters = <tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
array([[[[0.30780134],
         [0.30780134],
         [0.307801...9736],
         [0.38439736]],

        [[0.30780134],
         [0.30780134],
         [0.30780134]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_GaussianBlur2d.call().
E       
E       [1m{{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 5 vs 3 [Op:DepthwiseConv2dNative] name: [0m
E       
E       Arguments received by tensorflow_GaussianBlur2d.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:268: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.GaussianBlur2d
________________________________________________________________________________ test_GuidedBlur[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_GuidedBlur(target_framework, mode, backend_compile):
        print("kornia.filters.GuidedBlur")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledGuidedBlur = ivy.transpile(kornia.filters.GuidedBlur, source="torch", target=target_framework)
    
        guidance = torch.rand(2, 3, 5, 5)
        x = torch.rand(2, 3, 5, 5)
        torch_out = kornia.filters.GuidedBlur(3, 0.1)(guidance, x)
    
        transpiled_guidance = _nest_torch_tensor_to_new_framework(guidance, target_framework)
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledGuidedBlur(3, 0.1)(transpiled_guidance, transpiled_x)

kornia/test_filters.py:886: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_GuidedBlur(kernel_size=3, eps=0.1, border_type=reflect, subsample=1)
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.78991926, 0.81181526, 0.33220357, 0.53175324, 0.140...        [6.18445873e-03, 4.45231199e-01, 4.63403761e-01,
          5.21380842e-01, 8.63990307e-01]]]], dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x562215c7b500, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_GuidedBlur(kernel_size=3, eps=0.1, border_type=reflect, subsample=1), <tf.Tensor: shape=(2, 3, 5, 5), dtyp...        [6.18445873e-03, 4.45231199e-01, 4.63403761e-01,
          5.21380842e-01, 8.63990307e-01]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_GuidedBlur(kernel_size=3, eps=0.1, border_type=reflect, subsample=1), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.78991926, 0.81181526, 0.33220357, 0.53175324, 0.140...        [6.18445873e-03, 4.45231199e-01, 4.63403761e-01,
          5.21380842e-01, 8.63990307e-01]]]], dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_GuidedBlur(kernel_size=3, eps=0.1, border_type=reflect, subsample=1), <tf.Tensor: shape=(2, 3, 5, 5), dtyp...        [6.18445873e-03, 4.45231199e-01, 4.63403761e-01,
          5.21380842e-01, 8.63990307e-01]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_GuidedBlur(kernel_size=3, eps=0.1, border_type=reflect, subsample=1), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.78991926, 0.81181526, 0.33220357, 0.53175324, 0.140...        [6.18445873e-03, 4.45231199e-01, 4.63403761e-01,
          5.21380842e-01, 8.63990307e-01]]]], dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.78991926, 0.81181526, 0.33220357, 0.53175324, 0.1403....7917735 , 0.9132323 ],
         [0.21664315, 0.73251307, 0.98564667, 0.31811666, 0.93644506]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (guidance, input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_GuidedBlur(kernel_size=3, eps=0.1, border_type=reflect, subsample=1),)
kwargs = {'guidance': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.78991926, 0.81181526, 0.33220357, 0.531...        [6.18445873e-03, 4.45231199e-01, 4.63403761e-01,
          5.21380842e-01, 8.63990307e-01]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_GuidedBlur(kernel_size=3, eps=0.1, border_type=reflect, subsample=1)
guidance = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.78991926, 0.81181526, 0.33220357, 0.53175324, 0.1403....7917735 , 0.9132323 ],
         [0.21664315, 0.73251307, 0.98564667, 0.31811666, 0.93644506]]]],
      dtype=float32)>
input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[5.27226985e-01, 4.91946876e-01, 9.77167249e-01,
      ...         [6.18445873e-03, 4.45231199e-01, 4.63403761e-01,
          5.21380842e-01, 8.63990307e-01]]]], dtype=float32)>

    def call(self, guidance, input):
>       return tensorflow_guided_blur(
            guidance,
            input,
            self.kernel_size,
            self.eps,
            self.border_type,
            self.subsample,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/guided.py:253: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

guidance = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.78991926, 0.81181526, 0.33220357, 0.53175324, 0.1403....7917735 , 0.9132323 ],
         [0.21664315, 0.73251307, 0.98564667, 0.31811666, 0.93644506]]]],
      dtype=float32)>
input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[5.27226985e-01, 4.91946876e-01, 9.77167249e-01,
      ...         [6.18445873e-03, 4.45231199e-01, 4.63403761e-01,
          5.21380842e-01, 8.63990307e-01]]]], dtype=float32)>
kernel_size = 3, eps = 0.1, border_type = 'reflect', subsample = 1

    def tensorflow_guided_blur(
        guidance, input, kernel_size, eps, border_type="reflect", subsample=1
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(guidance)
        tensorflow_KORNIA_CHECK_SHAPE(guidance, ["B", "C", "H", "W"])
        if input is not guidance:
            tensorflow_KORNIA_CHECK_IS_TENSOR(input)
            tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
            tensorflow_KORNIA_CHECK(
                tensorflow_shape_frnt_(guidance)[0] == tensorflow_shape_frnt_(input)[0]
                and tensorflow_shape_frnt_(guidance)[-2:]
                == tensorflow_shape_frnt_(input)[-2:],
                "guidance and input should have the same batch size and spatial dimensions",
            )
        if tensorflow_shape_frnt_(guidance)[1] == 1:
            return tensorflow__guided_blur_grayscale_guidance(
                guidance, input, kernel_size, eps, border_type, subsample
            )
        else:
>           return tensorflow__guided_blur_multichannel_guidance(
                guidance, input, kernel_size, eps, border_type, subsample
            )

Translated_Outputs/tensorflow_outputs/kornia/filters/guided.py:228: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

guidance = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.78991926, 0.81181526, 0.33220357, 0.53175324, 0.1403....7917735 , 0.9132323 ],
         [0.21664315, 0.73251307, 0.98564667, 0.31811666, 0.93644506]]]],
      dtype=float32)>
input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[5.27226985e-01, 4.91946876e-01, 9.77167249e-01,
      ...         [6.18445873e-03, 4.45231199e-01, 4.63403761e-01,
          5.21380842e-01, 8.63990307e-01]]]], dtype=float32)>
kernel_size = (3, 3), eps = 0.1, border_type = 'reflect', subsample = 1

    def tensorflow__guided_blur_multichannel_guidance(
        guidance, input, kernel_size, eps, border_type="reflect", subsample=1
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from .blur import tensorflow_box_blur
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flatten_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_eye_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_diag_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_new_full_frnt_
        from ...ivy.functional.frontends.torch.linalg import tensorflow_solve_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_interpolate_frnt,
        )
        from ...ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_einsum_frnt,
        )
    
        guidance_sub, input_sub, kernel_size = tensorflow__preprocess_fast_guided_blur(
            guidance, input, kernel_size, subsample
        )
        B, C, H, W = tensorflow_shape_frnt_(guidance_sub)
        mean_I = tensorflow_permute_frnt_(
>           tensorflow_box_blur(guidance_sub, kernel_size, border_type), 0, 2, 3, 1
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/guided.py:128: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.78991926, 0.81181526, 0.33220357, 0.53175324, 0.1403....7917735 , 0.9132323 ],
         [0.21664315, 0.73251307, 0.98564667, 0.31811666, 0.93644506]]]],
      dtype=float32)>
kernel_size = (3, 3), border_type = 'reflect', separable = False

    def tensorflow_box_blur(input, kernel_size, border_type="reflect", separable=False):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from .kernels import tensorflow__unpack_2d_ks
        from .kernels import tensorflow_get_box_kernel1d
        from .filter import tensorflow_filter2d_separable
        from .kernels import tensorflow_get_box_kernel2d
        from .filter import tensorflow_filter2d
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        if separable:
            ky, kx = tensorflow__unpack_2d_ks(kernel_size)
            kernel_y = tensorflow_get_box_kernel1d(
                ky, device=input.device, dtype=input.dtype
            )
            kernel_x = tensorflow_get_box_kernel1d(
                kx, device=input.device, dtype=input.dtype
            )
            out = tensorflow_filter2d_separable(input, kernel_x, kernel_y, border_type)
        else:
            kernel = tensorflow_get_box_kernel2d(
                kernel_size, device=input.device, dtype=input.dtype
            )
>           out = tensorflow_filter2d(input, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/blur.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 7, 7), dtype=float32, numpy=
array([[[[0.951696  , 0.46140736, 0.951696  , 0.8867351 , 0.8159...     [0.38709754, 0.6948516 , 0.38709754, 0.32382274, 0.7917735 ,
          0.9132323 , 0.7917735 ]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0.11111111, 0.11111111, 0.11111111],
        [0.11111111, 0.11111111, 0.11111111],
        [0.11111111, 0.11111111, 0.11111111]]], dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 7, 7), dtype=float32, numpy=
array([[[[0.951696  , 0.46140736, 0.951696  , 0.8867351 , 0.8159...     [0.38709754, 0.6948516 , 0.38709754, 0.32382274, 0.7917735 ,
          0.9132323 , 0.7917735 ]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111],
         [0.11111...1111],
         [0.11111111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 7, 7), dtype=float32, numpy=
array([[[[0.951696  , 0.46140736, 0.951696  , 0.8867351 , 0.8159...     [0.38709754, 0.6948516 , 0.38709754, 0.32382274, 0.7917735 ,
          0.9132323 , 0.7917735 ]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111],
         [0.11111...1111],
         [0.11111111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 3, 7, 7), dtype=float32, numpy=
array([[[[0.951696  , 0.46140736, 0.951696  , 0.8867351 , 0.815...11111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5400a74550>
tensorflow_get_item = <function tensorflow_get_item at 0x7f54006620e0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(2, 3, 7, 7), dtype=float32, numpy=
array([[[[0.951696  , 0.46140736, 0.951696  , 0.8867351 , 0.815...11111, 0.11111111, 0.11111111],
         [0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f54006620e0>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f54241539a0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5400a74550>
tensorflow_asarray = <function tensorflow_asarray at 0x7f5400a764d0>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 7, 7, 3), dtype=float32, numpy=
array([[[[0.951696  , 0.8410155 , 0.29978245],
         [0.46140...735 ],
         [0.7591718 , 0.6780902 , 0.9132323 ],
         [0.98346686, 0.21984088, 0.7917735 ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 1, 3), dtype=float32, numpy=
array([[[[0.11111111, 0.11111111, 0.11111111]],

        [[0.111...]],

        [[0.11111111, 0.11111111, 0.11111111]],

        [[0.11111111, 0.11111111, 0.11111111]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:379: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 7, 7, 3), dtype=float32, numpy=
array([[[[0.951696  , 0.8410155 , 0.29978245],
         [0.4614...11111]],

        [[0.11111111],
         [0.11111111],
         [0.11111111]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f5400a74550>
tensorflow_get_item = <function tensorflow_get_item at 0x7f54006620e0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 7, 3, 7), dtype=float32, numpy=
array([[[[0.951696  , 0.81181526, 0.951696  , 0.8835073 , 0.2416...     [0.6134531 , 0.95701206, 0.6134531 , 0.01118243, 0.7917735 ,
          0.31811666, 0.7917735 ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 3, 1), dtype=float32, numpy=
array([[[[0.11111111],
         [0.11111111],
         [0.111111...1111],
         [0.11111111]],

        [[0.11111111],
         [0.11111111],
         [0.11111111]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_GuidedBlur.call().
E       
E       [1m{{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 7 vs 3 [Op:DepthwiseConv2dNative] name: [0m
E       
E       Arguments received by tensorflow_GuidedBlur.call():
E         â€¢ guidance=tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)
E         â€¢ input=tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:268: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.GuidedBlur
________________________________________________________________________________ test_MotionBlur[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_MotionBlur(target_framework, mode, backend_compile):
        print("kornia.filters.MotionBlur")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledMotionBlur = ivy.transpile(kornia.filters.MotionBlur, source="torch", target=target_framework)
    
        x = torch.rand(2, 3, 5, 7)
        torch_out = kornia.filters.MotionBlur(3, 35., 0.5)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledMotionBlur(3, 35., 0.5)(transpiled_x)

kornia/test_filters.py:922: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MotionBlur (kernel_size=3, angle=35.0, direction=0.5, border_type=constant)
args = (<tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[0.9636489 , 0.16938835, 0.8144732 , 0.23582834, 0.202...   [0.53535086, 0.1021629 , 0.6238943 , 0.01061916, 0.09078407,
          0.20457214, 0.64503634]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x56221d0685d0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MotionBlur (kernel_size=3, angle=35.0, direction=0.5, border_type=constant), <tf.Tensor: shape=(2, 3, 5, 7...    [0.53535086, 0.1021629 , 0.6238943 , 0.01061916, 0.09078407,
          0.20457214, 0.64503634]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MotionBlur (kernel_size=3, angle=35.0, direction=0.5, border_type=constant), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[0.9636489 , 0.16938835, 0.8144732 , 0.23582834, 0.202...   [0.53535086, 0.1021629 , 0.6238943 , 0.01061916, 0.09078407,
          0.20457214, 0.64503634]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MotionBlur (kernel_size=3, angle=35.0, direction=0.5, border_type=constant), <tf.Tensor: shape=(2, 3, 5, 7...    [0.53535086, 0.1021629 , 0.6238943 , 0.01061916, 0.09078407,
          0.20457214, 0.64503634]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MotionBlur (kernel_size=3, angle=35.0, direction=0.5, border_type=constant), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[0.9636489 , 0.16938835, 0.8144732 , 0.23582834, 0.202...   [0.53535086, 0.1021629 , 0.6238943 , 0.01061916, 0.09078407,
          0.20457214, 0.64503634]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[0.9636489 , 0.16938835, 0.8144732 , 0.23582834, 0.2029...     [0.53535086, 0.1021629 , 0.6238943 , 0.01061916, 0.09078407,
          0.20457214, 0.64503634]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MotionBlur (kernel_size=3, angle=35.0, direction=0.5, border_type=constant),)
kwargs = {'x': <tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[0.9636489 , 0.16938835, 0.8144732 , 0.23582834, ...    [0.53535086, 0.1021629 , 0.6238943 , 0.01061916, 0.09078407,
          0.20457214, 0.64503634]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MotionBlur (kernel_size=3, angle=35.0, direction=0.5, border_type=constant)
x = <tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[0.9636489 , 0.16938835, 0.8144732 , 0.23582834, 0.2029...     [0.53535086, 0.1021629 , 0.6238943 , 0.01061916, 0.09078407,
          0.20457214, 0.64503634]]]], dtype=float32)>

    def call(self, x):
>       return tensorflow_motion_blur(
            x, self.kernel_size, self.angle, self.direction, self.border_type
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/motion.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 5, 7), dtype=float32, numpy=
array([[[[0.9636489 , 0.16938835, 0.8144732 , 0.23582834, 0.2029...     [0.53535086, 0.1021629 , 0.6238943 , 0.01061916, 0.09078407,
          0.20457214, 0.64503634]]]], dtype=float32)>
kernel_size = 3, angle = 35.0, direction = 0.5, border_type = 'constant', mode = 'nearest'

    def tensorflow_motion_blur(
        input, kernel_size, angle, direction, border_type="constant", mode="nearest"
    ):
        from .kernels_geometry import tensorflow_get_motion_kernel2d
        from .filter import tensorflow_filter2d
    
        kernel = tensorflow_get_motion_kernel2d(kernel_size, angle, direction, mode)
>       return tensorflow_filter2d(input, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/motion.py:408: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 7, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.    ...0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ]]]],
      dtype=float32)>
kernel = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0.        , 0.        , 0.16666667],
        [0.        , 0.33333334, 0.        ],
        [0.5       , 0.        , 0.        ]]], dtype=float32)>
border_type = 'constant', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 7, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.    ...0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.16666667],
         [0.     ...6667],
         [0.        , 0.33333334, 0.        ],
         [0.5       , 0.        , 0.        ]]]], dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 7, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.    ...0.        , 0.        , 0.        ,
          0.        , 0.        , 0.        , 0.        ]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 3, 3), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.16666667],
         [0.     ...6667],
         [0.        , 0.33333334, 0.        ],
         [0.5       , 0.        , 0.        ]]]], dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 3, 7, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.   ...     , 0.33333334, 0.        ],
         [0.5       , 0.        , 0.        ]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f540062b640>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5400651870>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(2, 3, 7, 9), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.   ...     , 0.33333334, 0.        ],
         [0.5       , 0.        , 0.        ]]]], dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f5400651870>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f540062bd90>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f540062b640>
tensorflow_asarray = <function tensorflow_asarray at 0x7f540072b910>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 7, 9, 3), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        ],
         [0.     ...    ],
         [0.        , 0.        , 0.        ],
         [0.        , 0.        , 0.        ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 1, 3), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        ]],

        [[0.   ...]],

        [[0.        , 0.        , 0.        ]],

        [[0.        , 0.        , 0.        ]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:379: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 7, 9, 3), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        ],
         [0.    ...     ]],

        [[0.        ],
         [0.        ],
         [0.        ]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f540062b640>
tensorflow_get_item = <function tensorflow_get_item at 0x7f5400651870>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 9, 3, 7), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.    ...     [0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.        ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(3, 3, 3, 1), dtype=float32, numpy=
array([[[[0.        ],
         [0.        ],
         [0.      ...    ],
         [0.        ]],

        [[0.        ],
         [0.        ],
         [0.        ]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_MotionBlur.call().
E       
E       [1m{{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 7 vs 3 [Op:DepthwiseConv2dNative] name: [0m
E       
E       Arguments received by tensorflow_MotionBlur.call():
E         â€¢ x=tf.Tensor(shape=(2, 3, 5, 7), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:268: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.MotionBlur
________________________________________________________________________________ test_UnsharpMask[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_UnsharpMask(target_framework, mode, backend_compile):
        print("kornia.filters.UnsharpMask")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledUnsharpMask = ivy.transpile(kornia.filters.UnsharpMask, source="torch", target=target_framework)
    
        x = torch.rand(2, 3, 5, 5)
        torch_out = kornia.filters.UnsharpMask((3, 3), (1.5, 1.5))(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledUnsharpMask((3, 3), (1.5, 1.5))(transpiled_x)

kornia/test_filters.py:939: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask()
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.50332826, 0.55376005, 0.22372091, 0.75427157, 0.685...213766 , 0.70719427],
         [0.690262  , 0.65474695, 0.80982655, 0.34639758, 0.35020494]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f54006651b0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_UnsharpMask(), <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.50332826, 0.55376005, 0.2...8213766 , 0.70719427],
         [0.690262  , 0.65474695, 0.80982655, 0.34639758, 0.35020494]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.50332826, 0.55376005, 0.22372091, 0.75427157, 0.685...213766 , 0.70719427],
         [0.690262  , 0.65474695, 0.80982655, 0.34639758, 0.35020494]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_UnsharpMask(), <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.50332826, 0.55376005, 0.2...8213766 , 0.70719427],
         [0.690262  , 0.65474695, 0.80982655, 0.34639758, 0.35020494]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.50332826, 0.55376005, 0.22372091, 0.75427157, 0.685...213766 , 0.70719427],
         [0.690262  , 0.65474695, 0.80982655, 0.34639758, 0.35020494]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.50332826, 0.55376005, 0.22372091, 0.75427157, 0.6858....8213766 , 0.70719427],
         [0.690262  , 0.65474695, 0.80982655, 0.34639758, 0.35020494]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_UnsharpMask(),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.50332826, 0.55376005, 0.22372091, 0.754271...8213766 , 0.70719427],
         [0.690262  , 0.65474695, 0.80982655, 0.34639758, 0.35020494]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask()
input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.50332826, 0.55376005, 0.22372091, 0.75427157, 0.6858....8213766 , 0.70719427],
         [0.690262  , 0.65474695, 0.80982655, 0.34639758, 0.35020494]]]],
      dtype=float32)>

    def call(self, input):
>       return tensorflow_unsharp_mask(
            input, self.kernel_size, self.sigma, self.border_type
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/unsharp.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.50332826, 0.55376005, 0.22372091, 0.75427157, 0.6858....8213766 , 0.70719427],
         [0.690262  , 0.65474695, 0.80982655, 0.34639758, 0.35020494]]]],
      dtype=float32)>
kernel_size = (3, 3), sigma = (1.5, 1.5), border_type = 'reflect'

    def tensorflow_unsharp_mask(input, kernel_size, sigma, border_type="reflect"):
        from .kernels import tensorflow_gaussian
    
>       data_blur: typing.Any = tensorflow_gaussian.gaussian_blur2d(
            input, kernel_size, sigma, border_type
        )
E       AttributeError: Exception encountered when calling tensorflow_UnsharpMask.call().
E       
E       [1m'function' object has no attribute 'gaussian_blur2d'[0m
E       
E       Arguments received by tensorflow_UnsharpMask.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/filters/unsharp.py:37: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.UnsharpMask
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_filters.py::test_blur_pool2d[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:lo...
FAILED kornia/test_filters.py::test_box_blur[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:local...
FAILED kornia/test_filters.py::test_gaussian_blur2d[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/jo...
FAILED kornia/test_filters.py::test_guided_blur[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:lo...
FAILED kornia/test_filters.py::test_max_blur_pool2d[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/jo...
FAILED kornia/test_filters.py::test_motion_blur[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:lo...
FAILED kornia/test_filters.py::test_unsharp_mask[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'gaussian_blur2d'
FAILED kornia/test_filters.py::test_canny[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'gaussian_blur2d'
FAILED kornia/test_filters.py::test_laplacian[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:loca...
FAILED kornia/test_filters.py::test_Laplacian[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_Laplacian.call().
FAILED kornia/test_filters.py::test_Canny[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_Canny.call().
FAILED kornia/test_filters.py::test_DexiNed[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_DexiNed.call().
FAILED kornia/test_filters.py::test_BoxBlur[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_BoxBlur.call().
FAILED kornia/test_filters.py::test_GaussianBlur2d[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_GaussianBlur2d...
FAILED kornia/test_filters.py::test_GuidedBlur[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_GuidedBlur.call().
FAILED kornia/test_filters.py::test_MotionBlur[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_MotionBlur.call().
FAILED kornia/test_filters.py::test_UnsharpMask[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_UnsharpMask.call().
============================================================================== 17 failed, 27 passed in 807.72s (0:13:27) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

kornia/geometry/test_ransac.py .                                                                                                                                                                 [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 1 passed in 218.82s (0:03:38) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 15 items

kornia/test_contrib.py F...FFF.F...F..                                                                                                                                                           [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_compute_padding[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_compute_padding(target_framework, mode, backend_compile):
        trace_args = (
            (4, 3),
            (3, 3),
        )
        trace_kwargs = {}
        test_args = (
            (8, 5),
            (4, 4),
        )
        test_kwargs = {}
>       _test_function(
            kornia.contrib.compute_padding,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_contrib.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function compute_padding at 0x7f583e59bb50>, trace_args = ((4, 3), (3, 3)), trace_kwargs = {}, test_args = ((8, 5), (4, 4)), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function compute_padding at 0x7f583e59bb50>, trace_args = ((4, 3), (3, 3)), trace_kwargs = {}, test_args = ((8, 5), (4, 4)), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

original_size = (4, 3), window_size = (3, 3), stride = (3, 3)

    def tensorflow_compute_padding(original_size, window_size, stride=None):
        from ...torch.nn.modules.utils import _pair
    
        original_size = cast(Tuple[int, int], _pair(original_size))
        window_size = cast(Tuple[int, int], _pair(window_size))
        if stride is None:
            stride = window_size
        stride = cast(Tuple[int, int], _pair(stride))
        remainder_vertical = (original_size[0] - window_size[0]) % stride[0]
        remainder_horizontal = (original_size[1] - window_size[1]) % stride[1]
        if remainder_vertical != 0:
            vertical_padding = stride[0] - remainder_vertical
        else:
            vertical_padding = 0
        if remainder_horizontal != 0:
            horizontal_padding = stride[1] - remainder_horizontal
        else:
            horizontal_padding = 0
        if vertical_padding % 2 == 0:
            top_padding = bottom_padding = vertical_padding // 2
        else:
            top_padding = vertical_padding // 2
            bottom_padding = ceil(vertical_padding / 2)
        if horizontal_padding % 2 == 0:
            left_padding = right_padding = horizontal_padding // 2
        else:
            left_padding = horizontal_padding // 2
            right_padding = ceil(horizontal_padding / 2)
        padding = (
            int(top_padding),
            int(bottom_padding),
            int(left_padding),
            int(right_padding),
        )
>       return cast(FullPadType, padding)
E       NameError: name 'FullPadType' is not defined

Translated_Outputs/tensorflow_outputs/kornia/contrib/extract_patches.py:66: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.extract_patches.compute_padding
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/ivy/ivy/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.
  warnings.warn(
______________________________________________________________________________ test_diamond_square[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_diamond_square(target_framework, mode, backend_compile):
        trace_args = ((1, 1, 8, 8),)
        trace_kwargs = {
            "roughness": 0.5,
            "random_scale": 1.0,
            "normalize_range": (0.0, 1.0),
            "random_fn": torch.ones,
        }
        test_args = ((5, 1, 8, 8),)
        test_kwargs = {
            "roughness": 0.7,
            "random_scale": 0.9,
            "normalize_range": (-1.0, 1.0),
            "random_fn": torch.ones,
        }
>       _test_function(
            kornia.contrib.diamond_square,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_contrib.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function diamond_square at 0x7f583e59af80>, trace_args = ((1, 1, 8, 8),)
trace_kwargs = {'normalize_range': (0.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f585995b900>, 'random_scale': 1.0, 'roughness': 0.5}, test_args = ((5, 1, 8, 8),)
test_kwargs = {'normalize_range': (-1.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f585995b900>, 'random_scale': 0.9, 'roughness': 0.7}, target = 'tensorflow'
backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function diamond_square at 0x7f583e59af80>, trace_args = ((1, 1, 8, 8),)
trace_kwargs = {'normalize_range': (0.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f585995b900>, 'random_scale': 1.0, 'roughness': 0.5}, test_args = ((5, 1, 8, 8),)
test_kwargs = {'normalize_range': (-1.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f585995b900>, 'random_scale': 0.9, 'roughness': 0.7}, target = 'tensorflow'
backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

output_size = (1, 1, 8, 8), roughness = <tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[0.5]]]], dtype=float32)>
random_scale = <tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[1.]]]], dtype=float32)>, random_fn = <built-in method ones of type object at 0x7f585995b900>, normalize_range = (0.0, 1.0)
device = None, dtype = None

    def tensorflow_diamond_square(
        output_size,
        roughness=0.5,
        random_scale=1.0,
        random_fn=tensorflow_rand_frnt,
        normalize_range=None,
        device=None,
        dtype=None,
    ):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..enhance.normalize import tensorflow_normalize_min_max
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
    
        tensorflow_KORNIA_CHECK(len(output_size) == 4, "output_size must be (B,C,H,W)")
        if not isinstance(random_scale, (tensorflow.Tensor, tensorflow.Variable)):
            random_scale = tensorflow_to_frnt_(
                tensorflow.convert_to_tensor([[[[random_scale]]]]), device, dtype
            )
            random_scale = tensorflow_expand_frnt_(
                random_scale, [output_size[0] * output_size[1], 1, 1, 1]
            )
        else:
            tensorflow_KORNIA_CHECK_IS_TENSOR(random_scale)
            random_scale = tensorflow_view_frnt_(random_scale, -1, 1, 1, 1)
            random_scale = tensorflow_expand_frnt_(
                random_scale, [output_size[0], output_size[1], 1, 1]
            )
            random_scale = tensorflow_reshape_frnt_(random_scale, [-1, 1, 1, 1])
        if not isinstance(roughness, (tensorflow.Tensor, tensorflow.Variable)):
            roughness = tensorflow_to_frnt_(
                tensorflow.convert_to_tensor([[[[roughness]]]]), device, dtype
            )
            roughness = tensorflow_expand_frnt_(
                roughness, [output_size[0] * output_size[1], 1, 1, 1]
            )
        else:
            roughness = tensorflow_view_frnt_(roughness, -1, 1, 1, 1)
            roughness = tensorflow_expand_frnt_(
                roughness, [output_size[0], output_size[1], 1, 1]
            )
            roughness = tensorflow_reshape_frnt_(roughness, [-1, 1, 1, 1])
        width, height = output_size[-2:][0], output_size[-2:][1]
        num_samples: typing.Any = 1
        for x in output_size[:-2]:
            num_samples = num_samples * x
        p2_width: typing.Any = 2 ** math.ceil(math.log2(width - 1)) + 1
        p2_height: typing.Any = 2 ** math.ceil(math.log2(height - 1)) + 1
        recursion_depth: typing.Any = int(
            min(math.log2(p2_width - 1) - 1, math.log2(p2_height - 1) - 1)
        )
        seed_width: typing.Any = (p2_width - 1) // 2**recursion_depth + 1
        seed_height: typing.Any = (p2_height - 1) // 2**recursion_depth + 1
>       img: typing.Any = random_scale * tensorflow__diamond_square_seed(
            num_samples, seed_width, seed_height, random_fn, device, dtype
        )

Translated_Outputs/tensorflow_outputs/kornia/contrib/diamond_square.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[1.]]]], dtype=float32)>, tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]]))
kwargs = {}, arg = tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]])

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[1.]]]], dtype=float32)>, tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]]))
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arrays_and_dtypes = [<class 'numpy.float32'>, tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]])]

    def _result_type(*arrays_and_dtypes):
      """Returns the resulting type given a set of arrays."""
    
      def preprocess_float(x):
        if is_prefer_float32():
          if isinstance(x, float):
            return np.float32(x)
          elif isinstance(x, complex):
            return np.complex64(x)
        return x
    
      arrays_and_dtypes = [preprocess_float(x) for x in arrays_and_dtypes]
>     dtype = np.result_type(*arrays_and_dtypes)
E     TypeError: Cannot interpret 'tensor([[[[0.3333, 1.0000, 0.3333],
E               [1.0000, 0.3333, 1.0000],
E               [0.3333, 1.0000, 0.3333]]]])' as a data type

/opt/fw/tensorflow/tensorflow/python/ops/numpy_ops/np_dtypes.py:190: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.diamond_square.diamond_square
_______________________________________________________________________________ test_EdgeDetector[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_EdgeDetector(target_framework, mode, backend_compile):
        print("kornia.contrib.EdgeDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledEdgeDetector = ivy.transpile(kornia.contrib.EdgeDetector, source="torch", target=target_framework)
    
        torch_detector = kornia.contrib.EdgeDetector()
>       transpiled_detector = TranspiledEdgeDetector()

kornia/test_contrib.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_EdgeDetector()

    def __init__(self):
        from ..filters.dexined import tensorflow_DexiNed
    
        self.super___init__(
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.model = tensorflow_DexiNed(pretrained=True)

Translated_Outputs/tensorflow_outputs/kornia/contrib/edge_detection.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
args = (), kwargs = {'pretrained': True}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
pretrained = True

    @tensorflow_store_config_info
    def __init__(self, pretrained):
        from ...torch.nn.modules.pooling import tensorflow_MaxPool2d
    
        self.super___init__(
            pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.block_1 = tensorflow_DoubleConvBlock(3, 32, 64, stride=2)
        self.block_2 = tensorflow_DoubleConvBlock(64, 128, use_act=False)
        self.dblock_3 = tensorflow__DenseBlock(2, 128, 256)
        self.dblock_4 = tensorflow__DenseBlock(3, 256, 512)
        self.dblock_5 = tensorflow__DenseBlock(3, 512, 512)
        self.dblock_6 = tensorflow__DenseBlock(3, 512, 256)
        self.maxpool = tensorflow_MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.side_1 = tensorflow_SingleConvBlock(64, 128, 2)
        self.side_2 = tensorflow_SingleConvBlock(128, 256, 2)
        self.side_3 = tensorflow_SingleConvBlock(256, 512, 2)
        self.side_4 = tensorflow_SingleConvBlock(512, 512, 1)
        self.side_5 = tensorflow_SingleConvBlock(512, 256, 1)
        self.pre_dense_2 = tensorflow_SingleConvBlock(128, 256, 2)
        self.pre_dense_3 = tensorflow_SingleConvBlock(128, 256, 1)
        self.pre_dense_4 = tensorflow_SingleConvBlock(256, 512, 1)
        self.pre_dense_5 = tensorflow_SingleConvBlock(512, 512, 1)
        self.pre_dense_6 = tensorflow_SingleConvBlock(512, 256, 1)
        self.up_block_1 = tensorflow_UpConvBlock(64, 1)
        self.up_block_2 = tensorflow_UpConvBlock(128, 1)
        self.up_block_3 = tensorflow_UpConvBlock(256, 2)
        self.up_block_4 = tensorflow_UpConvBlock(512, 3)
        self.up_block_5 = tensorflow_UpConvBlock(512, 4)
        self.up_block_6 = tensorflow_UpConvBlock(256, 4)
        self.block_cat = tensorflow_SingleConvBlock(6, 1, stride=1, use_bs=False)
        if pretrained:
>           self.load_from_file(url)

Translated_Outputs/tensorflow_outputs/kornia/filters/dexined.py:1096: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
path_file = 'http://cmp.felk.cvut.cz/~mishkdmy/models/DexiNed_BIPED_10.pth'

    def load_from_file(self, path_file):
        from ..utils.helpers import tensorflow_map_location_to_cpu
    
>       pretrained_dict = torch.hub.load_state_dict_from_url(
            path_file, map_location=tensorflow_map_location_to_cpu
        )
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/filters/dexined.py:1103: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.EdgeDetector
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "http://cmp.felk.cvut.cz/~mishkdmy/models/DexiNed_BIPED_10.pth" to /root/.cache/torch/hub/checkpoints/DexiNed_BIPED_10.pth

  0%|          | 0.00/135M [00:00<?, ?B/s]
  0%|          | 128k/135M [00:00<08:54, 264kB/s]
  0%|          | 256k/135M [00:00<05:24, 433kB/s]
  0%|          | 512k/135M [00:00<02:58, 789kB/s]
  1%|          | 896k/135M [00:00<01:49, 1.28MB/s]
  1%|â–         | 1.75M/135M [00:01<00:53, 2.58MB/s]
  3%|â–Ž         | 3.50M/135M [00:01<00:26, 5.16MB/s]
  5%|â–         | 6.50M/135M [00:01<00:14, 9.33MB/s]
  7%|â–‹         | 9.38M/135M [00:01<00:11, 11.9MB/s]
  9%|â–‰         | 12.4M/135M [00:01<00:09, 14.0MB/s]
 11%|â–ˆâ–        | 15.4M/135M [00:01<00:08, 15.5MB/s]
 14%|â–ˆâ–Ž        | 18.2M/135M [00:02<00:07, 16.3MB/s]
 16%|â–ˆâ–Œ        | 21.2M/135M [00:02<00:06, 17.1MB/s]
 18%|â–ˆâ–Š        | 24.1M/135M [00:02<00:06, 17.4MB/s]
 20%|â–ˆâ–ˆ        | 27.1M/135M [00:02<00:06, 17.8MB/s]
 22%|â–ˆâ–ˆâ–       | 30.0M/135M [00:02<00:06, 17.7MB/s]
 24%|â–ˆâ–ˆâ–       | 32.8M/135M [00:03<00:06, 17.5MB/s]
 27%|â–ˆâ–ˆâ–‹       | 35.8M/135M [00:03<00:05, 17.9MB/s]
 29%|â–ˆâ–ˆâ–‰       | 38.8M/135M [00:03<00:05, 18.2MB/s]
 31%|â–ˆâ–ˆâ–ˆ       | 41.8M/135M [00:03<00:05, 18.4MB/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 44.4M/135M [00:03<00:05, 17.9MB/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 47.4M/135M [00:03<00:05, 18.2MB/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 50.4M/135M [00:04<00:04, 18.4MB/s]
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 53.2M/135M [00:04<00:04, 18.3MB/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 56.2M/135M [00:04<00:04, 18.5MB/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 59.2M/135M [00:04<00:04, 18.6MB/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 62.1M/135M [00:04<00:04, 18.5MB/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 65.1M/135M [00:04<00:03, 18.3MB/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 68.0M/135M [00:05<00:03, 18.2MB/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 70.9M/135M [00:05<00:03, 18.1MB/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 73.8M/135M [00:05<00:03, 18.1MB/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 76.8M/135M [00:05<00:03, 18.4MB/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 79.8M/135M [00:05<00:03, 18.5MB/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 82.8M/135M [00:05<00:02, 18.5MB/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 85.8M/135M [00:06<00:02, 18.7MB/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 88.8M/135M [00:06<00:02, 18.7MB/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 91.8M/135M [00:06<00:02, 18.8MB/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 94.8M/135M [00:06<00:02, 18.8MB/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 97.6M/135M [00:06<00:02, 18.8MB/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 101M/135M [00:06<00:01, 18.7MB/s] 
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 104M/135M [00:07<00:01, 18.6MB/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 107M/135M [00:07<00:01, 18.7MB/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 110M/135M [00:07<00:01, 18.0MB/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 113M/135M [00:07<00:01, 18.3MB/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 116M/135M [00:07<00:01, 17.9MB/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 118M/135M [00:07<00:00, 17.9MB/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 122M/135M [00:08<00:00, 18.2MB/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 124M/135M [00:08<00:00, 18.4MB/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 127M/135M [00:08<00:00, 17.7MB/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 130M/135M [00:08<00:00, 18.0MB/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 133M/135M [00:08<00:00, 18.0MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135M/135M [00:08<00:00, 16.1MB/s]
_______________________________________________________________________________ test_FaceDetector[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_FaceDetector(target_framework, mode, backend_compile):
        print("kornia.contrib.FaceDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledFaceDetector = ivy.transpile(kornia.contrib.FaceDetector, source="torch", target=target_framework)
    
        torch_detector = kornia.contrib.FaceDetector()
>       transpiled_detector = TranspiledFaceDetector()

kornia/test_contrib.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_FaceDetector(), top_k = 5000, confidence_threshold = 0.3, nms_threshold = 0.3, keep_top_k = 750

    def __init__(
        self, top_k=5000, confidence_threshold=0.3, nms_threshold=0.3, keep_top_k=750
    ):
        from ..geometry.bbox import tensorflow_nms
    
        self.super___init__(
            top_k=top_k,
            confidence_threshold=confidence_threshold,
            nms_threshold=nms_threshold,
            keep_top_k=keep_top_k,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.top_k = top_k
        self.confidence_threshold = confidence_threshold
        self.nms_threshold = nms_threshold
        self.keep_top_k = keep_top_k
        self.config = {
            "name": "YuFaceDetectNet",
            "min_sizes": [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],
            "steps": [8, 16, 32, 64],
            "variance": [0.1, 0.2],
            "clip": False,
        }
        self.min_sizes = [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]]
        self.steps = [8, 16, 32, 64]
        self.variance = [0.1, 0.2]
        self.clip = False
>       self.model = tensorflow_YuFaceDetectNet("test", pretrained=True)

Translated_Outputs/tensorflow_outputs/kornia/contrib/face_detection.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_YuFaceDetectNet(
  (model0): tensorflow_Conv_head(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...nv2): tensorflow_ConvDPUnit(
        (conv1): KerasConv2D()
        (conv2): KerasDepthWiseConv2D()
      )
    )
  )
)
args = ('test',), kwargs = {'pretrained': True}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_YuFaceDetectNet(
  (model0): tensorflow_Conv_head(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...nv2): tensorflow_ConvDPUnit(
        (conv1): KerasConv2D()
        (conv2): KerasDepthWiseConv2D()
      )
    )
  )
)
phase = 'test', pretrained = True

    @tensorflow_store_config_info
    def __init__(self, phase, pretrained):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.init import tensorflow_xavier_normal_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_data_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_fill__frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_normal__frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_zero__frnt_
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            phase,
            pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.phase = phase
        self.num_classes = 2
        self.model0 = tensorflow_Conv_head(3, 16, 16)
        self.model1 = tensorflow_Conv4layerBlock(16, 64)
        self.model2 = tensorflow_Conv4layerBlock(64, 64)
        self.model3 = tensorflow_Conv4layerBlock(64, 64)
        self.model4 = tensorflow_Conv4layerBlock(64, 64)
        self.model5 = tensorflow_Conv4layerBlock(64, 64)
        self.model6 = tensorflow_Conv4layerBlock(64, 64)
        self.head = tensorflow_Sequential(
            tensorflow_Conv4layerBlock(64, 3 * (14 + 2 + 1), False),
            tensorflow_Conv4layerBlock(64, 2 * (14 + 2 + 1), False),
            tensorflow_Conv4layerBlock(64, 2 * (14 + 2 + 1), False),
            tensorflow_Conv4layerBlock(64, 3 * (14 + 2 + 1), False),
        )
        if self.phase == "train":
            for m in self.modules():
                if isinstance(m, (KerasConv2D,)):
                    if m.bias is not None:
                        m.weight = tensorflow_xavier_normal_(
                            tensorflow_data_frnt_(m.weight)
                        )
                        m.bias = tensorflow_fill__frnt_(
                            tensorflow_data_frnt_(m.bias), 0.02
                        )
                    else:
                        m.weight = tensorflow_normal__frnt_(
                            tensorflow_data_frnt_(m.weight), 0, 0.01
                        )
                elif isinstance(m, (KerasBatchNorm2D,)):
                    m.weight = tensorflow_fill__frnt_(
                        tensorflow_data_frnt_(m.weight), 1
                    )
                    m.bias = tensorflow_zero__frnt_(tensorflow_data_frnt_(m.bias))
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                url, map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/contrib/face_detection.py:680: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.FaceDetector
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/kornia/data/raw/main/yunet_final.pth" to /root/.cache/torch/hub/checkpoints/yunet_final.pth

  0%|          | 0.00/396k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396k/396k [00:00<00:00, 67.8MB/s]
__________________________________________________________________________________ test_KMeans[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KMeans(target_framework, mode, backend_compile):
        print("kornia.contrib.KMeans")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKMeans = ivy.transpile(kornia.contrib.KMeans, source="torch", target=target_framework)
    
        torch_kmeans = kornia.contrib.KMeans(3, None, 10e-4, 100, 0)
        transpiled_kmeans = TranspiledKMeans(3, None, 10e-4, 100, 0)
    
        torch_x1 = torch.rand((1000, 5))
        torch_x2 = torch.rand((10, 5))
        transpiled_x1 = _array_to_new_backend(torch_x1, target_framework)
        transpiled_x2 = _array_to_new_backend(torch_x2, target_framework)
    
        torch_kmeans.fit(torch_x1)
        torch_predictions = torch_kmeans.predict(torch_x2)
    
>       transpiled_kmeans.fit(transpiled_x1)

kornia/test_contrib.py:245: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Translated_Outputs.tensorflow_outputs.kornia.contrib.kmeans.tensorflow_KMeans object at 0x7f58385f3bb0>
X = <tf.Tensor: shape=(1000, 5), dtype=float32, numpy=
array([[0.4962566 , 0.7682218 , 0.08847743, 0.13203049, 0.30742282]...5, 0.49248123, 0.4287302 ],
       [0.3786084 , 0.04217941, 0.28761953, 0.5166052 , 0.11317676]],
      dtype=float32)>

    def fit(self, X):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_argmin_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_nonzero_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_index_select_frnt,
        )
        from ...ivy.functional.frontends.torch.random_sampling import (
            tensorflow_randint_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
    
        tensorflow_KORNIA_CHECK_SHAPE(X, ["N", "D"])
        if self._cluster_centers is None:
            self._cluster_centers = self._initialise_cluster_centers(
                X, self.num_clusters
            )
        else:
            tensorflow_KORNIA_CHECK(
                tensorflow_shape_frnt_(X)[1]
                == tensorflow_shape_frnt_(self._cluster_centers)[1],
                f"Dimensions at position 1 of X and cluster_centers do not match.                 {tensorflow_shape_frnt_(X)[1]} != {tensorflow_shape_frnt_(self._cluster_centers)[1]}",
            )
        current_centers = self._cluster_centers
        previous_centers: typing.Any = None
        iteration: typing.Any = 0
        while True:
            distance: typing.Any = self._pairwise_euclidean_distance(X, current_centers)
            cluster_assignment = tensorflow_argmin_frnt_(distance, -1)
            previous_centers = tensorflow_clone_frnt_(current_centers)
            for index in range(self.num_clusters):
                selected = tensorflow_squeeze_frnt_(
                    tensorflow_nonzero_frnt(cluster_assignment == index)
                )
                selected = tensorflow_index_select_frnt(X, 0, selected)
                if tensorflow_shape_frnt_(selected)[0] == 0:
                    selected = X[tensorflow_randint_frnt(len(X), (1,), device=X.device)]
>               current_centers[index] = tensorflow_mean_frnt_(selected, dim=0)
E               TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment

Translated_Outputs/tensorflow_outputs/kornia/contrib/kmeans.py:143: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.KMeans
_______________________________________________________________________________ test_ImageStitcher[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ImageStitcher(target_framework, mode, backend_compile):
        print("kornia.contrib.ImageStitcher")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledLoFTR = ivy.transpile(kornia.feature.LoFTR, source="torch", target=target_framework)

kornia/test_contrib.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.feature.loftr.loftr.LoFTR'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, line 77)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.ImageStitcher
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_contrib.py::test_compute_padding[tensorflow-s2s-False] - NameError: name 'FullPadType' is not defined
FAILED kornia/test_contrib.py::test_diamond_square[tensorflow-s2s-False] - TypeError: Cannot interpret 'tensor([[[[0.3333, 1.0000, 0.3333],
FAILED kornia/test_contrib.py::test_EdgeDetector[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_contrib.py::test_FaceDetector[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_contrib.py::test_KMeans[tensorflow-s2s-False] - TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment
FAILED kornia/test_contrib.py::test_ImageStitcher[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, lin...
=============================================================================== 6 failed, 9 passed in 723.00s (0:12:02) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 7 items

kornia/test_morphology.py .......                                                                                                                                                                [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 7 passed in 119.98s (0:01:59) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/geometry/test_line.py ..                                                                                                                                                                  [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
===================================================================================== 2 passed in 92.33s (0:01:32) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 69 items

kornia/test_color.py ...........F...........F......F.............F...........F.....F.....F                                                                                                       [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________________ test_rgb_to_hls[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_rgb_to_hls(target_framework, mode, backend_compile):
        # Note: We test this function with requires_grad=True,
        # because otherwise we simply get an empty_like tensor
        # with garbage values on each run leading to test failures
        trace_args = (
            torch.rand(1, 3, 4, 5).requires_grad_(True),
        )
        trace_kwargs = {'eps': 1e-8}
        test_args = (
            torch.rand(5, 3, 4, 5).requires_grad_(True),
        )
        test_kwargs = {'eps': 1e-8}
>       _test_function(
            kornia.color.rgb_to_hls,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_color.py:291: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_hls at 0x7fc8f7a2dea0>
trace_args = (tensor([[[[3.1455e-01, 3.0261e-01, 5.9530e-02, 9.0832e-01, 6.0174e-01],
          [3.4461e-01, 3.3433e-01, 8.5404e-02...1, 4.6485e-01],
          [7.5383e-01, 7.3471e-01, 1.8451e-01, 2.4195e-01, 6.9957e-01]]]],
       requires_grad=True),)
trace_kwargs = {'eps': 1e-08}
test_args = (tensor([[[[0.0558, 0.8030, 0.9444, 0.6842, 0.3188],
          [0.4130, 0.1538, 0.5460, 0.4615, 0.0227],
          [0.... [0.8426, 0.7474, 0.2702, 0.7790, 0.9144],
          [0.6478, 0.3267, 0.3183, 0.7128, 0.5731]]]], requires_grad=True),)
test_kwargs = {'eps': 1e-08}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_hls at 0x7fc8f7a2dea0>
trace_args = (tensor([[[[3.1455e-01, 3.0261e-01, 5.9530e-02, 9.0832e-01, 6.0174e-01],
          [3.4461e-01, 3.3433e-01, 8.5404e-02...1, 4.6485e-01],
          [7.5383e-01, 7.3471e-01, 1.8451e-01, 2.4195e-01, 6.9957e-01]]]],
       requires_grad=True),)
trace_kwargs = {'eps': 1e-08}
test_args = (tensor([[[[0.0558, 0.8030, 0.9444, 0.6842, 0.3188],
          [0.4130, 0.1538, 0.5460, 0.4615, 0.0227],
          [0.... [0.8426, 0.7474, 0.2702, 0.7790, 0.9144],
          [0.6478, 0.3267, 0.3183, 0.7128, 0.5731]]]], requires_grad=True),)
test_kwargs = {'eps': 1e-08}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(1, 3, 4, 5), dtype=float32, numpy=
array([[[[3.1455159e-01, 3.0261445e-01, 5.9530497e-02, 9.0832365...01],
         [7.5383306e-01, 7.3470974e-01, 1.8450934e-01, 2.4195123e-01,
          6.9956946e-01]]]], dtype=float32)>
eps = 1e-08

    def tensorflow_rgb_to_hls(image, eps=1e-08):
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.pointwise_ops import sub
        from ..core._backend import where
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_requires_grad_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_empty_like_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_add_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_mul_frnt
    
        if not isinstance(image, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(image)}")
        if len(tensorflow_shape_frnt_(image)) < 3 or tensorflow_shape_frnt_(image)[-3] != 3:
            raise ValueError(
                f"Input size must have a shape of (*, 3, H, W). Got {tensorflow_shape_frnt_(image)}"
            )
        _RGB2HSL_IDX = tensor(
            [[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype
        )
        _img_max: typing.Any = tensorflow_max_frnt_(image, -3)
        maxc = _img_max[0]
        imax = _img_max[1]
        minc: typing.Any = tensorflow_min_frnt_(image, -3)[0]
        if tensorflow_requires_grad_frnt_(image):
            l_ = maxc + minc
            s = maxc - minc
            h = l_
            image_hls = l_
        else:
            image_hls = tensorflow_empty_like_frnt(image)
            h, l_, s = (
                image_hls[..., 0, :, :],
                image_hls[..., 1, :, :],
                image_hls[..., 2, :, :],
            )
            tensorflow_add_frnt(maxc, minc, out=l_)
            sub(maxc, minc, out=s)
        im = image / tensorflow_unsqueeze_frnt_(s + eps, -3)
        s = s / (where(l_ < 1.0, l_, 2.0 - l_) + eps)
        l_ = l_ / 2
        r, g, b = im[..., 0, :, :], im[..., 1, :, :], im[..., 2, :, :]
        cond = imax[..., None, :, :] == _RGB2HSL_IDX
        if tensorflow_requires_grad_frnt_(image):
            h = (g - b) % 6 * cond[..., 0, :, :]
        else:
>           tensorflow_mul_frnt((g - b) % 6, cond[..., 0, :, :], out=h)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[ 1.,  4.,  2.,  0.,  4.],
        [ 2.,  2., -0.,  2.,  2.],
        [ 2.,  4.,  0.,  0.,  0.],
        [-0.,  2.,  4.,  4.,  2.]]], dtype=float32)>
other = <tf.Tensor: shape=(1, 4, 5), dtype=bool, numpy=
array([[[ True, False, False,  True, False],
        [False, False, False, False, False],
        [ True, False, False, False, False],
        [False,  True, False,  True,  True]]])>

    def tensorflow_mul_frnt(input, other, *, out=None):
        from ...backends.tensorflow.elementwise import tensorflow_multiply
    
>       return tensorflow_multiply(input, other, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/pointwise_ops.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x1 = <tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[ 1.,  4.,  2.,  0.,  4.],
        [ 2.,  2., -0.,  2.,  2.],
        [ 2.,  4.,  0.,  0.,  0.],
        [-0.,  2.,  4.,  4.,  2.]]], dtype=float32)>
x2 = <tf.Tensor: shape=(1, 4, 5), dtype=bool, numpy=
array([[[ True, False, False,  True, False],
        [False, False, False, False, False],
        [ True, False, False, False, False],
        [False,  True, False,  True,  True]]])>

    def tensorflow_multiply(
        x1: Union[float, tensorflow.Tensor, tensorflow.Variable],
        x2: Union[float, tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.data_type import tensorflow_default_dtype_bknd
        from ...ivy.general import tensorflow_is_array_bknd
        from .creation import tensorflow_asarray
    
        oirg_x1 = x1
        oirg_x2 = x2
        try:
            dtype = (
                x1.dtype
                if hasattr(x1, "dtype")
                else x2.dtype
                if hasattr(x2, "dtype")
                else tensorflow_default_dtype_bknd()
            )
            if not tensorflow_is_array_bknd(x1):
                x1 = tensorflow_asarray(x1, dtype=dtype)
            if not tensorflow_is_array_bknd(x2):
                x2 = tensorflow_asarray(x2, dtype=dtype)
        except:
            x1 = oirg_x1
            x2 = oirg_x2
>       return tensorflow.math.multiply(x1, x2)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/elementwise.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[ 1.,  4.,  2.,  0.,  4.],
        [ 2.,  2., -0.,  2.,  2...se, False, False, False],
        [ True, False, False, False, False],
        [False,  True, False,  True,  True]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      if not ops.is_auto_dtype_conversion_enabled():
>       return op(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/ops/weak_tensor_ops.py:142: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[ 1.,  4.,  2.,  0.,  4.],
        [ 2.,  2., -0.,  2.,  2...se, False, False, False],
        [ True, False, False, False, False],
        [False,  True, False,  True,  True]]])>)
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a bool tensor [Op:Mul] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.hls.rgb_to_hls
_______________________________________________________________________________ test_rgb_to_yuv420[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_rgb_to_yuv420(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 4, 6),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(5, 3, 4, 6),
        )
        test_kwargs = {}
>       _test_function(
            kornia.color.rgb_to_yuv420,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_color.py:559: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_yuv420 at 0x7fc8f7a2fbe0>
trace_args = (tensor([[[[0.0070, 0.6732, 0.3852, 0.1270, 0.8509, 0.2648],
          [0.8396, 0.3974, 0.0969, 0.2330, 0.2040, 0.6792...     [0.9922, 0.5270, 0.5268, 0.9549, 0.9726, 0.6397],
          [0.1492, 0.3102, 0.9850, 0.8557, 0.7680, 0.5133]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[2.1144e-01, 1.6802e-01, 3.6393e-01, 2.4383e-01, 6.5737e-01,
           2.2277e-01],
          [6.9444e-01,...       8.6505e-01],
          [3.0595e-01, 9.0156e-01, 2.9094e-01, 6.7145e-01, 4.4958e-01,
           9.6273e-01]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_yuv420 at 0x7fc8f7a2fbe0>
trace_args = (tensor([[[[0.0070, 0.6732, 0.3852, 0.1270, 0.8509, 0.2648],
          [0.8396, 0.3974, 0.0969, 0.2330, 0.2040, 0.6792...     [0.9922, 0.5270, 0.5268, 0.9549, 0.9726, 0.6397],
          [0.1492, 0.3102, 0.9850, 0.8557, 0.7680, 0.5133]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[2.1144e-01, 1.6802e-01, 3.6393e-01, 2.4383e-01, 6.5737e-01,
           2.2277e-01],
          [6.9444e-01,...       8.6505e-01],
          [3.0595e-01, 9.0156e-01, 2.9094e-01, 6.7145e-01, 4.4958e-01,
           9.6273e-01]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[[0.3809, 0.7528, 0.4257, 0.4053, 0.3573, 0.3850],
          [0.5554, 0.2224, 0.2108, 0.2121, 0.2390, 0.6607...       [ 0.0358,  0.0960,  0.1240]],

         [[ 0.0013, -0.0903,  0.0783],
          [-0.0977, -0.2385,  0.0046]]]]))
transpiled_x = (<tf.Tensor: shape=(1, 1, 4, 6), dtype=float32, numpy=
array([[[[0.38085458, 0.7527503 , 0.42568338, 0.4052737 , 0.357...        [[-0.18123618,  0.10679004, -0.26013818],
         [-0.05165327,  0.10386258, -0.06009604]]]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[0.38085458, 0.7527503 , 0.42568338, 0.4052737 , 0.35733816,
          0.3850391 ],
         [0.5553894 , 0....
        [[ 0.00126985, -0.09031231,  0.0782582 ],
         [-0.09772895, -0.23851433,  0.00455649]]]], dtype=float32))
y = (array([[[[0.38085458, 0.7527503 , 0.42568338, 0.4052737 , 0.35733816,
          0.3850391 ],
         [0.5553894 , 0....
        [[-0.18123618,  0.10679004, -0.26013818],
         [-0.05165327,  0.10386258, -0.06009604]]]], dtype=float32))
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7fc8f0fd1d80>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[ 0.02150079,  0.09916829,  0.00779785],
         [ 0.03580609,  0.09602891,  0.12395314]],

        [[ 0.00126985, -0.09031231,  0.0782582 ],
         [-0.09772895, -0.23851433,  0.00455649]]]], dtype=float32)
y = array([[[[ 0.13920748,  0.05383041,  0.14974032],
         [-0.08434897,  0.10548735,  0.02033846]],

        [[-0.18123618,  0.10679004, -0.26013818],
         [-0.05165327,  0.10386258, -0.06009604]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.yuv.rgb_to_yuv420
________________________________________________________________________________ test_raw_to_rgb[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_raw_to_rgb(target_framework, mode, backend_compile):
        print("kornia.color.raw_to_rgb")
    
        transpiled_raw_to_rgb = ivy.transpile(kornia.color.raw_to_rgb, source="torch", target=target_framework)
        TranspiledCFA = ivy.transpile(kornia.color.CFA, source="torch", target=target_framework)
    
        torch_x = torch.rand(5, 1, 4, 6)
        transpiled_x = _array_to_new_backend(torch_x, target_framework)
    
        torch_out = kornia.color.raw_to_rgb(torch_x, kornia.color.CFA.RG)
        transpiled_out = transpiled_raw_to_rgb(transpiled_x, TranspiledCFA.RG)
    
>       _to_numpy_and_allclose(torch_out, transpiled_out)

kornia/test_color.py:713: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0.2797, 0.2797, 0.2137, 0.1476, 0.2576, 0.3675],
          [0.2797, 0.2797, 0.2137, 0.1476, 0.2576, 0.3675]...       [0.1213, 0.4136, 0.7059, 0.6764, 0.6469, 0.6469],
          [0.1213, 0.4136, 0.7059, 0.6764, 0.6469, 0.6469]]]])
transpiled_x = <tf.Tensor: shape=(5, 3, 4, 6), dtype=float32, numpy=
array([[[[0.27972525, 0.27972525, 0.2136654 , 0.16331527, 0.2889...8634 ],
         [0.1212638 , 0.33006757, 0.66415346, 0.6763888 , 0.6468634 ,
          0.6468634 ]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0.27972525, 0.27972525, 0.2136654 , 0.14760554, 0.25757337,
          0.3675412 ],
         [0.27972525, 0.2...68634 ],
         [0.1212638 , 0.413589  , 0.7059142 , 0.6763888 , 0.6468634 ,
          0.6468634 ]]]], dtype=float32)
y = array([[[[0.27972525, 0.27972525, 0.2136654 , 0.16331527, 0.28899276,
          0.3675412 ],
         [0.27972525, 0.2...68634 ],
         [0.1212638 , 0.33006757, 0.66415346, 0.6763888 , 0.6468634 ,
          0.6468634 ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.raw_to_rgb
_________________________________________________________________________________ test_RgbToHls[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RgbToHls(target_framework, mode, backend_compile):
        args = (
            torch.rand(2, 3, 4, 5),
        )
>       _test_color_class(
            kornia.color.RgbToHls,
            args,
            target_framework,
            backend_compile,
            tolerance=1e-3,
        )

kornia/test_color.py:908: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'kornia.color.hls.RgbToHls'>
args = (tensor([[[[0.3670, 0.1055, 0.6548, 0.0590, 0.6150],
          [0.4094, 0.6335, 0.5095, 0.3413, 0.0983],
          [0...., 0.8904],
          [0.8020, 0.9799, 0.4792, 0.3133, 0.5752],
          [0.1728, 0.2693, 0.7169, 0.2056, 0.0292]]]]),)
target = 'tensorflow', backend_compile = False, tolerance = 0.001, init_args = ()

    def _test_color_class(
        cls,
        args,
        target,
        backend_compile=False,
        tolerance=1e-3,
        init_args=(),
    ):
        print(f"{cls.__module__}.{cls.__name__}")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(cls, source="torch", target=target)
    
        torch_obj = cls(*init_args)
        transpiled_obj = transpiled_cls(*init_args)
    
        torch_out = torch_obj(*args)
        transpile_args = _nest_torch_tensor_to_new_framework(args, target)
>       transpiled_out = transpiled_obj(*transpile_args)

kornia/test_color.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls()
args = (<tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.366978  , 0.1055057 , 0.65479875, 0.05903447, 0.615...1325245, 0.57518065],
         [0.17277664, 0.26928437, 0.7169265 , 0.20560169, 0.02917004]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7fc8f09fb440, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RgbToHls(), <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.366978  , 0.1055057 , 0.6547...31325245, 0.57518065],
         [0.17277664, 0.26928437, 0.7169265 , 0.20560169, 0.02917004]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.366978  , 0.1055057 , 0.65479875, 0.05903447, 0.615...1325245, 0.57518065],
         [0.17277664, 0.26928437, 0.7169265 , 0.20560169, 0.02917004]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RgbToHls(), <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.366978  , 0.1055057 , 0.6547...31325245, 0.57518065],
         [0.17277664, 0.26928437, 0.7169265 , 0.20560169, 0.02917004]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.366978  , 0.1055057 , 0.65479875, 0.05903447, 0.615...1325245, 0.57518065],
         [0.17277664, 0.26928437, 0.7169265 , 0.20560169, 0.02917004]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.366978  , 0.1055057 , 0.65479875, 0.05903447, 0.6150....31325245, 0.57518065],
         [0.17277664, 0.26928437, 0.7169265 , 0.20560169, 0.02917004]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (image)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RgbToHls(),)
kwargs = {'image': <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.366978  , 0.1055057 , 0.65479875, 0.059034...31325245, 0.57518065],
         [0.17277664, 0.26928437, 0.7169265 , 0.20560169, 0.02917004]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls()
image = <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.366978  , 0.1055057 , 0.65479875, 0.05903447, 0.6150....31325245, 0.57518065],
         [0.17277664, 0.26928437, 0.7169265 , 0.20560169, 0.02917004]]]],
      dtype=float32)>

    def call(self, image):
>       return tensorflow_rgb_to_hls(image)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.366978  , 0.1055057 , 0.65479875, 0.05903447, 0.6150....31325245, 0.57518065],
         [0.17277664, 0.26928437, 0.7169265 , 0.20560169, 0.02917004]]]],
      dtype=float32)>
eps = 1e-08

    def tensorflow_rgb_to_hls(image, eps=1e-08):
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.pointwise_ops import sub
        from ..core._backend import where
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_requires_grad_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_empty_like_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_add_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_mul_frnt
    
        if not isinstance(image, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(image)}")
        if len(tensorflow_shape_frnt_(image)) < 3 or tensorflow_shape_frnt_(image)[-3] != 3:
            raise ValueError(
                f"Input size must have a shape of (*, 3, H, W). Got {tensorflow_shape_frnt_(image)}"
            )
        _RGB2HSL_IDX = tensor(
            [[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype
        )
        _img_max: typing.Any = tensorflow_max_frnt_(image, -3)
        maxc = _img_max[0]
        imax = _img_max[1]
        minc: typing.Any = tensorflow_min_frnt_(image, -3)[0]
        if tensorflow_requires_grad_frnt_(image):
            l_ = maxc + minc
            s = maxc - minc
            h = l_
            image_hls = l_
        else:
            image_hls = tensorflow_empty_like_frnt(image)
            h, l_, s = (
                image_hls[..., 0, :, :],
                image_hls[..., 1, :, :],
                image_hls[..., 2, :, :],
            )
            tensorflow_add_frnt(maxc, minc, out=l_)
            sub(maxc, minc, out=s)
        im = image / tensorflow_unsqueeze_frnt_(s + eps, -3)
        s = s / (where(l_ < 1.0, l_, 2.0 - l_) + eps)
        l_ = l_ / 2
        r, g, b = im[..., 0, :, :], im[..., 1, :, :], im[..., 2, :, :]
        cond = imax[..., None, :, :] == _RGB2HSL_IDX
        if tensorflow_requires_grad_frnt_(image):
            h = (g - b) % 6 * cond[..., 0, :, :]
        else:
>           tensorflow_mul_frnt((g - b) % 6, cond[..., 0, :, :], out=h)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:86: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 5), dtype=float32, numpy=
array([[[ 4., -0.,  2.,  2.,  2.],
        [ 4., -0.,  4.,  4.,  4....     [ 0.,  2., -0.,  2.,  2.],
        [ 4.,  2.,  4.,  2., -0.],
        [ 0.,  2.,  2.,  0.,  4.]]], dtype=float32)>
other = <tf.Tensor: shape=(2, 4, 5), dtype=bool, numpy=
array([[[False, False, False, False,  True],
        [False,  True,  T...rue, False, False, False],
        [False, False, False,  True, False],
        [ True,  True, False, False, False]]])>

    def tensorflow_mul_frnt(input, other, *, out=None):
        from ...backends.tensorflow.elementwise import tensorflow_multiply
    
>       return tensorflow_multiply(input, other, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/pointwise_ops.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x1 = <tf.Tensor: shape=(2, 4, 5), dtype=float32, numpy=
array([[[ 4., -0.,  2.,  2.,  2.],
        [ 4., -0.,  4.,  4.,  4....     [ 0.,  2., -0.,  2.,  2.],
        [ 4.,  2.,  4.,  2., -0.],
        [ 0.,  2.,  2.,  0.,  4.]]], dtype=float32)>
x2 = <tf.Tensor: shape=(2, 4, 5), dtype=bool, numpy=
array([[[False, False, False, False,  True],
        [False,  True,  T...rue, False, False, False],
        [False, False, False,  True, False],
        [ True,  True, False, False, False]]])>

    def tensorflow_multiply(
        x1: Union[float, tensorflow.Tensor, tensorflow.Variable],
        x2: Union[float, tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.data_type import tensorflow_default_dtype_bknd
        from ...ivy.general import tensorflow_is_array_bknd
        from .creation import tensorflow_asarray
    
        oirg_x1 = x1
        oirg_x2 = x2
        try:
            dtype = (
                x1.dtype
                if hasattr(x1, "dtype")
                else x2.dtype
                if hasattr(x2, "dtype")
                else tensorflow_default_dtype_bknd()
            )
            if not tensorflow_is_array_bknd(x1):
                x1 = tensorflow_asarray(x1, dtype=dtype)
            if not tensorflow_is_array_bknd(x2):
                x2 = tensorflow_asarray(x2, dtype=dtype)
        except:
            x1 = oirg_x1
            x2 = oirg_x2
>       return tensorflow.math.multiply(x1, x2)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RgbToHls.call().
E       
E       [1mcannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a bool tensor [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_RgbToHls.call():
E         â€¢ image=tf.Tensor(shape=(2, 3, 4, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/elementwise.py:290: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.hls.RgbToHls
________________________________________________________________________________ test_RgbToYuv420[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RgbToYuv420(target_framework, mode, backend_compile):
        args = (
            torch.rand(2, 3, 4, 6),
        )
>       _test_color_class(
            kornia.color.RgbToYuv420,
            args,
            target_framework,
            backend_compile,
            tolerance=1e-3,
        )

kornia/test_color.py:1064: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'kornia.color.yuv.RgbToYuv420'>
args = (tensor([[[[0.6091, 0.8564, 0.9064, 0.4457, 0.5106, 0.4488],
          [0.0460, 0.1677, 0.4497, 0.6641, 0.4334, 0.3595...     [0.2940, 0.4755, 0.1333, 0.4254, 0.8786, 0.0427],
          [0.2817, 0.1327, 0.4823, 0.8405, 0.5310, 0.5369]]]]),)
target = 'tensorflow', backend_compile = False, tolerance = 0.001, init_args = ()

    def _test_color_class(
        cls,
        args,
        target,
        backend_compile=False,
        tolerance=1e-3,
        init_args=(),
    ):
        print(f"{cls.__module__}.{cls.__name__}")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(cls, source="torch", target=target)
    
        torch_obj = cls(*init_args)
        transpiled_obj = transpiled_cls(*init_args)
    
        torch_out = torch_obj(*args)
        transpile_args = _nest_torch_tensor_to_new_framework(args, target)
        transpiled_out = transpiled_obj(*transpile_args)
    
        orig_np = _nest_array_to_numpy(torch_out)
        graph_np = _nest_array_to_numpy(transpiled_out)
    
>       _check_allclose(orig_np, graph_np, tolerance=tolerance)

kornia/test_color.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[0.42352328, 0.85636824, 0.841634  , 0.5008135 , 0.59313214,
          0.7053717 ],
         [0.42408556, 0....
        [[-0.00706986, -0.1731129 , -0.03619061],
         [ 0.16373175, -0.00163108,  0.04516541]]]], dtype=float32))
y = (array([[[[0.42352328, 0.85636824, 0.841634  , 0.5008135 , 0.59313214,
          0.7053717 ],
         [0.42408556, 0....
        [[ 0.08930989, -0.20896624,  0.09553318],
         [ 0.06373861,  0.07620307, -0.12492578]]]], dtype=float32))
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7fc8f0d52cc0>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[ 0.10063431, -0.18228471,  0.04217613],
         [ 0.01930543, -0.02107029,  0.00581146]],

        [[-0.038...

        [[-0.00706986, -0.1731129 , -0.03619061],
         [ 0.16373175, -0.00163108,  0.04516541]]]], dtype=float32)
y = array([[[[ 0.045084  ,  0.00158674,  0.01698055],
         [ 0.02309939,  0.02164765, -0.14382601]],

        [[ 0.044...

        [[ 0.08930989, -0.20896624,  0.09553318],
         [ 0.06373861,  0.07620307, -0.12492578]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.yuv.RgbToYuv420
_________________________________________________________________________________ test_RawToRgb[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RawToRgb(target_framework, mode, backend_compile):
        print("kornia.color.RawToRgb")
    
        transpiled_RawToRgb = ivy.transpile(kornia.color.RawToRgb, source="torch", target=target_framework)
        TranspiledCFA = ivy.transpile(kornia.color.CFA, source="torch", target=target_framework)
    
        torch_x = torch.rand(2, 1, 4, 6)
        transpiled_x = _array_to_new_backend(torch_x, target_framework)
    
        torch_out = kornia.color.RawToRgb(kornia.color.CFA.RG)(torch_x)
        transpiled_out = transpiled_RawToRgb(TranspiledCFA.RG)(transpiled_x)
    
>       _to_numpy_and_allclose(torch_out, transpiled_out)

kornia/test_color.py:1152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0.3659, 0.3659, 0.5247, 0.6836, 0.7045, 0.7253],
          [0.3659, 0.3659, 0.5247, 0.6836, 0.7045, 0.7253]...       [0.5273, 0.2960, 0.0646, 0.2953, 0.5260, 0.5260],
          [0.5273, 0.2960, 0.0646, 0.2953, 0.5260, 0.5260]]]])
transpiled_x = <tf.Tensor: shape=(2, 3, 4, 6), dtype=float32, numpy=
array([[[[0.36586487, 0.36586487, 0.5247332 , 0.6865809 , 0.7104...0483 ],
         [0.5273164 , 0.3620736 , 0.09768519, 0.29534248, 0.5260483 ,
          0.5260483 ]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0.36586487, 0.36586487, 0.5247332 , 0.68360144, 0.70445764,
          0.7253139 ],
         [0.36586487, 0.3...60483 ],
         [0.5273164 , 0.29597652, 0.06463665, 0.29534248, 0.5260483 ,
          0.5260483 ]]]], dtype=float32)
y = array([[[[0.36586487, 0.36586487, 0.5247332 , 0.6865809 , 0.7104166 ,
          0.7253139 ],
         [0.36586487, 0.3...60483 ],
         [0.5273164 , 0.3620736 , 0.09768519, 0.29534248, 0.5260483 ,
          0.5260483 ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.RawToRgb
______________________________________________________________________________ test_apply_colormap[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_apply_colormap(target_framework, mode, backend_compile):
        print("kornia.color.ColorMap")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledColorMapType = ivy.transpile(kornia.color.ColorMapType, source="torch", target=target_framework)
        TranspiledColorMap = ivy.transpile(kornia.color.ColorMap, source="torch", target=target_framework)
        transpiled_apply_colormap = ivy.transpile(kornia.color.apply_colormap, source="torch", target=target_framework)
    
        torch_x = torch.tensor([[[0, 1, 2], [15, 25, 33], [128, 158, 188]]])
        transpiled_x = _array_to_new_backend(torch_x, target_framework)
    
        colormap = kornia.color.ColorMap(base=kornia.color.ColorMapType.autumn)
        torch_out = kornia.color.apply_colormap(torch_x, colormap)
    
        colormap = TranspiledColorMap(base=TranspiledColorMapType.autumn)
>       transpiled_out = transpiled_apply_colormap(transpiled_x, colormap)

kornia/test_color.py:1250: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input_tensor = <tf.Tensor: shape=(1, 1, 9), dtype=float32, numpy=
array([[[0.        , 0.00392157, 0.00784314, 0.05882353, 0.09803922,
         0.12941177, 0.5019608 , 0.61960787, 0.7372549 ]]], dtype=float32)>
colormap = <Translated_Outputs.tensorflow_outputs.kornia.color.colormap.tensorflow_ColorMap object at 0x7fc8f0c576d0>

    def tensorflow_apply_colormap(input_tensor, colormap):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze__frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_div__frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_linspace_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
    
        tensorflow_KORNIA_CHECK(
            isinstance(input_tensor, (tensorflow.Tensor, tensorflow.Variable)),
            f"`input_tensor` must be a Tensor. Got: {type(input_tensor)}",
        )
        valid_types = [
            tf.float16,
            tf.float32,
            tf.float64,
            tf.uint8,
            tf.int32,
            tf.int64,
            tf.int16,
        ]
        tensorflow_KORNIA_CHECK(
            input_tensor.dtype in valid_types,
            f"`input_tensor` must be a {valid_types}. Got: {input_tensor.dtype}",
        )
        tensorflow_KORNIA_CHECK(
            len(tensorflow_shape_frnt_(input_tensor)) in (3, 4),
            "Wrong input tensor dimension.",
        )
        if len(tensorflow_shape_frnt_(input_tensor)) == 3:
            input_tensor = tensorflow_unsqueeze__frnt_(input_tensor, 0)
        B, C, H, W = tensorflow_shape_frnt_(input_tensor)
        input_tensor = tensorflow_reshape_frnt_(input_tensor, B, C, -1)
        max_value = 1.0 if tensorflow_max_frnt_(input_tensor) <= 1.0 else 255.0
        input_tensor = tensorflow_div__frnt_(
            tensorflow_float_frnt_(input_tensor), max_value
        )
        colors = tensorflow_permute_frnt_(colormap.colors, 1, 0)
        num_colors, channels_cmap = tensorflow_shape_frnt_(colors)
        keys = tensorflow_linspace_frnt(
            0.0, 1.0, num_colors - 1, device=input_tensor.device, dtype=input_tensor.dtype
        )
        indices = tensorflow_expand_frnt_(
>           tensorflow_unsqueeze_frnt_(torch.bucketize(input_tensor, keys), -1),
            -1,
            -1,
            -1,
            3,
        )
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/color/colormap.py:205: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.ColorMap
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_color.py::test_rgb_to_hls[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a fl...
FAILED kornia/test_color.py::test_rgb_to_yuv420[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_color.py::test_raw_to_rgb[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_color.py::test_RgbToHls[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RgbToHls.call().
FAILED kornia/test_color.py::test_RgbToYuv420[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_color.py::test_RawToRgb[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_color.py::test_apply_colormap[tensorflow-s2s-False] - NameError: name 'torch' is not defined
=============================================================================== 7 failed, 62 passed in 811.23s (0:13:31) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 4 items

kornia/geometry/test_liegroup.py ..F.                                                                                                                                                            [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________________ test_So2[tensorflow-s2s-False] ____________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_So2(target_framework, mode, backend_compile):
        print("kornia.geometry.liegroup.So2")
    
        if backend_compile:
            pytest.skip()
    
        real_part = torch.tensor([1.0], requires_grad=True)
        imaginary_part = torch.tensor([2.0], requires_grad=True)
        complex_number = torch.complex(real_part, imaginary_part)
        torch_so2 = kornia.geometry.liegroup.So2(complex_number)
    
        TranspiledSo2 = ivy.transpile(kornia.geometry.liegroup.So2, source="torch", target=target_framework)
        transpiled_complex_number = _nest_torch_tensor_to_new_framework(complex_number, target_framework)
        transpiled_so2 = TranspiledSo2(transpiled_complex_number)
    
        # Test .matrix()
        torch_matrix = torch_so2.matrix()
        transpiled_matrix = transpiled_so2.matrix()
        _to_numpy_and_allclose(torch_matrix, transpiled_matrix)
    
        # Test .inverse()
        torch_inverse = torch_so2.inverse()
        transpiled_inverse = transpiled_so2.inverse()
        _to_numpy_and_allclose(torch_inverse.z, transpiled_inverse.z)
    
        # Test .log()
        torch_log = torch_so2.log()
        transpiled_log = transpiled_so2.log()
        _to_numpy_and_allclose(torch_log, transpiled_log)
    
        # Test .__mul__()
        other_real_part = torch.tensor([0.5], requires_grad=True)
        other_imaginary_part = torch.tensor([0.5], requires_grad=True)
        other_complex_number = torch.complex(other_real_part, other_imaginary_part)
        other_torch_so2 = kornia.geometry.liegroup.So2(other_complex_number)
    
        transpiled_other_complex_number = _nest_torch_tensor_to_new_framework(other_complex_number, target_framework)
        transpiled_other_so2 = TranspiledSo2(transpiled_other_complex_number)
    
        torch_composed_so2 = torch_so2 * other_torch_so2
        transpiled_composed_so2 = transpiled_so2 * transpiled_other_so2
        _to_numpy_and_allclose(torch_composed_so2.z, transpiled_composed_so2.z)
    
        # Test .adjoint()
        torch_adjoint = torch_so2.adjoint()
>       transpiled_adjoint = transpiled_so2.adjoint()

kornia/geometry/test_liegroup.py:202: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Variable 'Variable:0' shape=(1,) dtype=complex64, numpy=array([1.+2.j], dtype=complex64)>

    def adjoint(self):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_real_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
>       batch_size = len(self.z) if len(tensorflow_shape_frnt_(self.z)) > 0 else None
E       TypeError: object of type 'ResourceVariable' has no len()

Translated_Outputs/tensorflow_outputs/kornia/geometry/liegroup/so2.py:195: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.liegroup.So2
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_liegroup.py::test_So2[tensorflow-s2s-False] - TypeError: object of type 'ResourceVariable' has no len()
=============================================================================== 1 failed, 3 passed in 248.93s (0:04:08) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 68 items

kornia/augmentation/test_augmentation.py .......F..........F..F.....F.F.FFF.........FF...FFFFF.....F...F..FFF                                                                                    [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________________ test_RandomClahe[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomClahe(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomClahe")
    
        init_args = ()
        init_kwargs = {}
        call_args = (torch.rand(2, 3, 10, 20),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomClahe,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:215: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.clahe.RandomClahe'>, target = 'tensorflow', init_args = (), init_kwargs = {}
call_args = (tensor([[[[0.0024, 0.5001, 0.5356,  ..., 0.5981, 0.8090, 0.0514],
          [0.6214, 0.7227, 0.6514,  ..., 0.5567, 0...., 0.7387, 0.6150,  ..., 0.8266, 0.2624, 0.8007],
          [0.3947, 0.5569, 0.8105,  ..., 0.9006, 0.2370, 0.9411]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
args = (<tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0.5981309 ...
         [0.39470887, 0.5568804 , 0.81049496, ..., 0.90055996,
          0.23703748, 0.9411453 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x560a67408390, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slo...,
         [0.39470887, 0.5568804 , 0.81049496, ..., 0.90055996,
          0.23703748, 0.9411453 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0.5981309 ...
         [0.39470887, 0.5568804 , 0.81049496, ..., 0.90055996,
          0.23703748, 0.9411453 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slo...,
         [0.39470887, 0.5568804 , 0.81049496, ..., 0.90055996,
          0.23703748, 0.9411453 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0.5981309 ...
         [0.39470887, 0.5568804 , 0.81049496, ..., 0.90055996,
          0.23703748, 0.9411453 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0.5981309 ,...],
         [0.39470887, 0.5568804 , 0.81049496, ..., 0.90055996,
          0.23703748, 0.9411453 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0...,
         [0.39470887, 0.5568804 , 0.81049496, ..., 0.90055996,
          0.23703748, 0.9411453 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
input = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0.5981309 ,...],
         [0.39470887, 0.5568804 , 0.81049496, ..., 0.90055996,
          0.23703748, 0.9411453 ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 0.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff20c468a60>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff20c2b72e0>
tensor = <function tensorflow_tensor_frnt at 0x7ff20d178ee0>
in_tensor = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0.5981309 ,...],
         [0.39470887, 0.5568804 , 0.81049496, ..., 0.90055996,
          0.23703748, 0.9411453 ]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([2, 3, 10, 20]), batch_shape = ivy.frontends.torch.Size([2, 3, 10, 20]), flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
in_tensor = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0.5981309 ,...],
         [0.39470887, 0.5568804 , 0.81049496, ..., 0.90055996,
          0.23703748, 0.9411453 ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 0.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
input = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0.5981309 ,...],
         [0.39470887, 0.5568804 , 0.81049496, ..., 0.90055996,
          0.23703748, 0.9411453 ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 0.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}
transform = <tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],

       [[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff20c468a60>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7ff20c469870>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7ff20d1fb1c0>, tensorflow_get_item = <function tensorflow_get_item at 0x7ff20db34160>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7ff20da7e440>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7ff20d1fba30>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7ff20d1f9240>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
            output = self.apply_transform(in_tensor, params, flags, transform=transform)
        elif not tensorflow_any_frnt_(to_apply):
            output = self.apply_non_transform(
                in_tensor, params, flags, transform=transform
            )
        else:
            output = self.apply_non_transform(
                in_tensor, params, flags, transform=transform
            )
>           applied = self.apply_transform(
                tensorflow_get_item(in_tensor, to_apply),
                params,
                flags,
                transform=transform
                if transform is None
                else tensorflow_get_item(transform, to_apply),
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:616: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
input = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[2.38579512e-03, 5.00097275e-01, 5.35640776e-01,
    ...          9.85323608e-01, 5.16467154e-01, 9.56102014e-01,
          9.86698508e-01, 6.89007580e-01]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 0.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....enhance.equalization import tensorflow_equalize_clahe
    
        clip_limit = float(params["clip_limit_factor"][0])
>       return tensorflow_equalize_clahe(
            input, clip_limit, flags["grid_size"], flags["slow_and_differentiable"]
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/clahe.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[2.38579512e-03, 5.00097275e-01, 5.35640776e-01,
    ...          9.85323608e-01, 5.16467154e-01, 9.56102014e-01,
          9.86698508e-01, 6.89007580e-01]]]], dtype=float32)>
args = (40.0, (8, 8), False), kwargs = {}, tensorflow_numel_frnt_ = <function tensorflow_numel_frnt_ at 0x7ff20c4683a0>, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff20c468a60>
tensorflow_view_frnt_ = <function tensorflow_view_frnt_ at 0x7ff20c47a170>, input_shape = ivy.frontends.torch.Size([1, 3, 10, 20])

    @wraps(f)
    def _wrapper(input, *args, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_numel_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
        if tensorflow_numel_frnt_(input) == 0:
            raise ValueError("Invalid input tensor, it is empty.")
        input_shape = tensorflow_shape_frnt_(input)
        input = tensorflow__to_bchw(input)
>       output = f(input, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/utils/image.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[2.38579512e-03, 5.00097275e-01, 5.35640776e-01,
    ...          9.85323608e-01, 5.16467154e-01, 9.56102014e-01,
          9.86698508e-01, 6.89007580e-01]]]], dtype=float32)>
clip_limit = 40.0, grid_size = (8, 8), slow_and_differentiable = False

    @tensorflow_perform_keep_shape_image
    def tensorflow_equalize_clahe(
        input, clip_limit=40.0, grid_size=(8, 8), slow_and_differentiable=False
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_as_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
    
        if not isinstance(clip_limit, (float,)):
            raise TypeError(f"Input clip_limit type is not float. Got {type(clip_limit)}")
        if not isinstance(grid_size, (tuple,)):
            raise TypeError(f"Input grid_size type is not Tuple. Got {type(grid_size)}")
        if len(grid_size) != 2:
            raise TypeError(
                f"Input grid_size is not a Tuple with 2 elements. Got {len(grid_size)}"
            )
        if isinstance(grid_size[0], (float,)) or isinstance(grid_size[1], (float,)):
            raise TypeError("Input grid_size type is not valid, must be a Tuple[int, int].")
        if grid_size[0] <= 0 or grid_size[1] <= 0:
            raise ValueError(f"Input grid_size elements must be positive. Got {grid_size}")
        imgs: typing.Any = input
>       hist_tiles, img_padded = tensorflow__compute_tiles(imgs, grid_size, True)

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:513: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

imgs = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[2.38579512e-03, 5.00097275e-01, 5.35640776e-01,
    ...          9.85323608e-01, 5.16467154e-01, 9.56102014e-01,
          9.86698508e-01, 6.89007580e-01]]]], dtype=float32)>
grid_size = (8, 8), even_tile_size = True

    def tensorflow__compute_tiles(imgs, grid_size, even_tile_size=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_pad_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unfold_frnt_
    
        batch: typing.Any = imgs
        h, w = tensorflow_shape_frnt_(batch)[-2:][0], tensorflow_shape_frnt_(batch)[-2:][1]
        kernel_vert: typing.Any = math.ceil(h / grid_size[0])
        kernel_horz: typing.Any = math.ceil(w / grid_size[1])
        if even_tile_size:
            kernel_vert = kernel_vert + (1 if kernel_vert % 2 else 0)
            kernel_horz = kernel_horz + (1 if kernel_horz % 2 else 0)
        pad_vert = kernel_vert * grid_size[0] - h
        pad_horz = kernel_horz * grid_size[1] - w
        if (
            pad_vert > tensorflow_shape_frnt_(batch)[-2]
            or pad_horz > tensorflow_shape_frnt_(batch)[-1]
        ):
            raise ValueError(
                "Cannot compute tiles on the image according to the given grid size"
            )
        if pad_vert > 0 or pad_horz > 0:
            batch = tensorflow_pad_frnt(batch, [0, pad_horz, 0, pad_vert], mode="reflect")
        c: typing.Any = tensorflow_shape_frnt_(batch)[-3]
        tiles: typing.Any = tensorflow_contiguous_frnt_(
            tensorflow_squeeze_frnt_(
>               tensorflow_unfold_frnt_(
                    tensorflow_unfold_frnt_(
                        tensorflow_unfold_frnt_(batch, 1, c, c), 2, kernel_vert, kernel_vert
                    ),
                    3,
                    kernel_horz,
                    kernel_horz,
                ),
                1,
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 1, 2, 16, 32), dtype=float32, numpy=
array([[[[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0.....90310884, 0.69293404, 0.5861191 , ..., 0.2771142 ,
            0.475573  , 0.6308777 ]]]]]], dtype=float32)>, 3, 4, 4)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7ff20dee97e0>
array_like = <tf.Tensor: shape=(1, 1, 1, 2, 16, 32), dtype=float32, numpy=
array([[[[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0.5...        [0.90310884, 0.69293404, 0.5861191 , ..., 0.2771142 ,
            0.475573  , 0.6308777 ]]]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 1, 1, 2, 16, 32), dtype=float32, numpy=
array([[[[[[0.0023858 , 0.5000973 , 0.5356408 , ..., 0.5...        [0.90310884, 0.69293404, 0.5861191 , ..., 0.2771142 ,
            0.475573  , 0.6308777 ]]]]]], dtype=float32)>
dimension = 3, size = 4, step = 4

    @tensorflow_handle_methods
    def tensorflow_unfold_frnt_(tensor, dimension, size, step):
        from ...backends.tensorflow.general import tensorflow_get_item
        from ...ivy.general import tensorflow_set_item_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_stack_frnt
    
        slices = []
        self_shape = tuple(tensorflow_shape_frnt_(tensor))
        for i in range(0, tensorflow_get_item(self_shape, dimension) - size + 1, step):
            slicing = [slice(None)] * len(tensorflow_shape_frnt_(tensor))
            slicing = tensorflow_set_item_bknd(slicing, dimension, slice(i, i + size))
            slices.append(tensorflow_get_item(tensor, tuple(slicing)))
>       stacked = tensorflow_stack_frnt(slices, dim=dimension)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:679: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = [], dim = 3

    def tensorflow_stack_frnt(tensors, dim=0, *, out=None):
        from ...backends.tensorflow.manipulation import tensorflow_stack
    
>       return tensorflow_stack(tensors, axis=dim, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ([],), kwargs = {'axis': 3, 'out': None}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7ff20dee97e0>, array_like = []

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
>           array_like = array_like[0]
E           IndexError: Exception encountered when calling tensorflow_RandomClahe.call().
E           
E           [1mlist index out of range[0m
E           
E           Arguments received by tensorflow_RandomClahe.call():
E             â€¢ input=tf.Tensor(shape=(2, 3, 10, 20), dtype=float32)
E             â€¢ params=None
E             â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:183: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomClahe
______________________________________________________________________ test_RandomLinearCornerIllumination[tensorflow-s2s-False] _______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomLinearCornerIllumination(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomLinearCornerIllumination")
    
        init_args = ()
        init_kwargs = {"gain": 0.25, "p": 1.}
        call_args = (torch.ones(1, 3, 3, 3) * 0.5,)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomLinearCornerIllumination,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:435: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.linear_illumination.RandomLinearCornerIllumination'>, target = 'tensorflow', init_args = (), init_kwargs = {'gain': 0.25, 'p': 1.0}
call_args = (tensor([[[[0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000]],

       ...00]],

         [[0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0...  [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x560a685615c0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=Fals...   [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0...  [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=Fals...   [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0...  [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0....    [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
  ...   [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0....    [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>
params = None, kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff20c6e4310>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff20e036050>
tensor = <function tensorflow_tensor_frnt at 0x7ff20c6d6560>
in_tensor = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0....    [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 3, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 3, 3, 3])

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
>           params = self.forward_parameters(batch_shape)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), batch_shape = ivy.frontends.torch.Size([1, 3, 3, 3])

    def forward_parameters(self, batch_shape):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        batch_prob = self.__batch_prob_generator__(
            batch_shape, self.p, self.p_batch, self.same_on_batch
        )
        to_apply = batch_prob > 0.5
>       _params = self.generate_parameters(
            tuple(
                (
                    int(tensorflow_item_frnt_(tensorflow_sum_frnt_(to_apply))),
                    *batch_shape[1:],
                )
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:199: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), batch_shape = (1, 3, 3, 3)

    def generate_parameters(self, batch_shape):
        if self._param_generator is not None:
>           return self._param_generator(batch_shape, self.same_on_batch)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), args = ((1, 3, 3, 3), False), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x560a68de2b10, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ion.py', lineno=46, function='__call__', code_context=['            return call_fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), v = None, buffers = None, args = ((1, 3, 3, 3), False), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), v = None, buffers = None, args = ((1, 3, 3, 3), False), kwargs = {}, replace_v = False, replace_buffers = False
call_signature = <Signature (batch_shape, same_on_batch=False)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), batch_shape = (1, 3, 3, 3), same_on_batch = False

    def call(self, batch_shape, same_on_batch=False):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...utils.param_validation import tensorflow__common_param_check
        from ....utils.helpers import tensorflow__extract_device_dtype
        from ...utils.helpers import tensorflow__adapted_rsampling
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_where_frnt,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from .....ivy.functional.frontends.torch.tensor import (
            tensorflow_unsqueeze_frnt_,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_linspace_frnt,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_zeros_frnt,
        )
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....enhance.normalize import tensorflow_normalize
    
        batch_size, channels, height, width = (
            batch_shape[0],
            batch_shape[1],
            batch_shape[2],
            batch_shape[3],
        )
        tensorflow__common_param_check(batch_size, same_on_batch)
        _device, _dtype = tensorflow__extract_device_dtype([self.gain, self.sign])
        gain_factor = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 1, 1, 1), self.gain_sampler, same_on_batch
            ),
            device=_device,
            dtype=_dtype,
        )
        sign = tensorflow_to_frnt_(
            tensorflow_where_frnt(
                tensorflow__adapted_rsampling(
                    (batch_size, 1, 1, 1), self.sign_sampler, same_on_batch
                )
                >= 0.0,
                tensorflow_tensor_frnt(1),
                tensorflow_tensor_frnt(-1),
            ),
            device=_device,
            dtype=_dtype,
        )
        directions = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 1, 1, 1), self.directions_sampler, same_on_batch
            ),
            device=_device,
            dtype=tf.int8,
        )
        y_grad = tensorflow_expand_frnt_(
            tensorflow_unsqueeze_frnt_(tensorflow_linspace_frnt(0, 1, height), 1),
            channels,
            height,
            width,
        )
        x_grad = tensorflow_expand_frnt_(
            tensorflow_unsqueeze_frnt_(tensorflow_linspace_frnt(0, 1, width), 0),
            channels,
            height,
            width,
        )
        gradient = tensorflow_zeros_frnt(batch_shape)
        for _b in range(batch_size):
            if tensorflow_get_item(directions, _b) == 0:
                gradient = tensorflow_set_item_bknd(gradient, _b, x_grad + y_grad)
            elif tensorflow_get_item(directions, _b) == 1:
                gradient = tensorflow_set_item_bknd(gradient, _b, -x_grad + y_grad)
            elif tensorflow_get_item(directions, _b) == 2:
                gradient = tensorflow_set_item_bknd(gradient, _b, x_grad - y_grad)
            elif tensorflow_get_item(directions, _b) == 3:
                gradient = tensorflow_set_item_bknd(gradient, _b, 1 - (x_grad + y_grad))
>       gradient = sign * gain_factor * tensorflow_normalize.normalize_min_max(gradient)
E       AttributeError: Exception encountered when calling tensorflow_LinearCornerIlluminationGenerator.call().
E       
E       [1m'function' object has no attribute 'normalize_min_max'[0m
E       
E       Arguments received by tensorflow_LinearCornerIlluminationGenerator.call():
E         â€¢ batch_shape=('1', '3', '3', '3')
E         â€¢ same_on_batch=False

Translated_Outputs/tensorflow_outputs/kornia/augmentation/random_generator/_2d/linear_illumination.py:147: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomLinearCornerIllumination
_____________________________________________________________________________ test_RandomMotionBlur[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomMotionBlur(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomMotionBlur")
    
        init_args = (3, 35., 0.5)
        init_kwargs = {"p": 1.}
        call_args = (torch.ones(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomMotionBlur,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:495: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.motion_blur.RandomMotionBlur'>, target = 'tensorflow', init_args = (3, 35.0, 0.5), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]]]]),), call_kwargs = {}
deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff20c72d840, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border..., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border..., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>
params = None, kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff20e034310>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff207f3a7a0>
tensor = <function tensorflow_tensor_frnt at 0x7ff20da3beb0>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5])

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
>           params = self.forward_parameters(batch_shape)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5])

    def forward_parameters(self, batch_shape):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        batch_prob = self.__batch_prob_generator__(
            batch_shape, self.p, self.p_batch, self.same_on_batch
        )
        to_apply = batch_prob > 0.5
>       _params = self.generate_parameters(
            tuple(
                (
                    int(tensorflow_item_frnt_(tensorflow_sum_frnt_(to_apply))),
                    *batch_shape[1:],
                )
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:199: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest), batch_shape = (1, 1, 5, 5)

    def generate_parameters(self, batch_shape):
        from ....core._backend import tensor
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .....ivy.functional.frontends.torch.random_sampling import (
            tensorflow_randint_frnt,
        )
    
        params = super().generate_parameters(batch_shape)
        params = tensorflow_set_item_bknd(
            params,
            "idx",
            tensor([0])
            if batch_shape[0] == 0
>           else tensorflow_randint_frnt(batch_shape[0], (1,)),
        )
E       TypeError: Exception encountered when calling tensorflow_RandomMotionBlur.call().
E       
E       [1mtensorflow_randint_frnt() missing 1 required positional argument: 'size'[0m
E       
E       Arguments received by tensorflow_RandomMotionBlur.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/motion_blur.py:70: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomMotionBlur
________________________________________________________________________________ test_RandomRain[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRain(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomRain")
    
        init_args = ()
        init_kwargs = {"p": 1, "drop_height": (1,2), "drop_width": (1,2), "number_of_drops": (1,1)}
        call_args = (torch.rand(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomRain,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:615: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.random_rain.RandomRain'>, target = 'tensorflow', init_args = ()
init_kwargs = {'drop_height': (1, 2), 'drop_width': (1, 2), 'number_of_drops': (1, 1), 'p': 1}
call_args = (tensor([[[[0.1405, 0.9339, 0.2713, 0.9601, 0.0871],
          [0.5025, 0.4023, 0.8561, 0.0698, 0.0759],
          [0...., 0.8147],
          [0.9663, 0.2501, 0.6282, 0.8797, 0.5748],
          [0.6036, 0.4318, 0.0245, 0.2725, 0.9110]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.087...7965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x560a681e77a0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=...87965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.087...7965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=...87965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.087...7965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.0871....87965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.960111...87965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.0871....87965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff207fb81f0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff20db94ca0>
tensor = <function tensorflow_tensor_frnt at 0x7ff20df75cf0>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.0871....87965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.0871....87965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.0871....87965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff207fb81f0>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7ff207fb9240>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7ff21240f490>, tensorflow_get_item = <function tensorflow_get_item at 0x7ff2122445e0>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7ff2122376d0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7ff21240ff40>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7ff21240fa30>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
image = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.0871....87965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, image, params, flags, transform=None):
        from ....core.check import tensorflow_KORNIA_CHECK
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.reduction_ops import (
            tensorflow_all_frnt,
        )
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_abs_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_linspace_frnt,
        )
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        tensorflow_KORNIA_CHECK(
            tensorflow_shape_frnt_(image)[1] in {3, 1},
            "Number of color channels should be 1 or 3.",
        )
        tensorflow_KORNIA_CHECK(
            bool(
                tensorflow_all_frnt(
                    params["drop_height_factor"] <= tensorflow_shape_frnt_(image)[2]
                )
                and tensorflow_all_frnt(params["drop_height_factor"] > 0)
            ),
            "Height of drop should be greater than zero and less than image height.",
        )
        tensorflow_KORNIA_CHECK(
            bool(
                tensorflow_all_frnt(
                    tensorflow_abs_frnt(params["drop_width_factor"])
                    <= tensorflow_shape_frnt_(image)[3]
                )
            ),
            "Width of drop should be less than image width.",
        )
        modeified_img = tensorflow_clone_frnt_(image)
        for i in range(tensorflow_shape_frnt_(image)[0]):
            number_of_drops: typing.Any = int(
                tensorflow_get_item(params["number_of_drops_factor"], i)
            )
            coordinates_of_drops: typing.Any = tensorflow_get_item(
                tensorflow_get_item(params["coordinates_factor"], i),
                slice(None, number_of_drops, None),
            )
            height_of_drop: typing.Any = int(
                tensorflow_get_item(params["drop_height_factor"], i)
            )
            width_of_drop: typing.Any = int(
                tensorflow_get_item(params["drop_width_factor"], i)
            )
            random_y_coords = coordinates_of_drops[:, 0] * (
                tensorflow_shape_frnt_(image)[2] - height_of_drop - 1
            )
            if width_of_drop > 0:
                random_x_coords = coordinates_of_drops[:, 1] * (
                    tensorflow_shape_frnt_(image)[3] - width_of_drop - 1
                )
            else:
                random_x_coords = (
                    coordinates_of_drops[:, 1]
                    * (tensorflow_shape_frnt_(image)[3] + width_of_drop - 1)
                    - width_of_drop
                )
            coords = tensorflow_to_frnt_(
                tensorflow_cat_frnt(
                    [random_y_coords[None], random_x_coords[None]], dim=0
                ),
                image.device,
                dtype=tf.int64,
            )
            size_of_line: typing.Any = max(height_of_drop, abs(width_of_drop))
            x = tensorflow_to_frnt_(
                tensorflow_linspace_frnt(
                    start=0, end=height_of_drop, steps=size_of_line, dtype=tf.int64
                ),
                image.device,
            )
            y = tensorflow_to_frnt_(
                tensorflow_linspace_frnt(
                    start=0, end=width_of_drop, steps=size_of_line, dtype=tf.int64
                ),
                image.device,
            )
            for k in range(tensorflow_shape_frnt_(x)[0]):
>               modeified_img = tensorflow_set_item_bknd(
                    modeified_img,
                    (i, slice(None, None, None), coords[0] + x[k], coords[1] + y[k]),
                    200 / 255,
                )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/random_rain.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inp = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.0871....87965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>
query = (0, slice(None, None, None), <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>), val = 0.7843137254901961, kwargs = {}

    @functools.wraps(fn)
    def wrapper(inp, query, val, **kwargs):
        try:
            inp.__setitem__(query, val)
            res = inp
        except IndexError:
            raise
        except Exception:
>           res = fn(inp, query, val, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.087...hape=(1,), dtype=int64, numpy=array([1])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>), 0.7843137254901961)
kwargs = {}, handle_mixed_in_backend = True

    @functools.wraps(fn)
    def _handle_partial_mixed_function(*args, **kwargs):
        handle_mixed_in_backend = False
        if not hasattr(fn, "partial_mixed_handler"):
            handle_mixed_in_backend = True
        else:
            compos = getattr(fn, "compos")
            condition = getattr(fn, "partial_mixed_handler")
        if handle_mixed_in_backend or condition(*args, **kwargs):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.14046687, 0.9339073 , 0.27125502, 0.96011126, 0.0871....87965393, 0.5748354 ],
         [0.60361874, 0.43175465, 0.02446926, 0.27249897, 0.9109846 ]]]],
      dtype=float32)>
query = (0, slice(None, None, None), <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>)
val = <tf.Tensor: shape=(), dtype=float32, numpy=0.78431374>

    @tensorflow_handle_set_item
    @tensorflow_handle_partial_mixed_function
    def tensorflow_set_item_bknd(
        x: Union[tensorflow.Tensor, tf.Tensor],
        query: Union[tensorflow.Tensor, tf.Tensor, Tuple],
        val: Union[tensorflow.Tensor, tf.Tensor],
        /,
        *,
        copy: Optional[bool] = False,
    ):
        from ..backends.tensorflow.creation import tensorflow_asarray
        from ..backends.tensorflow.creation import tensorflow_copy_array
        from .data_type import tensorflow_is_bool_dtype_bknd
        from ..backends.tensorflow.manipulation import tensorflow_tile
        from ..backends.tensorflow.searching import tensorflow_nonzero
        from ..backends.tensorflow.general import tensorflow_shape
        from ...data_classes.array.data_type import tensorflow_astype_bknd_
        from ..backends.tensorflow.general import tensorflow_scatter_nd
    
        if copy:
            x = tensorflow_copy_array(x)
        if not tensorflow_is_array_bknd(val):
            val = tensorflow_asarray(val)
        if 0 in x.shape or 0 in val.shape:
            return x
        if tensorflow_is_array_bknd(query) and tensorflow_is_bool_dtype_bknd(query):
            if not len(query.shape):
                query = tensorflow_tile(query, (x.shape[0],))
            indices = tensorflow_nonzero(query, as_tuple=False)
        else:
            indices, target_shape, _ = tensorflow__parse_query_bknd(
                query, tensorflow_shape(x, as_array=True), scatter=True
            )
            if indices is None:
                return x
        val = tensorflow_astype_bknd_(val, x.dtype)
>       ret = tensorflow_scatter_nd(indices, val, reduction="replace", out=x)

Translated_Outputs/tensorflow_outputs/ivy/functional/ivy/general.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

indices = <tf.Tensor: shape=(1, 1, 4), dtype=int64, numpy=array([[[0, 1, 2, 0]]])>, updates = <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.78431374]], dtype=float32)>, shape = None

    def tensorflow_scatter_nd(
        indices: Union[tensorflow.Tensor, tensorflow.Variable],
        updates: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Optional[Union[tf.TensorShape, Sequence[int]]] = None,
        *,
        reduction: str = "sum",
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.general import tensorflow_exists_bknd
        from ...ivy.data_type import tensorflow_promote_types_bknd
        from .data_type import tensorflow_as_native_dtype
        from ...ivy.general import tensorflow__broadcast_to_bknd
        from ....utils.assertions import tensorflow_check_equal
        from .elementwise import tensorflow_multiply
    
        updates_dtype = updates.dtype
        if tensorflow_exists_bknd(out):
            dtype = tensorflow_promote_types_bknd(out.dtype, updates_dtype)
        updates = tensorflow.cast(
            updates,
            tensorflow_as_native_dtype(dtype)
            if tensorflow_exists_bknd(out)
            else updates_dtype,
        )
        expected_shape = (
            list(tensorflow.shape(indices)[:-1])
            + list(out.shape[tensorflow.shape(indices)[-1] :])
            if tensorflow_exists_bknd(out)
            else list(tensorflow.shape(indices)[:-1])
            + list(shape[tensorflow.shape(indices)[-1] :])
        )
        updates = tensorflow__broadcast_to_bknd(updates, expected_shape)
        if len(updates.shape) == 0:
            indices = tensorflow.expand_dims(indices, 0)
            updates = tensorflow.expand_dims(updates, 0)
        target = out
        target_given = tensorflow_exists_bknd(target)
        if tensorflow_exists_bknd(shape) and target_given:
            tensorflow_check_equal(tuple(target.shape), tuple(shape), as_array=False)
        if not target_given:
            shape = list(shape) if tensorflow_exists_bknd(shape) else list(out.shape)
            target = tensorflow.zeros(shape, dtype=updates.dtype)
        if reduction == "sum":
            res = tensorflow.tensor_scatter_nd_add(target, indices, updates)
        elif reduction == "min":
            res = tensorflow.tensor_scatter_nd_min(target, indices, updates)
        elif reduction == "max":
            res = tensorflow.tensor_scatter_nd_max(target, indices, updates)
        elif reduction == "mul":
            updates = tensorflow_multiply(tensorflow_gather_nd(target, indices), updates)
            res = tensorflow.tensor_scatter_nd_update(target, indices, updates)
        elif reduction == "replace":
>           res = tensorflow.tensor_scatter_nd_update(target, indices, updates)
E           tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomRain.call().
E           
E           [1m{{function_node __wrapped__TensorScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,0] = [0, 1, 2, 0] does not index into shape [1,1,5,5] [Op:TensorScatterUpdate] name: [0m
E           
E           Arguments received by tensorflow_RandomRain.call():
E             â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E             â€¢ params=None
E             â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/general.py:328: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomRain
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-13 13:48:56.758546: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at scatter_nd_op.cc:218 : INVALID_ARGUMENT: indices[0,0] = [0, 1, 2, 0] does not index into shape [1,1,5,5]
_________________________________________________________________________ test_RandomSaltAndPepperNoise[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSaltAndPepperNoise(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSaltAndPepperNoise")
    
        init_args = ()
        init_kwargs = {"amount": 0.5, "salt_vs_pepper": 0.5, "p": 1.}
        call_args = (torch.rand(1, 3, 3, 3),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSaltAndPepperNoise,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:655: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.salt_pepper_noise.RandomSaltAndPepperNoise'>, target = 'tensorflow', init_args = ()
init_kwargs = {'amount': 0.5, 'p': 1.0, 'salt_vs_pepper': 0.5}
call_args = (tensor([[[[0.2906, 0.7945, 0.3666],
          [0.0985, 0.7833, 0.5776],
          [0.4990, 0.3406, 0.3036]],

       ...66]],

         [[0.5978, 0.8716, 0.1906],
          [0.6721, 0.8264, 0.1782],
          [0.5047, 0.1652, 0.1147]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.29058862, 0.79448295, 0.3666432 ],
         [0.0984...67],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff207d53240, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=...567],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.29058862, 0.79448295, 0.3666432 ],
         [0.0984...67],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=...567],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.29058862, 0.79448295, 0.3666432 ],
         [0.0984...67],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.29058862, 0.79448295, 0.3666432 ],
         [0.09847...8567],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.29058862, 0.79448295, 0.3666432 ],
       ...567],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.29058862, 0.79448295, 0.3666432 ],
         [0.09847...8567],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...ue, False]],

        [[ True, False, False],
         [ True, False, False],
         [ True,  True, False]]]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff20cadcca0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff2048e2b00>
tensor = <function tensorflow_tensor_frnt at 0x7ff20c1d4310>
in_tensor = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.29058862, 0.79448295, 0.3666432 ],
         [0.09847...8567],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 3, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 3, 3, 3]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.29058862, 0.79448295, 0.3666432 ],
         [0.09847...8567],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...ue, False]],

        [[ True, False, False],
         [ True, False, False],
         [ True,  True, False]]]])>, ...}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.29058862, 0.79448295, 0.3666432 ],
         [0.09847...8567],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...ue, False]],

        [[ True, False, False],
         [ True, False, False],
         [ True,  True, False]]]])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff20cadcca0>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7ff20cadcaf0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7ff20cadf6d0>, tensorflow_get_item = <function tensorflow_get_item at 0x7ff20482ee60>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7ff2047e5fc0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7ff20cadf880>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7ff20cadfa30>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.29058862, 0.79448295, 0.3666432 ],
         [0.09847...8567],
         [0.6720547 , 0.8264082 , 0.17822021],
         [0.5047272 , 0.16522795, 0.11468494]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...ue, False]],

        [[ True, False, False],
         [ True, False, False],
         [ True,  True, False]]]])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....core.check import tensorflow_KORNIA_CHECK
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        tensorflow_KORNIA_CHECK(
            len(tensorflow_shape_frnt_(input)) in (3, 4), "Wrong input dimension."
        )
        if len(tensorflow_shape_frnt_(input)) == 3:
            input = input[None, :, :, :]
        tensorflow_KORNIA_CHECK(
            tensorflow_shape_frnt_(input)[1] in {3, 1},
            "Number of color channels should be 1 or 3.",
        )
        noisy_image = tensorflow_clone_frnt_(input)
        noisy_image = tensorflow_set_item_bknd(
>           noisy_image, params["mask_salt"].to(input.device), 1.0
        )
E       AttributeError: Exception encountered when calling tensorflow_RandomSaltAndPepperNoise.call().
E       
E       [1m'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'to'[0m
E       
E       Arguments received by tensorflow_RandomSaltAndPepperNoise.call():
E         â€¢ input=tf.Tensor(shape=(1, 3, 3, 3), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/salt_pepper_noise.py:96: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSaltAndPepperNoise
______________________________________________________________________________ test_RandomSharpness[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSharpness(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSharpness")
    
        init_args = (1.,)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSharpness,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:695: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.sharpness.RandomSharpness'>, target = 'tensorflow', init_args = (1.0,), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[0.2800, 0.3439, 0.0747, 0.7647, 0.9083],
          [0.7818, 0.6757, 0.4750, 0.8342, 0.5951],
          [0...., 0.0947],
          [0.5300, 0.3144, 0.6058, 0.0947, 0.9123],
          [0.0293, 0.7459, 0.6648, 0.3576, 0.4995]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.2800213 , 0.3438599 , 0.07466358, 0.7647492 , 0.908...947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff20c188840, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(1, 1, 5, 5), d...0947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.2800213 , 0.3438599 , 0.07466358, 0.7647492 , 0.908...947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(1, 1, 5, 5), d...0947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.2800213 , 0.3438599 , 0.07466358, 0.7647492 , 0.908...947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.2800213 , 0.3438599 , 0.07466358, 0.7647492 , 0.9082....0947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.2800213 , 0.3438599 , 0.07466358, 0.764749...0947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.2800213 , 0.3438599 , 0.07466358, 0.7647492 , 0.9082....0947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...py=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.31582546], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff212404f70>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff212237370>
tensor = <function tensorflow_tensor_frnt at 0x7ff2123fb880>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.2800213 , 0.3438599 , 0.07466358, 0.7647492 , 0.9082....0947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.2800213 , 0.3438599 , 0.07466358, 0.7647492 , 0.9082....0947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...py=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.31582546], dtype=float32)>}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.2800213 , 0.3438599 , 0.07466358, 0.7647492 , 0.9082....0947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...py=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.31582546], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff212404f70>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7ff207ba75b0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7ff207b59240>, tensorflow_get_item = <function tensorflow_get_item at 0x7ff2128afd90>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7ff212982ef0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7ff207b58ee0>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7ff207b58d30>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.2800213 , 0.3438599 , 0.07466358, 0.7647492 , 0.9082....0947237 , 0.9123394 ],
         [0.02932936, 0.7459117 , 0.6648449 , 0.3576023 , 0.49948174]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...py=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.31582546], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        factor = params["sharpness"]
>       return sharpness(input, factor)
E       NameError: Exception encountered when calling tensorflow_RandomSharpness.call().
E       
E       [1mname 'sharpness' is not defined[0m
E       
E       Arguments received by tensorflow_RandomSharpness.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/sharpness.py:43: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSharpness
________________________________________________________________________________ test_RandomSnow[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSnow(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSnow")
    
        init_args = ()
        init_kwargs = {"p": 1.0, "snow_coefficient": (0.1, 0.6), "brightness": (1.0, 5.0)}
        call_args = (torch.rand(2, 3, 4, 4),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSnow,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:715: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.random_snow.RandomSnow'>, target = 'tensorflow', init_args = ()
init_kwargs = {'brightness': (1.0, 5.0), 'p': 1.0, 'snow_coefficient': (0.1, 0.6)}
call_args = (tensor([[[[0.2261, 0.5820, 0.7496, 0.4913],
          [0.3232, 0.9764, 0.6079, 0.0465],
          [0.7573, 0.5897, 0...., 0.9804, 0.7133, 0.4174],
          [0.9944, 0.5422, 0.2321, 0.5890],
          [0.4135, 0.0166, 0.2448, 0.6605]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.22607327, 0.5820191 , 0.74960345, 0.49131644],
    ...4219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff212307840, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), <...54219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.22607327, 0.5820191 , 0.74960345, 0.49131644],
    ...4219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), <...54219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.22607327, 0.5820191 , 0.74960345, 0.49131644],
    ...4219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.22607327, 0.5820191 , 0.74960345, 0.49131644],
     ....54219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.22607327, 0.5820191 , 0.74960345, 0.491316...54219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.22607327, 0.5820191 , 0.74960345, 0.49131644],
     ....54219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:...4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.29681852, 0.4684637 ], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff207cf5900>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff207c44dc0>
tensor = <function tensorflow_tensor_frnt at 0x7ff207d669e0>
in_tensor = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.22607327, 0.5820191 , 0.74960345, 0.49131644],
     ....54219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([2, 3, 4, 4]), batch_shape = ivy.frontends.torch.Size([2, 3, 4, 4]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.22607327, 0.5820191 , 0.74960345, 0.49131644],
     ....54219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:...4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.29681852, 0.4684637 ], dtype=float32)>}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.22607327, 0.5820191 , 0.74960345, 0.49131644],
     ....54219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:...4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.29681852, 0.4684637 ], dtype=float32)>}
flags = {}
transform = <tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],

       [[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff207cf5900>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7ff207cf5ab0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7ff207cf4940>, tensorflow_get_item = <function tensorflow_get_item at 0x7ff2122c3b50>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7ff20480fb50>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7ff207cf4700>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7ff207cf45e0>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.22607327, 0.5820191 , 0.74960345, 0.49131644],
     ....54219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:...4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.29681852, 0.4684637 ], dtype=float32)>}
flags = {}
transform = <tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],

       [[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....core.check import tensorflow_KORNIA_CHECK
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....color.hls import tensorflow_rgb_to_hls
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_zeros_like_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_where_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clamp_frnt_
        from ....color.hls import tensorflow_hls_to_rgb
    
        tensorflow_KORNIA_CHECK(
            tensorflow_shape_frnt_(input)[1] == 3,
            "Number of color channels should be 3.",
        )
        tensorflow_KORNIA_CHECK(
            len(tensorflow_shape_frnt_(input)) in (3, 4), "Wrong input dimension."
        )
        if len(tensorflow_shape_frnt_(input)) == 3:
            input = input[None, :, :, :]
>       input_HLS = tensorflow_rgb_to_hls(input)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/random_snow.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.22607327, 0.5820191 , 0.74960345, 0.49131644],
     ....54219276, 0.23213965, 0.58902955],
         [0.41352767, 0.01660573, 0.24480677, 0.66052276]]]],
      dtype=float32)>
eps = 1e-08

    def tensorflow_rgb_to_hls(image, eps=1e-08):
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.pointwise_ops import sub
        from ..core._backend import where
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_requires_grad_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_empty_like_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_add_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_mul_frnt
    
        if not isinstance(image, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(image)}")
        if len(tensorflow_shape_frnt_(image)) < 3 or tensorflow_shape_frnt_(image)[-3] != 3:
            raise ValueError(
                f"Input size must have a shape of (*, 3, H, W). Got {tensorflow_shape_frnt_(image)}"
            )
        _RGB2HSL_IDX = tensor(
            [[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype
        )
        _img_max: typing.Any = tensorflow_max_frnt_(image, -3)
        maxc = _img_max[0]
        imax = _img_max[1]
        minc: typing.Any = tensorflow_min_frnt_(image, -3)[0]
        if tensorflow_requires_grad_frnt_(image):
            l_ = maxc + minc
            s = maxc - minc
            h = l_
            image_hls = l_
        else:
            image_hls = tensorflow_empty_like_frnt(image)
            h, l_, s = (
                image_hls[..., 0, :, :],
                image_hls[..., 1, :, :],
                image_hls[..., 2, :, :],
            )
            tensorflow_add_frnt(maxc, minc, out=l_)
            sub(maxc, minc, out=s)
        im = image / tensorflow_unsqueeze_frnt_(s + eps, -3)
        s = s / (where(l_ < 1.0, l_, 2.0 - l_) + eps)
        l_ = l_ / 2
        r, g, b = im[..., 0, :, :], im[..., 1, :, :], im[..., 2, :, :]
        cond = imax[..., None, :, :] == _RGB2HSL_IDX
        if tensorflow_requires_grad_frnt_(image):
            h = (g - b) % 6 * cond[..., 0, :, :]
        else:
>           tensorflow_mul_frnt((g - b) % 6, cond[..., 0, :, :], out=h)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[ 4.,  4.,  2.,  4.],
        [-0., -0., -0.,  4.],
      ...,
        [False, False, False, False],
        [False, False,  True,  True],
        [ True, False, False, False]]])>)
kwargs = {'out': <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[0., 0., 0., 0.],
        [0., 0., 0., 0.],
       ...    [[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]]], dtype=float32)>}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7ff207c456c0>
array_like = <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[ 4.,  4.,  2.,  4.],
        [-0., -0., -0.,  4.],
       ...  0.,  4.],
        [ 0.,  2.,  2.,  4.],
        [ 2.,  0.,  4.,  0.],
        [ 4.,  4.,  4., -0.]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[ 4.,  4.,  2.,  4.],
        [-0., -0., -0.,  4.],
       ...  0.,  4.],
        [ 0.,  2.,  2.,  4.],
        [ 2.,  0.,  4.,  0.],
        [ 4.,  4.,  4., -0.]]], dtype=float32)>
other = <tf.Tensor: shape=(2, 4, 4), dtype=bool, numpy=
array([[[False,  True,  True, False],
        [False,  True, False, Fa...],
        [False, False, False, False],
        [False, False,  True,  True],
        [ True, False, False, False]]])>

    @tensorflow_handle_methods
    def tensorflow_mul_frnt(input, other, *, out=None):
        from ...backends.tensorflow.elementwise import tensorflow_multiply
    
>       return tensorflow_multiply(input, other, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/pointwise_ops.py:142: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x1 = <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[ 4.,  4.,  2.,  4.],
        [-0., -0., -0.,  4.],
       ...  0.,  4.],
        [ 0.,  2.,  2.,  4.],
        [ 2.,  0.,  4.,  0.],
        [ 4.,  4.,  4., -0.]]], dtype=float32)>
x2 = <tf.Tensor: shape=(2, 4, 4), dtype=bool, numpy=
array([[[False,  True,  True, False],
        [False,  True, False, Fa...],
        [False, False, False, False],
        [False, False,  True,  True],
        [ True, False, False, False]]])>

    def tensorflow_multiply(
        x1: Union[float, tensorflow.Tensor, tensorflow.Variable],
        x2: Union[float, tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .creation import tensorflow_asarray
        from ...ivy.data_type import tensorflow_default_dtype_bknd
        from ...ivy.general import tensorflow_is_array_bknd
    
        oirg_x1 = x1
        oirg_x2 = x2
        try:
            dtype = (
                x1.dtype
                if hasattr(x1, "dtype")
                else x2.dtype
                if hasattr(x2, "dtype")
                else tensorflow_default_dtype_bknd()
            )
            if not tensorflow_is_array_bknd(x1):
                x1 = tensorflow_asarray(x1, dtype=dtype)
            if not tensorflow_is_array_bknd(x2):
                x2 = tensorflow_asarray(x2, dtype=dtype)
        except:
            x1 = oirg_x1
            x2 = oirg_x2
>       return tensorflow.math.multiply(x1, x2)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomSnow.call().
E       
E       [1mcannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a bool tensor [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_RandomSnow.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 4, 4), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/elementwise.py:540: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSnow
______________________________________________________________________________ test_RandomSolarize[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSolarize(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSolarize")
    
        init_args = (0.1, 0.1)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSolarize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:735: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.solarize.RandomSolarize'>, target = 'tensorflow', init_args = (0.1, 0.1), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[0.3726, 0.2768, 0.6722, 0.5014, 0.6490],
          [0.3453, 0.5038, 0.8405, 0.9915, 0.8517],
          [0...., 0.7701],
          [0.5104, 0.9194, 0.7350, 0.2416, 0.8014],
          [0.6757, 0.6040, 0.6091, 0.1191, 0.5271]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.37258267, 0.276765  , 0.67221785, 0.5013985 , 0.649...4163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff20c188a40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=...24163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.37258267, 0.276765  , 0.67221785, 0.5013985 , 0.649...4163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=...24163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.37258267, 0.276765  , 0.67221785, 0.5013985 , 0.649...4163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.37258267, 0.276765  , 0.67221785, 0.5013985 , 0.6490....24163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.37258267, 0.276765  , 0.67221785, 0.501398...24163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.37258267, 0.276765  , 0.67221785, 0.5013985 , 0.6490....24163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.072157], dtype=float32)>, 'batch_prob': <tf.Tenso...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.4293499], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff20c181900>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff207afa5f0>
tensor = <function tensorflow_tensor_frnt at 0x7ff207a376d0>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.37258267, 0.276765  , 0.67221785, 0.5013985 , 0.6490....24163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.37258267, 0.276765  , 0.67221785, 0.5013985 , 0.6490....24163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.072157], dtype=float32)>, 'batch_prob': <tf.Tenso...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.4293499], dtype=float32)>}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.37258267, 0.276765  , 0.67221785, 0.5013985 , 0.6490....24163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.072157], dtype=float32)>, 'batch_prob': <tf.Tenso...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.4293499], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff20c181900>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7ff20c1812d0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7ff212283880>, tensorflow_get_item = <function tensorflow_get_item at 0x7ff204736950>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7ff2047acdc0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7ff212283c70>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7ff212281480>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.37258267, 0.276765  , 0.67221785, 0.5013985 , 0.6490....24163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.072157], dtype=float32)>, 'batch_prob': <tf.Tenso...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.4293499], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....enhance.adjust import tensorflow_solarize
    
        thresholds = params["thresholds"]
        additions: typing.Any
        if "additions" in params:
            additions = params["additions"]
        else:
            additions = None
>       return tensorflow_solarize(input, thresholds, additions)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/solarize.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.37258267, 0.276765  , 0.67221785, 0.5013985 , 0.6490....24163848, 0.80136853],
         [0.6756824 , 0.6039572 , 0.60914403, 0.1191051 , 0.52707696]]]],
      dtype=float32)>
thresholds = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.4293499], dtype=float32)>, additions = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.072157], dtype=float32)>

    def tensorflow_solarize(input, thresholds=0.5, additions=None):
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_as_tensor_frnt
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_all_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clamp_frnt_
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
        if not isinstance(thresholds, (float, tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(
                f"The factor should be either a float or Tensor. Got {type(thresholds)}"
            )
        if isinstance(thresholds, (float,)):
            thresholds = tensorflow_as_tensor_frnt(thresholds)
        if additions is not None:
            if not isinstance(additions, (float, tensorflow.Tensor, tensorflow.Variable)):
                raise TypeError(
                    f"The factor should be either a float or Tensor. Got {type(additions)}"
                )
            if isinstance(additions, (float,)):
                additions = tensorflow_as_tensor_frnt(additions)
>           if not tensorflow_all_frnt((additions < 0.5) * (additions > -0.5)):

Translated_Outputs/tensorflow_outputs/kornia/enhance/adjust.py:379: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>, <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>), kwargs = {}
arg = <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomSolarize.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_RandomSolarize.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSolarize
_____________________________________________________________________________ test_RandomResizedCrop[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomResizedCrop(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomResizedCrop")
    
        init_args = ()
        init_kwargs = {"size": (3, 3), "scale": (3., 3.), "ratio": (2., 2.), "p": 1., "cropping_mode": "resample"}
        call_args = (torch.tensor([[[0., 1., 2.],
                                    [3., 4., 5.],
                                    [6., 7., 8.]]]),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomResizedCrop,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:951: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resized_crop.RandomResizedCrop'>, target = 'tensorflow', init_args = ()
init_kwargs = {'cropping_mode': 'resample', 'p': 1.0, 'ratio': (2.0, 2.0), 'scale': (3.0, 3.0), ...}, call_args = (tensor([[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
args = (<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>,), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff1f4b70c40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_...e=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
v = None, buffers = None, args = (<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>,), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_...e=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
v = None, buffers = None, args = (<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>,), kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>, replace_v = False, replace_buffers = False
call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros),)
kwargs = {'input': <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
input = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>, params = None, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff1f4e56c20>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff20461dfc0>
tensor = <function tensorflow_tensor_frnt at 0x7ff20c7a3be0>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3])

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
>           params = self.forward_parameters(batch_shape)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3])

    def forward_parameters(self, batch_shape):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        batch_prob = self.__batch_prob_generator__(
            batch_shape, self.p, self.p_batch, self.same_on_batch
        )
        to_apply = batch_prob > 0.5
>       _params = self.generate_parameters(
            tuple(
                (
                    int(tensorflow_item_frnt_(tensorflow_sum_frnt_(to_apply))),
                    *batch_shape[1:],
                )
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:199: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
batch_shape = (1, 1, 3, 3)

    def generate_parameters(self, batch_shape):
        if self._param_generator is not None:
>           return self._param_generator(batch_shape, self.same_on_batch)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), args = ((1, 1, 3, 3), False), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff1f4b70e40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ion.py', lineno=46, function='__call__', code_context=['            return call_fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), v = None, buffers = None, args = ((1, 1, 3, 3), False), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), v = None, buffers = None, args = ((1, 1, 3, 3), False), kwargs = {}, replace_v = False, replace_buffers = False
call_signature = <Signature (batch_shape, same_on_batch=False)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), batch_shape = (1, 1, 3, 3), same_on_batch = False

    def call(self, batch_shape, same_on_batch=False):
        from ....utils.helpers import tensorflow__extract_device_dtype
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...utils.helpers import tensorflow__adapted_rsampling
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_exp_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_floor_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_round_frnt_
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_sqrt_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_int_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_cumsum_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_bool_frnt_
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_arange_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_round_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_where_frnt_
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ....core._backend import tensor
        from ....core._backend import zeros
    
        batch_size = batch_shape[0]
        size = batch_shape[-2], batch_shape[-1]
        _device, _dtype = tensorflow__extract_device_dtype([self.scale, self.ratio])
        if batch_size == 0:
            return {
                "src": zeros([0, 4, 2], device=_device, dtype=_dtype),
                "dst": zeros([0, 4, 2], device=_device, dtype=_dtype),
                "size": zeros([0, 2], device=_device, dtype=_dtype),
            }
        rand = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 10), self.rand_sampler, same_on_batch
            ),
            device=_device,
            dtype=_dtype,
        )
        area = (
            (rand * (self.scale[1] - self.scale[0]) + self.scale[0]) * size[0] * size[1]
        )
        log_ratio = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 10), self.log_ratio_sampler, same_on_batch
            ),
            device=_device,
            dtype=_dtype,
        )
        aspect_ratio = tensorflow_exp_frnt(log_ratio)
        w = tensorflow_floor_frnt_(
            tensorflow_round_frnt_(tensorflow_sqrt_frnt(area * aspect_ratio))
        )
        h = tensorflow_floor_frnt_(
            tensorflow_round_frnt_(tensorflow_sqrt_frnt(area / aspect_ratio))
        )
>       cond = tensorflow_int_frnt_((0 < w) * (w < size[0]) * (0 < h) * (h < size[1]))

Translated_Outputs/tensorflow_outputs/kornia/augmentation/random_generator/_2d/crop.py:328: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 10), dtype=bool, numpy=
array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,
...e=(1, 10), dtype=bool, numpy=
array([[False, False, False, False, False, False, False, False, False,
        False]])>)
kwargs = {}, arg = <tf.Tensor: shape=(1, 10), dtype=bool, numpy=
array([[False, False, False, False, False, False, False, False, False,
        False]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_ResizedCropGenerator.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_ResizedCropGenerator.call():
E         â€¢ batch_shape=('1', '1', '3', '3')
E         â€¢ same_on_batch=False

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomResizedCrop
______________________________________________________________________________ test_RandomRotation[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRotation(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomRotation")
    
        init_args = ()
        init_kwargs = {"degrees": 45.0, "p": 1.}
        call_args = (torch.tensor([[1., 0., 0., 2.],
                                   [0., 0., 0., 0.],
                                   [0., 1., 2., 0.],
                                   [0., 0., 1., 2.]]),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomRotation,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:974: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.rotation.RandomRotation'>, target = 'tensorflow', init_args = (), init_kwargs = {'degrees': 45.0, 'p': 1.0}
call_args = (tensor([[1., 0., 0., 2.],
        [0., 0., 0., 0.],
        [0., 1., 2., 0.],
        [0., 0., 1., 2.]]),), call_kwargs = {}, deterministic_output = False, backend_compile = False
tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
args = (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>,), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff1f4b72a40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=Tru...=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True), v = None, buffers = None
args = (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>,), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=Tru...=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True), v = None, buffers = None
args = (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>,), kwargs = {}
first_arr = <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>, replace_v = False
replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True),)
kwargs = {'input': <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...ay([9.452705], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff2042b29e0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff1f4e3f7f0>
tensor = <function tensorflow_tensor_frnt at 0x7ff2042a84c0>
in_tensor = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([4, 4]), batch_shape = ivy.frontends.torch.Size([1, 1, 4, 4]), flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
in_tensor = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...ay([9.452705], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...ay([9.452705], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...ay([9.452705], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....geometry.transform.affwarp import tensorflow__compute_tensor_center
        from ....geometry.transform.affwarp import tensorflow__compute_rotation_matrix
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        angles: typing.Any = tensorflow_to_frnt_(params["degrees"], input)
        center: typing.Any = tensorflow__compute_tensor_center(input)
        rotation_mat: typing.Any = tensorflow__compute_rotation_matrix(
>           angles, center.expand(tensorflow_shape_frnt_(angles)[0], -1)
        )
E       AttributeError: Exception encountered when calling tensorflow_RandomRotation.call().
E       
E       [1m'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'expand'[0m
E       
E       Arguments received by tensorflow_RandomRotation.call():
E         â€¢ input=tf.Tensor(shape=(4, 4), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/rotation.py:67: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomRotation
______________________________________________________________________________ test_RandomCutMixV2[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomCutMixV2(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomCutMixV2")
    
        input = torch.rand(2, 1, 3, 3)
        input[0] = torch.ones((1, 3, 3))
        label = torch.tensor([0, 1])
    
        init_args = ()
        init_kwargs = {"data_keys": ["input", "class"]}
        call_args = (input, label)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomCutMixV2,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1058: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.cutmix.RandomCutMixV2'>, target = 'tensorflow', init_args = (), init_kwargs = {'data_keys': ['input', 'class']}
call_args = (tensor([[[[1.0000, 1.0000, 1.0000],
          [1.0000, 1.0000, 1.0000],
          [1.0000, 1.0000, 1.0000]]],


        [[[0.1595, 0.3372, 0.7843],
          [0.9987, 0.6868, 0.3676],
          [0.0058, 0.8987, 0.6066]]]]), tensor([0, 1]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
>       transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)

kornia/augmentation/test_augmentation.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation._2d.mix.cutmix.RandomCutMixV2'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: source code not available

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomCutMixV2
_______________________________________________________________________________ test_RandomJigsaw[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomJigsaw(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomJigsaw")
    
        init_args = ((4, 4),)
        init_kwargs = {}
        call_args = (torch.randn(8, 3, 256, 256),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomJigsaw,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1078: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.jigsaw.RandomJigsaw'>, target = 'tensorflow', init_args = ((4, 4),), init_kwargs = {}
call_args = (tensor([[[[-9.2646e-01,  7.3287e-02, -8.9232e-01,  ...,  2.9964e-01,
           -1.9188e-01,  4.5195e-03],
          ...1e-01],
          [-4.2386e-01,  9.5506e-01,  7.6255e-01,  ...,  4.9147e-01,
           -1.2470e+00, -6.0032e-02]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4))
args = (<tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[-9.26461160e-01,  7.32873976e-02, -8.92315686e-01...124e-01,  7.62552738e-01, ...,
           4.91471559e-01, -1.24697649e+00, -6.00315668e-02]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff1f4937c40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), <tf.Tensor: shape=(8, 3, ...0124e-01,  7.62552738e-01, ...,
           4.91471559e-01, -1.24697649e+00, -6.00315668e-02]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[-9.26461160e-01,  7.32873976e-02, -8.92315686e-01...124e-01,  7.62552738e-01, ...,
           4.91471559e-01, -1.24697649e+00, -6.00315668e-02]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), <tf.Tensor: shape=(8, 3, ...0124e-01,  7.62552738e-01, ...,
           4.91471559e-01, -1.24697649e+00, -6.00315668e-02]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[-9.26461160e-01,  7.32873976e-02, -8.92315686e-01...124e-01,  7.62552738e-01, ...,
           4.91471559e-01, -1.24697649e+00, -6.00315668e-02]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[-9.26461160e-01,  7.32873976e-02, -8.92315686e-01,...60124e-01,  7.62552738e-01, ...,
           4.91471559e-01, -1.24697649e+00, -6.00315668e-02]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)),)
kwargs = {'input': <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[-9.26461160e-01,  7.32873976e-02, -8.923...0124e-01,  7.62552738e-01, ...,
           4.91471559e-01, -1.24697649e+00, -6.00315668e-02]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[-9.26461160e-01,  7.32873976e-02, -8.923...0124e-01,  7.62552738e-01, ...,
           4.91471559e-01, -1.24697649e+00, -6.00315668e-02]]]],
      dtype=float32)>}

    def bind(self, /, *args, **kwargs):
        """Get a BoundArguments object, that maps the passed `args`
        and `kwargs` to the function's signature.  Raises `TypeError`
        if the passed arguments can not be bound.
        """
>       return self._bind(args, kwargs)

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[-9.26461160e-01,  7.32873976e-02, -8.923...0124e-01,  7.62552738e-01, ...,
           4.91471559e-01, -1.24697649e+00, -6.00315668e-02]]]],
      dtype=float32)>}

    def _bind(self, args, kwargs, *, partial=False):
        """Private method. Don't use directly."""
    
        arguments = {}
    
        parameters = iter(self.parameters.values())
        parameters_ex = ()
        arg_vals = iter(args)
    
        while True:
            # Let's iterate through the positional arguments and corresponding
            # parameters
            try:
                arg_val = next(arg_vals)
            except StopIteration:
                # No more positional arguments
                try:
                    param = next(parameters)
                except StopIteration:
                    # No more parameters. That's it. Just need to check that
                    # we have no `kwargs` after this while loop
                    break
                else:
                    if param.kind == _VAR_POSITIONAL:
                        # That's OK, just empty *args.  Let's start parsing
                        # kwargs
                        break
                    elif param.name in kwargs:
                        if param.kind == _POSITIONAL_ONLY:
                            msg = '{arg!r} parameter is positional only, ' \
                                  'but was passed as a keyword'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
                        parameters_ex = (param,)
                        break
                    elif (param.kind == _VAR_KEYWORD or
                                                param.default is not _empty):
                        # That's fine too - we have a default value for this
                        # parameter.  So, lets start parsing `kwargs`, starting
                        # with the current parameter
                        parameters_ex = (param,)
                        break
                    else:
                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,
                        # not in `kwargs`
                        if partial:
                            parameters_ex = (param,)
                            break
                        else:
                            msg = 'missing a required argument: {arg!r}'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
            else:
                # We have a positional argument to process
                try:
                    param = next(parameters)
                except StopIteration:
                    raise TypeError('too many positional arguments') from None
                else:
                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):
                        # Looks like we have no parameter for this positional
                        # argument
                        raise TypeError(
                            'too many positional arguments') from None
    
                    if param.kind == _VAR_POSITIONAL:
                        # We have an '*args'-like argument, let's fill it with
                        # all positional arguments we have left and move on to
                        # the next phase
                        values = [arg_val]
                        values.extend(arg_vals)
                        arguments[param.name] = tuple(values)
                        break
    
                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:
                        raise TypeError(
                            'multiple values for argument {arg!r}'.format(
                                arg=param.name)) from None
    
                    arguments[param.name] = arg_val
    
        # Now, we iterate through the remaining parameters to process
        # keyword arguments
        kwargs_param = None
        for param in itertools.chain(parameters_ex, parameters):
            if param.kind == _VAR_KEYWORD:
                # Memorize that we have a '**kwargs'-like parameter
                kwargs_param = param
                continue
    
            if param.kind == _VAR_POSITIONAL:
                # Named arguments don't refer to '*args'-like parameters.
                # We only arrive here if the positional arguments ended
                # before reaching the last parameter before *args.
                continue
    
            param_name = param.name
            try:
                arg_val = kwargs.pop(param_name)
            except KeyError:
                # We have no value for this parameter.  It's fine though,
                # if it has a default value, or it is an '*args'-like
                # parameter, left alone by the processing of positional
                # arguments.
                if (not partial and param.kind != _VAR_POSITIONAL and
                                                    param.default is _empty):
                    raise TypeError('missing a required argument: {arg!r}'. \
                                    format(arg=param_name)) from None
    
            else:
                if param.kind == _POSITIONAL_ONLY:
                    # This should never happen in case of a properly built
                    # Signature object (but let's have this check here
                    # to ensure correct behaviour just in case)
                    raise TypeError('{arg!r} parameter is positional only, '
                                    'but was passed as a keyword'. \
                                    format(arg=param.name))
    
                arguments[param_name] = arg_val
    
        if kwargs:
            if kwargs_param is not None:
                # Process our '**kwargs'-like parameter
                arguments[kwargs_param.name] = kwargs
            else:
>               raise TypeError(
                    'got an unexpected keyword argument {arg!r}'.format(
                        arg=next(iter(kwargs))))
E               TypeError: got an unexpected keyword argument 'input'

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3166: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomJigsaw
_______________________________________________________________________________ test_RandomMixUpV2[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomMixUpV2(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomMixUpV2")
    
        init_args = ()
        init_kwargs = {"data_keys": ["input", "class"]}
        call_args = (torch.rand(2, 1, 3, 3), torch.tensor([0, 1]))
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomMixUpV2,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1098: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.mixup.RandomMixUpV2'>, target = 'tensorflow', init_args = (), init_kwargs = {'data_keys': ['input', 'class']}
call_args = (tensor([[[[0.8188, 0.8633, 0.2785],
          [0.9426, 0.7165, 0.3806],
          [0.9121, 0.5495, 0.6013]]],


        [[[0.8972, 0.5588, 0.8043],
          [0.9034, 0.7715, 0.8325],
          [0.4620, 0.3171, 0.9290]]]]), tensor([0, 1]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.81875265, 0.86333096, 0.27850914],
         [0.9425...   [0.46196282, 0.3171296 , 0.9290299 ]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x560a69e13690, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 1, 3, 3), d...   [0.46196282, 0.3171296 , 0.9290299 ]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.81875265, 0.86333096, 0.27850914],
         [0.9425...   [0.46196282, 0.3171296 , 0.9290299 ]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 1, 3, 3), d...   [0.46196282, 0.3171296 , 0.9290299 ]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.81875265, 0.86333096, 0.27850914],
         [0.9425...   [0.46196282, 0.3171296 , 0.9290299 ]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.81875265, 0.86333096, 0.27850914],
         [0.94256...3124],
         [0.9034053 , 0.77145624, 0.8324862 ],
         [0.46196282, 0.3171296 , 0.9290299 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.81875265, 0.86333096, 0.27850914],
       ...6282, 0.3171296 , 0.9290299 ]]]], dtype=float32)>, 'params': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.81875265, 0.86333096, 0.27850914],
       ...124],
         [0.9034053 , 0.77145624, 0.8324862 ],
         [0.46196282, 0.3171296 , 0.9290299 ]]]], dtype=float32)>}

    def bind(self, /, *args, **kwargs):
        """Get a BoundArguments object, that maps the passed `args`
        and `kwargs` to the function's signature.  Raises `TypeError`
        if the passed arguments can not be bound.
        """
>       return self._bind(args, kwargs)

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.81875265, 0.86333096, 0.27850914],
       ...124],
         [0.9034053 , 0.77145624, 0.8324862 ],
         [0.46196282, 0.3171296 , 0.9290299 ]]]], dtype=float32)>}

    def _bind(self, args, kwargs, *, partial=False):
        """Private method. Don't use directly."""
    
        arguments = {}
    
        parameters = iter(self.parameters.values())
        parameters_ex = ()
        arg_vals = iter(args)
    
        while True:
            # Let's iterate through the positional arguments and corresponding
            # parameters
            try:
                arg_val = next(arg_vals)
            except StopIteration:
                # No more positional arguments
                try:
                    param = next(parameters)
                except StopIteration:
                    # No more parameters. That's it. Just need to check that
                    # we have no `kwargs` after this while loop
                    break
                else:
                    if param.kind == _VAR_POSITIONAL:
                        # That's OK, just empty *args.  Let's start parsing
                        # kwargs
                        break
                    elif param.name in kwargs:
                        if param.kind == _POSITIONAL_ONLY:
                            msg = '{arg!r} parameter is positional only, ' \
                                  'but was passed as a keyword'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
                        parameters_ex = (param,)
                        break
                    elif (param.kind == _VAR_KEYWORD or
                                                param.default is not _empty):
                        # That's fine too - we have a default value for this
                        # parameter.  So, lets start parsing `kwargs`, starting
                        # with the current parameter
                        parameters_ex = (param,)
                        break
                    else:
                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,
                        # not in `kwargs`
                        if partial:
                            parameters_ex = (param,)
                            break
                        else:
                            msg = 'missing a required argument: {arg!r}'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
            else:
                # We have a positional argument to process
                try:
                    param = next(parameters)
                except StopIteration:
                    raise TypeError('too many positional arguments') from None
                else:
                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):
                        # Looks like we have no parameter for this positional
                        # argument
                        raise TypeError(
                            'too many positional arguments') from None
    
                    if param.kind == _VAR_POSITIONAL:
                        # We have an '*args'-like argument, let's fill it with
                        # all positional arguments we have left and move on to
                        # the next phase
                        values = [arg_val]
                        values.extend(arg_vals)
                        arguments[param.name] = tuple(values)
                        break
    
                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:
                        raise TypeError(
                            'multiple values for argument {arg!r}'.format(
                                arg=param.name)) from None
    
                    arguments[param.name] = arg_val
    
        # Now, we iterate through the remaining parameters to process
        # keyword arguments
        kwargs_param = None
        for param in itertools.chain(parameters_ex, parameters):
            if param.kind == _VAR_KEYWORD:
                # Memorize that we have a '**kwargs'-like parameter
                kwargs_param = param
                continue
    
            if param.kind == _VAR_POSITIONAL:
                # Named arguments don't refer to '*args'-like parameters.
                # We only arrive here if the positional arguments ended
                # before reaching the last parameter before *args.
                continue
    
            param_name = param.name
            try:
                arg_val = kwargs.pop(param_name)
            except KeyError:
                # We have no value for this parameter.  It's fine though,
                # if it has a default value, or it is an '*args'-like
                # parameter, left alone by the processing of positional
                # arguments.
                if (not partial and param.kind != _VAR_POSITIONAL and
                                                    param.default is _empty):
                    raise TypeError('missing a required argument: {arg!r}'. \
                                    format(arg=param_name)) from None
    
            else:
                if param.kind == _POSITIONAL_ONLY:
                    # This should never happen in case of a properly built
                    # Signature object (but let's have this check here
                    # to ensure correct behaviour just in case)
                    raise TypeError('{arg!r} parameter is positional only, '
                                    'but was passed as a keyword'. \
                                    format(arg=param.name))
    
                arguments[param_name] = arg_val
    
        if kwargs:
            if kwargs_param is not None:
                # Process our '**kwargs'-like parameter
                arguments[kwargs_param.name] = kwargs
            else:
>               raise TypeError(
                    'got an unexpected keyword argument {arg!r}'.format(
                        arg=next(iter(kwargs))))
E               TypeError: got an unexpected keyword argument 'input'

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3166: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomMixUpV2
_______________________________________________________________________________ test_RandomMosaic[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomMosaic(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomMosaic")
    
        init_args = ((300, 300),)
        init_kwargs = {"data_keys": ["input", "bbox_xyxy"]}
        call_args = (
            torch.randn(8, 3, 224, 224),
            torch.tensor([[
                [70, 5, 150, 100],
                [60, 180, 175, 220],
            ]]).repeat(8, 1, 1),
        )
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomMosaic,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.mosaic.RandomMosaic'>, target = 'tensorflow', init_args = ((300, 300),), init_kwargs = {'data_keys': ['input', 'bbox_xyxy']}
call_args = (tensor([[[[ 1.4642, -1.2033, -0.6195,  ..., -0.2955,  0.0243,  0.4747],
          [ 0.0116, -0.9446, -0.1096,  ..., -...[ 70,   5, 150, 100],
         [ 60, 180, 175, 220]],

        [[ 70,   5, 150, 100],
         [ 60, 180, 175, 220]]]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0, ..._size=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice)
args = (<tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 1.464156  , -1.2032562 , -0.6194901 , ..., -0.29... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff1f4df5040, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0,... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0, ..._size=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice)
v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 1.464156  , -1.2032562 , -0.6194901 , ..., -0.29... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0,... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0, ..._size=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice)
v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 1.464156  , -1.2032562 , -0.6194901 , ..., -0.29... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 1.464156  , -1.2032562 , -0.6194901 , ..., -0.295...      [ 0.63769096,  1.0068017 ,  0.41328746, ...,  0.6982985 ,
          -0.80875844,  0.5135008 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0,...ize=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice),)
kwargs = {'input': <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 1.464156  , -1.2032562 , -0.6194901 , .... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 1.464156  , -1.2032562 , -0.6194901 , ....     [ 0.63769096,  1.0068017 ,  0.41328746, ...,  0.6982985 ,
          -0.80875844,  0.5135008 ]]]], dtype=float32)>}

    def bind(self, /, *args, **kwargs):
        """Get a BoundArguments object, that maps the passed `args`
        and `kwargs` to the function's signature.  Raises `TypeError`
        if the passed arguments can not be bound.
        """
>       return self._bind(args, kwargs)

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 1.464156  , -1.2032562 , -0.6194901 , ....     [ 0.63769096,  1.0068017 ,  0.41328746, ...,  0.6982985 ,
          -0.80875844,  0.5135008 ]]]], dtype=float32)>}

    def _bind(self, args, kwargs, *, partial=False):
        """Private method. Don't use directly."""
    
        arguments = {}
    
        parameters = iter(self.parameters.values())
        parameters_ex = ()
        arg_vals = iter(args)
    
        while True:
            # Let's iterate through the positional arguments and corresponding
            # parameters
            try:
                arg_val = next(arg_vals)
            except StopIteration:
                # No more positional arguments
                try:
                    param = next(parameters)
                except StopIteration:
                    # No more parameters. That's it. Just need to check that
                    # we have no `kwargs` after this while loop
                    break
                else:
                    if param.kind == _VAR_POSITIONAL:
                        # That's OK, just empty *args.  Let's start parsing
                        # kwargs
                        break
                    elif param.name in kwargs:
                        if param.kind == _POSITIONAL_ONLY:
                            msg = '{arg!r} parameter is positional only, ' \
                                  'but was passed as a keyword'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
                        parameters_ex = (param,)
                        break
                    elif (param.kind == _VAR_KEYWORD or
                                                param.default is not _empty):
                        # That's fine too - we have a default value for this
                        # parameter.  So, lets start parsing `kwargs`, starting
                        # with the current parameter
                        parameters_ex = (param,)
                        break
                    else:
                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,
                        # not in `kwargs`
                        if partial:
                            parameters_ex = (param,)
                            break
                        else:
                            msg = 'missing a required argument: {arg!r}'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
            else:
                # We have a positional argument to process
                try:
                    param = next(parameters)
                except StopIteration:
                    raise TypeError('too many positional arguments') from None
                else:
                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):
                        # Looks like we have no parameter for this positional
                        # argument
                        raise TypeError(
                            'too many positional arguments') from None
    
                    if param.kind == _VAR_POSITIONAL:
                        # We have an '*args'-like argument, let's fill it with
                        # all positional arguments we have left and move on to
                        # the next phase
                        values = [arg_val]
                        values.extend(arg_vals)
                        arguments[param.name] = tuple(values)
                        break
    
                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:
                        raise TypeError(
                            'multiple values for argument {arg!r}'.format(
                                arg=param.name)) from None
    
                    arguments[param.name] = arg_val
    
        # Now, we iterate through the remaining parameters to process
        # keyword arguments
        kwargs_param = None
        for param in itertools.chain(parameters_ex, parameters):
            if param.kind == _VAR_KEYWORD:
                # Memorize that we have a '**kwargs'-like parameter
                kwargs_param = param
                continue
    
            if param.kind == _VAR_POSITIONAL:
                # Named arguments don't refer to '*args'-like parameters.
                # We only arrive here if the positional arguments ended
                # before reaching the last parameter before *args.
                continue
    
            param_name = param.name
            try:
                arg_val = kwargs.pop(param_name)
            except KeyError:
                # We have no value for this parameter.  It's fine though,
                # if it has a default value, or it is an '*args'-like
                # parameter, left alone by the processing of positional
                # arguments.
                if (not partial and param.kind != _VAR_POSITIONAL and
                                                    param.default is _empty):
                    raise TypeError('missing a required argument: {arg!r}'. \
                                    format(arg=param_name)) from None
    
            else:
                if param.kind == _POSITIONAL_ONLY:
                    # This should never happen in case of a properly built
                    # Signature object (but let's have this check here
                    # to ensure correct behaviour just in case)
                    raise TypeError('{arg!r} parameter is positional only, '
                                    'but was passed as a keyword'. \
                                    format(arg=param.name))
    
                arguments[param_name] = arg_val
    
        if kwargs:
            if kwargs_param is not None:
                # Process our '**kwargs'-like parameter
                arguments[kwargs_param.name] = kwargs
            else:
>               raise TypeError(
                    'got an unexpected keyword argument {arg!r}'.format(
                        arg=next(iter(kwargs))))
E               TypeError: got an unexpected keyword argument 'input'

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3166: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomMosaic
___________________________________________________________________________ test_RandomTransplantation[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomTransplantation(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomTransplantation")
    
        init_args = ()
        init_kwargs = {"p": 1.}
        call_args = (torch.randn(2, 3, 5, 5), torch.randint(0, 3, (2, 5, 5)))
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomTransplantation,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.transplantation.RandomTransplantation'>, target = 'tensorflow', init_args = (), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[ 0.0297, -0.8402,  0.5900, -0.9448,  0.4508],
          [ 0.8289,  0.3207,  0.4729, -1.3481,  0.3146],
   ...0, 2, 2, 1],
         [1, 0, 1, 2, 0],
         [1, 0, 2, 0, 1],
         [0, 2, 1, 0, 2],
         [2, 1, 0, 1, 0]]]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.02967906, -0.84016305,  0.5900121 , -0.9447646 ,
 ...1, 0, 2, 2, 1],
        [1, 0, 1, 2, 0],
        [1, 0, 2, 0, 1],
        [0, 2, 1, 0, 2],
        [2, 1, 0, 1, 0]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff1f484f240, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=floa...1, 0, 2, 2, 1],
        [1, 0, 1, 2, 0],
        [1, 0, 2, 0, 1],
        [0, 2, 1, 0, 2],
        [2, 1, 0, 1, 0]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.02967906, -0.84016305,  0.5900121 , -0.9447646 ,
 ...1, 0, 2, 2, 1],
        [1, 0, 1, 2, 0],
        [1, 0, 2, 0, 1],
        [0, 2, 1, 0, 2],
        [2, 1, 0, 1, 0]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=floa...1, 0, 2, 2, 1],
        [1, 0, 1, 2, 0],
        [1, 0, 2, 0, 1],
        [0, 2, 1, 0, 2],
        [2, 1, 0, 1, 0]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.02967906, -0.84016305,  0.5900121 , -0.9447646 ,
 ...1, 0, 2, 2, 1],
        [1, 0, 1, 2, 0],
        [1, 0, 2, 0, 1],
        [0, 2, 1, 0, 2],
        [2, 1, 0, 1, 0]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.02967906, -0.84016305,  0.5900121 , -0.9447646 ,
  ...  1.584572  ],
         [-2.3618531 ,  2.1391535 , -0.9958717 ,  1.1088691 ,
           0.5754823 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.02967906, -0.84016305,  0.5900121 , -0.94...1, 0, 2, 2, 1],
        [1, 0, 1, 2, 0],
        [1, 0, 2, 0, 1],
        [0, 2, 1, 0, 2],
        [2, 1, 0, 1, 0]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False)
params = <tf.Tensor: shape=(2, 5, 5), dtype=int64, numpy=
array([[[2, 0, 2, 0, 2],
        [1, 0, 2, 2, 1],
        [2, 2, 2, 1...[1, 0, 2, 2, 1],
        [1, 0, 1, 2, 0],
        [1, 0, 2, 0, 1],
        [0, 2, 1, 0, 2],
        [2, 1, 0, 1, 0]]])>
data_keys = None, input = ()
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.02967906, -0.84016305,  0.5900121 , -0.94... 1.584572  ],
         [-2.3618531 ,  2.1391535 , -0.9958717 ,  1.1088691 ,
           0.5754823 ]]]], dtype=float32)>}
tensorflow_get_item = <function tensorflow_get_item at 0x7ff1f4d3acb0>, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff1f49432e0>
tensorflow_clone_frnt_ = <function tensorflow_clone_frnt_ at 0x7ff1f4940a60>, tensorflow__validate_input_dtype = <function tensorflow__validate_input_dtype at 0x7ff1f46af0a0>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7ff1f4e2ab90>, keys = [<tensorflow_DataKey.IMAGE: 0>, <tensorflow_DataKey.MASK: 1>]

    def call(self, *input, params=None, data_keys=None, **kwargs):
        from ....constants import tensorflow_DataKey
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...utils.helpers import tensorflow__validate_input_dtype
        from .....ivy.functional.frontends.torch.tensor import (
            tensorflow_index_put_frnt_,
        )
    
        keys: typing.Any
        if data_keys is None:
            keys = self.data_keys
        else:
            keys = [tensorflow_DataKey.get(inp) for inp in data_keys]
        if params is None:
            mask: typing.Any = tensorflow_get_item(
                input, keys.index(tensorflow_DataKey.MASK)
            )
            self._params = self.forward_parameters(tensorflow_shape_frnt_(mask))
        else:
            self._params = params
>       if any(
            k not in self._params
            for k in ["acceptor_indices", "donor_indices", "selection"]
        ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:242: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7ff2128c0610>

    if any(
>       k not in self._params
        for k in ["acceptor_indices", "donor_indices", "selection"]
    ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(5, 5), dtype=int64, numpy=
array([[2, 0, 2, 0, 2],
       [1, 0, 2, 2, 1],
       [2, 2, 2, 1, 0],
       [0, 0, 1, 2, 0],
       [1, 2, 2, 0, 0]])>, 'acceptor_indices')
kwargs = {}, arg = 'acceptor_indices'

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       TypeError: Exception encountered when calling tensorflow_RandomTransplantation.call().
E       
E       [1mdata type 'acceptor_indices' not understood[0m
E       
E       Arguments received by tensorflow_RandomTransplantation.call():
E         â€¢ input=<class 'inspect._empty'>
E         â€¢ params=tf.Tensor(shape=(2, 5, 5), dtype=int64)
E         â€¢ data_keys=None
E         â€¢ kwargs={'input': 'tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)'}

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomTransplantation
_____________________________________________________________________________ test_RandomRotation3D[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRotation3D(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomRotation3D")
    
        init_args = ((15., 20., 20.),)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 3, 3, 3),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomRotation3D,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1264: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._3d.geometric.rotation.RandomRotation3D'>, target = 'tensorflow', init_args = ((15.0, 20.0, 20.0),), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[[0.7971, 0.8303, 0.3958],
           [0.8028, 0.7885, 0.9747],
           [0.7197, 0.6193, 0.6938]],

    ...,

          [[0.4430, 0.1685, 0.6763],
           [0.7504, 0.8705, 0.6930],
           [0.1619, 0.3353, 0.6836]]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.7970566 , 0.8302813 , 0.39577913],
          [0...,
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff1f41c2240, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, a...],
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.7970566 , 0.8302813 , 0.39577913],
          [0...,
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, a...],
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.7970566 , 0.8302813 , 0.39577913],
          [0...,
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.7970566 , 0.8302813 , 0.39577913],
          [0.... ],
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.7970566 , 0.8302813 , 0.39577913],
   ...],
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.7970566 , 0.8302813 , 0.39577913],
          [0.... ],
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...14669], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.509762], dtype=float32)>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff1f43ebe20>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff1f48470a0>
tensor = <function tensorflow_tensor_frnt at 0x7ff1f45185e0>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.7970566 , 0.8302813 , 0.39577913],
          [0.... ],
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 3, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3, 3]), flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.7970566 , 0.8302813 , 0.39577913],
          [0.... ],
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...14669], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.509762], dtype=float32)>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/base.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.7970566 , 0.8302813 , 0.39577913],
          [0.... ],
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...14669], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.509762], dtype=float32)>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/base.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.7970566 , 0.8302813 , 0.39577913],
          [0.... ],
          [0.75039667, 0.87046766, 0.6930221 ],
          [0.16194898, 0.33530337, 0.68363404]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...14669], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-17.509762], dtype=float32)>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....geometry.transform.affwarp import tensorflow__compute_tensor_center3d
        from ....geometry.transform.affwarp import tensorflow__compute_rotation_matrix3d
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        yaw: typing.Any = tensorflow_to_frnt_(params["yaw"], input)
        pitch: typing.Any = tensorflow_to_frnt_(params["pitch"], input)
        roll: typing.Any = tensorflow_to_frnt_(params["roll"], input)
        center: typing.Any = tensorflow__compute_tensor_center3d(input)
        rotation_mat: typing.Any = tensorflow__compute_rotation_matrix3d(
>           yaw, pitch, roll, center.expand(tensorflow_shape_frnt_(yaw)[0], -1)
        )
E       AttributeError: Exception encountered when calling tensorflow_RandomRotation3D.call().
E       
E       [1m'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'expand'[0m
E       
E       Arguments received by tensorflow_RandomRotation3D.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 3, 3, 3), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/geometric/rotation.py:65: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomRotation3D
__________________________________________________________________________ test_RandomTransplantation3D[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomTransplantation3D(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomTransplantation3D")
    
        init_args = ()
        init_kwargs = {"p": 1.}
        call_args = (torch.randn(2, 3, 5, 5), torch.randint(0, 3, (2, 5, 5)))
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomTransplantation3D,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1344: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._3d.mix.transplantation.RandomTransplantation3D'>, target = 'tensorflow', init_args = (), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[-1.5167, -0.7282,  0.1546,  0.7797, -0.2509],
          [-0.5404, -1.6361,  0.7422,  0.6606, -0.9847],
   ...1, 2, 2, 1],
         [0, 0, 1, 2, 0],
         [2, 1, 0, 0, 0],
         [2, 1, 2, 1, 1],
         [2, 2, 1, 2, 1]]]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.5166594 , -0.72816753,  0.15457644,  0.7797271 ,
 ...2, 1, 2, 2, 1],
        [0, 0, 1, 2, 0],
        [2, 1, 0, 0, 0],
        [2, 1, 2, 1, 1],
        [2, 2, 1, 2, 1]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff1ecbaba40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=fl...2, 1, 2, 2, 1],
        [0, 0, 1, 2, 0],
        [2, 1, 0, 0, 0],
        [2, 1, 2, 1, 1],
        [2, 2, 1, 2, 1]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.5166594 , -0.72816753,  0.15457644,  0.7797271 ,
 ...2, 1, 2, 2, 1],
        [0, 0, 1, 2, 0],
        [2, 1, 0, 0, 0],
        [2, 1, 2, 1, 1],
        [2, 2, 1, 2, 1]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=fl...2, 1, 2, 2, 1],
        [0, 0, 1, 2, 0],
        [2, 1, 0, 0, 0],
        [2, 1, 2, 1, 1],
        [2, 2, 1, 2, 1]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.5166594 , -0.72816753,  0.15457644,  0.7797271 ,
 ...2, 1, 2, 2, 1],
        [0, 0, 1, 2, 0],
        [2, 1, 0, 0, 0],
        [2, 1, 2, 1, 1],
        [2, 2, 1, 2, 1]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.5166594 , -0.72816753,  0.15457644,  0.7797271 ,
  ...  0.70137393],
         [ 0.5494364 , -0.71509343,  1.5095766 , -0.27276143,
          -0.39673355]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.5166594 , -0.72816753,  0.15457644,  0.77...2, 1, 2, 2, 1],
        [0, 0, 1, 2, 0],
        [2, 1, 0, 0, 0],
        [2, 1, 2, 1, 1],
        [2, 2, 1, 2, 1]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False)
params = <tf.Tensor: shape=(2, 5, 5), dtype=int64, numpy=
array([[[1, 2, 0, 0, 2],
        [0, 2, 2, 0, 1],
        [0, 2, 1, 0...[2, 1, 2, 2, 1],
        [0, 0, 1, 2, 0],
        [2, 1, 0, 0, 0],
        [2, 1, 2, 1, 1],
        [2, 2, 1, 2, 1]]])>
data_keys = None, input = ()
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.5166594 , -0.72816753,  0.15457644,  0.77... 0.70137393],
         [ 0.5494364 , -0.71509343,  1.5095766 , -0.27276143,
          -0.39673355]]]], dtype=float32)>}
tensorflow_get_item = <function tensorflow_get_item at 0x7ff2041f2cb0>, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff1f4405870>
tensorflow_clone_frnt_ = <function tensorflow_clone_frnt_ at 0x7ff1f4404820>, tensorflow__validate_input_dtype = <function tensorflow__validate_input_dtype at 0x7ff1ecc69f30>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7ff1ecea6050>, keys = [<tensorflow_DataKey.IMAGE: 0>, <tensorflow_DataKey.MASK: 1>]

    def call(self, *input, params=None, data_keys=None, **kwargs):
        from ....constants import tensorflow_DataKey
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...utils.helpers import tensorflow__validate_input_dtype
        from .....ivy.functional.frontends.torch.tensor import (
            tensorflow_index_put_frnt_,
        )
    
        keys: typing.Any
        if data_keys is None:
            keys = self.data_keys
        else:
            keys = [tensorflow_DataKey.get(inp) for inp in data_keys]
        if params is None:
            mask: typing.Any = tensorflow_get_item(
                input, keys.index(tensorflow_DataKey.MASK)
            )
            self._params = self.forward_parameters(tensorflow_shape_frnt_(mask))
        else:
            self._params = params
>       if any(
            k not in self._params
            for k in ["acceptor_indices", "donor_indices", "selection"]
        ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:242: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7ff1f4747130>

    if any(
>       k not in self._params
        for k in ["acceptor_indices", "donor_indices", "selection"]
    ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(5, 5), dtype=int64, numpy=
array([[1, 2, 0, 0, 2],
       [0, 2, 2, 0, 1],
       [0, 2, 1, 0, 1],
       [1, 0, 1, 1, 0],
       [0, 0, 2, 1, 1]])>, 'acceptor_indices')
kwargs = {}, arg = 'acceptor_indices'

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       TypeError: Exception encountered when calling tensorflow_RandomTransplantation3D.call().
E       
E       [1mdata type 'acceptor_indices' not understood[0m
E       
E       Arguments received by tensorflow_RandomTransplantation3D.call():
E         â€¢ input=<class 'inspect._empty'>
E         â€¢ params=tf.Tensor(shape=(2, 5, 5), dtype=int64)
E         â€¢ data_keys=None
E         â€¢ kwargs={'input': 'tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)'}

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomTransplantation3D
______________________________________________________________________________ test_LongestMaxSize[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LongestMaxSize(target_framework, mode, backend_compile):
        print("kornia.augmentation.LongestMaxSize")
    
        init_args = (100,)
        init_kwargs = {}
        call_args = (torch.rand(10, 3, 200, 200),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.LongestMaxSize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=True,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1404: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resize.LongestMaxSize'>, target = 'tensorflow', init_args = (100,), init_kwargs = {}
call_args = (tensor([[[[0.6619, 0.4777, 0.2084,  ..., 0.1139, 0.3895, 0.0737],
          [0.6563, 0.2683, 0.2207,  ..., 0.0599, 0...., 0.3473, 0.2475,  ..., 0.7966, 0.1813, 0.9936],
          [0.5651, 0.9108, 0.6062,  ..., 0.3926, 0.7649, 0.5403]]]]),)
call_kwargs = {}, deterministic_output = True, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
args = (<tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.66186804, 0.47769427, 0.20836747, ..., 0.11394...
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff1ec88b040, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bili...,
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.66186804, 0.47769427, 0.20836747, ..., 0.11394...
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bili...,
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.66186804, 0.47769427, 0.20836747, ..., 0.11394...
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.66186804, 0.47769427, 0.20836747, ..., 0.113948...],
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False),)
kwargs = {'input': <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.66186804, 0.47769427, 0.20836747, ......,
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.66186804, 0.47769427, 0.20836747, ..., 0.113948...],
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff1f4153400>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff1f4941e10>
tensor = <function tensorflow_tensor_frnt at 0x7ff1eca01510>
in_tensor = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.66186804, 0.47769427, 0.20836747, ..., 0.113948...],
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([10, 3, 200, 200]), batch_shape = ivy.frontends.torch.Size([10, 3, 200, 200])
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
in_tensor = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.66186804, 0.47769427, 0.20836747, ..., 0.113948...],
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.66186804, 0.47769427, 0.20836747, ..., 0.113948...],
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.66186804, 0.47769427, 0.20836747, ..., 0.113948...],
         [0.56510955, 0.91076374, 0.60617113, ..., 0.3925706 ,
          0.7648697 , 0.54028547]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_as_tensor_frnt,
        )
        from ....geometry.transform.imgwarp import tensorflow_get_perspective_transform
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
    
>       if params["output_size"] == tensorflow_shape_frnt_(input)[-2:]:
E       ValueError: Exception encountered when calling tensorflow_LongestMaxSize.call().
E       
E       [1mThe truth value of an array with more than one element is ambiguous. Use a.any() or a.all()[0m
E       
E       Arguments received by tensorflow_LongestMaxSize.call():
E         â€¢ input=tf.Tensor(shape=(10, 3, 200, 200), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/resize.py:67: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.LongestMaxSize
__________________________________________________________________________________ test_Resize[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Resize(target_framework, mode, backend_compile):
        print("kornia.augmentation.Resize")
    
        init_args = ((100, 100),)
        init_kwargs = {}
        call_args = (torch.rand(10, 3, 50, 50),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.Resize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=True,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1424: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resize.Resize'>, target = 'tensorflow', init_args = ((100, 100),), init_kwargs = {}
call_args = (tensor([[[[4.9729e-01, 1.3363e-01, 1.6647e-01,  ..., 3.7177e-01,
           1.7108e-01, 4.4398e-02],
          [5.981... 2.4175e-01],
          [8.4577e-01, 4.2222e-02, 9.7776e-01,  ..., 6.8863e-01,
           6.7190e-01, 9.7810e-01]]]]),)
call_kwargs = {}, deterministic_output = True, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.97289717e-01, 1.33629799e-01, 1.66470110e-01, .....2220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x560a6d215050, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resamp...22220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.97289717e-01, 1.33629799e-01, 1.66470110e-01, .....2220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resamp...22220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.97289717e-01, 1.33629799e-01, 1.66470110e-01, .....2220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.97289717e-01, 1.33629799e-01, 1.66470110e-01, .......22220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False),)
kwargs = {'input': <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.97289717e-01, 1.33629799e-01, 1.6647011...22220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.97289717e-01, 1.33629799e-01, 1.66470110e-01, .......22220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff1f4153400>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff1f4941e10>
tensor = <function tensorflow_tensor_frnt at 0x7ff1eca01510>
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.97289717e-01, 1.33629799e-01, 1.66470110e-01, .......22220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([10, 3, 50, 50]), batch_shape = ivy.frontends.torch.Size([10, 3, 50, 50])
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.97289717e-01, 1.33629799e-01, 1.66470110e-01, .......22220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.97289717e-01, 1.33629799e-01, 1.66470110e-01, .......22220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.97289717e-01, 1.33629799e-01, 1.66470110e-01, .......22220826e-02, 9.77757454e-01, ...,
          6.88629150e-01, 6.71901584e-01, 9.78100717e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_as_tensor_frnt,
        )
        from ....geometry.transform.imgwarp import tensorflow_get_perspective_transform
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
    
>       if params["output_size"] == tensorflow_shape_frnt_(input)[-2:]:
E       ValueError: Exception encountered when calling tensorflow_Resize.call().
E       
E       [1mThe truth value of an array with more than one element is ambiguous. Use a.any() or a.all()[0m
E       
E       Arguments received by tensorflow_Resize.call():
E         â€¢ input=tf.Tensor(shape=(10, 3, 50, 50), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/resize.py:67: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.Resize
______________________________________________________________________________ test_SmallestMaxSize[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SmallestMaxSize(target_framework, mode, backend_compile):
        print("kornia.augmentation.SmallestMaxSize")
    
        init_args = (100,)
        init_kwargs = {}
        call_args = (torch.rand(10, 3, 50, 50),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.SmallestMaxSize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=True,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resize.SmallestMaxSize'>, target = 'tensorflow', init_args = (100,), init_kwargs = {}
call_args = (tensor([[[[9.5246e-01, 6.4759e-03, 2.2856e-01,  ..., 2.1528e-01,
           4.7277e-01, 1.3760e-01],
          [6.590... 6.4718e-01],
          [4.1642e-01, 9.7380e-01, 7.6272e-01,  ..., 3.7232e-01,
           1.7648e-01, 3.6222e-01]]]]),)
call_kwargs = {}, deterministic_output = True, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.52463984e-01, 6.47592545e-03, 2.28556156e-01, .....3801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7ff1ec593640, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bi...73801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.52463984e-01, 6.47592545e-03, 2.28556156e-01, .....3801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bi...73801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.52463984e-01, 6.47592545e-03, 2.28556156e-01, .....3801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.52463984e-01, 6.47592545e-03, 2.28556156e-01, .......73801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False),)
kwargs = {'input': <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.52463984e-01, 6.47592545e-03, 2.2855615...73801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.52463984e-01, 6.47592545e-03, 2.28556156e-01, .......73801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7ff1ec8eb9a0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7ff1ecc93eb0>
tensor = <function tensorflow_tensor_frnt at 0x7ff1ec8f3640>
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.52463984e-01, 6.47592545e-03, 2.28556156e-01, .......73801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([10, 3, 50, 50]), batch_shape = ivy.frontends.torch.Size([10, 3, 50, 50])
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.52463984e-01, 6.47592545e-03, 2.28556156e-01, .......73801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.52463984e-01, 6.47592545e-03, 2.28556156e-01, .......73801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.52463984e-01, 6.47592545e-03, 2.28556156e-01, .......73801613e-01, 7.62718499e-01, ...,
          3.72321427e-01, 1.76476836e-01, 3.62219810e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_as_tensor_frnt,
        )
        from ....geometry.transform.imgwarp import tensorflow_get_perspective_transform
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
    
>       if params["output_size"] == tensorflow_shape_frnt_(input)[-2:]:
E       ValueError: Exception encountered when calling tensorflow_SmallestMaxSize.call().
E       
E       [1mThe truth value of an array with more than one element is ambiguous. Use a.any() or a.all()[0m
E       
E       Arguments received by tensorflow_SmallestMaxSize.call():
E         â€¢ input=tf.Tensor(shape=(10, 3, 50, 50), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/resize.py:67: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.SmallestMaxSize
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/augmentation/test_augmentation.py::test_RandomClahe[tensorflow-s2s-False] - IndexError: Exception encountered when calling tensorflow_RandomClahe.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomLinearCornerIllumination[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_LinearCornerIlluminationGe...
FAILED kornia/augmentation/test_augmentation.py::test_RandomMotionBlur[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_RandomMotionBlur.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomRain[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_...
FAILED kornia/augmentation/test_augmentation.py::test_RandomSaltAndPepperNoise[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_RandomSaltAndPepperNoise.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomSharpness[tensorflow-s2s-False] - NameError: Exception encountered when calling tensorflow_RandomSharpness.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomSnow[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_...
FAILED kornia/augmentation/test_augmentation.py::test_RandomSolarize[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorf...
FAILED kornia/augmentation/test_augmentation.py::test_RandomResizedCrop[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tens...
FAILED kornia/augmentation/test_augmentation.py::test_RandomRotation[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_RandomRotation.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomCutMixV2[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: source code not available
FAILED kornia/augmentation/test_augmentation.py::test_RandomJigsaw[tensorflow-s2s-False] - TypeError: got an unexpected keyword argument 'input'
FAILED kornia/augmentation/test_augmentation.py::test_RandomMixUpV2[tensorflow-s2s-False] - TypeError: got an unexpected keyword argument 'input'
FAILED kornia/augmentation/test_augmentation.py::test_RandomMosaic[tensorflow-s2s-False] - TypeError: got an unexpected keyword argument 'input'
FAILED kornia/augmentation/test_augmentation.py::test_RandomTransplantation[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_RandomTransplantation.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomRotation3D[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_RandomRotation3D.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomTransplantation3D[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_RandomTransplantation3D.call().
FAILED kornia/augmentation/test_augmentation.py::test_LongestMaxSize[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_LongestMaxSize.call().
FAILED kornia/augmentation/test_augmentation.py::test_Resize[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_Resize.call().
FAILED kornia/augmentation/test_augmentation.py::test_SmallestMaxSize[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_SmallestMaxSize.call().
============================================================================== 20 failed, 48 passed in 1846.37s (0:30:46) ==============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/geometry/test_linalg.py ........                                                                                                                                                          [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 8 passed in 110.58s (0:01:50) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 35 items

kornia/test_losses.py F................FFF.F.............                                                                                                                                        [100%]

=============================================================================================== FAILURES ===============================================================================================
_________________________________________________________________________________ test_ssim_loss[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ssim_loss(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 4, 5, 5),
            torch.rand(1, 4, 5, 5),
            5,
        )
        trace_kwargs = {'max_val': 1.0, 'eps': 1e-12, 'reduction': 'mean', 'padding': 'same'}
        test_args = (
            torch.rand(5, 4, 5, 5),
            torch.rand(5, 4, 5, 5),
            7,
        )
        test_kwargs = {'max_val': 1.0, 'eps': 1e-12, 'reduction': 'mean', 'padding': 'same'}
>       _test_function(
            kornia.losses.ssim_loss,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_losses.py:29: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function ssim_loss at 0x7f5003fb72e0>
trace_args = (tensor([[[[0.1540, 0.5195, 0.5850, 0.5686, 0.4269],
          [0.9785, 0.3518, 0.6036, 0.3333, 0.6617],
          [0....0.1797],
          [0.6934, 0.4227, 0.3733, 0.4037, 0.6561],
          [0.8025, 0.0138, 0.5826, 0.7110, 0.5347]]]]), 5)
trace_kwargs = {'eps': 1e-12, 'max_val': 1.0, 'padding': 'same', 'reduction': 'mean'}
test_args = (tensor([[[[0.6093, 0.3322, 0.5907, 0.9569, 0.3625],
          [0.4171, 0.3910, 0.3488, 0.4119, 0.1310],
          [0....01, 4.7468e-01, 1.7225e-01, 4.7432e-01],
          [4.8205e-01, 3.5037e-01, 9.3286e-01, 1.4298e-01, 1.3152e-01]]]]), 7)
test_kwargs = {'eps': 1e-12, 'max_val': 1.0, 'padding': 'same', 'reduction': 'mean'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False
deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function ssim_loss at 0x7f5003fb72e0>
trace_args = (tensor([[[[0.1540, 0.5195, 0.5850, 0.5686, 0.4269],
          [0.9785, 0.3518, 0.6036, 0.3333, 0.6617],
          [0....0.1797],
          [0.6934, 0.4227, 0.3733, 0.4037, 0.6561],
          [0.8025, 0.0138, 0.5826, 0.7110, 0.5347]]]]), 5)
trace_kwargs = {'eps': 1e-12, 'max_val': 1.0, 'padding': 'same', 'reduction': 'mean'}
test_args = (tensor([[[[0.6093, 0.3322, 0.5907, 0.9569, 0.3625],
          [0.4171, 0.3910, 0.3488, 0.4119, 0.1310],
          [0....01, 4.7468e-01, 1.7225e-01, 4.7432e-01],
          [4.8205e-01, 3.5037e-01, 9.3286e-01, 1.4298e-01, 1.3152e-01]]]]), 7)
test_kwargs = {'eps': 1e-12, 'max_val': 1.0, 'padding': 'same', 'reduction': 'mean'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

img1 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.15402246, 0.5195033 , 0.58501756, 0.5685529 , 0.4269....50214684, 0.05400628],
         [0.02261436, 0.35319418, 0.6055907 , 0.4764629 , 0.7820191 ]]]],
      dtype=float32)>
img2 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.9368327 , 0.63875574, 0.07084757, 0.23638195, 0.3854....40372264, 0.6561174 ],
         [0.80249   , 0.01375943, 0.5825807 , 0.71104413, 0.5346947 ]]]],
      dtype=float32)>
window_size = 5, max_val = 1.0, eps = 1e-12, reduction = 'mean', padding = 'same'

    def tensorflow_ssim_loss(
        img1, img2, window_size, max_val=1.0, eps=1e-12, reduction="mean", padding="same"
    ):
        from ..metrics.ssim import tensorflow_ssim
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_clamp_frnt
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_mean_frnt
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
    
>       ssim_map: typing.Any = tensorflow_ssim(
            img1, img2, window_size, max_val, eps, padding
        )

Translated_Outputs/tensorflow_outputs/kornia/losses/ssim.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

img1 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.15402246, 0.5195033 , 0.58501756, 0.5685529 , 0.4269....50214684, 0.05400628],
         [0.02261436, 0.35319418, 0.6055907 , 0.4764629 , 0.7820191 ]]]],
      dtype=float32)>
img2 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.9368327 , 0.63875574, 0.07084757, 0.23638195, 0.3854....40372264, 0.6561174 ],
         [0.80249   , 0.01375943, 0.5825807 , 0.71104413, 0.5346947 ]]]],
      dtype=float32)>
window_size = 5, max_val = 1.0, eps = 1e-12, padding = 'same'

    def tensorflow_ssim(img1, img2, window_size, max_val=1.0, eps=1e-12, padding="same"):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..filters.kernels import tensorflow_get_gaussian_kernel1d
        from ..filters.filter import tensorflow_filter2d_separable
        from ..filters.filter import tensorflow__compute_padding
    
        if not isinstance(img1, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input img1 type is not a torch.Tensor. Got {type(img1)}")
        if not isinstance(img2, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input img2 type is not a torch.Tensor. Got {type(img2)}")
        if not isinstance(max_val, (float,)):
            raise TypeError(f"Input max_val type is not a float. Got {type(max_val)}")
        if not len(tensorflow_shape_frnt_(img1)) == 4:
            raise ValueError(
                f"Invalid img1 shape, we expect BxCxHxW. Got: {tensorflow_shape_frnt_(img1)}"
            )
        if not len(tensorflow_shape_frnt_(img2)) == 4:
            raise ValueError(
                f"Invalid img2 shape, we expect BxCxHxW. Got: {tensorflow_shape_frnt_(img2)}"
            )
        if not tensorflow_shape_frnt_(img1) == tensorflow_shape_frnt_(img2):
            raise ValueError(
                f"img1 and img2 shapes must be the same. Got: {tensorflow_shape_frnt_(img1)} and {tensorflow_shape_frnt_(img2)}"
            )
        kernel: typing.Any = tensorflow_get_gaussian_kernel1d(
            window_size, 1.5, device=img1.device, dtype=img1.dtype
        )
        C1: typing.Any = (0.01 * max_val) ** 2
        C2: typing.Any = (0.03 * max_val) ** 2
>       mu1: typing.Any = tensorflow_filter2d_separable(img1, kernel, kernel)

Translated_Outputs/tensorflow_outputs/kornia/metrics/ssim.py:76: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.15402246, 0.5195033 , 0.58501756, 0.5685529 , 0.4269....50214684, 0.05400628],
         [0.02261436, 0.35319418, 0.6055907 , 0.4764629 , 0.7820191 ]]]],
      dtype=float32)>
kernel_x = <tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]],
      dtype=float32)>
kernel_y = <tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]],
      dtype=float32)>, border_type = 'reflect', normalized = False
padding = 'same'

    def tensorflow_filter2d_separable(
        input, kernel_x, kernel_y, border_type="reflect", normalized=False, padding="same"
    ):
>       out_x = tensorflow_filter2d(
            input, kernel_x[..., None, :], border_type, normalized, padding
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.58501756, 0.5195033 , 0.15402246, 0.5195033 , 0.5850...0.02261436, 0.35319418, 0.6055907 ,
          0.4764629 , 0.7820191 , 0.4764629 , 0.6055907 ]]]],
      dtype=float32)>
kernel = <tf.Tensor: shape=(1, 1, 5), dtype=float32, numpy=
array([[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]],
      dtype=float32)>, border_type = 'reflect', normalized = False
padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.58501756, 0.5195033 , 0.15402246, 0.5195033 , 0.5850...0.02261436, 0.35319418, 0.6055907 ,
          0.4764629 , 0.7820191 , 0.4764629 , 0.6055907 ]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 1, 5), dtype=float32, numpy=
array([[[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.1200...88074, 0.12007838]]],


       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 4

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.58501756, 0.5195033 , 0.15402246, 0.5195033 , 0.5850...0.02261436, 0.35319418, 0.6055907 ,
          0.4764629 , 0.7820191 , 0.4764629 , 0.6055907 ]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 1, 5), dtype=float32, numpy=
array([[[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.1200...88074, 0.12007838]]],


       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 4

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.58501756, 0.5195033 , 0.15402246, 0.5195033 , 0.585...

       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f4ffd9abbe0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f4ffd994790>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.58501756, 0.5195033 , 0.15402246, 0.5195033 , 0.585...

       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f4ffd9ab880>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f4ffd9abbe0>, tensorflow_asarray = <function tensorflow_asarray at 0x7f4ffd985c60>
tensorflow_get_item = <function tensorflow_get_item at 0x7f4ffd994790>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 5, 9, 4), dtype=float32, numpy=
array([[[[0.58501756, 0.751763  , 0.83627415, 0.4839759 ],
     ....72261786, 0.37744635, 0.4764629 ],
         [0.641649  , 0.49093562, 0.09350437, 0.6055907 ]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(1, 5, 1, 4), dtype=float32, numpy=
array([[[[0.12007838, 0.12007838, 0.12007838, 0.12007838]],

   ...3388074, 0.23388074, 0.23388074]],

        [[0.12007838, 0.12007838, 0.12007838, 0.12007838]]]],
      dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 9, 4), dtype=float32, numpy=
array([[[[0.58501756, 0.751763  , 0.83627415, 0.4839759 ],
    ...2007838],
         [0.12007838],
         [0.12007838],
         [0.12007838]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f4ffd9abbe0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f4ffd994790>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 9, 4, 5), dtype=float32, numpy=
array([[[[0.58501756, 0.6035892 , 0.70078546, 0.42347634, 0.6416....3873021 , 0.09350437],
         [0.4839759 , 0.10881978, 0.01539022, 0.73277485, 0.6055907 ]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(1, 5, 4, 1), dtype=float32, numpy=
array([[[[0.12007838],
         [0.12007838],
         [0.120078...8074]],

        [[0.12007838],
         [0.12007838],
         [0.12007838],
         [0.12007838]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 9, 4, 5), dtype=float32, numpy=
array([[[[0.58501756, 0.6035892 , 0.70078546, 0.42347634, 0.641...0.12007838],
         [0.12007838]]]], dtype=float32)>, [1, 1, 1, 1], [(0, 0), (0, 0), (0, 0), (0, 0)], 'NHWC', [1, 1])
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 5 vs 4 [Op:DepthwiseConv2dNative] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.losses.ssim.ssim_loss
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/ivy/ivy/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.
  warnings.warn(
______________________________________________________________________________ test_HausdorffERLoss[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HausdorffERLoss(target_framework, mode, backend_compile):
        print("kornia.losses.HausdorffERLoss")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHausdorffERLoss = ivy.transpile(kornia.losses.HausdorffERLoss, source="torch", target=target_framework)
    
        torch_loss_fn = kornia.losses.HausdorffERLoss()
        transpiled_loss_fn = TranspiledHausdorffERLoss()
    
        torch_args = (
            torch.randn(5, 3, 20, 20),
            (torch.rand(5, 1, 20, 20) * 2).long(),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_res = torch_loss_fn(*torch_args)
>       transpiled_res = transpiled_loss_fn(*transpiled_args)

kornia/test_losses.py:446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
args = (<tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[-1.35627043e+00,  1.56926560e+00, -1.48539748e-02, ...        ...,
         [0, 1, 1, ..., 0, 1, 1],
         [1, 0, 1, ..., 0, 0, 1],
         [1, 0, 0, ..., 1, 0, 1]]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f4ffe6b9640, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss(), <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[-1.35627043e+00,  1.5...        ...,
         [0, 1, 1, ..., 0, 1, 1],
         [1, 0, 1, ..., 0, 0, 1],
         [1, 0, 0, ..., 1, 0, 1]]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[-1.35627043e+00,  1.56926560e+00, -1.48539748e-02, ...        ...,
         [0, 1, 1, ..., 0, 1, 1],
         [1, 0, 1, ..., 0, 0, 1],
         [1, 0, 0, ..., 1, 0, 1]]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss(), <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[-1.35627043e+00,  1.5...        ...,
         [0, 1, 1, ..., 0, 1, 1],
         [1, 0, 1, ..., 0, 0, 1],
         [1, 0, 0, ..., 1, 0, 1]]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[-1.35627043e+00,  1.56926560e+00, -1.48539748e-02, ...        ...,
         [0, 1, 1, ..., 0, 1, 1],
         [1, 0, 1, ..., 0, 0, 1],
         [1, 0, 0, ..., 1, 0, 1]]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[-1.35627043e+00,  1.56926560e+00, -1.48539748e-02, ....92066e-01, -9.89434361e-01, ...,
           1.02773869e+00, -1.82507050e+00, -1.49818921e+00]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (pred, target)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss(),)
kwargs = {'pred': <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[-1.35627043e+00,  1.56926560e+00, -1.485397...        ...,
         [0, 1, 1, ..., 0, 1, 1],
         [1, 0, 1, ..., 0, 0, 1],
         [1, 0, 0, ..., 1, 0, 1]]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
pred = <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[-1.35627043e+00,  1.56926560e+00, -1.48539748e-02, ....92066e-01, -9.89434361e-01, ...,
           1.02773869e+00, -1.82507050e+00, -1.49818921e+00]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20), dtype=int64, numpy=
array([[[[1, 0, 1, ..., 0, 0, 0],
         [0, 1, 0, ..., 0, 0, ...         ...,
         [0, 1, 1, ..., 0, 1, 1],
         [1, 0, 1, ..., 0, 0, 1],
         [1, 0, 0, ..., 1, 0, 1]]]])>

    def call(self, pred, target):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
    
        if tensorflow_dim_frnt_(pred) != 4:
            raise ValueError(
                f"Only 2D images supported. Got {tensorflow_dim_frnt_(pred)}."
            )
        if not (
            tensorflow_max_frnt_(target) < tensorflow_size_frnt_(pred, 1)
            and tensorflow_min_frnt_(target) >= 0
            and target.dtype == tf.int64
        ):
            raise ValueError(
                f"Expect long type target value in range (0, {tensorflow_size_frnt_(pred, 1)}). ({tensorflow_min_frnt_(target)}, {tensorflow_max_frnt_(target)})"
            )
>       return super().call(pred, target)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:537: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
pred = <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[-1.35627043e+00,  1.56926560e+00, -1.48539748e-02, ....92066e-01, -9.89434361e-01, ...,
           1.02773869e+00, -1.82507050e+00, -1.49818921e+00]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20), dtype=int64, numpy=
array([[[[1, 0, 1, ..., 0, 0, 0],
         [0, 1, 0, ..., 0, 0, ...         ...,
         [0, 1, 1, ..., 0, 1, 1],
         [1, 0, 1, ..., 0, 0, 1],
         [1, 0, 0, ..., 1, 0, 1]]]])>

    def call(self, pred, target):
        from ..core._backend import where
        from ..core._backend import stack
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
    
        if not (
            tensorflow_shape_frnt_(pred)[2:] == tensorflow_shape_frnt_(target)[2:]
            and tensorflow_size_frnt_(pred, 0) == tensorflow_size_frnt_(target, 0)
            and tensorflow_size_frnt_(target, 1) == 1
        ):
            raise ValueError(
                f"Prediction and target need to be of same size, and target should not be one-hot.Got {tensorflow_shape_frnt_(pred)} and {tensorflow_shape_frnt_(target)}."
            )
        if tensorflow_size_frnt_(pred, 1) < tensorflow_item_frnt_(
            tensorflow_max_frnt_(target)
        ):
            raise ValueError("Invalid target value.")
        out = stack(
>           [
                self.perform_erosion(
                    tensorflow_get_item(
                        pred, (slice(None, None, None), slice(i, i + 1, None))
                    ),
                    where(
                        target == i,
                        tensor(1, device=target.device, dtype=target.dtype),
                        tensor(0, device=target.device, dtype=target.dtype),
                    ),
                )
                for i in range(tensorflow_size_frnt_(pred, 1))
            ]
        )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <range_iterator object at 0x7f4ffd5fed90>

        [
>           self.perform_erosion(
                tensorflow_get_item(
                    pred, (slice(None, None, None), slice(i, i + 1, None))
                ),
                where(
                    target == i,
                    tensor(1, device=target.device, dtype=target.dtype),
                    tensor(0, device=target.device, dtype=target.dtype),
                ),
            )
            for i in range(tensorflow_size_frnt_(pred, 1))
        ]
    )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
pred = <tf.Tensor: shape=(5, 1, 20, 20), dtype=float32, numpy=
array([[[[-1.3562704 ,  1.5692656 , -0.01485397, ..., -0.48331...      [-0.73744255,  0.14787403,  1.2485743 , ..., -0.153814  ,
          -0.4567056 , -2.0753925 ]]]], dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20), dtype=int64, numpy=
array([[[[0, 1, 0, ..., 1, 1, 1],
         [1, 0, 1, ..., 1, 1, ...         ...,
         [1, 0, 0, ..., 1, 0, 0],
         [0, 1, 0, ..., 1, 1, 0],
         [0, 1, 1, ..., 0, 1, 0]]]])>

    def perform_erosion(self, pred, target):
        from ..core._backend import as_tensor
        from ..core._backend import zeros_like
        from ..core._backend import where
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_ones_like_v_0p4p0_and_above_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
    
        bound = (pred - target) ** 2
        kernel = as_tensor(self.kernel, device=pred.device, dtype=pred.dtype)
        eroded = zeros_like(bound, device=pred.device, dtype=pred.dtype)
        mask = tensorflow_ones_like_v_0p4p0_and_above_frnt(
            bound, device=pred.device, dtype=tf.bool
        )
        padding = (tensorflow_size_frnt_(kernel, -1) - 1) // 2
        for k in range(self.k):
>           dilation = self.conv(bound, weight=kernel, padding=padding, groups=1)
E           TypeError: Exception encountered when calling tensorflow_HausdorffERLoss.call().
E           
E           [1mtensorflow_conv2d_frnt() got multiple values for argument 'weight'[0m
E           
E           Arguments received by tensorflow_HausdorffERLoss.call():
E             â€¢ pred=tf.Tensor(shape=(5, 3, 20, 20), dtype=float32)
E             â€¢ target=tf.Tensor(shape=(5, 1, 20, 20), dtype=int64)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:82: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.losses.HausdorffERLoss
_____________________________________________________________________________ test_HausdorffERLoss3D[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HausdorffERLoss3D(target_framework, mode, backend_compile):
        print("kornia.losses.HausdorffERLoss3D")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHausdorffERLoss3D = ivy.transpile(kornia.losses.HausdorffERLoss3D, source="torch", target=target_framework)
    
        torch_loss_fn = kornia.losses.HausdorffERLoss3D()
        transpiled_loss_fn = TranspiledHausdorffERLoss3D()
    
        torch_args = (
            torch.randn(5, 3, 20, 20, 20),
            (torch.rand(5, 1, 20, 20, 20) * 2).long(),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_res = torch_loss_fn(*torch_args)
>       transpiled_res = transpiled_loss_fn(*transpiled_args)

kornia/test_losses.py:471: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
args = (<tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 1.90677911e-01,  1.54916406e+00, -5.52952528e...    ...,
          [0, 0, 0, ..., 0, 0, 0],
          [1, 1, 1, ..., 0, 0, 1],
          [0, 0, 1, ..., 0, 1, 1]]]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f4ffe25e9f0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss3D(), <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 1.90677911e-0...    ...,
          [0, 0, 0, ..., 0, 0, 0],
          [1, 1, 1, ..., 0, 0, 1],
          [0, 0, 1, ..., 0, 1, 1]]]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 1.90677911e-01,  1.54916406e+00, -5.52952528e...    ...,
          [0, 0, 0, ..., 0, 0, 0],
          [1, 1, 1, ..., 0, 0, 1],
          [0, 0, 1, ..., 0, 1, 1]]]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss3D(), <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 1.90677911e-0...    ...,
          [0, 0, 0, ..., 0, 0, 0],
          [1, 1, 1, ..., 0, 0, 1],
          [0, 0, 1, ..., 0, 1, 1]]]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 1.90677911e-01,  1.54916406e+00, -5.52952528e...    ...,
          [0, 0, 0, ..., 0, 0, 0],
          [1, 1, 1, ..., 0, 0, 1],
          [0, 0, 1, ..., 0, 1, 1]]]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 1.90677911e-01,  1.54916406e+00, -5.52952528e-...449e+00, -8.74141157e-01, ...,
           -6.51087314e-02, -1.15337849e+00,  8.58689994e-02]]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (pred, target)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss3D(),)
kwargs = {'pred': <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 1.90677911e-01,  1.54916406e+00, -5.5...    ...,
          [0, 0, 0, ..., 0, 0, 0],
          [1, 1, 1, ..., 0, 0, 1],
          [0, 0, 1, ..., 0, 1, 1]]]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
pred = <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 1.90677911e-01,  1.54916406e+00, -5.52952528e-...449e+00, -8.74141157e-01, ...,
           -6.51087314e-02, -1.15337849e+00,  8.58689994e-02]]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=int64, numpy=
array([[[[[1, 1, 1, ..., 0, 0, 1],
          [1, 1, 0, ..., ...     ...,
          [0, 0, 0, ..., 0, 0, 0],
          [1, 1, 1, ..., 0, 0, 1],
          [0, 0, 1, ..., 0, 1, 1]]]]])>

    def call(self, pred, target):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
    
        if tensorflow_dim_frnt_(pred) != 5:
            raise ValueError(
                f"Only 3D images supported. Got {tensorflow_dim_frnt_(pred)}."
            )
>       return super().call(pred, target)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:564: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
pred = <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 1.90677911e-01,  1.54916406e+00, -5.52952528e-...449e+00, -8.74141157e-01, ...,
           -6.51087314e-02, -1.15337849e+00,  8.58689994e-02]]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=int64, numpy=
array([[[[[1, 1, 1, ..., 0, 0, 1],
          [1, 1, 0, ..., ...     ...,
          [0, 0, 0, ..., 0, 0, 0],
          [1, 1, 1, ..., 0, 0, 1],
          [0, 0, 1, ..., 0, 1, 1]]]]])>

    def call(self, pred, target):
        from ..core._backend import where
        from ..core._backend import stack
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
    
        if not (
            tensorflow_shape_frnt_(pred)[2:] == tensorflow_shape_frnt_(target)[2:]
            and tensorflow_size_frnt_(pred, 0) == tensorflow_size_frnt_(target, 0)
            and tensorflow_size_frnt_(target, 1) == 1
        ):
            raise ValueError(
                f"Prediction and target need to be of same size, and target should not be one-hot.Got {tensorflow_shape_frnt_(pred)} and {tensorflow_shape_frnt_(target)}."
            )
        if tensorflow_size_frnt_(pred, 1) < tensorflow_item_frnt_(
            tensorflow_max_frnt_(target)
        ):
            raise ValueError("Invalid target value.")
        out = stack(
>           [
                self.perform_erosion(
                    tensorflow_get_item(
                        pred, (slice(None, None, None), slice(i, i + 1, None))
                    ),
                    where(
                        target == i,
                        tensor(1, device=target.device, dtype=target.dtype),
                        tensor(0, device=target.device, dtype=target.dtype),
                    ),
                )
                for i in range(tensorflow_size_frnt_(pred, 1))
            ]
        )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <range_iterator object at 0x7f4ffe254390>

        [
>           self.perform_erosion(
                tensorflow_get_item(
                    pred, (slice(None, None, None), slice(i, i + 1, None))
                ),
                where(
                    target == i,
                    tensor(1, device=target.device, dtype=target.dtype),
                    tensor(0, device=target.device, dtype=target.dtype),
                ),
            )
            for i in range(tensorflow_size_frnt_(pred, 1))
        ]
    )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
pred = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 1.90677911e-01,  1.54916406e+00, -5.52952528e-...009e-02,  3.77943009e-01, ...,
            1.51114017e-01, -1.98083043e-01, -8.23665380e-01]]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=int64, numpy=
array([[[[[0, 0, 0, ..., 1, 1, 0],
          [0, 0, 1, ..., ...     ...,
          [1, 1, 1, ..., 1, 1, 1],
          [0, 0, 0, ..., 1, 1, 0],
          [1, 1, 0, ..., 1, 0, 0]]]]])>

    def perform_erosion(self, pred, target):
        from ..core._backend import as_tensor
        from ..core._backend import zeros_like
        from ..core._backend import where
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_ones_like_v_0p4p0_and_above_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
    
        bound = (pred - target) ** 2
        kernel = as_tensor(self.kernel, device=pred.device, dtype=pred.dtype)
        eroded = zeros_like(bound, device=pred.device, dtype=pred.dtype)
        mask = tensorflow_ones_like_v_0p4p0_and_above_frnt(
            bound, device=pred.device, dtype=tf.bool
        )
        padding = (tensorflow_size_frnt_(kernel, -1) - 1) // 2
        for k in range(self.k):
>           dilation = self.conv(bound, weight=kernel, padding=padding, groups=1)
E           TypeError: Exception encountered when calling tensorflow_HausdorffERLoss3D.call().
E           
E           [1mtensorflow_conv3d_frnt() got multiple values for argument 'weight'[0m
E           
E           Arguments received by tensorflow_HausdorffERLoss3D.call():
E             â€¢ pred=tf.Tensor(shape=(5, 3, 20, 20, 20), dtype=float32)
E             â€¢ target=tf.Tensor(shape=(5, 1, 20, 20, 20), dtype=int64)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:86: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.losses.HausdorffERLoss3D
_________________________________________________________________________________ test_SSIMLoss[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SSIMLoss(target_framework, mode, backend_compile):
        print("kornia.losses.SSIMLoss")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSSIMLoss = ivy.transpile(kornia.losses.SSIMLoss, source="torch", target=target_framework)
    
        torch_loss_fn = kornia.losses.SSIMLoss(5)
        transpiled_loss_fn = TranspiledSSIMLoss(5)
    
        torch_args = (
            torch.rand(1, 4, 5, 5),
            torch.rand(1, 4, 5, 5),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_res = torch_loss_fn(*torch_args)
>       transpiled_res = transpiled_loss_fn(*transpiled_args)

kornia/test_losses.py:496: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SSIMLoss()
args = (<tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.02646267, 0.7363431 , 0.9920731 , 0.28324366, 0.527...699103  , 0.28130335],
         [0.27611083, 0.16323024, 0.76004493, 0.6368226 , 0.6570088 ]]]],
      dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55d96fdc8420, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SSIMLoss(), <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.02646267, 0.7363431 , 0.9920...699103  , 0.28130335],
         [0.27611083, 0.16323024, 0.76004493, 0.6368226 , 0.6570088 ]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SSIMLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.02646267, 0.7363431 , 0.9920731 , 0.28324366, 0.527...699103  , 0.28130335],
         [0.27611083, 0.16323024, 0.76004493, 0.6368226 , 0.6570088 ]]]],
      dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SSIMLoss(), <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.02646267, 0.7363431 , 0.9920...699103  , 0.28130335],
         [0.27611083, 0.16323024, 0.76004493, 0.6368226 , 0.6570088 ]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SSIMLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.02646267, 0.7363431 , 0.9920731 , 0.28324366, 0.527...699103  , 0.28130335],
         [0.27611083, 0.16323024, 0.76004493, 0.6368226 , 0.6570088 ]]]],
      dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.02646267, 0.7363431 , 0.9920731 , 0.28324366, 0.5275....47970527, 0.14698082],
         [0.21168607, 0.56656647, 0.56430477, 0.457393  , 0.28976035]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (img1, img2)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SSIMLoss(),)
kwargs = {'img1': <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.02646267, 0.7363431 , 0.9920731 , 0.2832436...699103  , 0.28130335],
         [0.27611083, 0.16323024, 0.76004493, 0.6368226 , 0.6570088 ]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SSIMLoss()
img1 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.02646267, 0.7363431 , 0.9920731 , 0.28324366, 0.5275....47970527, 0.14698082],
         [0.21168607, 0.56656647, 0.56430477, 0.457393  , 0.28976035]]]],
      dtype=float32)>
img2 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.77879226, 0.2577836 , 0.9668816 , 0.5921759 , 0.9944....699103  , 0.28130335],
         [0.27611083, 0.16323024, 0.76004493, 0.6368226 , 0.6570088 ]]]],
      dtype=float32)>

    def call(self, img1, img2):
>       return tensorflow_ssim_loss(
            img1,
            img2,
            self.window_size,
            self.max_val,
            self.eps,
            self.reduction,
            self.padding,
        )

Translated_Outputs/tensorflow_outputs/kornia/losses/ssim.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

img1 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.02646267, 0.7363431 , 0.9920731 , 0.28324366, 0.5275....47970527, 0.14698082],
         [0.21168607, 0.56656647, 0.56430477, 0.457393  , 0.28976035]]]],
      dtype=float32)>
img2 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.77879226, 0.2577836 , 0.9668816 , 0.5921759 , 0.9944....699103  , 0.28130335],
         [0.27611083, 0.16323024, 0.76004493, 0.6368226 , 0.6570088 ]]]],
      dtype=float32)>
window_size = 5, max_val = 1.0, eps = 1e-12, reduction = 'mean', padding = 'same'

    def tensorflow_ssim_loss(
        img1, img2, window_size, max_val=1.0, eps=1e-12, reduction="mean", padding="same"
    ):
        from ..metrics.ssim import tensorflow_ssim
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_clamp_frnt
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_mean_frnt
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
    
>       ssim_map: typing.Any = tensorflow_ssim(
            img1, img2, window_size, max_val, eps, padding
        )

Translated_Outputs/tensorflow_outputs/kornia/losses/ssim.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

img1 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.02646267, 0.7363431 , 0.9920731 , 0.28324366, 0.5275....47970527, 0.14698082],
         [0.21168607, 0.56656647, 0.56430477, 0.457393  , 0.28976035]]]],
      dtype=float32)>
img2 = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.77879226, 0.2577836 , 0.9668816 , 0.5921759 , 0.9944....699103  , 0.28130335],
         [0.27611083, 0.16323024, 0.76004493, 0.6368226 , 0.6570088 ]]]],
      dtype=float32)>
window_size = 5, max_val = 1.0, eps = 1e-12, padding = 'same'

    def tensorflow_ssim(img1, img2, window_size, max_val=1.0, eps=1e-12, padding="same"):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..filters.kernels import tensorflow_get_gaussian_kernel1d
        from ..filters.filter import tensorflow_filter2d_separable
        from ..filters.filter import tensorflow__compute_padding
    
        if not isinstance(img1, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input img1 type is not a torch.Tensor. Got {type(img1)}")
        if not isinstance(img2, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input img2 type is not a torch.Tensor. Got {type(img2)}")
        if not isinstance(max_val, (float,)):
            raise TypeError(f"Input max_val type is not a float. Got {type(max_val)}")
        if not len(tensorflow_shape_frnt_(img1)) == 4:
            raise ValueError(
                f"Invalid img1 shape, we expect BxCxHxW. Got: {tensorflow_shape_frnt_(img1)}"
            )
        if not len(tensorflow_shape_frnt_(img2)) == 4:
            raise ValueError(
                f"Invalid img2 shape, we expect BxCxHxW. Got: {tensorflow_shape_frnt_(img2)}"
            )
        if not tensorflow_shape_frnt_(img1) == tensorflow_shape_frnt_(img2):
            raise ValueError(
                f"img1 and img2 shapes must be the same. Got: {tensorflow_shape_frnt_(img1)} and {tensorflow_shape_frnt_(img2)}"
            )
        kernel: typing.Any = tensorflow_get_gaussian_kernel1d(
            window_size, 1.5, device=img1.device, dtype=img1.dtype
        )
        C1: typing.Any = (0.01 * max_val) ** 2
        C2: typing.Any = (0.03 * max_val) ** 2
>       mu1: typing.Any = tensorflow_filter2d_separable(img1, kernel, kernel)

Translated_Outputs/tensorflow_outputs/kornia/metrics/ssim.py:76: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 5), dtype=float32, numpy=
array([[[[0.02646267, 0.7363431 , 0.9920731 , 0.28324366, 0.5275....47970527, 0.14698082],
         [0.21168607, 0.56656647, 0.56430477, 0.457393  , 0.28976035]]]],
      dtype=float32)>
kernel_x = <tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]],
      dtype=float32)>
kernel_y = <tf.Tensor: shape=(1, 5), dtype=float32, numpy=
array([[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]],
      dtype=float32)>, border_type = 'reflect', normalized = False
padding = 'same'

    def tensorflow_filter2d_separable(
        input, kernel_x, kernel_y, border_type="reflect", normalized=False, padding="same"
    ):
>       out_x = tensorflow_filter2d(
            input, kernel_x[..., None, :], border_type, normalized, padding
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.9920731 , 0.7363431 , 0.02646267, 0.7363431 , 0.9920...0.21168607, 0.56656647, 0.56430477,
          0.457393  , 0.28976035, 0.457393  , 0.56430477]]]],
      dtype=float32)>
kernel = <tf.Tensor: shape=(1, 1, 5), dtype=float32, numpy=
array([[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]],
      dtype=float32)>, border_type = 'reflect', normalized = False
padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.9920731 , 0.7363431 , 0.02646267, 0.7363431 , 0.9920...0.21168607, 0.56656647, 0.56430477,
          0.457393  , 0.28976035, 0.457393  , 0.56430477]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 1, 5), dtype=float32, numpy=
array([[[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.1200...88074, 0.12007838]]],


       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 4

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.9920731 , 0.7363431 , 0.02646267, 0.7363431 , 0.9920...0.21168607, 0.56656647, 0.56430477,
          0.457393  , 0.28976035, 0.457393  , 0.56430477]]]],
      dtype=float32)>
weight = <tf.Tensor: shape=(4, 1, 1, 5), dtype=float32, numpy=
array([[[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.1200...88074, 0.12007838]]],


       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 4

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.9920731 , 0.7363431 , 0.02646267, 0.7363431 , 0.992...

       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f4ffd7ac670>
tensorflow_get_item = <function tensorflow_get_item at 0x7f4ffd7eae60>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 4, 5, 9), dtype=float32, numpy=
array([[[[0.9920731 , 0.7363431 , 0.02646267, 0.7363431 , 0.992...

       [[[0.12007838, 0.23388074, 0.2920817 , 0.23388074, 0.12007838]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f4ffd7ac310>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f4ffd7ac670>, tensorflow_asarray = <function tensorflow_asarray at 0x7f4ffd7af520>
tensorflow_get_item = <function tensorflow_get_item at 0x7f4ffd7eae60>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 5, 9, 4), dtype=float32, numpy=
array([[[[0.9920731 , 0.00724292, 0.54400396, 0.22857124],
     ....9678094 , 0.9370469 , 0.457393  ],
         [0.7429477 , 0.9829428 , 0.5552568 , 0.56430477]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(1, 5, 1, 4), dtype=float32, numpy=
array([[[[0.12007838, 0.12007838, 0.12007838, 0.12007838]],

   ...3388074, 0.23388074, 0.23388074]],

        [[0.12007838, 0.12007838, 0.12007838, 0.12007838]]]],
      dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 9, 4), dtype=float32, numpy=
array([[[[0.9920731 , 0.00724292, 0.54400396, 0.22857124],
    ...2007838],
         [0.12007838],
         [0.12007838],
         [0.12007838]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f4ffd7ac670>
tensorflow_get_item = <function tensorflow_get_item at 0x7f4ffd7eae60>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 9, 4, 5), dtype=float32, numpy=
array([[[[0.9920731 , 0.94948566, 0.06265187, 0.04035634, 0.7429....84615207, 0.5552568 ],
         [0.22857124, 0.63099366, 0.29386514, 0.73985684, 0.56430477]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(1, 5, 4, 1), dtype=float32, numpy=
array([[[[0.12007838],
         [0.12007838],
         [0.120078...8074]],

        [[0.12007838],
         [0.12007838],
         [0.12007838],
         [0.12007838]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SSIMLoss.call().
E       
E       [1m{{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 5 vs 4 [Op:DepthwiseConv2dNative] name: [0m
E       
E       Arguments received by tensorflow_SSIMLoss.call():
E         â€¢ img1=tf.Tensor(shape=(1, 4, 5, 5), dtype=float32)
E         â€¢ img2=tf.Tensor(shape=(1, 4, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.losses.SSIMLoss
________________________________________________________________________________ test_MS_SSIMLoss[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_MS_SSIMLoss(target_framework, mode, backend_compile):
        print("kornia.losses.MS_SSIMLoss")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledMS_SSIMLoss = ivy.transpile(kornia.losses.MS_SSIMLoss, source="torch", target=target_framework)
    
        torch_loss_fn = kornia.losses.MS_SSIMLoss()
        transpiled_loss_fn = TranspiledMS_SSIMLoss()
    
        torch_args = (
            torch.rand(1, 3, 5, 5),
            torch.rand(1, 3, 5, 5),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_res = torch_loss_fn(*torch_args)
>       transpiled_res = transpiled_loss_fn(*transpiled_args)

kornia/test_losses.py:546: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss()
args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.42997968, 0.20629436, 0.761...41622365, 0.14149094],
         [0.96916485, 0.4641257 , 0.00615019, 0.8803732 , 0.45866394]]]],
      dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55d970953830, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MS_SSIMLoss(), <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.4...41622365, 0.14149094],
         [0.96916485, 0.4641257 , 0.00615019, 0.8803732 , 0.45866394]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.42997968, 0.20629436, 0.761...41622365, 0.14149094],
         [0.96916485, 0.4641257 , 0.00615019, 0.8803732 , 0.45866394]]]],
      dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MS_SSIMLoss(), <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.4...41622365, 0.14149094],
         [0.96916485, 0.4641257 , 0.00615019, 0.8803732 , 0.45866394]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.42997968, 0.20629436, 0.761...41622365, 0.14149094],
         [0.96916485, 0.4641257 , 0.00615019, 0.8803732 , 0.45866394]]]],
      dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.42997968, 0.20629436, 0.7619....95736647, 0.2624539 ],
         [0.2559471 , 0.34299988, 0.3668217 , 0.39188415, 0.03921139]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (img1, img2)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MS_SSIMLoss(),)
kwargs = {'img1': <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.42997968, 0.2062943...41622365, 0.14149094],
         [0.96916485, 0.4641257 , 0.00615019, 0.8803732 , 0.45866394]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss()
img1 = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.42997968, 0.20629436, 0.7619....95736647, 0.2624539 ],
         [0.2559471 , 0.34299988, 0.3668217 , 0.39188415, 0.03921139]]]],
      dtype=float32)>
img2 = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.82807595, 0.6410548 , 0.06159717, 0.24578512, 0.9334....41622365, 0.14149094],
         [0.96916485, 0.4641257 , 0.00615019, 0.8803732 , 0.45866394]]]],
      dtype=float32)>

    def call(self, img1, img2):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_prod_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.loss_functions import (
            tensorflow_l1_loss_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_mean_frnt
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
    
        if not isinstance(img1, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a torch.Tensor. Got {type(img1)}")
        if not isinstance(img2, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Output type is not a torch.Tensor. Got {type(img2)}")
        if not len(tensorflow_shape_frnt_(img1)) == len(tensorflow_shape_frnt_(img2)):
            raise ValueError(
                f"Input shapes should be same. Got {type(img1)} and {type(img2)}."
            )
        g_masks: typing.Any = []
        CH: typing.Any = tensorflow_shape_frnt_(img1)[-3]
>       mux = tensorflow_conv2d_frnt(img1, g_masks, groups=CH, padding=self.pad)

Translated_Outputs/tensorflow_outputs/kornia/losses/ms_ssim.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.42997968, 0.20629436, 0.7619....95736647, 0.2624539 ],
         [0.2559471 , 0.34299988, 0.3668217 , 0.39188415, 0.03921139]]]],
      dtype=float32)>
weight = [], bias = None, stride = 1, padding = 16, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.42997968, 0.20629436, 0.7619....95736647, 0.2624539 ],
         [0.2559471 , 0.34299988, 0.3668217 , 0.39188415, 0.03921139]]]],
      dtype=float32)>
weight = [], bias = None, stride = 1, padding = [(16, 16), (16, 16)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.42997968, 0.20629436, 0.761...   [0.2559471 , 0.34299988, 0.3668217 , 0.39188415, 0.03921139]]]],
      dtype=float32)>, [], 1, [(16, 16), (16, 16)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f4ffe967010>
tensorflow_get_item = <function tensorflow_get_item at 0x7f4ffe4e2f80>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.42997968, 0.20629436, 0.761...      dtype=float32)>, <tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)>, 1, [(16, 16), (16, 16)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f4ffd77dbd0>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f4ffe967010>, tensorflow_asarray = <function tensorflow_asarray at 0x7f4ffd58f640>
tensorflow_get_item = <function tensorflow_get_item at 0x7f4ffe4e2f80>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.35870343, 0.08790129, 0.42997968, 0.20629436, 0.7619....95736647, 0.2624539 ],
         [0.2559471 , 0.34299988, 0.3668217 , 0.39188415, 0.03921139]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)>, strides = 1, padding = [(16, 16), (16, 16)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
>           filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
E           tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_MS_SSIMLoss.call().
E           
E           [1m{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:CPU:0}} transpose expects a vector of size 1. But input(1) is a vector of size 4 [Op:Transpose][0m
E           
E           Arguments received by tensorflow_MS_SSIMLoss.call():
E             â€¢ img1=tf.Tensor(shape=(1, 3, 5, 5), dtype=float32)
E             â€¢ img2=tf.Tensor(shape=(1, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:164: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.losses.MS_SSIMLoss
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-13 13:40:09.945350: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at transpose_op.cc:142 : INVALID_ARGUMENT: transpose expects a vector of size 1. But input(1) is a vector of size 4
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_losses.py::test_ssim_loss[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:local...
FAILED kornia/test_losses.py::test_HausdorffERLoss[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_HausdorffERLoss.call().
FAILED kornia/test_losses.py::test_HausdorffERLoss3D[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_HausdorffERLoss3D.call().
FAILED kornia/test_losses.py::test_SSIMLoss[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SSIMLoss.call().
FAILED kornia/test_losses.py::test_MS_SSIMLoss[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_MS_SSIMLoss.call().
=============================================================================== 5 failed, 30 passed in 529.95s (0:08:49) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 19 items

kornia/geometry/test_camera.py ...................                                                                                                                                               [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 19 passed in 196.85s (0:03:16) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 6 items

kornia/geometry/test_depth.py ...FF.                                                                                                                                                             [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________ test_unproject_meshgrid[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_unproject_meshgrid(target_framework, mode, backend_compile):
        trace_args = (
            4,
            4,
            torch.eye(3),
        )
        trace_kwargs = {'normalize_points': False, 'device': 'cpu', 'dtype': torch.float32}
        test_args = (
            5,
            5,
            torch.eye(3),
        )
        test_kwargs = {'normalize_points': False, 'device': 'cpu', 'dtype': torch.float32}
>       _test_function(
            kornia.geometry.depth.unproject_meshgrid,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_depth.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unproject_meshgrid at 0x7fa055089360>, trace_args = (4, 4, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
trace_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, test_args = (5, 5, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
test_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unproject_meshgrid at 0x7fa055089360>, trace_args = (4, 4, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
trace_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, test_args = (5, 5, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
test_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

height = 4, width = 4, camera_matrix = <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>, normalize_points = False
device = 'cpu', dtype = torch.float32

    def tensorflow_unproject_meshgrid(
        height, width, camera_matrix, normalize_points=False, device=None, dtype=None
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ..utils.grid import tensorflow_create_meshgrid
        from .conversions import tensorflow_normalize_points_with_intrinsics
        from .conversions import tensorflow_convert_points_to_homogeneous
        from ..core._backend import normalize
    
        tensorflow_KORNIA_CHECK_SHAPE(camera_matrix, ["3", "3"])
        points_uv: typing.Any = tensorflow_squeeze_frnt_(
>           tensorflow_create_meshgrid(
                height, width, normalized_coordinates=False, device=device, dtype=dtype
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/geometry/depth.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

height = 4, width = 4, normalized_coordinates = False, device = 'cpu', dtype = torch.float32

    def tensorflow_create_meshgrid(
        height, width, normalized_coordinates=True, device=None, dtype=None
    ):
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_linspace_frnt
        from ._compat import tensorflow_torch_meshgrid
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
    
>       xs: typing.Any = tensorflow_linspace_frnt(
            0, width - 1, width, device=device, dtype=dtype
        )

Translated_Outputs/tensorflow_outputs/kornia/utils/grid.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

start = 0, end = 3, steps = 4

    def tensorflow_linspace_frnt(
        start,
        end,
        steps,
        *,
        out=None,
        dtype=None,
        device=None,
        layout=None,
        requires_grad=False,
    ):
        from .dtype import tensorflow_get_default_dtype_frnt
        from ...backends.tensorflow.creation import tensorflow_linspace
    
        dtype = tensorflow_get_default_dtype_frnt() if dtype is None else dtype
>       return tensorflow_linspace(
            start, end, num=steps, dtype=dtype, device=device, out=out
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/creation_ops.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

dtype = torch.float32, args = (0, 3), kwargs = {'device': 'cpu', 'num': 4, 'out': None}, tensorflow_default_dtype_bknd = <function tensorflow_default_dtype_bknd at 0x7fa04e647400>
tensorflow_exists_bknd = <function tensorflow_exists_bknd at 0x7fa04e647d90>, arr = None

    @functools.wraps(fn)
    def _infer_dtype(*args, dtype=None, **kwargs):
        from .functional.ivy.data_type import tensorflow_default_dtype_bknd
        from .functional.ivy.general import tensorflow_exists_bknd
    
        arr = (
            None
            if tensorflow_exists_bknd(dtype)
            else tensorflow__get_first_array(*args, **kwargs)
        )
        dtype = tensorflow_default_dtype_bknd(dtype=dtype, item=arr, as_native=True)
>       return fn(*args, dtype=dtype, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [0, 3], kwargs = {'device': 'cpu', 'dtype': torch.float32, 'num': 4, 'out': None}, tensorflow_get_item = <function tensorflow_get_item at 0x7fa04de105e0>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fa04e647c70>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7fa04e5a0040>
tensorflow_asarray = <function tensorflow_asarray at 0x7fa04e5a1ab0>, num_args = 2
type_hints = mappingproxy(OrderedDict([('start', <Parameter "start: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.pyt..."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['start', 'stop', 'num', 'axis', 'endpoint', 'dtype', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, float], typing.Unio..., float], <class 'int'>, typing.Optional[int], <class 'bool'>, <class 'tensorflow.python.framework.dtypes.DType'>, ...]
device = None, i = 1

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

start = 0, stop = 3, num = 4

    @tensorflow_infer_dtype
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_linspace(
        start: Union[tensorflow.Tensor, tensorflow.Variable, float],
        stop: Union[tensorflow.Tensor, tensorflow.Variable, float],
        /,
        num: int,
        *,
        axis: Optional[int] = None,
        endpoint: bool = True,
        dtype: tensorflow.DType,
        device: Optional[str] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        if axis is None:
            axis = -1
>       start = tensorflow.cast(tensorflow.constant(start), dtype=dtype)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/creation.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(), dtype=int32, numpy=0>,), kwargs = {'dtype': torch.float32}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

type_value = torch.float32

    @tf_export("dtypes.as_dtype", "as_dtype")
    def as_dtype(type_value):
      """Converts the given `type_value` to a `tf.DType`.
    
      Inputs can be existing `tf.DType` objects, a [`DataType`
      enum](https://www.tensorflow.org/code/tensorflow/core/framework/types.proto),
      a string type name, or a
      [`numpy.dtype`](https://numpy.org/doc/stable/reference/generated/numpy.dtype.html).
    
      Examples:
      >>> tf.as_dtype(2)  # Enum value for float64.
      tf.float64
    
      >>> tf.as_dtype('float')
      tf.float32
    
      >>> tf.as_dtype(np.int32)
      tf.int32
    
      Note: `DType` values are interned (i.e. a single instance of each dtype is
      stored in a map). When passed a new `DType` object, `as_dtype` always returns
      the interned value.
    
      Args:
        type_value: A value that can be converted to a `tf.DType` object.
    
      Returns:
        A `DType` corresponding to `type_value`.
    
      Raises:
        TypeError: If `type_value` cannot be converted to a `DType`.
      """
      if isinstance(type_value, DType):
        if type_value._handle_data is None:  # pylint:disable=protected-access
          return _INTERN_TABLE[type_value.as_datatype_enum]
        else:
          return type_value
    
      if isinstance(type_value, np.dtype):
        try:
          return _NP_TO_TF[type_value.type]
        except KeyError:
          pass
    
      try:
        return _ANY_TO_TF[type_value]
      except (KeyError, TypeError):
        # TypeError indicates that type_value is not hashable.
        pass
    
      if hasattr(type_value, "dtype"):
        try:
          return _NP_TO_TF[np.dtype(type_value.dtype).type]
        except (KeyError, TypeError):
          pass
    
      if isinstance(type_value, _dtypes.DType):
        return _INTERN_TABLE[type_value.as_datatype_enum]
    
>     raise TypeError(f"Cannot convert the argument `type_value`: {type_value!r} "
                      "to a TensorFlow DType.")
E     TypeError: Cannot convert the argument `type_value`: torch.float32 to a TensorFlow DType.

/opt/fw/tensorflow/tensorflow/python/framework/dtypes.py:852: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.depth.unproject_meshgrid
_____________________________________________________________________________ test_depth_to_normals[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_depth_to_normals(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 1, 4, 4),
            torch.eye(3)[None],
        )
        trace_kwargs = {'normalize_points': False}
        test_args = (
            torch.rand(1, 1, 5, 5),
            torch.eye(3)[None],
        )
        test_kwargs = {'normalize_points': False}
>       _test_function(
            kornia.geometry.depth.depth_to_normals,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_depth.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function depth_to_normals at 0x7fa055089480>
trace_args = (tensor([[[[0.6080, 0.5898, 0.2863, 0.9456],
          [0.8040, 0.4778, 0.1561, 0.7705],
          [0.7569, 0.3861, 0....          [0.5163, 0.0418, 0.3964, 0.4459]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
trace_kwargs = {'normalize_points': False}
test_args = (tensor([[[[0.5892, 0.1946, 0.2600, 0.8091, 0.7675],
          [0.1347, 0.3959, 0.7526, 0.5771, 0.4761],
          [0....  [0.8419, 0.8427, 0.8259, 0.9963, 0.7480]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
test_kwargs = {'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function depth_to_normals at 0x7fa055089480>
trace_args = (tensor([[[[0.6080, 0.5898, 0.2863, 0.9456],
          [0.8040, 0.4778, 0.1561, 0.7705],
          [0.7569, 0.3861, 0....          [0.5163, 0.0418, 0.3964, 0.4459]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
trace_kwargs = {'normalize_points': False}
test_args = (tensor([[[[0.5892, 0.1946, 0.2600, 0.8091, 0.7675],
          [0.1347, 0.3959, 0.7526, 0.5771, 0.4761],
          [0....  [0.8419, 0.8427, 0.8259, 0.9963, 0.7480]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
test_kwargs = {'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

depth = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[0.60800725, 0.5897932 , 0.2862802 , 0.9456407 ],
     ....38611776, 0.05158049, 0.19377577],
         [0.5163445 , 0.04175413, 0.39636344, 0.4459113 ]]]],
      dtype=float32)>
camera_matrix = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, normalize_points = False

    def tensorflow_depth_to_normals(depth, camera_matrix, normalize_points=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..filters.sobel import tensorflow_sobel
        from ...ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_cross_frnt,
        )
        from ..core._backend import normalize
    
        if not isinstance(depth, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input depht type is not a Tensor. Got {type(depth)}.")
        if not (
            len(tensorflow_shape_frnt_(depth)) == 4
            and tensorflow_shape_frnt_(depth)[-3] == 1
        ):
            raise ValueError(
                f"Input depth musth have a shape (B, 1, H, W). Got: {tensorflow_shape_frnt_(depth)}"
            )
        if not isinstance(camera_matrix, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(
                f"Input camera_matrix type is not a Tensor. Got {type(camera_matrix)}."
            )
        if not (
            len(tensorflow_shape_frnt_(camera_matrix)) == 3
            and tensorflow_shape_frnt_(camera_matrix)[-2:] == (3, 3)
        ):
            raise ValueError(
                f"Input camera_matrix must have a shape (B, 3, 3). Got: {tensorflow_shape_frnt_(camera_matrix)}."
            )
        xyz: typing.Any = tensorflow_depth_to_3d(depth, camera_matrix, normalize_points)
>       gradients: typing.Any = tensorflow_sobel.spatial_gradient(xyz)
E       AttributeError: 'function' object has no attribute 'spatial_gradient'

Translated_Outputs/tensorflow_outputs/kornia/geometry/depth.py:153: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.depth.depth_to_normals
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/geometry/depth.py:205: DeprecationWarning: Since kornia 0.7.0 the `depth_to_3d` is deprecated in favor of `depth_to_3d_v2`. This function will be replaced with the `depth_to_3d_v2` behaviour, where the that does not require the creation of a meshgrid. The return shape can be not backward compatible between these implementations.
  xyz: Tensor = depth_to_3d(depth, camera_matrix, normalize_points)  # Bx3xHxW
/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/kornia/geometry/depth.py:152: DeprecationWarning: Since kornia 0.7.0 the `tensorflow_depth_to_3d` is deprecated in favor of `depth_to_3d_v2`. This function will be replaced with the `depth_to_3d_v2` behaviour, where the that does not require the creation of a meshgrid. The return shape can be not backward compatible between these implementations.
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_depth.py::test_unproject_meshgrid[tensorflow-s2s-False] - TypeError: Cannot convert the argument `type_value`: torch.float32 to a TensorFlow DType.
FAILED kornia/geometry/test_depth.py::test_depth_to_normals[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'spatial_gradient'
=============================================================================== 2 failed, 4 passed in 165.74s (0:02:45) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 75 items

kornia/test_feature.py .....F......F.............F...FFF.FF.F......FFFFFFFFF.FFFFF..FFFF.FFFF.....                                                                                               [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________ test_get_laf_descriptors[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_laf_descriptors(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 1, 32, 32),
            torch.rand(1, 3, 2, 2),
            kornia.feature.HardNet8(pretrained=False),
        )
        trace_kwargs = {'patch_size': 32, 'grayscale_descriptor': True}
        test_args = (
            torch.rand(5, 1, 32, 32),
            torch.rand(5, 3, 2, 2),
            kornia.feature.HardNet8(pretrained=False),
        )
        test_kwargs = {'patch_size': 32, 'grayscale_descriptor': True}
>       _test_function(
            kornia.feature.get_laf_descriptors,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_feature.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function get_laf_descriptors at 0x7f99a9772830>
trace_args = (tensor([[[[0.0949, 0.8876, 0.3750,  ..., 0.3842, 0.3055, 0.4075],
          [0.9748, 0.6365, 0.1441,  ..., 0.7698, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
trace_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}
test_args = (tensor([[[[0.3440, 0.6549, 0.2509,  ..., 0.2164, 0.4162, 0.7511],
          [0.1046, 0.5685, 0.9944,  ..., 0.2894, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
test_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function get_laf_descriptors at 0x7f99a9772830>
trace_args = (tensor([[[[0.0949, 0.8876, 0.3750,  ..., 0.3842, 0.3055, 0.4075],
          [0.9748, 0.6365, 0.1441,  ..., 0.7698, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
trace_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}
test_args = (tensor([[[[0.3440, 0.6549, 0.2509,  ..., 0.2164, 0.4162, 0.7511],
          [0.1046, 0.5685, 0.9944,  ..., 0.2894, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
test_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
>       orig_out = fn(*trace_args, **trace_kwargs)

helpers.py:253: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

img = tensor([[[[0.0949, 0.8876, 0.3750,  ..., 0.3842, 0.3055, 0.4075],
          [0.9748, 0.6365, 0.1441,  ..., 0.7698, 0.4...58, 0.8314, 0.5957,  ..., 0.0809, 0.7857, 0.3349],
          [0.6022, 0.2167, 0.5975,  ..., 0.3604, 0.8954, 0.7159]]]])
lafs = tensor([[[[0.8998, 0.6329],
          [0.2556, 0.5746]],

         [[0.9058, 0.5396],
          [0.8913, 0.9155]],

         [[0.6144, 0.8990],
          [0.7250, 0.4469]]]])
patch_descriptor = HardNet8(
  (features): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=Fal...e=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
)
patch_size = 32, grayscale_descriptor = True

    def get_laf_descriptors(
        img: Tensor, lafs: Tensor, patch_descriptor: Module, patch_size: int = 32, grayscale_descriptor: bool = True
    ) -> Tensor:
        r"""Function to get local descriptors, corresponding to LAFs (keypoints).
    
        Args:
            img: image features with shape :math:`(B,C,H,W)`.
            lafs: local affine frames :math:`(B,N,2,3)`.
            patch_descriptor: patch descriptor module, e.g. :class:`~kornia.feature.SIFTDescriptor`
                or :class:`~kornia.feature.HardNet`.
            patch_size: patch size in pixels, which descriptor expects.
            grayscale_descriptor: True if ``patch_descriptor`` expects single-channel image.
    
        Returns:
            Local descriptors of shape :math:`(B,N,D)` where :math:`D` is descriptor size.
        """
>       KORNIA_CHECK_LAF(lafs)

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/feature/integrated.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

laf = tensor([[[[0.8998, 0.6329],
          [0.2556, 0.5746]],

         [[0.9058, 0.5396],
          [0.8913, 0.9155]],

         [[0.6144, 0.8990],
          [0.7250, 0.4469]]]]), raises = True

    def KORNIA_CHECK_LAF(laf: Tensor, raises: bool = True) -> bool:
        """Check whether a Local Affine Frame (laf) has a valid shape.
    
        Args:
            laf: local affine frame tensor to evaluate.
            raises: bool indicating whether an exception should be raised upon failure.
    
        Raises:
            Exception: if the input laf does not have a shape :math:`(B,N,2,3)` and raises is True.
    
        Example:
            >>> lafs = torch.rand(2, 10, 2, 3)
            >>> KORNIA_CHECK_LAF(lafs)
            True
        """
>       return KORNIA_CHECK_SHAPE(laf, ["B", "N", "2", "3"], raises)

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/core/check.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = tensor([[[[0.8998, 0.6329],
          [0.2556, 0.5746]],

         [[0.9058, 0.5396],
          [0.8913, 0.9155]],

         [[0.6144, 0.8990],
          [0.7250, 0.4469]]]])
shape = ['B', 'N', '2', '3'], raises = True

    def KORNIA_CHECK_SHAPE(x: Tensor, shape: list[str], raises: bool = True) -> bool:
        """Check whether a tensor has a specified shape.
    
        The shape can be specified with a implicit or explicit list of strings.
        The guard also check whether the variable is a type `Tensor`.
    
        Args:
            x: the tensor to evaluate.
            shape: a list with strings with the expected shape.
            raises: bool indicating whether an exception should be raised upon failure.
    
        Raises:
            Exception: if the input tensor is has not the expected shape and raises is True.
    
        Example:
            >>> x = torch.rand(2, 3, 4, 4)
            >>> KORNIA_CHECK_SHAPE(x, ["B", "C", "H", "W"])  # implicit
            True
    
            >>> x = torch.rand(2, 3, 4, 4)
            >>> KORNIA_CHECK_SHAPE(x, ["2", "3", "H", "W"])  # explicit
            True
        """
        if "*" == shape[0]:
            shape_to_check = shape[1:]
            x_shape_to_check = x.shape[-len(shape) + 1 :]
        elif "*" == shape[-1]:
            shape_to_check = shape[:-1]
            x_shape_to_check = x.shape[: len(shape) - 1]
        else:
            shape_to_check = shape
            x_shape_to_check = x.shape
    
        if len(x_shape_to_check) != len(shape_to_check):
            if raises:
                raise TypeError(f"{x} shape must be [{shape}]. Got {x.shape}")
            else:
                return False
    
        for i in range(len(x_shape_to_check)):
            # The voodoo below is because torchscript does not like
            # that dim can be both int and str
            dim_: str = shape_to_check[i]
            if not dim_.isnumeric():
                continue
            dim = int(dim_)
            if x_shape_to_check[i] != dim:
                if raises:
>                   raise TypeError(f"{x} shape must be [{shape}]. Got {x.shape}")
E                   TypeError: tensor([[[[0.8998, 0.6329],
E                             [0.2556, 0.5746]],
E                   
E                            [[0.9058, 0.5396],
E                             [0.8913, 0.9155]],
E                   
E                            [[0.6144, 0.8990],
E                             [0.7250, 0.4469]]]]) shape must be [['B', 'N', '2', '3']]. Got torch.Size([1, 3, 2, 2])

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/core/check.py:80: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.integrated.get_laf_descriptors
_______________________________________________________________________ test_extract_patches_from_pyramid[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_extract_patches_from_pyramid(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 32, 32),
            torch.rand(1, 5, 2, 3),
        )
        trace_kwargs = {'PS': 32}
        test_args = (
            torch.rand(1, 3, 64, 64),  # TODO: changing the batch size of these causes the trace_graph test to fail
            torch.rand(1, 5, 2, 3),
        )
        test_kwargs = {'PS': 16}
>       _test_function(
            kornia.feature.extract_patches_from_pyramid,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_feature.py:317: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function extract_patches_from_pyramid at 0x7f99a96f9360>
trace_args = (tensor([[[[0.9300, 0.6640, 0.6233,  ..., 0.9580, 0.9851, 0.9004],
          [0.1694, 0.4442, 0.2868,  ..., 0.8792, 0....539],
          [0.9865, 0.9174, 0.0953]],

         [[0.6206, 0.8435, 0.2734],
          [0.8194, 0.8711, 0.0951]]]]))
trace_kwargs = {'PS': 32}
test_args = (tensor([[[[0.6751, 0.3695, 0.6738,  ..., 0.5359, 0.9386, 0.8480],
          [0.7348, 0.7410, 0.1936,  ..., 0.0159, 0....425],
          [0.8183, 0.2255, 0.5065]],

         [[0.2174, 0.6151, 0.0603],
          [0.8960, 0.9252, 0.7028]]]]))
test_kwargs = {'PS': 16}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function extract_patches_from_pyramid at 0x7f99a96f9360>
trace_args = (tensor([[[[0.9300, 0.6640, 0.6233,  ..., 0.9580, 0.9851, 0.9004],
          [0.1694, 0.4442, 0.2868,  ..., 0.8792, 0....539],
          [0.9865, 0.9174, 0.0953]],

         [[0.6206, 0.8435, 0.2734],
          [0.8194, 0.8711, 0.0951]]]]))
trace_kwargs = {'PS': 32}
test_args = (tensor([[[[0.6751, 0.3695, 0.6738,  ..., 0.5359, 0.9386, 0.8480],
          [0.7348, 0.7410, 0.1936,  ..., 0.0159, 0....425],
          [0.8183, 0.2255, 0.5065]],

         [[0.2174, 0.6151, 0.0603],
          [0.8960, 0.9252, 0.7028]]]]))
test_kwargs = {'PS': 16}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

img = <tf.Tensor: shape=(1, 3, 32, 32), dtype=float32, numpy=
array([[[[0.9300396 , 0.6640446 , 0.62331903, ..., 0.957966  ,...],
         [0.96038336, 0.7150846 , 0.53028435, ..., 0.5910305 ,
          0.21587718, 0.06378388]]]], dtype=float32)>
laf = <tf.Tensor: shape=(1, 5, 2, 3), dtype=float32, numpy=
array([[[[0.86932385, 0.29143894, 0.15585315],
         [0.50334...38]],

        [[0.6205604 , 0.8435113 , 0.27337468],
         [0.8194184 , 0.8711015 , 0.09509856]]]], dtype=float32)>
PS = 32, normalize_lafs_before_extraction = True

    def tensorflow_extract_patches_from_pyramid(
        img, laf, PS=32, normalize_lafs_before_extraction=True
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_LAF
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_long_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clamp_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_log2_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_zeros_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_grid_sample_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...ivy.functional.frontends.torch.tensor import (
            tensorflow_masked_scatter__frnt_,
        )
        from ..geometry.transform.pyramid import tensorflow_pyrdown
    
        tensorflow_KORNIA_CHECK_LAF(laf)
        if normalize_lafs_before_extraction:
            nlaf = tensorflow_normalize_laf(laf, img)
        else:
            nlaf = laf
        B, N, _, _ = tensorflow_size_frnt_(laf)
        _, ch, h, w = tensorflow_size_frnt_(img)
        scale = (
            2.0
            * tensorflow_get_laf_scale(tensorflow_denormalize_laf(nlaf, img))
            / float(PS)
        )
        max_level = min(tensorflow_size_frnt_(img, 2), tensorflow_size_frnt_(img, 3)) // PS
        pyr_idx = tensorflow_long_frnt_(
            tensorflow_clamp_frnt_(
                tensorflow_log2_frnt_(scale), min=0.0, max=max(0, max_level - 1)
            )
        )
        cur_img = img
        cur_pyr_level = 0
        out = tensorflow_to_frnt_(
            tensorflow_to_frnt_(tensorflow_zeros_frnt(B, N, ch, PS, PS), nlaf.dtype),
            nlaf.device,
        )
        we_are_in_business = True
        while we_are_in_business:
            _, ch, h, w = tensorflow_size_frnt_(cur_img)
            for i in range(B):
                scale_mask = tensorflow_squeeze_frnt_(
                    tensorflow_get_item(pyr_idx, i) == cur_pyr_level
                )
                if (
                    tensorflow_item_frnt_(
                        tensorflow_sum_frnt_(tensorflow_float_frnt_(scale_mask))
                    )
                    == 0
                ):
                    continue
                scale_mask = tensorflow_view_frnt_(scale_mask > 0, -1)
                grid = tensorflow_generate_patch_grid_from_normalized_LAF(
                    tensorflow_get_item(cur_img, slice(i, i + 1, None)),
                    tensorflow_get_item(
                        nlaf,
                        (
                            slice(i, i + 1, None),
                            scale_mask,
                            slice(None, None, None),
                            slice(None, None, None),
                        ),
                    ),
                    PS,
                )
                patches = tensorflow_grid_sample_frnt(
                    tensorflow_expand_frnt_(
                        tensorflow_get_item(cur_img, slice(i, i + 1, None)),
                        tensorflow_shape_frnt_(grid)[0],
                        ch,
                        h,
                        w,
                    ),
                    grid,
                    padding_mode="border",
                    align_corners=False,
                )
                out = tensorflow_set_item_bknd(
                    out,
                    i,
                    tensorflow_masked_scatter__frnt_(
                        tensorflow_get_item(out, i),
                        tensorflow_view_frnt_(scale_mask, -1, 1, 1, 1),
                        patches,
                    ),
                )
            we_are_in_business = (
                min(tensorflow_size_frnt_(cur_img, 2), tensorflow_size_frnt_(cur_img, 3))
                >= PS
            )
            if not we_are_in_business:
                break
>           cur_img = tensorflow_pyrdown(cur_img)

Translated_Outputs/tensorflow_outputs/kornia/feature/laf.py:240: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 32, 32), dtype=float32, numpy=
array([[[[0.9300396 , 0.6640446 , 0.62331903, ..., 0.957966  ,...],
         [0.96038336, 0.7150846 , 0.53028435, ..., 0.5910305 ,
          0.21587718, 0.06378388]]]], dtype=float32)>
border_type = 'reflect', align_corners = False, factor = 2.0

    def tensorflow_pyrdown(input, border_type="reflect", align_corners=False, factor=2.0):
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...filters.filter import tensorflow_filter2d
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_interpolate_frnt,
        )
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        kernel: typing.Any = tensorflow__get_pyramid_gaussian_kernel()
        _, _, height, width = tensorflow_shape_frnt_(input)
>       x_blur: typing.Any = tensorflow_filter2d(input, kernel, border_type)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 36, 36), dtype=float32, numpy=
array([[[[0.4237218 , 0.12281144, 0.8137539 , ..., 0.11904824,...],
         [0.19360584, 0.6397535 , 0.35186636, ..., 0.12396413,
          0.768997  , 0.6606639 ]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=
array([[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625... 0.0625    , 0.015625  ],
        [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]],
      dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
>       output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 36, 36), dtype=float32, numpy=
array([[[[0.4237218 , 0.12281144, 0.8137539 , ..., 0.11904824,...],
         [0.19360584, 0.6397535 , 0.35186636, ..., 0.12396413,
          0.768997  , 0.6606639 ]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.0039....0625    , 0.015625  ],
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>
bias = None, stride = 1, padding = 0, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 36, 36), dtype=float32, numpy=
array([[[[0.4237218 , 0.12281144, 0.8137539 , ..., 0.11904824,...],
         [0.19360584, 0.6397535 , 0.35186636, ..., 0.12396413,
          0.768997  , 0.6606639 ]]]], dtype=float32)>
weight = <tf.Tensor: shape=(3, 1, 5, 5), dtype=float32, numpy=
array([[[[0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.0039....0625    , 0.015625  ],
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>
bias = None, stride = 1, padding = [(0, 0), (0, 0)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 36, 36), dtype=float32, numpy=
array([[[[0.4237218 , 0.12281144, 0.8137539 , ..., 0.11904824...,
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f99a257f6d0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f99a25b2200>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 36, 36), dtype=float32, numpy=
array([[[[0.4237218 , 0.12281144, 0.8137539 , ..., 0.11904824...,
         [0.00390625, 0.015625  , 0.0234375 , 0.015625  , 0.00390625]]]],
      dtype=float32)>, 1, [(0, 0), (0, 0)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_get_item = <function tensorflow_get_item at 0x7f99a25b2200>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99a257f370>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f99a257f6d0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f99a25ee680>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 36, 36, 3), dtype=float32, numpy=
array([[[[0.4237218 , 0.3558262 , 0.5455306 ],
         [0.122...6413],
         [0.9713983 , 0.93328077, 0.768997  ],
         [0.9856869 , 0.51884776, 0.6606639 ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 1, 3), dtype=float32, numpy=
array([[[[0.00390625, 0.00390625, 0.00390625]],

        [[0.015...]],

        [[0.015625  , 0.015625  , 0.015625  ]],

        [[0.00390625, 0.00390625, 0.00390625]]]], dtype=float32)>
strides = 1, padding = [(0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
            filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
        num_channels = x.shape[1] if data_format == "channel_first" else x.shape[-1]
        if filters.shape[-2] != num_channels // feature_group_count:
            raise Exception(
                f"given feature_group_count {feature_group_count} expected input channel of the filter to be {num_channels // feature_group_count} but got {filters.shape[-2]}"
            )
        if num_channels % feature_group_count != 0:
            raise Exception(
                f"input channel should be divisible by feature group count {feature_group_count} but got input channel {num_channels}"
            )
        permuted_x = False
        if data_format == "channel_first" and (
            tensorflow_dev(x) == "cpu" or feature_group_count != 1
        ):
            x = tensorflow.transpose(x, (0, *range(2, dims + 2), 1))
            data_format = "channel_last"
            permuted_x = True
        data_format = tensorflow__get_x_data_format_bknd(dims, data_format)
        x = tensorflow__x_dil_before_conv(x, dims, x_dilations, data_format)
        if dims == 2:
            padding = tensorflow__extend_2d_padding(padding, data_format)
            if feature_group_count == 1:
                res = tensorflow.nn.conv2d(
                    x,
                    filters,
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )
            else:
                if not isinstance(padding, str):
                    padding = padding[1:-1]
>               res = tensorflow_depthwise_conv2d(
                    x,
                    tensorflow.transpose(filters, (0, 1, 3, 2)),
                    strides,
                    padding,
                    data_format=data_format,
                    dilations=dilations,
                )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 36, 36, 3), dtype=float32, numpy=
array([[[[0.4237218 , 0.3558262 , 0.5455306 ],
         [0.12...625  ]],

        [[0.00390625],
         [0.00390625],
         [0.00390625]]]], dtype=float32)>, 1, [(0, 0), (0, 0)])
kwargs = {'data_format': 'NCHW', 'dilations': 1}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f99a257f6d0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f99a25b2200>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
                os.environ = tensorflow_set_item_bknd(
                    os.environ, "DATA_FORMAT", "channels_last"
                )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 36, 3, 36), dtype=float32, numpy=
array([[[[0.4237218 , 0.2867983 , 0.62331903, ..., 0.5165699 ,...],
         [0.4140522 , 0.3155918 , 0.7523885 , ..., 0.5910305 ,
          0.9245961 , 0.6606639 ]]]], dtype=float32)>
filters = <tf.Tensor: shape=(5, 5, 3, 1), dtype=float32, numpy=
array([[[[0.00390625],
         [0.00390625],
         [0.003906...25  ],
         [0.015625  ]],

        [[0.00390625],
         [0.00390625],
         [0.00390625]]]], dtype=float32)>
strides = [1, 1, 1, 1], padding = [(0, 0), (0, 0), (0, 0), (0, 0)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    def tensorflow_depthwise_conv2d(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        data_format: str = "NHWC",
        dilations: Union[int, Tuple[int, int]] = 1,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
    
        strides = [strides] * 2 if isinstance(strides, int) else strides
        dilations = [dilations] * 2 if isinstance(dilations, int) else dilations
        permuted_x = False
        if data_format == "NCHW" and tensorflow_dev(x) == "cpu":
            x = tensorflow.transpose(x, (0, 2, 3, 1))
            data_format = "NHWC"
            permuted_x = True
        if tensorflow.rank(filters) == 3:
            filters = tensorflow.expand_dims(filters, -1)
        padding = tensorflow__extend_2d_padding(padding, data_format)
        strides = [1, strides[0], strides[1], 1]
>       res = tensorflow.nn.depthwise_conv2d(
            x, filters, strides, padding, data_format, dilations
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:134: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 36, 3, 36), dtype=float32, numpy=
array([[[[0.4237218 , 0.2867983 , 0.62331903, ..., 0.5165699 ...0.00390625],
         [0.00390625]]]], dtype=float32)>, [1, 1, 1, 1], [(0, 0), (0, 0), (0, 0), (0, 0)], 'NHWC', [1, 1])
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNative_device_/job:localhost/replica:0/task:0/device:CPU:0}} input and filter must have the same depth: 36 vs 3 [Op:DepthwiseConv2dNative] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.laf.extract_patches_from_pyramid
____________________________________________________________________________ test_laf_is_inside_image[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_laf_is_inside_image(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 5, 2, 3),
            torch.rand(1, 1, 32, 32),
        )
        trace_kwargs = {'border': 0}
        test_args = (
            torch.rand(2, 10, 2, 3),
            torch.rand(2, 1, 64, 64),
        )
        test_kwargs = {'border': 1}
>       _test_function(
            kornia.feature.laf_is_inside_image,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_feature.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function laf_is_inside_image at 0x7f99a96f93f0>
trace_args = (tensor([[[[0.8455, 0.2093, 0.8265],
          [0.9602, 0.8185, 0.2311]],

         [[0.3115, 0.0959, 0.7209],
       ...6, 0.9948, 0.3324,  ..., 0.7753, 0.6586, 0.8991],
          [0.8712, 0.1268, 0.7869,  ..., 0.8758, 0.8349, 0.1573]]]]))
trace_kwargs = {'border': 0}
test_args = (tensor([[[[0.2647, 0.6176, 0.5472],
          [0.4641, 0.0603, 0.5879]],

         [[0.4349, 0.2522, 0.1842],
       ...7, 0.6554, 0.7309,  ..., 0.6057, 0.9141, 0.9504],
          [0.6417, 0.1117, 0.0605,  ..., 0.6904, 0.7448, 0.7984]]]]))
test_kwargs = {'border': 1}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function laf_is_inside_image at 0x7f99a96f93f0>
trace_args = (tensor([[[[0.8455, 0.2093, 0.8265],
          [0.9602, 0.8185, 0.2311]],

         [[0.3115, 0.0959, 0.7209],
       ...6, 0.9948, 0.3324,  ..., 0.7753, 0.6586, 0.8991],
          [0.8712, 0.1268, 0.7869,  ..., 0.8758, 0.8349, 0.1573]]]]))
trace_kwargs = {'border': 0}
test_args = (tensor([[[[0.2647, 0.6176, 0.5472],
          [0.4641, 0.0603, 0.5879]],

         [[0.4349, 0.2522, 0.1842],
       ...7, 0.6554, 0.7309,  ..., 0.6057, 0.9141, 0.9504],
          [0.6417, 0.1117, 0.0605,  ..., 0.6904, 0.7448, 0.7984]]]]))
test_kwargs = {'border': 1}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

laf = <tf.Tensor: shape=(1, 5, 2, 3), dtype=float32, numpy=
array([[[[0.8455152 , 0.20929575, 0.8264938 ],
         [0.96021...07]],

        [[0.5909813 , 0.17695844, 0.60590637],
         [0.8491949 , 0.5860209 , 0.03092533]]]], dtype=float32)>
images = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.40241408, 0.8937466 , 0.021716  , ..., 0.5719111 ,...],
         [0.87115496, 0.12676114, 0.7868658 , ..., 0.87576616,
          0.8349425 , 0.15731037]]]], dtype=float32)>
border = 0

    def tensorflow_laf_is_inside_image(laf, images, border=0):
        from ..core.check import tensorflow_KORNIA_CHECK_LAF
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
    
        tensorflow_KORNIA_CHECK_LAF(laf)
        _, _, h, w = tensorflow_size_frnt_(images)
        pts = tensorflow_laf_to_boundary_points(laf, 12)
        good_lafs_mask = (
>           (pts[..., 0] >= border)
            * (pts[..., 0] <= w - border)
            * (pts[..., 1] >= border)
            * (pts[..., 1] <= h - border)
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/laf.py:523: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 12), dtype=bool, numpy=
array([[[ True,  True,  True,  True,  True,  True,  True,  True, Fal...rue,  True],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True]]])>)
kwargs = {}
arg = <tf.Tensor: shape=(1, 5, 12), dtype=bool, numpy=
array([[[ True,  True,  True,  True,  True,  True,  True,  True,  Tru...True,  True],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True]]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 12), dtype=bool, numpy=
array([[[ True,  True,  True,  True,  True,  True,  True,  True, Fal...rue,  True],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True]]])>)
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E     	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.laf.laf_is_inside_image
____________________________________________________________________________ test_DenseSIFTDescriptor[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DenseSIFTDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.DenseSIFTDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDenseSIFTDescriptor = ivy.transpile(kornia.feature.DenseSIFTDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(2, 1, 200, 300)
        torch_out = kornia.feature.DenseSIFTDescriptor()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDenseSIFTDescriptor()(transpiled_x)

kornia/test_feature.py:736: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2)
args = (<tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.0295037 , 0.8079376 , 0.94552076, ..., 0.813822...
         [0.5375857 , 0.29383594, 0.13505864, ..., 0.5446509 ,
          0.17293823, 0.47552562]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f99a2498ec0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipv...,
         [0.5375857 , 0.29383594, 0.13505864, ..., 0.5446509 ,
          0.17293823, 0.47552562]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.0295037 , 0.8079376 , 0.94552076, ..., 0.813822...
         [0.5375857 , 0.29383594, 0.13505864, ..., 0.5446509 ,
          0.17293823, 0.47552562]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipv...,
         [0.5375857 , 0.29383594, 0.13505864, ..., 0.5446509 ,
          0.17293823, 0.47552562]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.0295037 , 0.8079376 , 0.94552076, ..., 0.813822...
         [0.5375857 , 0.29383594, 0.13505864, ..., 0.5446509 ,
          0.17293823, 0.47552562]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.0295037 , 0.8079376 , 0.94552076, ..., 0.8138227...],
         [0.5375857 , 0.29383594, 0.13505864, ..., 0.5446509 ,
          0.17293823, 0.47552562]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2),)
kwargs = {'input': <tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.0295037 , 0.8079376 , 0.94552076, ...,...,
         [0.5375857 , 0.29383594, 0.13505864, ..., 0.5446509 ,
          0.17293823, 0.47552562]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2)
input = <tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.0295037 , 0.8079376 , 0.94552076, ..., 0.8138227...],
         [0.5375857 , 0.29383594, 0.13505864, ..., 0.5446509 ,
          0.17293823, 0.47552562]]]], dtype=float32)>

    def call(self, input):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ..filters.sobel import tensorflow_spatial_gradient
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_atan2_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_floor_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clamp__frnt_
        from ..constants import pi
        from ..core._backend import concatenate
        from ..core._backend import normalize
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "1", "H", "W"])
        B, CH, W, H = tensorflow_size_frnt_(input)
>       self.bin_pooling_kernel = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.bin_pooling_kernel, input.dtype), input.device
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/siftdesc.py:169: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (KerasConv2D(), '/job:localhost/replica:0/task:0/device:CPU:0'), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99a17e3a30>, array_like = KerasConv2D()
pattern = '_bknd_|_bknd|_frnt_|_frnt', fn_name = 'to'

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
            return fn(*args, **kwargs)
        else:
            pattern = "_bknd_|_bknd|_frnt_|_frnt"
            fn_name = extract_function_name(re.sub(pattern, "", fn.__name__))
            try:
                new_fn = getattr(array_like, fn_name)
                if not callable(new_fn):
                    return new_fn
                return new_fn(*args[1:], **kwargs)
            except AttributeError:
>               return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:195: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = KerasConv2D(), args = ('/job:localhost/replica:0/task:0/device:CPU:0',), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99a17e3a30>
tensorflow_dev = <function tensorflow_dev at 0x7f99a179ce50>, tensorflow_dtype = <function tensorflow_dtype at 0x7f99a179cd30>
tensorflow_check_elem_in_list = <function tensorflow_check_elem_in_list at 0x7f99a21d4310>, tensorflow_as_ivy_dev = <function tensorflow_as_ivy_dev at 0x7f99a179cee0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f99a1796e60>, _all_ivy_dtypes_str = ('int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', ...)
device = '/job:localhost/replica:0/task:0/device:CPU:0', dtype = None, arg = '/job:localhost/replica:0/task:0/device:CPU:0'

    @tensorflow_handle_methods
    def tensorflow_to_frnt_(tensor, *args, **kwargs):
        from ...ivy.general import tensorflow_is_array_bknd
        from ...backends.tensorflow.device import tensorflow_dev
        from ...backends.tensorflow.data_type import tensorflow_dtype
        from ....utils.assertions import tensorflow_check_elem_in_list
        from ...backends.tensorflow.device import tensorflow_as_ivy_dev
        from ...backends.tensorflow.creation import tensorflow_asarray
        from ....__init__ import _all_ivy_dtypes_str
    
        device = None
        dtype = None
        for arg in args:
            if hasattr(arg, "ivy_array") or tensorflow_is_array_bknd(arg):
                device = tensorflow_dev(arg)
                dtype = tensorflow_dtype(arg)
            elif (
                isinstance(arg, (tf.DType,))
                or isinstance(arg, (str,))
                and hasattr(arg, "as_native_dtype")
                or arg in _all_ivy_dtypes_str
            ):
                dtype = arg
            elif isinstance(arg, (str, str, str)):
                if isinstance(arg, (str,)) and not isinstance(arg, (str, str)):
                    tensorflow_check_elem_in_list(
                        arg,
                        [
                            "cpu",
                            "cuda",
                            "mps",
                            "xpu",
                            "mkldnn",
                            "opengl",
                            "opencl",
                            "ideep",
                            "hip",
                            "ve",
                            "ort",
                            "mlc",
                            "xla",
                            "lazy",
                            "vulkan",
                            "meta",
                            "hpu",
                        ],
                    )
                device = arg
        if "device" in kwargs:
            device = kwargs["device"]
        if "dtype" in kwargs:
            dtype = kwargs["dtype"]
        if (dtype is None or tensor.dtype == dtype) and (
>           device is None or tensor.device == tensorflow_as_ivy_dev(device)
        ):

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D(), name = 'device'

    def __getattribute__(self, name):
        built = object.__getattribute__(self, "__dict__").get("built", False)
        if built:
            attr_map = {"weight": "kernel", "out_channels": "filters"}
        else:
            attr_map = {
                "out_channels": "filters",
            }
    
        new_name = attr_map[name] if name in attr_map else name
>       return super().__getattribute__(new_name)
E       AttributeError: Exception encountered when calling tensorflow_DenseSIFTDescriptor.call().
E       
E       [1m'KerasConv2D' object has no attribute 'device'[0m
E       
E       Arguments received by tensorflow_DenseSIFTDescriptor.call():
E         â€¢ input=tf.Tensor(shape=(2, 1, 200, 300), dtype=float32)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:465: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.DenseSIFTDescriptor
______________________________________________________________________________ test_SIFTDescriptor[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SIFTDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.SIFTDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSIFTDescriptor = ivy.transpile(kornia.feature.SIFTDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(23, 1, 41, 41)
        torch_out = kornia.feature.SIFTDescriptor()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSIFTDescriptor()(transpiled_x)

kornia/test_feature.py:753: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2)
args = (<tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.9588079 , 0.5468601 , 0.80513865, ..., 0.8412458...
         [0.24023849, 0.04273599, 0.39804947, ..., 0.12586164,
          0.7864558 , 0.20213121]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f99a1a00040, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), <tf.Tensor:...,
         [0.24023849, 0.04273599, 0.39804947, ..., 0.12586164,
          0.7864558 , 0.20213121]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.9588079 , 0.5468601 , 0.80513865, ..., 0.8412458...
         [0.24023849, 0.04273599, 0.39804947, ..., 0.12586164,
          0.7864558 , 0.20213121]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), <tf.Tensor:...,
         [0.24023849, 0.04273599, 0.39804947, ..., 0.12586164,
          0.7864558 , 0.20213121]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.9588079 , 0.5468601 , 0.80513865, ..., 0.8412458...
         [0.24023849, 0.04273599, 0.39804947, ..., 0.12586164,
          0.7864558 , 0.20213121]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.9588079 , 0.5468601 , 0.80513865, ..., 0.8412458 ...],
         [0.24023849, 0.04273599, 0.39804947, ..., 0.12586164,
          0.7864558 , 0.20213121]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2),)
kwargs = {'input': <tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.9588079 , 0.5468601 , 0.80513865, ..., ...,
         [0.24023849, 0.04273599, 0.39804947, ..., 0.12586164,
          0.7864558 , 0.20213121]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2)
input = <tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.9588079 , 0.5468601 , 0.80513865, ..., 0.8412458 ...],
         [0.24023849, 0.04273599, 0.39804947, ..., 0.12586164,
          0.7864558 , 0.20213121]]]], dtype=float32)>

    def call(self, input):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ..filters.sobel import tensorflow_spatial_gradient
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_atan2_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_as_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_as_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_floor_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_clamp_frnt,
        )
        from ..constants import pi
        from ..core._backend import concatenate
        from ..core._backend import normalize
    
        tensorflow_KORNIA_CHECK_SHAPE(
            input, ["B", "1", f"{self.patch_size}", f"{self.patch_size}"]
        )
        B: typing.Any = tensorflow_shape_frnt_(input)[0]
>       self.pk = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.pk, input.dtype), input.device
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/siftdesc.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (KerasConv2D(), '/job:localhost/replica:0/task:0/device:CPU:0'), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99a1822290>, array_like = KerasConv2D()
pattern = '_bknd_|_bknd|_frnt_|_frnt', fn_name = 'to'

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
            return fn(*args, **kwargs)
        else:
            pattern = "_bknd_|_bknd|_frnt_|_frnt"
            fn_name = extract_function_name(re.sub(pattern, "", fn.__name__))
            try:
                new_fn = getattr(array_like, fn_name)
                if not callable(new_fn):
                    return new_fn
                return new_fn(*args[1:], **kwargs)
            except AttributeError:
>               return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:195: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = KerasConv2D(), args = ('/job:localhost/replica:0/task:0/device:CPU:0',), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99a1822290>
tensorflow_dev = <function tensorflow_dev at 0x7f99a1793640>, tensorflow_dtype = <function tensorflow_dtype at 0x7f99a1793520>
tensorflow_check_elem_in_list = <function tensorflow_check_elem_in_list at 0x7f99a19fe680>, tensorflow_as_ivy_dev = <function tensorflow_as_ivy_dev at 0x7f99a17936d0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f99a17916c0>, _all_ivy_dtypes_str = ('int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', ...)
device = '/job:localhost/replica:0/task:0/device:CPU:0', dtype = None, arg = '/job:localhost/replica:0/task:0/device:CPU:0'

    @tensorflow_handle_methods
    def tensorflow_to_frnt_(tensor, *args, **kwargs):
        from ...ivy.general import tensorflow_is_array_bknd
        from ...backends.tensorflow.device import tensorflow_dev
        from ...backends.tensorflow.data_type import tensorflow_dtype
        from ....utils.assertions import tensorflow_check_elem_in_list
        from ...backends.tensorflow.device import tensorflow_as_ivy_dev
        from ...backends.tensorflow.creation import tensorflow_asarray
        from ....__init__ import _all_ivy_dtypes_str
    
        device = None
        dtype = None
        for arg in args:
            if hasattr(arg, "ivy_array") or tensorflow_is_array_bknd(arg):
                device = tensorflow_dev(arg)
                dtype = tensorflow_dtype(arg)
            elif (
                isinstance(arg, (tf.DType,))
                or isinstance(arg, (str,))
                and hasattr(arg, "as_native_dtype")
                or arg in _all_ivy_dtypes_str
            ):
                dtype = arg
            elif isinstance(arg, (str, str, str)):
                if isinstance(arg, (str,)) and not isinstance(arg, (str, str)):
                    tensorflow_check_elem_in_list(
                        arg,
                        [
                            "cpu",
                            "cuda",
                            "mps",
                            "xpu",
                            "mkldnn",
                            "opengl",
                            "opencl",
                            "ideep",
                            "hip",
                            "ve",
                            "ort",
                            "mlc",
                            "xla",
                            "lazy",
                            "vulkan",
                            "meta",
                            "hpu",
                        ],
                    )
                device = arg
        if "device" in kwargs:
            device = kwargs["device"]
        if "dtype" in kwargs:
            dtype = kwargs["dtype"]
        if (dtype is None or tensor.dtype == dtype) and (
>           device is None or tensor.device == tensorflow_as_ivy_dev(device)
        ):

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D(), name = 'device'

    def __getattribute__(self, name):
        built = object.__getattribute__(self, "__dict__").get("built", False)
        if built:
            attr_map = {"weight": "kernel", "out_channels": "filters"}
        else:
            attr_map = {
                "out_channels": "filters",
            }
    
        new_name = attr_map[name] if name in attr_map else name
>       return super().__getattribute__(new_name)
E       AttributeError: Exception encountered when calling tensorflow_SIFTDescriptor.call().
E       
E       [1m'KerasConv2D' object has no attribute 'device'[0m
E       
E       Arguments received by tensorflow_SIFTDescriptor.call():
E         â€¢ input=tf.Tensor(shape=(23, 1, 41, 41), dtype=float32)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:465: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SIFTDescriptor
_______________________________________________________________________________ test_MKDDescriptor[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_MKDDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.MKDDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledMKDDescriptor = ivy.transpile(kornia.feature.MKDDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(23, 1, 32, 32)
        torch_out = kornia.feature.MKDDescriptor()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledMKDDescriptor()(transpiled_x)

kornia/test_feature.py:770: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_MKDDescriptor' object has no attribute 'output_dims'") raised in repr()] tensorflow_MKDDescriptor object at 0x7f99a1ab94b0>, patch_size = 32
kernel_type = 'concat', whitening = 'pcawt', training_set = 'liberty', output_dims = 128

    def __init__(
        self,
        patch_size=32,
        kernel_type="concat",
        whitening="pcawt",
        training_set="liberty",
        output_dims=128,
    ):
        from ..filters.gaussian import tensorflow_GaussianBlur2d
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ..utils.helpers import tensorflow_map_location_to_cpu
    
        self.super___init__(
            patch_size=patch_size,
            kernel_type=kernel_type,
            whitening=whitening,
            training_set=training_set,
            output_dims=output_dims,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size: typing.Any = patch_size
        self.kernel_type: typing.Any = kernel_type
        self.whitening: typing.Any = whitening
        self.training_set: typing.Any = training_set
        self.sigma = 1.4 * (patch_size / 64)
        self.smoothing = tensorflow_GaussianBlur2d(
            (5, 5), (self.sigma, self.sigma), "replicate"
        )
        self.gradients = tensorflow_MKDGradients()
        polar_s: typing.Any = "polar"
        cart_s: typing.Any = "cart"
        self.parametrizations = (
            [polar_s, cart_s] if self.kernel_type == "concat" else [self.kernel_type]
        )
        self.odims: typing.Any = 0
        relative_orientations = {polar_s: True, cart_s: False}
        self.feats = {}
        for parametrization in self.parametrizations:
            gradient_embedding = tensorflow_EmbedGradients(
                patch_size=patch_size,
                relative=tensorflow_get_item(relative_orientations, parametrization),
            )
>           spatial_encoding = tensorflow_ExplicitSpacialEncoding(
                kernel_type=parametrization,
                fmap_size=patch_size,
                in_dims=gradient_embedding.kernel.d,
            )

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:2227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_ExplicitSpacialEncoding' object has no attribute 'out_dims'") raised in repr()] tensorflow_ExplicitSpacialEncoding object at 0x7f99a28b7940>, args = ()
kwargs = {'fmap_size': 32, 'in_dims': 7, 'kernel_type': 'polar'}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_ExplicitSpacialEncoding' object has no attribute 'out_dims'") raised in repr()] tensorflow_ExplicitSpacialEncoding object at 0x7f99a28b7940>, kernel_type = 'polar'
fmap_size = 32, in_dims = 7, do_gmask = True, do_l2 = True

    @tensorflow_store_config_info
    def __init__(
        self, kernel_type="polar", fmap_size=32, in_dims=7, do_gmask=True, do_l2=True
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        self.super___init__(
            kernel_type=kernel_type,
            fmap_size=fmap_size,
            in_dims=in_dims,
            do_gmask=do_gmask,
            do_l2=do_l2,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if kernel_type not in ["polar", "cart"]:
            raise NotImplementedError(
                f"{kernel_type} is not valid, use polar or cart)."
            )
        self.kernel_type = kernel_type
        self.fmap_size = fmap_size
        self.in_dims = in_dims
        self.do_gmask = do_gmask
        self.do_l2 = do_l2
        self.grid = tensorflow_get_grid_dict(fmap_size)
        self.gmask = None
>       emb = tensorflow_spatial_kernel_embedding(self.kernel_type, self.grid)

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:1322: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kernel_type = 'polar'
grids = {'x': <tf.Tensor: shape=(32, 32), dtype=float32, numpy=
array([[-1.        , -0.9354839 , -0.87096775, ...,  0.8709677...,
       [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
         0.81871915,  0.7853981 ]], dtype=float32)>}

    def tensorflow_spatial_kernel_embedding(kernel_type, grids):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_select_frnt_
        from ..constants import pi
    
        factors = {"phi": 1.0, "rho": pi / sqrt2, "x": pi / 2, "y": pi / 2}
        if kernel_type == "cart":
            coeffs_ = "xy"
            params_ = ["x", "y"]
        elif kernel_type == "polar":
            coeffs_ = "rhophi"
            params_ = ["phi", "rho"]
        keys = list(grids.keys())
        patch_size = tensorflow_shape_frnt_(tensorflow_get_item(grids, keys[0]))[-1]
        grids_normed = {k: (v * tensorflow_get_item(factors, k)) for k, v in grids.items()}
        grids_normed = {
            k: tensorflow_float_frnt_(
                tensorflow_unsqueeze_frnt_(tensorflow_unsqueeze_frnt_(v, 0), 0)
            )
            for k, v in grids_normed.items()
        }
        vm_a = tensorflow_VonMisesKernel(
            patch_size=patch_size, coeffs=tensorflow_get_item(COEFFS, coeffs_)
        )
        vm_b = tensorflow_VonMisesKernel(
            patch_size=patch_size, coeffs=tensorflow_get_item(COEFFS, coeffs_)
        )
        emb_a = tensorflow_squeeze_frnt_(
>           vm_a(tensorflow_get_item(grids_normed, params_[0]))
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:1281: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234])
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.8542...    [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55dc95618630, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...n='test_MKDDescriptor', code_context=['    transpiled_out = TranspiledMKDDescriptor()(transpiled_x)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), <tf.Tensor: shape=(1, ...     [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.8542...    [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), <tf.Tensor: shape=(1, ...     [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.8542...    [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.85425...      [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]),)
kwargs = {'x': <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0...     [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234])
x = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.85425...      [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>

    def call(self, x):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ..core._backend import cos
        from ..core._backend import sin
    
        if not isinstance(x, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(x)}")
        if not len(tensorflow_shape_frnt_(x)) == 4 or tensorflow_shape_frnt_(x)[1] != 1:
            raise ValueError(
                f"Invalid input shape, we expect Bx1xHxW. Got: {tensorflow_shape_frnt_(x)}"
            )
        if not isinstance(self.emb0, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Emb0 type is not a Tensor. Got {type(x)}")
        emb0 = tensorflow_repeat_frnt_(
            tensorflow_to_frnt_(self.emb0, x), tensorflow_size_frnt_(x, 0), 1, 1, 1
        )
        frange = tensorflow_to_frnt_(self.frange, x) * x
        emb1 = cos(frange)
        emb2 = sin(frange)
        embedding = tensorflow_cat_frnt([emb0, emb1, emb2], dim=1)
>       embedding = self.pt_weights * embedding

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:514: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), name = 'pt_weights'

    @tf.autograph.experimental.do_not_convert
    def __getattr__(self, name):
        if name == "v":
            if not super().__getattribute__("_v") and not getattr(  # noqa: E501
                self, "_built", False
            ):
                return self._build_and_return_v(
                    *self._args, dynamic_backend=self._dynamic_backend, **self._kwargs
                )
    
        _dict = super().__getattribute__("__dict__")
        if name in _dict:
            return _dict[name]
    
        elif "_v" in _dict and name in _dict["_v"]:
            return _dict["_v"][name]
    
>       return super().__getattribute__(name)
E       AttributeError: Exception encountered when calling tensorflow_VonMisesKernel.call().
E       
E       [1m'tensorflow_VonMisesKernel' object has no attribute 'pt_weights'[0m
E       
E       Arguments received by tensorflow_VonMisesKernel.call():
E         â€¢ x=tf.Tensor(shape=(1, 1, 32, 32), dtype=float32)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:998: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.MKDDescriptor
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/manyids2/mkd_pytorch/raw/master/mkd_pytorch/mkd-concat-64.pth" to /root/.cache/torch/hub/checkpoints/mkd-concat-64.pth

  0%|          | 0.00/1.31M [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.31M/1.31M [00:00<00:00, 39.9MB/s]
_________________________________________________________________________________ test_HardNet8[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HardNet8(target_framework, mode, backend_compile):
        print("kornia.feature.HardNet8")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHardNet8 = ivy.transpile(kornia.feature.HardNet8, source="torch", target=target_framework)
    
        x = torch.rand(16, 1, 32, 32)
        torch_out = kornia.feature.HardNet8()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledHardNet8()(transpiled_x)

kornia/test_feature.py:804: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.01206678, 0.72626114, 0.31289655, ..., 0.5588491...
         [0.8296071 , 0.4235121 , 0.795543  , ..., 0.42104614,
          0.00980794, 0.80813664]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55dc93c0a7d0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2):...,
         [0.8296071 , 0.4235121 , 0.795543  , ..., 0.42104614,
          0.00980794, 0.80813664]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.01206678, 0.72626114, 0.31289655, ..., 0.5588491...
         [0.8296071 , 0.4235121 , 0.795543  , ..., 0.42104614,
          0.00980794, 0.80813664]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2):...,
         [0.8296071 , 0.4235121 , 0.795543  , ..., 0.42104614,
          0.00980794, 0.80813664]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.01206678, 0.72626114, 0.31289655, ..., 0.5588491...
         [0.8296071 , 0.4235121 , 0.795543  , ..., 0.42104614,
          0.00980794, 0.80813664]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2):...    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
),)
kwargs = {'input': <tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.01206678, 0.72626114, 0.31289655, ..., ...,
         [0.8296071 , 0.4235121 , 0.795543  , ..., 0.42104614,
          0.00980794, 0.80813664]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
input = <tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.01206678, 0.72626114, 0.31289655, ..., 0.55884916...],
         [0.8296071 , 0.4235121 , 0.795543  , ..., 0.42104614,
          0.00980794, 0.80813664]]]], dtype=float32)>

    def call(self, input):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.nn.functional.non_linear_activation_functions import (
            tensorflow_normalize_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_mm_frnt,
        )
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "1", "32", "32"])
        x_norm: typing.Any = self._normalize_input(input)
        x_features: typing.Any = self.features(x_norm)
        mean: typing.Any = []
        components: typing.Any = []
        x_prePCA = tensorflow_normalize_frnt(
            tensorflow_view_frnt_(x_features, tensorflow_size_frnt_(x_features, 0), -1)
        )
>       pca = tensorflow_mm_frnt(x_prePCA - mean, components)

Translated_Outputs/tensorflow_outputs/kornia/feature/hardnet.py:868: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(16, 512), dtype=float32, numpy=
array([[0.04238852, 0.06113916, 0.10164564, ..., 0.03897177, 0.051...8],
       [0.00533167, 0.02199571, 0.06502116, ..., 0.04346371, 0.02383793,
        0.04798829]], dtype=float32)>, [])
kwargs = {}, arg = []

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_HardNet8.call().
E       
E       [1m{{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [16,512] vs. [0] [Op:Sub] name: [0m
E       
E       Arguments received by tensorflow_HardNet8.call():
E         â€¢ input=tf.Tensor(shape=(16, 1, 32, 32), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.HardNet8
___________________________________________________________________________________ test_HyNet[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HyNet(target_framework, mode, backend_compile):
        print("kornia.feature.HyNet")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledHyNet = ivy.transpile(kornia.feature.HyNet, source="torch", target=target_framework)

kornia/test_feature.py:815: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.feature.hynet.HyNet'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_mixed_handler = local_response_norm.partial_mixed_handler = lambda x, size, **kwargs: size % 2 != 0

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.HyNet
__________________________________________________________________________________ test_SOSNet[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SOSNet(target_framework, mode, backend_compile):
        print("kornia.feature.SOSNet")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledSOSNet = ivy.transpile(kornia.feature.SOSNet, source="torch", target=target_framework)

kornia/test_feature.py:849: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.feature.sosnet.SOSNet'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_mixed_handler = local_response_norm.partial_mixed_handler = lambda x, size, **kwargs: size % 2 != 0

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SOSNet
__________________________________________________________________________ test_MultiResolutionDetector[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_MultiResolutionDetector(target_framework, mode, backend_compile):
        print("kornia.feature.MultiResolutionDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledMultiResolutionDetector = ivy.transpile(kornia.feature.MultiResolutionDetector, source="torch", target=target_framework)
        TranspiledKeyNet = ivy.transpile(kornia.feature.KeyNet, source="torch", target=target_framework)
    
        model = kornia.feature.KeyNet()
        transpiled_model = TranspiledKeyNet()
    
        x = torch.rand(1, 1, 32, 32) * 10.
        torch_out = kornia.feature.MultiResolutionDetector(model)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_out = TranspiledMultiResolutionDetector(transpiled_model)(transpiled_x)
    
>       _to_numpy_and_shape_allclose(torch_out, transpiled_out)

kornia/test_feature.py:980: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[[15.6444,  0.0000, 19.9111],
          [-0.0000, 15.6444, 10.6667]],

         [[15.6444,  0.0000, 15.6444]....0000, 16.0000],
          [-0.0000, 22.0000, 16.0000]]]]), tensor([[0.0340, 0.0323, 0.0089]], grad_fn=<CatBackward0>))
transpiled_x = (<tf.Tensor: shape=(1, 2, 2, 3), dtype=float32, numpy=
array([[[[15.644444,  0.      , 10.666667],
         [-0.      ...]]], dtype=float32)>, <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.4642749 , 0.45495427]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_shape_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_shape_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[15.644444,  0.      , 19.911112],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.   ...    [-0.      , 22.      , 16.      ]]]], dtype=float32), array([[0.03397305, 0.03228738, 0.00889231]], dtype=float32))
y = (array([[[[15.644444,  0.      , 10.666667],
         [-0.      , 15.644444, 18.48889 ]],

        [[15.644444,  0.   ...6668],
         [-0.      , 15.644444, 18.48889 ]]]], dtype=float32), array([[0.4642749 , 0.45495427]], dtype=float32))
tolerance = 0.001

    def _check_shape_allclose(x, y, tolerance=1e-3):
        """
        Checks that all array shapes are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x.shape, y.shape, atol=tolerance), "numpy array shapes are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7f99a15c4900>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[15.644444,  0.      , 19.911112],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.    ...17.066668]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
y = array([[[[15.644444,  0.      , 10.666667],
         [-0.      , 15.644444, 18.48889 ]],

        [[15.644444,  0.      , 17.066668],
         [-0.      , 15.644444, 18.48889 ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"

helpers.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[15.644444,  0.      , 19.911112],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.    ...17.066668]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
b = array([[[[15.644444,  0.      , 10.666667],
         [-0.      , 15.644444, 18.48889 ]],

        [[15.644444,  0.      , 17.066668],
         [-0.      , 15.644444, 18.48889 ]]]], dtype=float32)
rtol = 1e-05, atol = 0.001, equal_nan = False

    @array_function_dispatch(_allclose_dispatcher)
    def allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
        """
        Returns True if two arrays are element-wise equal within a tolerance.
    
        The tolerance values are positive, typically very small numbers.  The
        relative difference (`rtol` * abs(`b`)) and the absolute difference
        `atol` are added together to compare against the absolute difference
        between `a` and `b`.
    
        NaNs are treated as equal if they are in the same place and if
        ``equal_nan=True``.  Infs are treated as equal if they are in the same
        place and of the same sign in both arrays.
    
        Parameters
        ----------
        a, b : array_like
            Input arrays to compare.
        rtol : float
            The relative tolerance parameter (see Notes).
        atol : float
            The absolute tolerance parameter (see Notes).
        equal_nan : bool
            Whether to compare NaN's as equal.  If True, NaN's in `a` will be
            considered equal to NaN's in `b` in the output array.
    
            .. versionadded:: 1.10.0
    
        Returns
        -------
        allclose : bool
            Returns True if the two arrays are equal within the given
            tolerance; False otherwise.
    
        See Also
        --------
        isclose, all, any, equal
    
        Notes
        -----
        If the following equation is element-wise True, then allclose returns
        True.
    
         absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
    
        The above equation is not symmetric in `a` and `b`, so that
        ``allclose(a, b)`` might be different from ``allclose(b, a)`` in
        some rare cases.
    
        The comparison of `a` and `b` uses standard broadcasting, which
        means that `a` and `b` need not have the same shape in order for
        ``allclose(a, b)`` to evaluate to True.  The same is true for
        `equal` but not `array_equal`.
    
        `allclose` is not defined for non-numeric data types.
        `bool` is considered a numeric data-type for this purpose.
    
        Examples
        --------
        >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])
        False
        >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])
        True
        >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])
        False
        >>> np.allclose([1.0, np.nan], [1.0, np.nan])
        False
        >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
        True
    
        """
>       res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))

/opt/fw/mxnet/numpy/core/numeric.py:2241: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[15.644444,  0.      , 19.911112],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.    ...17.066668]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
b = array([[[[15.644444,  0.      , 10.666667],
         [-0.      , 15.644444, 18.48889 ]],

        [[15.644444,  0.      , 17.066668],
         [-0.      , 15.644444, 18.48889 ]]]], dtype=float32)
rtol = 1e-05, atol = 0.001, equal_nan = False

    @array_function_dispatch(_isclose_dispatcher)
    def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
        """
        Returns a boolean array where two arrays are element-wise equal within a
        tolerance.
    
        The tolerance values are positive, typically very small numbers.  The
        relative difference (`rtol` * abs(`b`)) and the absolute difference
        `atol` are added together to compare against the absolute difference
        between `a` and `b`.
    
        .. warning:: The default `atol` is not appropriate for comparing numbers
                     that are much smaller than one (see Notes).
    
        Parameters
        ----------
        a, b : array_like
            Input arrays to compare.
        rtol : float
            The relative tolerance parameter (see Notes).
        atol : float
            The absolute tolerance parameter (see Notes).
        equal_nan : bool
            Whether to compare NaN's as equal.  If True, NaN's in `a` will be
            considered equal to NaN's in `b` in the output array.
    
        Returns
        -------
        y : array_like
            Returns a boolean array of where `a` and `b` are equal within the
            given tolerance. If both `a` and `b` are scalars, returns a single
            boolean value.
    
        See Also
        --------
        allclose
        math.isclose
    
        Notes
        -----
        .. versionadded:: 1.7.0
    
        For finite values, isclose uses the following equation to test whether
        two floating point values are equivalent.
    
         absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
    
        Unlike the built-in `math.isclose`, the above equation is not symmetric
        in `a` and `b` -- it assumes `b` is the reference value -- so that
        `isclose(a, b)` might be different from `isclose(b, a)`. Furthermore,
        the default value of atol is not zero, and is used to determine what
        small values should be considered close to zero. The default value is
        appropriate for expected values of order unity: if the expected values
        are significantly smaller than one, it can result in false positives.
        `atol` should be carefully selected for the use case at hand. A zero value
        for `atol` will result in `False` if either `a` or `b` is zero.
    
        `isclose` is not defined for non-numeric data types.
        `bool` is considered a numeric data-type for this purpose.
    
        Examples
        --------
        >>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])
        array([ True, False])
        >>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])
        array([ True, True])
        >>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])
        array([False,  True])
        >>> np.isclose([1.0, np.nan], [1.0, np.nan])
        array([ True, False])
        >>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
        array([ True, True])
        >>> np.isclose([1e-8, 1e-7], [0.0, 0.0])
        array([ True, False])
        >>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)
        array([False, False])
        >>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])
        array([ True,  True])
        >>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)
        array([False,  True])
        """
        def within_tol(x, y, atol, rtol):
            with errstate(invalid='ignore'), _no_nep50_warning():
                return less_equal(abs(x-y), atol + rtol * abs(y))
    
        x = asanyarray(a)
        y = asanyarray(b)
    
        # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
        # This will cause casting of x later. Also, make sure to allow subclasses
        # (e.g., for numpy.ma).
        # NOTE: We explicitly allow timedelta, which used to work. This could
        #       possibly be deprecated. See also gh-18286.
        #       timedelta works if `atol` is an integer or also a timedelta.
        #       Although, the default tolerances are unlikely to be useful
        if y.dtype.kind != "m":
            dt = multiarray.result_type(y, 1.)
            y = asanyarray(y, dtype=dt)
    
        xfin = isfinite(x)
        yfin = isfinite(y)
        if all(xfin) and all(yfin):
>           return within_tol(x, y, atol, rtol)

/opt/fw/mxnet/numpy/core/numeric.py:2351: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[15.644444,  0.      , 19.911112],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.    ...17.066668]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
y = array([[[[15.644444,  0.      , 10.666667],
         [-0.      , 15.644444, 18.48889 ]],

        [[15.644444,  0.      , 17.066668],
         [-0.      , 15.644444, 18.48889 ]]]], dtype=float32)
atol = 0.001, rtol = 1e-05

    def within_tol(x, y, atol, rtol):
        with errstate(invalid='ignore'), _no_nep50_warning():
>           return less_equal(abs(x-y), atol + rtol * abs(y))
E           ValueError: operands could not be broadcast together with shapes (1,3,2,3) (1,2,2,3)

/opt/fw/mxnet/numpy/core/numeric.py:2332: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.MultiResolutionDetector
____________________________________________________________________________ test_ScaleSpaceDetector[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ScaleSpaceDetector(target_framework, mode, backend_compile):
        print("kornia.feature.ScaleSpaceDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledScaleSpaceDetector = ivy.transpile(kornia.feature.ScaleSpaceDetector, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 32, 32) * 10.
        torch_out = kornia.feature.ScaleSpaceDetector()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledScaleSpaceDetector()(transpiled_x)

kornia/test_feature.py:995: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[9.719516 , 8.198648 , 3.7372375, ..., 6.755016 , 4....723 ],
         [5.9377317, 2.8344011, 3.928619 , ..., 7.850444 , 8.774974 ,
          2.4317293]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55dc90a9df20, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...4723 ],
         [5.9377317, 2.8344011, 3.928619 , ..., 7.850444 , 8.774974 ,
          2.4317293]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[9.719516 , 8.198648 , 3.7372375, ..., 6.755016 , 4....723 ],
         [5.9377317, 2.8344011, 3.928619 , ..., 7.850444 , 8.774974 ,
          2.4317293]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...4723 ],
         [5.9377317, 2.8344011, 3.928619 , ..., 7.850444 , 8.774974 ,
          2.4317293]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[9.719516 , 8.198648 , 3.7372375, ..., 6.755016 , 4....723 ],
         [5.9377317, 2.8344011, 3.928619 , ..., 7.850444 , 8.774974 ,
          2.4317293]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[9.719516 , 8.198648 , 3.7372375, ..., 6.755016 , 4.0...44723 ],
         [5.9377317, 2.8344011, 3.928619 , ..., 7.850444 , 8.774974 ,
          2.4317293]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (img, mask=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...es=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF()),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[9.719516 , 8.198648 , 3.7372375, ..., 6.7550...4723 ],
         [5.9377317, 2.8344011, 3.928619 , ..., 7.850444 , 8.774974 ,
          2.4317293]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
img = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[9.719516 , 8.198648 , 3.7372375, ..., 6.755016 , 4.0...44723 ],
         [5.9377317, 2.8344011, 3.928619 , ..., 7.850444 , 8.774974 ,
          2.4317293]]]], dtype=float32)>
mask = None

    def call(self, img, mask=None):
>       responses, lafs = self.detect(img, self.num_features, mask)

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:274: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
img = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[9.719516 , 8.198648 , 3.7372375, ..., 6.755016 , 4.0...44723 ],
         [5.9377317, 2.8344011, 3.928619 , ..., 7.850444 , 8.774974 ,
          2.4317293]]]], dtype=float32)>
num_feats = 500, mask = None

    def detect(self, img, num_feats, mask=None):
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.comparison_ops import (
            tensorflow_topk_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from .laf import tensorflow_laf_is_inside_image
        from ..core._backend import eye
        from ..core._backend import concatenate
    
        dev: typing.Any = img.device
        dtype: typing.Any = img.dtype
        sigmas: typing.Any
        sp, sigmas, _ = self.scale_pyr(img)
        all_responses: typing.Any = []
        all_lafs: typing.Any = []
        px_size = 0.5 if self.scale_pyr.double_image else 1.0
        for oct_idx, octave in enumerate(sp):
            sigmas_oct = tensorflow_get_item(sigmas, oct_idx)
            B, CH, L, H, W = tensorflow_size_frnt_(octave)
            if self.scale_space_response:
                oct_resp = self.resp(octave, tensorflow_view_frnt_(sigmas_oct, -1))
            else:
                oct_resp = tensorflow_view_frnt_(
                    self.resp(
                        tensorflow_reshape_frnt_(
                            tensorflow_permute_frnt_(octave, 0, 2, 1, 3, 4),
                            B * L,
                            CH,
                            H,
                            W,
                        ),
                        tensorflow_view_frnt_(sigmas_oct, -1),
                    ),
                    B,
                    L,
                    CH,
                    H,
                    W,
                )
                oct_resp = tensorflow_permute_frnt_(oct_resp, 0, 2, 1, 3, 4)
                if (
                    isinstance(
                        self.scale_pyr.extra_levels,
                        (tensorflow.Tensor, tensorflow.Variable),
                    )
                    and self.scale_pyr.extra_levels % 2 != 0
                ):
                    oct_resp = oct_resp[:, :, :-1]
            if mask is not None:
                oct_mask: typing.Any = tensorflow__create_octave_mask(
                    mask, tensorflow_shape_frnt_(oct_resp)
                )
                oct_resp = oct_mask * oct_resp
            coord_max: typing.Any
            response_max: typing.Any
            coord_max, response_max = self.nms(oct_resp)
            if self.minima_are_also_good:
                coord_min, response_min = self.nms(-oct_resp)
                take_min_mask = tensorflow_to_frnt_(
                    response_min > response_max, response_max.dtype
                )
                response_max = (
                    response_min * take_min_mask + (1 - take_min_mask) * response_max
                )
                coord_max = (
                    coord_min * tensorflow_unsqueeze_frnt_(take_min_mask, 2)
                    + (1 - tensorflow_unsqueeze_frnt_(take_min_mask, 2)) * coord_max
                )
            responses_flatten = tensorflow_view_frnt_(
                response_max, tensorflow_size_frnt_(response_max, 0), -1
            )
            max_coords_flatten = tensorflow_permute_frnt_(
                tensorflow_view_frnt_(
                    coord_max, tensorflow_size_frnt_(response_max, 0), 3, -1
                ),
                0,
                2,
                1,
            )
            if tensorflow_size_frnt_(responses_flatten, 1) > num_feats:
                resp_flat_best, idxs = tensorflow_topk_frnt(
                    responses_flatten, k=num_feats, dim=1
                )
                max_coords_best = tensorflow_gather_frnt(
                    max_coords_flatten,
                    1,
                    tensorflow_repeat_frnt_(
                        tensorflow_unsqueeze_frnt_(idxs, -1), 1, 1, 3
                    ),
                )
            else:
                resp_flat_best = responses_flatten
                max_coords_best = max_coords_flatten
            B, N = tensorflow_size_frnt_(resp_flat_best)
            if isinstance(
                self.scale_pyr.n_levels, (tensorflow.Tensor, tensorflow.Variable)
            ):
                num_levels = int(tensorflow_item_frnt_(self.scale_pyr.n_levels))
            elif isinstance(self.scale_pyr.n_levels, (int,)):
                num_levels = self.scale_pyr.n_levels
            else:
                raise TypeError(
                    f"Expected the scale pyramid module to have `n_levels` as a Tensor or int.Gotcha {type(self.scale_pyr.n_levels)}"
                )
            max_coords_best = tensorflow__scale_index_to_scale(
                max_coords_best, sigmas_oct, num_levels
            )
            rotmat = tensorflow_view_frnt_(eye(2, dtype=dtype, device=dev), 1, 1, 2, 2)
            current_lafs = concatenate(
                [
                    self.mr_size
                    * tensorflow_view_frnt_(max_coords_best[:, :, 0], B, N, 1, 1)
                    * rotmat,
                    tensorflow_view_frnt_(max_coords_best[:, :, 1:3], B, N, 2, 1),
                ],
                3,
            )
>           good_mask = tensorflow_laf_is_inside_image(current_lafs, octave[:, 0])

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:251: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

laf = <tf.Tensor: shape=(1, 500, 2, 3), dtype=float32, numpy=
array([[[[10.747913 ,  0.       ,  6.0804496],
         [ 0.  ...3 ]],

        [[27.151764 ,  0.       ,  5.004979 ],
         [ 0.       , 27.151764 , 29.005344 ]]]], dtype=float32)>
images = <tf.Tensor: shape=(1, 6, 32, 32), dtype=float32, numpy=
array([[[[6.282503 , 6.0320373, 5.5166283, ..., 5.4447546, 5.1...50318 ],
         [4.512528 , 4.5112214, 4.507833 , ..., 4.9612713, 4.9547753,
          4.9525404]]]], dtype=float32)>
border = 0

    def tensorflow_laf_is_inside_image(laf, images, border=0):
        from ..core.check import tensorflow_KORNIA_CHECK_LAF
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
    
        tensorflow_KORNIA_CHECK_LAF(laf)
        _, _, h, w = tensorflow_size_frnt_(images)
        pts = tensorflow_laf_to_boundary_points(laf, 12)
        good_lafs_mask = (
>           (pts[..., 0] >= border)
            * (pts[..., 0] <= w - border)
            * (pts[..., 1] >= border)
            * (pts[..., 1] <= h - border)
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/laf.py:523: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 500, 12), dtype=bool, numpy=
array([[[ True,  True,  True, ..., False, False,  True],
        [...,
        [ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True]]])>)
kwargs = {}
arg = <tf.Tensor: shape=(1, 500, 12), dtype=bool, numpy=
array([[[ True,  True,  True, ...,  True,  True,  True],
        [ ...],
        [ True,  True,  True, ...,  True,  True,  True],
        [ True,  True,  True, ...,  True,  True,  True]]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_ScaleSpaceDetector.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_ScaleSpaceDetector.call():
E         â€¢ img=tf.Tensor(shape=(1, 1, 32, 32), dtype=float32)
E         â€¢ mask=None

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.ScaleSpaceDetector
______________________________________________________________________________ test_KeyNetDetector[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KeyNetDetector(target_framework, mode, backend_compile):
        print("kornia.feature.KeyNetDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetDetector = ivy.transpile(kornia.feature.KeyNetDetector, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 32, 32)
        torch_out = kornia.feature.KeyNetDetector()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_out = TranspiledKeyNetDetector()(transpiled_x)
    
>       _to_numpy_and_shape_allclose(torch_out, transpiled_out)

kornia/test_feature.py:1014: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[[15.6444,  0.0000, 18.4889],
          [-0.0000, 15.6444, 19.2000]],

         [[15.6444,  0.0000, 10.6667]....0000, 15.0000],
          [-0.0000, 22.0000, 16.0000]]]]), tensor([[0.0247, 0.0234, 0.0206]], grad_fn=<CatBackward0>))
transpiled_x = (<tf.Tensor: shape=(1, 3, 2, 3), dtype=float32, numpy=
array([[[[15.644444,  0.      , 18.48889 ],
         [-0.      ...loat32)>, <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.03127904, 0.02871902, 0.0220203 ]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_shape_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_shape_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[15.644444,  0.      , 18.48889 ],
         [-0.      , 15.644444, 19.2     ]],

        [[15.644444,  0.   ...    [-0.      , 22.      , 16.      ]]]], dtype=float32), array([[0.02470037, 0.02343095, 0.02058304]], dtype=float32))
y = (array([[[[15.644444,  0.      , 18.48889 ],
         [-0.      , 15.644444, 20.622223]],

        [[15.644444,  0.   ...    [-0.      , 22.      , 16.      ]]]], dtype=float32), array([[0.03127904, 0.02871902, 0.0220203 ]], dtype=float32))
tolerance = 0.001

    def _check_shape_allclose(x, y, tolerance=1e-3):
        """
        Checks that all array shapes are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x.shape, y.shape, atol=tolerance), "numpy array shapes are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7f99a22cc280>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[15.644444,  0.      , 18.48889 ],
         [-0.      , 15.644444, 19.2     ]],

        [[15.644444,  0.    ...19.2     ]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
y = array([[[[15.644444,  0.      , 18.48889 ],
         [-0.      , 15.644444, 20.622223]],

        [[15.644444,  0.    ...14.222222]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.KeyNetDetector
_______________________________________________________________________________ test_LAFDescriptor[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LAFDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.LAFDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLAFDescriptor = ivy.transpile(kornia.feature.LAFDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 64, 64)
        lafs = torch.rand(1, 2, 2, 3)
        torch_out = kornia.feature.LAFDescriptor()(x, lafs)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_lafs = _nest_torch_tensor_to_new_framework(lafs, target_framework)
>       transpiled_out = TranspiledLAFDescriptor()(transpiled_x, transpiled_lafs)

kornia/test_feature.py:1031: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFDescriptor' object has no attribute 'descriptor'") raised in repr()] tensorflow_LAFDescriptor object at 0x7f99a217f760>, patch_descriptor_module = None
patch_size = 32, grayscale_descriptor = True

    def __init__(
        self, patch_descriptor_module=None, patch_size=32, grayscale_descriptor=True
    ):
        from .hardnet import tensorflow_HardNet
    
        self.super___init__(
            patch_descriptor_module=patch_descriptor_module,
            patch_size=patch_size,
            grayscale_descriptor=grayscale_descriptor,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if patch_descriptor_module is None:
>           patch_descriptor_module = tensorflow_HardNet(True)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): t...)
    (17): tensorflow_ReLU()
    (18): tensorflow_Dropout()
    (19): KerasConv2D()
    (20): KerasBatchNorm2D()
  )
)
pretrained = True

    def __init__(self, pretrained=False):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=128,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.3),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=8,
                strides=1,
                padding=0,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
        )
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["liberty_aug"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/hardnet.py:211: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LAFDescriptor
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /root/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth

  0%|          | 0.00/5.10M [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.10M/5.10M [00:00<00:00, 118MB/s]
___________________________________________________________________________________ test_SOLD2[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SOLD2(target_framework, mode, backend_compile):
        print("kornia.feature.SOLD2")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSOLD2 = ivy.transpile(kornia.feature.SOLD2, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 512, 512)
        torch_out = kornia.feature.SOLD2(pretrained=False)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSOLD2(pretrained=False)(transpiled_x)

kornia/test_feature.py:1048: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.32998133, 0.76423275, 0.3462487 , ..., 0.010286...
         [0.01614702, 0.8250682 , 0.15108567, ..., 0.40666348,
          0.0578047 , 0.5379731 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55dc9461dd50, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tens...,
         [0.01614702, 0.8250682 , 0.15108567, ..., 0.40666348,
          0.0578047 , 0.5379731 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.32998133, 0.76423275, 0.3462487 , ..., 0.010286...
         [0.01614702, 0.8250682 , 0.15108567, ..., 0.40666348,
          0.0578047 , 0.5379731 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tens...,
         [0.01614702, 0.8250682 , 0.15108567, ..., 0.40666348,
          0.0578047 , 0.5379731 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.32998133, 0.76423275, 0.3462487 , ..., 0.010286...
         [0.01614702, 0.8250682 , 0.15108567, ..., 0.40666348,
          0.0578047 , 0.5379731 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (img)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tens...   (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.32998133, 0.76423275, 0.3462487 , ..., 0...,
         [0.01614702, 0.8250682 , 0.15108567, ..., 0.40666348,
          0.0578047 , 0.5379731 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
img = <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.32998133, 0.76423275, 0.3462487 , ..., 0.0102869...],
         [0.01614702, 0.8250682 , 0.15108567, ..., 0.40666348,
          0.0578047 , 0.5379731 ]]]], dtype=float32)>

    def call(self, img):
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .sold2_detector import tensorflow_prob_to_junctions
        from .sold2_detector import tensorflow_line_map_to_segments
    
        tensorflow_KORNIA_CHECK_SHAPE(img, ["B", "1", "H", "W"])
        outputs = {}
        net_outputs = self.model(img)
        outputs = tensorflow_set_item_bknd(
            outputs, "junction_heatmap", net_outputs["junctions"]
        )
        outputs = tensorflow_set_item_bknd(
            outputs, "line_heatmap", net_outputs["heatmap"]
        )
        outputs = tensorflow_set_item_bknd(
            outputs, "dense_desc", net_outputs["descriptors"]
        )
        lines = []
        for junc_prob, heatmap in zip(net_outputs["junctions"], net_outputs["heatmap"]):
            junctions = tensorflow_prob_to_junctions(
                junc_prob,
                self.grid_size,
                self.junc_detect_thresh,
                self.max_num_junctions,
            )
>           line_map, junctions, _ = self.line_detector.detect(junctions, heatmap)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Translated_Outputs.tensorflow_outputs.kornia.feature.sold2.sold2_detector.tensorflow_LineSegmentDetectionModule object at 0x7f99a1b1a290>
junctions = <tf.Tensor: shape=(500, 2), dtype=float32, numpy=
array([[435.,   6.],
       [395.,   6.],
       [419.,   6.],
     ...     [355.,  86.],
       [211.,  62.],
       [387.,  62.],
       [339.,  94.],
       [ 75., 246.]], dtype=float32)>
heatmap = <tf.Tensor: shape=(512, 512), dtype=float32, numpy=
array([[0.90309614, 0.9322194 , 1.        , ..., 0.73929226, 0.478...667955],
       [0.8378961 , 0.5753635 , 0.8536058 , ..., 0.6735144 , 0.7176186 ,
        0.76200473]], dtype=float32)>

    def detect(self, junctions, heatmap):
        from ...core._backend import zeros
        from ...core._backend import where
        from ...core._backend import concatenate
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .structures import tensorflow_HeatMapRefineCfg
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_triu_frnt,
        )
        from ....ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_clamp_frnt,
        )
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_sqrt_frnt,
        )
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
        from ....ivy.functional.frontends.torch.reduction_ops import (
            tensorflow_mean_frnt,
        )
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        tensorflow_KORNIA_CHECK_SHAPE(heatmap, ["H", "W"])
        H, W = tensorflow_shape_frnt_(heatmap)
        device = junctions.device
        if self.use_heatmap_refinement and isinstance(
            self.heatmap_refine_cfg, (tensorflow_HeatMapRefineCfg,)
        ):
            if self.heatmap_refine_cfg.mode == "global":
                heatmap = self.refine_heatmap(
                    heatmap,
                    self.heatmap_refine_cfg.ratio,
                    self.heatmap_refine_cfg.valid_thresh,
                )
            elif self.heatmap_refine_cfg.mode == "local":
                heatmap = self.refine_heatmap_local(
                    heatmap,
                    self.heatmap_refine_cfg.num_blocks,
                    self.heatmap_refine_cfg.overlap_ratio,
                    self.heatmap_refine_cfg.ratio,
                    self.heatmap_refine_cfg.valid_thresh,
                )
        num_junctions = len(junctions)
        line_map_pred = zeros(
            [num_junctions, num_junctions], device=device, dtype=tf.int32
        )
        if num_junctions < 2:
            return line_map_pred, junctions, heatmap
        candidate_map = tensorflow_triu_frnt(
            tensorflow_ones_frnt(
                [num_junctions, num_junctions], device=device, dtype=tf.int32
            ),
            diagonal=1,
        )
        if self.use_candidate_suppression:
>           candidate_map = self.candidate_suppression(junctions, candidate_map)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2_detector.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Translated_Outputs.tensorflow_outputs.kornia.feature.sold2.sold2_detector.tensorflow_LineSegmentDetectionModule object at 0x7f99a1b1a290>
junctions = <tf.Tensor: shape=(500, 2), dtype=float32, numpy=
array([[435.,   6.],
       [395.,   6.],
       [419.,   6.],
     ...     [355.,  86.],
       [211.,  62.],
       [387.,  62.],
       [339.,  94.],
       [ 75., 246.]], dtype=float32)>
candidate_map = <tf.Tensor: shape=(500, 500), dtype=float32, numpy=
array([[0., 1., 1., ..., 1., 1., 1.],
       [0., 0., 1., ..., 1.,... 0., 0., ..., 0., 1., 1.],
       [0., 0., 0., ..., 0., 0., 1.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>

    def candidate_suppression(self, junctions, candidate_map):
        from ...core._backend import where
        from ...core._backend import sin
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_triu_frnt,
        )
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_unsqueeze_frnt,
        )
        from ....ivy.functional.frontends.torch.reduction_ops import (
            tensorflow_norm_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_einsum_frnt,
        )
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_acos_frnt,
        )
        from ....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_arange_frnt,
        )
    
        dist_tolerance = self.nms_dist_tolerance
        line_dist_map = (
            tensorflow_sum_frnt(
                (tensorflow_unsqueeze_frnt(junctions, dim=1) - junctions[None, ...])
                ** 2,
                dim=-1,
            )
            ** 0.5
        )
        seg_indexes = where(tensorflow_triu_frnt(candidate_map, diagonal=1))
        start_point_idxs = seg_indexes[0]
        end_point_idxs = seg_indexes[1]
        start_points = tensorflow_get_item(
            junctions, (start_point_idxs, slice(None, None, None))
        )
        end_points = tensorflow_get_item(
            junctions, (end_point_idxs, slice(None, None, None))
        )
        line_dists = tensorflow_get_item(
            line_dist_map, (start_point_idxs, end_point_idxs)
        )
        dir_vecs = (end_points - start_points) / tensorflow_norm_frnt(
            end_points - start_points, dim=-1
        )[..., None]
        cand_vecs = junctions[None, ...] - tensorflow_unsqueeze_frnt_(
            start_points, dim=1
        )
        cand_vecs_norm = tensorflow_norm_frnt(cand_vecs, dim=-1)
        proj = (
            tensorflow_einsum_frnt("bij,bjk->bik", cand_vecs, dir_vecs[..., None])
            / line_dists[..., None, None]
        )
>       proj_mask = (proj >= 0) * (proj <= 1)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2_detector.py:368: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(124750, 500, 1), dtype=bool, numpy=
array([[[ True],
        [ True],
        [ True],
        ......
       [[ True],
        [ True],
        [ True],
        ...,
        [ True],
        [ True],
        [ True]]])>)
kwargs = {}
arg = <tf.Tensor: shape=(124750, 500, 1), dtype=bool, numpy=
array([[[ True],
        [ True],
        [ True],
        ...,...

       [[ True],
        [ True],
        [ True],
        ...,
        [ True],
        [ True],
        [ True]]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SOLD2.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_SOLD2.call():
E         â€¢ img=tf.Tensor(shape=(1, 1, 512, 512), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SOLD2
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-13 14:05:50.451205: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 499000000 exceeds 10% of free system memory.
2024-09-13 14:05:50.797493: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 499000000 exceeds 10% of free system memory.
2024-09-13 14:05:50.829902: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 499000000 exceeds 10% of free system memory.
_______________________________________________________________________________ test_LocalFeature[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LocalFeature(target_framework, mode, backend_compile):
        print("kornia.feature.LocalFeature")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetDetector = ivy.transpile(
            kornia.feature.KeyNetDetector, source="torch", target=target_framework
        )
        TranspiledLAFDescriptor = ivy.transpile(
            kornia.feature.LAFDescriptor, source="torch", target=target_framework
        )
        TranspiledLocalFeature = ivy.transpile(
            kornia.feature.LocalFeature, source="torch", target=target_framework
        )
    
        torch_detector = kornia.feature.KeyNetDetector()
        torch_descriptor = kornia.feature.LAFDescriptor()
        transpiled_detector = TranspiledKeyNetDetector()
>       transpiled_descriptor = TranspiledLAFDescriptor()

kornia/test_feature.py:1072: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFDescriptor' object has no attribute 'descriptor'") raised in repr()] tensorflow_LAFDescriptor object at 0x7f99a2c53eb0>, patch_descriptor_module = None
patch_size = 32, grayscale_descriptor = True

    def __init__(
        self, patch_descriptor_module=None, patch_size=32, grayscale_descriptor=True
    ):
        from .hardnet import tensorflow_HardNet
    
        self.super___init__(
            patch_descriptor_module=patch_descriptor_module,
            patch_size=patch_size,
            grayscale_descriptor=grayscale_descriptor,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if patch_descriptor_module is None:
>           patch_descriptor_module = tensorflow_HardNet(True)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): t...)
    (17): tensorflow_ReLU()
    (18): tensorflow_Dropout()
    (19): KerasConv2D()
    (20): KerasBatchNorm2D()
  )
)
pretrained = True

    def __init__(self, pretrained=False):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=128,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.3),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=8,
                strides=1,
                padding=0,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
        )
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["liberty_aug"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/hardnet.py:211: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LocalFeature
______________________________________________________________________________ test_SOLD2_detector[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SOLD2_detector(target_framework, mode, backend_compile):
        print("kornia.feature.SOLD2_detector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSOLD2Detector = ivy.transpile(kornia.feature.SOLD2_detector, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 512, 512)
        torch_out = kornia.feature.SOLD2_detector(pretrained=False)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSOLD2Detector(pretrained=False)(transpiled_x)

kornia/test_feature.py:1095: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.72613376, 0.29401612, 0.01724291, ..., 0.296247...
         [0.6383344 , 0.6292224 , 0.6383597 , ..., 0.31218445,
          0.7087661 , 0.5007694 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f99789136a0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (n...,
         [0.6383344 , 0.6292224 , 0.6383597 , ..., 0.31218445,
          0.7087661 , 0.5007694 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.72613376, 0.29401612, 0.01724291, ..., 0.296247...
         [0.6383344 , 0.6292224 , 0.6383597 , ..., 0.31218445,
          0.7087661 , 0.5007694 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (n...,
         [0.6383344 , 0.6292224 , 0.6383597 , ..., 0.31218445,
          0.7087661 , 0.5007694 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.72613376, 0.29401612, 0.01724291, ..., 0.296247...
         [0.6383344 , 0.6292224 , 0.6383597 , ..., 0.31218445,
          0.7087661 , 0.5007694 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (img)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (n...ntDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.72613376, 0.29401612, 0.01724291, ..., 0...,
         [0.6383344 , 0.6292224 , 0.6383597 , ..., 0.31218445,
          0.7087661 , 0.5007694 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
img = <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.72613376, 0.29401612, 0.01724291, ..., 0.296247 ...],
         [0.6383344 , 0.6292224 , 0.6383597 , ..., 0.31218445,
          0.7087661 , 0.5007694 ]]]], dtype=float32)>

    def call(self, img):
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
    
        tensorflow_KORNIA_CHECK_SHAPE(img, ["B", "1", "H", "W"])
        outputs = {}
        net_outputs = self.model(img)
        outputs = tensorflow_set_item_bknd(
            outputs, "junction_heatmap", net_outputs["junctions"]
        )
        outputs = tensorflow_set_item_bknd(
            outputs, "line_heatmap", net_outputs["heatmap"]
        )
        lines = []
        for junc_prob, heatmap in zip(net_outputs["junctions"], net_outputs["heatmap"]):
            junctions = tensorflow_prob_to_junctions(
                junc_prob,
                self.grid_size,
                self.junc_detect_thresh,
                self.max_num_junctions,
            )
>           line_map, junctions, _ = self.line_detector.detect(junctions, heatmap)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2_detector.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Translated_Outputs.tensorflow_outputs.kornia.feature.sold2.sold2_detector.tensorflow_LineSegmentDetectionModule object at 0x7f99679bb8b0>
junctions = <tf.Tensor: shape=(500, 2), dtype=float32, numpy=
array([[498., 507.],
       [ 34., 419.],
       [ 34., 427.],
     ...     [234., 475.],
       [ 34.,  59.],
       [146., 467.],
       [ 42., 139.],
       [490., 203.]], dtype=float32)>
heatmap = <tf.Tensor: shape=(512, 512), dtype=float32, numpy=
array([[0.7104042 , 0.64647985, 0.4422554 , ..., 0.949013  , 0.657...018077],
       [0.58762765, 0.6475043 , 0.41069275, ..., 0.81893885, 0.59524584,
        0.35970882]], dtype=float32)>

    def detect(self, junctions, heatmap):
        from ...core._backend import zeros
        from ...core._backend import where
        from ...core._backend import concatenate
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .structures import tensorflow_HeatMapRefineCfg
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_triu_frnt,
        )
        from ....ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_clamp_frnt,
        )
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_sqrt_frnt,
        )
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
        from ....ivy.functional.frontends.torch.reduction_ops import (
            tensorflow_mean_frnt,
        )
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        tensorflow_KORNIA_CHECK_SHAPE(heatmap, ["H", "W"])
        H, W = tensorflow_shape_frnt_(heatmap)
        device = junctions.device
        if self.use_heatmap_refinement and isinstance(
            self.heatmap_refine_cfg, (tensorflow_HeatMapRefineCfg,)
        ):
            if self.heatmap_refine_cfg.mode == "global":
                heatmap = self.refine_heatmap(
                    heatmap,
                    self.heatmap_refine_cfg.ratio,
                    self.heatmap_refine_cfg.valid_thresh,
                )
            elif self.heatmap_refine_cfg.mode == "local":
                heatmap = self.refine_heatmap_local(
                    heatmap,
                    self.heatmap_refine_cfg.num_blocks,
                    self.heatmap_refine_cfg.overlap_ratio,
                    self.heatmap_refine_cfg.ratio,
                    self.heatmap_refine_cfg.valid_thresh,
                )
        num_junctions = len(junctions)
        line_map_pred = zeros(
            [num_junctions, num_junctions], device=device, dtype=tf.int32
        )
        if num_junctions < 2:
            return line_map_pred, junctions, heatmap
        candidate_map = tensorflow_triu_frnt(
            tensorflow_ones_frnt(
                [num_junctions, num_junctions], device=device, dtype=tf.int32
            ),
            diagonal=1,
        )
        if self.use_candidate_suppression:
>           candidate_map = self.candidate_suppression(junctions, candidate_map)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2_detector.py:571: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Translated_Outputs.tensorflow_outputs.kornia.feature.sold2.sold2_detector.tensorflow_LineSegmentDetectionModule object at 0x7f99679bb8b0>
junctions = <tf.Tensor: shape=(500, 2), dtype=float32, numpy=
array([[498., 507.],
       [ 34., 419.],
       [ 34., 427.],
     ...     [234., 475.],
       [ 34.,  59.],
       [146., 467.],
       [ 42., 139.],
       [490., 203.]], dtype=float32)>
candidate_map = <tf.Tensor: shape=(500, 500), dtype=float32, numpy=
array([[0., 1., 1., ..., 1., 1., 1.],
       [0., 0., 1., ..., 1.,... 0., 0., ..., 0., 1., 1.],
       [0., 0., 0., ..., 0., 0., 1.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>

    def candidate_suppression(self, junctions, candidate_map):
        from ...core._backend import where
        from ...core._backend import sin
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_triu_frnt,
        )
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_unsqueeze_frnt,
        )
        from ....ivy.functional.frontends.torch.reduction_ops import (
            tensorflow_norm_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_einsum_frnt,
        )
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_acos_frnt,
        )
        from ....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_arange_frnt,
        )
    
        dist_tolerance = self.nms_dist_tolerance
        line_dist_map = (
            tensorflow_sum_frnt(
                (tensorflow_unsqueeze_frnt(junctions, dim=1) - junctions[None, ...])
                ** 2,
                dim=-1,
            )
            ** 0.5
        )
        seg_indexes = where(tensorflow_triu_frnt(candidate_map, diagonal=1))
        start_point_idxs = seg_indexes[0]
        end_point_idxs = seg_indexes[1]
        start_points = tensorflow_get_item(
            junctions, (start_point_idxs, slice(None, None, None))
        )
        end_points = tensorflow_get_item(
            junctions, (end_point_idxs, slice(None, None, None))
        )
        line_dists = tensorflow_get_item(
            line_dist_map, (start_point_idxs, end_point_idxs)
        )
        dir_vecs = (end_points - start_points) / tensorflow_norm_frnt(
            end_points - start_points, dim=-1
        )[..., None]
        cand_vecs = junctions[None, ...] - tensorflow_unsqueeze_frnt_(
            start_points, dim=1
        )
        cand_vecs_norm = tensorflow_norm_frnt(cand_vecs, dim=-1)
        proj = (
            tensorflow_einsum_frnt("bij,bjk->bik", cand_vecs, dir_vecs[..., None])
            / line_dists[..., None, None]
        )
>       proj_mask = (proj >= 0) * (proj <= 1)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2_detector.py:821: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(124750, 500, 1), dtype=bool, numpy=
array([[[ True],
        [ True],
        [ True],
        ......
       [[False],
        [ True],
        [ True],
        ...,
        [ True],
        [ True],
        [ True]]])>)
kwargs = {}
arg = <tf.Tensor: shape=(124750, 500, 1), dtype=bool, numpy=
array([[[ True],
        [ True],
        [ True],
        ...,...

       [[False],
        [ True],
        [ True],
        ...,
        [ True],
        [ True],
        [ True]]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SOLD2_detector.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_SOLD2_detector.call():
E         â€¢ img=tf.Tensor(shape=(1, 1, 512, 512), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SOLD2_detector
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-13 14:10:34.017339: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 499000000 exceeds 10% of free system memory.
2024-09-13 14:10:34.346204: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 499000000 exceeds 10% of free system memory.
__________________________________________________________________________________ test_DeDoDe[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DeDoDe(target_framework, mode, backend_compile):
        print("kornia.feature.DeDoDe")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDeDoDe = ivy.transpile(kornia.feature.DeDoDe, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 256, 256)
        torch_out = kornia.feature.DeDoDe(amp_dtype=torch.float32)(x)
    
        ivy.set_backend(target_framework)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDeDoDe(amp_dtype=ivy.as_native_dtype("float32"))(transpiled_x)

kornia/test_feature.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DeDoDe(), detector_model = 'L', descriptor_model = 'G', amp_dtype = tf.float32

    def __init__(self, detector_model="L", descriptor_model="G", amp_dtype=tf.float16):
        from .dedode_models import tensorflow_get_detector
        from .dedode_models import tensorflow_get_descriptor
        from ...enhance.normalize import tensorflow_normalize
        from ....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
    
        self.super___init__(
            detector_model=detector_model,
            descriptor_model=descriptor_model,
            amp_dtype=amp_dtype,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.detector: typing.Any = tensorflow_get_detector(detector_model, amp_dtype)

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/dedode.py:68: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kind = 'L', amp_dtype = tf.float32

    def tensorflow_get_detector(kind="L", amp_dtype=tf.float16):
        if kind == "L":
>           return tensorflow_dedode_detector_L(amp_dtype)

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/dedode_models.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

amp_dtype = tf.float32

    def tensorflow_dedode_detector_L(amp_dtype=tf.float16):
        from ....torch.nn.modules.container import tensorflow_ModuleDict
        from .decoder import tensorflow_ConvRefiner
        from .encoder import tensorflow_VGG19
        from .decoder import tensorflow_Decoder
        from .detector import tensorflow_DeDoDeDetector
    
        NUM_PROTOTYPES = 1
        residual = True
        hidden_blocks = 8
        amp = True
        conv_refiner = tensorflow_ModuleDict(
            {
>               "8": tensorflow_ConvRefiner(
                    512,
                    512,
                    256 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
                "4": tensorflow_ConvRefiner(
                    256 + 256,
                    256,
                    128 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
                "2": tensorflow_ConvRefiner(
                    128 + 128,
                    128,
                    64 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
                "1": tensorflow_ConvRefiner(
                    64 + 64,
                    64,
                    1 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
            }
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/dedode_models.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvRefiner(), args = (512, 512, 257), kwargs = {'amp': True, 'amp_dtype': tf.float32, 'hidden_blocks': 8, 'residual': True}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvRefiner(), in_dim = 512, hidden_dim = 512, out_dim = 257, dw = True, kernel_size = 5, hidden_blocks = 8, amp = True, residual = True, amp_dtype = tf.float32

    @tensorflow_store_config_info
    def __init__(
        self,
        in_dim=6,
        hidden_dim=16,
        out_dim=2,
        dw=True,
        kernel_size=5,
        hidden_blocks=5,
        amp=True,
        residual=False,
        amp_dtype=tf.float16,
    ):
        from ....torch.nn.modules.container import tensorflow_Sequential
        from ....tensorflow__stateful_layers import KerasConv2D
    
        self.super___init__(
            in_dim=in_dim,
            hidden_dim=hidden_dim,
            out_dim=out_dim,
            dw=dw,
            kernel_size=kernel_size,
            hidden_blocks=hidden_blocks,
            amp=amp,
            residual=residual,
            amp_dtype=amp_dtype,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.block1 = self.create_block(in_dim, hidden_dim, dw=False, kernel_size=1)

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/decoder.py:466: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvRefiner(), in_dim = 512, out_dim = 512, dw = False, kernel_size = 1, bias = True
norm_type = <class 'Translated_Outputs.tensorflow_outputs.tensorflow__stateful_layers.KerasBatchNorm2D'>

    def create_block(
        self,
        in_dim,
        out_dim,
        dw=True,
        kernel_size=5,
        bias=True,
        norm_type=KerasBatchNorm2D,
    ):
        from ....torch.nn.modules.container import tensorflow_Sequential
        from ....torch.nn.modules.activation import tensorflow_ReLU
        from ....tensorflow__stateful_layers import KerasConv2D
        from ....tensorflow__stateful_layers import resolve_convolution
        from ....tensorflow__stateful_layers import KerasBatchNorm2D
    
        num_groups = 1 if not dw else in_dim
        if dw:
            if out_dim % in_dim != 0:
                raise Exception("outdim must be divisible by indim for depthwise")
        conv1 = resolve_convolution(
            in_channels=in_dim,
            filters=out_dim,
            kernel_size=kernel_size,
            strides=1,
            padding=kernel_size // 2,
            use_bias=bias,
            dilation_rate=1,
            groups=num_groups,
            padding_mode="zeros",
            data_format="channels_last",
        )
        norm = (
>           norm_type(out_dim)
            if norm_type is KerasBatchNorm2D
            else norm_type(num_channels=out_dim)
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/decoder.py:524: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasBatchNorm2D(), args = (512,), kwargs = {}

    def __init__(self, *args, **kwargs):
        self._previous_frame_info = None
    
        # pytorch layer attributes
>       self.num_features = kwargs.pop("num_features")
E       KeyError: 'num_features'

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:585: KeyError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.DeDoDe
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth" to /root/.cache/torch/hub/checkpoints/dinov2_vitl14_pretrain.pth

  0%|          | 0.00/1.13G [00:00<?, ?B/s]
  2%|â–         | 26.2M/1.13G [00:00<00:04, 275MB/s]
  5%|â–         | 53.0M/1.13G [00:00<00:04, 278MB/s]
  7%|â–‹         | 83.4M/1.13G [00:00<00:03, 296MB/s]
 10%|â–‰         | 115M/1.13G [00:00<00:03, 309MB/s] 
 12%|â–ˆâ–        | 144M/1.13G [00:00<00:03, 285MB/s]
 15%|â–ˆâ–Œ        | 175M/1.13G [00:00<00:03, 298MB/s]
 18%|â–ˆâ–Š        | 208M/1.13G [00:00<00:03, 310MB/s]
 21%|â–ˆâ–ˆ        | 241M/1.13G [00:00<00:03, 322MB/s]
 24%|â–ˆâ–ˆâ–Ž       | 274M/1.13G [00:00<00:02, 329MB/s]
 26%|â–ˆâ–ˆâ–‹       | 307M/1.13G [00:01<00:02, 334MB/s]
 29%|â–ˆâ–ˆâ–‰       | 339M/1.13G [00:01<00:02, 336MB/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 371M/1.13G [00:01<00:02, 335MB/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 403M/1.13G [00:01<00:02, 328MB/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 435M/1.13G [00:01<00:02, 323MB/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 466M/1.13G [00:01<00:02, 319MB/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 497M/1.13G [00:01<00:02, 322MB/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 529M/1.13G [00:01<00:02, 326MB/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 561M/1.13G [00:01<00:01, 329MB/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 593M/1.13G [00:01<00:01, 329MB/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 626M/1.13G [00:02<00:01, 334MB/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 659M/1.13G [00:02<00:01, 337MB/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 692M/1.13G [00:02<00:01, 340MB/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 724M/1.13G [00:02<00:01, 339MB/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 757M/1.13G [00:02<00:01, 333MB/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 789M/1.13G [00:02<00:01, 329MB/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 820M/1.13G [00:02<00:01, 324MB/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 851M/1.13G [00:02<00:01, 323MB/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 883M/1.13G [00:02<00:00, 325MB/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 914M/1.13G [00:02<00:00, 324MB/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 946M/1.13G [00:03<00:00, 328MB/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 979M/1.13G [00:03<00:00, 333MB/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 0.99G/1.13G [00:03<00:00, 336MB/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1.02G/1.13G [00:03<00:00, 331MB/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1.05G/1.13G [00:03<00:00, 332MB/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1.08G/1.13G [00:03<00:00, 332MB/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1.11G/1.13G [00:03<00:00, 330MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.13G/1.13G [00:03<00:00, 325MB/s]
___________________________________________________________________________________ test_DISK[tensorflow-s2s-False] ____________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DISK(target_framework, mode, backend_compile):
        print("kornia.feature.DISK")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDISK = ivy.transpile(kornia.feature.DISK, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 256, 256)
        torch_out = kornia.feature.DISK()(x)[0]
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDISK()(transpiled_x)

kornia/test_feature.py:1131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.16068512, 0.49403733, 0.20287901, ..., 0.552001...
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f99789b7c10, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDow...,
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.16068512, 0.49403733, 0.20287901, ..., 0.552001...
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDow...,
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.16068512, 0.49403733, 0.20287901, ..., 0.552001...
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.16068512, 0.49403733, 0.20287901, ..., 0.5520015...],
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (images, n=None, window_size=5, score_threshold=0.0, pad_if_not_divisible=False)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDow... tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
),)
kwargs = {'images': <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.16068512, 0.49403733, 0.20287901, ......,
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
images = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.16068512, 0.49403733, 0.20287901, ..., 0.5520015...],
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>
n = None, window_size = 5, score_threshold = 0.0, pad_if_not_divisible = False

    def call(
        self,
        images,
        n=None,
        window_size=5,
        score_threshold=0.0,
        pad_if_not_divisible=False,
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_pad_frnt,
        )
        from .detector import tensorflow_heatmap_to_keypoints
    
        B = tensorflow_shape_frnt_(images)[0]
        if pad_if_not_divisible:
            h, w = (
                tensorflow_shape_frnt_(images)[2:][0],
                tensorflow_shape_frnt_(images)[2:][1],
            )
            pd_h = 16 - h % 16 if h % 16 > 0 else 0
            pd_w = 16 - w % 16 if w % 16 > 0 else 0
            images = tensorflow_pad_frnt(images, (0, pd_w, 0, pd_h), value=0.0)
>       heatmaps, descriptors = self.heatmap_and_dense_descriptors(images)

Translated_Outputs/tensorflow_outputs/kornia/feature/disk/disk.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
images = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.16068512, 0.49403733, 0.20287901, ..., 0.5520015...],
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>

    def heatmap_and_dense_descriptors(self, images):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
>       unet_output = self.unet(images)

Translated_Outputs/tensorflow_outputs/kornia/feature/disk/disk.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.16068512, 0.49403733, 0.20287901, ..., 0.552001...
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f9998248b20, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ls.py', lineno=117, function='error_handler', code_context=['            return fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.16068512, 0.49403733, 0.20287901, ..., 0.552001...
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.16068512, 0.49403733, 0.20287901, ..., 0.552001...
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (inp)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
inp = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.16068512, 0.49403733, 0.20287901, ..., 0.5520015...],
         [0.7828492 , 0.18678433, 0.5034873 , ..., 0.8729647 ,
          0.9021613 , 0.36313576]]]], dtype=float32)>

    def call(self, inp):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
    
        if tensorflow_size_frnt_(inp, 1) != self.in_features:
            fmt = "Expected {} feature channels in input, got {}"
            msg = fmt.format(self.in_features, tensorflow_size_frnt_(inp, 1))
            raise ValueError(msg)
        input_size_divisor = 2 ** len(self.up)
        if (
            tensorflow_size_frnt_(inp, 2) % input_size_divisor != 0
            or tensorflow_size_frnt_(inp, 3) % input_size_divisor != 0
        ):
            raise ValueError(
                f"Input image shape must be divisible by {input_size_divisor} (got {tensorflow_size_frnt_(inp)}). This is not inherent to DISK, but to the U-Net architecture used in pretrained models. Please pad if necessary."
            )
        features = [inp]
        for layer in self.path_down:
>           features.append(layer(features[-1]))

Translated_Outputs/tensorflow_outputs/kornia/feature/disk/_unets/unet.py:91: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_TrivialDownsample()
  (1): tensorflow_Conv(
    (0): tensorflow_InstanceNorm2d()
    (1): tensorflow_PReLU()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
args = (<tf.Tensor: shape=(1, 16, 256, 256), dtype=float32, numpy=
array([[[[ 0.27643225,  0.09385319,  0.07813358, ...,  0.1...    [-0.07362368,  0.36673477,  0.773449  , ...,  0.5656305 ,
           0.26982853,  0.37786126]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f9978b6c040, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_TrivialDownsample()
  (1): tensorflow_Conv(
    (0): tensorflow_InstanceNorm2d()
    (1): tensorflow_PReLU()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 16, 256, 256), dtype=float32, numpy=
array([[[[ 0.27643225,  0.09385319,  0.07813358, ...,  0.1...    [-0.07362368,  0.36673477,  0.773449  , ...,  0.5656305 ,
           0.26982853,  0.37786126]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_TrivialDownsample()
  (1): tensorflow_Conv(
    (0): tensorflow_InstanceNorm2d()
    (1): tensorflow_PReLU()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 16, 256, 256), dtype=float32, numpy=
array([[[[ 0.27643225,  0.09385319,  0.07813358, ...,  0.1...    [-0.07362368,  0.36673477,  0.773449  , ...,  0.5656305 ,
           0.26982853,  0.37786126]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_TrivialDownsample()
  (1): tensorflow_Conv(
    (0): tensorflow_InstanceNorm2d()
    (1): tensorflow_PReLU()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
input = <tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 0.12946224,  0.0416526 ,  0.06454976, ...,  0.06...      [ 0.21680284,  0.57352567,  0.413912  , ...,  0.45840472,
           0.40458417,  0.24046278]]]], dtype=float32)>

    def call(self, input):
        for i, module in enumerate(self):
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_InstanceNorm2d()
  (1): tensorflow_PReLU()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
)
args = (<tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 0.12946224,  0.0416526 ,  0.06454976, ...,  0.0...    [ 0.21680284,  0.57352567,  0.413912  , ...,  0.45840472,
           0.40458417,  0.24046278]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55dc95509320, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_InstanceNorm2d()
  (1): tensorflow_PReLU()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 0.12946224,  0.0416526 ,  0.06454976, ...,  0.0...    [ 0.21680284,  0.57352567,  0.413912  , ...,  0.45840472,
           0.40458417,  0.24046278]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_InstanceNorm2d()
  (1): tensorflow_PReLU()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 0.12946224,  0.0416526 ,  0.06454976, ...,  0.0...    [ 0.21680284,  0.57352567,  0.413912  , ...,  0.45840472,
           0.40458417,  0.24046278]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_InstanceNorm2d()
  (1): tensorflow_PReLU()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
)
input = <tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 1.7653564 ,  0.57016027,  0.88181853, ...,  0.89...      [-3.539817  ,  0.52024317, -1.2964087 , ..., -0.79001236,
          -1.4025736 , -3.2705302 ]]]], dtype=float32)>

    def call(self, input):
        for i, module in enumerate(self):
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PReLU()
args = (<tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 1.7653564 ,  0.57016027,  0.88181853, ...,  0.8...    [-3.539817  ,  0.52024317, -1.2964087 , ..., -0.79001236,
          -1.4025736 , -3.2705302 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f9978b6ccf0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PReLU(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 1.7653564 ,  0.57016027,  0.88181853, ...,  0.8...    [-3.539817  ,  0.52024317, -1.2964087 , ..., -0.79001236,
          -1.4025736 , -3.2705302 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PReLU(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 1.7653564 ,  0.57016027,  0.88181853, ...,  0.8...    [-3.539817  ,  0.52024317, -1.2964087 , ..., -0.79001236,
          -1.4025736 , -3.2705302 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PReLU(), args = ()
kwargs = {'input': <tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 1.7653564 ,  0.57016027,  0.88181853, ...     [-3.539817  ,  0.52024317, -1.2964087 , ..., -0.79001236,
          -1.4025736 , -3.2705302 ]]]], dtype=float32)>}
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f99a1960f70>, DATA_FORMAT = 'channels_first'
fn_args_and_kwargs = {'input': <tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 1.7653564 ,  0.57016027,  0.88181853, ...     [-3.539817  ,  0.52024317, -1.2964087 , ..., -0.79001236,
          -1.4025736 , -3.2705302 ]]]], dtype=float32)>}
conv_block_start = <function tensorflow_handle_transpose_in_input_and_output.<locals>.transpose_wrapper.<locals>.<lambda> at 0x7f9978bf8670>, next_call_in_seq = None, conv_block_continued = None

    @functools.wraps(fn)
    def transpose_wrapper(self, *args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        kwargs_call = {
            key: val
            for key, val in kwargs.items()
            if key not in dict(original_signature.parameters)
        }
        fn_args_and_kwargs = {
            key: val for key, val in kwargs.items() if key not in kwargs_call
        }
        fn_args_and_kwargs.update(dict(zip(fn.__code__.co_varnames[1:], args)))
        conv_block_start = lambda f: any(
            substr in f.__qualname__
            for substr in CONV_FUNCS
            + NORM_FUNCS
            + POOL_FUNCS
            + KERAS_CONV_FUNCS
            + KERAS_NORM_FUNCS
            + KERAS_POOL_FUNCS
        )
        next_call_in_seq = tensorflow_get_next_func(self)
        name_of_next_call = (
            next_call_in_seq.__class__.__name__
            if hasattr(next_call_in_seq, "__class__")
            else ""
        )
        conv_block_continued = next_call_in_seq and any(
            substr in name_of_next_call for substr in CONV_BLOCK_FNS
        )
        if DATA_FORMAT == "channels_first" and conv_block_start(self.__class__):
            input = fn_args_and_kwargs["input"]
            if len(input.shape) > 4:
                transpose = tensorflow_TransposeType.CONV3D
            elif len(input.shape) > 3:
                transpose = tensorflow_TransposeType.CONV2D
            elif len(input.shape) > 2:
                transpose = tensorflow_TransposeType.CONV1D
            else:
                transpose = tensorflow_TransposeType.NO_TRANSPOSE
            fn_args_and_kwargs = tensorflow_set_item_bknd(
                fn_args_and_kwargs,
                "input",
                tensorflow_apply_transpose(input, transpose=transpose, pt_to_tf=True),
            )
            DATA_FORMAT = "channels_last"
            os.environ = tensorflow_set_item_bknd(
                os.environ, "DATA_FORMAT", DATA_FORMAT
            )
>       res = fn(self, **fn_args_and_kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:396: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PReLU()
input = <tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 1.7653564 ,  0.57016027,  0.88181853, ...,  0.89...      [-3.539817  ,  0.52024317, -1.2964087 , ..., -0.79001236,
          -1.4025736 , -3.2705302 ]]]], dtype=float32)>

    @tensorflow_handle_transpose_in_input_and_output
    def call(self, input):
        from ....ivy.functional.frontends.torch.nn.functional.non_linear_activation_functions import (
            tensorflow_prelu_frnt,
        )
    
>       return tensorflow_prelu_frnt(input, self.weight)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/activation.py:1220: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 1.7653564 ,  0.57016027,  0.88181853, ...,  0.89...      [-3.539817  ,  0.52024317, -1.2964087 , ..., -0.79001236,
          -1.4025736 , -3.2705302 ]]]], dtype=float32)>
weight = <tf.Variable 'Variable:0' shape=(16,) dtype=float32, numpy=
array([0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
       0.25, 0.25, 0.25, 0.25, 0.25], dtype=float32)>

    def tensorflow_prelu_frnt(input, weight):
        from .....backends.tensorflow.elementwise import tensorflow_add
        from .....backends.tensorflow.elementwise import tensorflow_maximum
        from .....backends.tensorflow.elementwise import tensorflow_multiply
        from .....backends.tensorflow.elementwise import tensorflow_minimum
    
        return tensorflow_add(
            tensorflow_maximum(0, input),
>           tensorflow_multiply(weight, tensorflow_minimum(0, input)),
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/non_linear_activation_functions.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x1 = <tf.Variable 'Variable:0' shape=(16,) dtype=float32, numpy=
array([0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
       0.25, 0.25, 0.25, 0.25, 0.25], dtype=float32)>
x2 = <tf.Tensor: shape=(1, 16, 128, 128), dtype=float32, numpy=
array([[[[ 0.        ,  0.        ,  0.        , ...,  0.  ...      [-3.539817  ,  0.        , -1.2964087 , ..., -0.79001236,
          -1.4025736 , -3.2705302 ]]]], dtype=float32)>

    def tensorflow_multiply(
        x1: Union[float, tensorflow.Tensor, tensorflow.Variable],
        x2: Union[float, tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.data_type import tensorflow_default_dtype_bknd
        from ...ivy.general import tensorflow_is_array_bknd
        from .creation import tensorflow_asarray
    
        oirg_x1 = x1
        oirg_x2 = x2
        try:
            dtype = (
                x1.dtype
                if hasattr(x1, "dtype")
                else x2.dtype
                if hasattr(x2, "dtype")
                else tensorflow_default_dtype_bknd()
            )
            if not tensorflow_is_array_bknd(x1):
                x1 = tensorflow_asarray(x1, dtype=dtype)
            if not tensorflow_is_array_bknd(x2):
                x2 = tensorflow_asarray(x2, dtype=dtype)
        except:
            x1 = oirg_x1
            x2 = oirg_x2
>       return tensorflow.math.multiply(x1, x2)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PReLU.call().
E       
E       [1m{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [16] vs. [1,16,128,128] [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_PReLU.call():
E         â€¢ input=tf.Tensor(shape=(1, 16, 128, 128), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/elementwise.py:441: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.DISK
________________________________________________________________________________ test_SIFTFeature[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SIFTFeature(target_framework, mode, backend_compile):
        print("kornia.feature.SIFTFeature")
    
        if backend_compile:
            pytest.skip()
    
        import os
        flag = os.environ.get("APPLY_TRANSPOSE_OPTIMIZATION")
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = "false"
    
        TranspiledSIFTFeature = ivy.transpile(kornia.feature.SIFTFeature, source="torch", target=target_framework)
    
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = flag
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.SIFTFeature(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSIFTFeature(num_features=5000)(transpiled_x)

kornia/test_feature.py:1177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeature(
  (detector): tensorflow_MultiResolutionDetector(
    (model): tensorflow_BlobDoGSingle, sigma...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.32229692, 0.5976798 , 0.05366158, ..., 0.321758...
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f9978b02650, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTFeature(
  (detector): tensorflow_MultiResolutionDetector(
    (model): tensorflow_BlobDoGSingle, sigm...,
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeature(
  (detector): tensorflow_MultiResolutionDetector(
    (model): tensorflow_BlobDoGSingle, sigma...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.32229692, 0.5976798 , 0.05366158, ..., 0.321758...
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTFeature(
  (detector): tensorflow_MultiResolutionDetector(
    (model): tensorflow_BlobDoGSingle, sigm...,
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeature(
  (detector): tensorflow_MultiResolutionDetector(
    (model): tensorflow_BlobDoGSingle, sigma...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.32229692, 0.5976798 , 0.05366158, ..., 0.321758...
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.32229692, 0.5976798 , 0.05366158, ..., 0.3217589...],
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (img, mask=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTFeature(
  (detector): tensorflow_MultiResolutionDetector(
    (model): tensorflow_BlobDoGSingle, sigm..._bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.32229692, 0.5976798 , 0.05366158, ..., 0...,
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeature(
  (detector): tensorflow_MultiResolutionDetector(
    (model): tensorflow_BlobDoGSingle, sigma...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
img = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.32229692, 0.5976798 , 0.05366158, ..., 0.3217589...],
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>
mask = None

    def call(self, img, mask=None):
        from .laf import tensorflow_scale_laf
    
>       lafs, responses = self.detector(img, mask)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:461: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MultiResolutionDetector(
  (model): tensorflow_BlobDoGSingle, sigma1=1.0, sigma2=1.6)
  (nms): tensorflow_N...tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08))
  (aff): tensorflow_PassLAF()
)
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.32229692, 0.5976798 , 0.05366158, ..., 0.321758...     [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>, None)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f9998248950, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MultiResolutionDetector(
  (model): tensorflow_BlobDoGSingle, sigma1=1.0, sigma2=1.6)
  (nms): tensorflow_N...tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08))
  (aff): tensorflow_PassLAF()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.32229692, 0.5976798 , 0.05366158, ..., 0.321758...     [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>, None)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MultiResolutionDetector(
  (model): tensorflow_BlobDoGSingle, sigma1=1.0, sigma2=1.6)
  (nms): tensorflow_N...tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08))
  (aff): tensorflow_PassLAF()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.32229692, 0.5976798 , 0.05366158, ..., 0.321758...     [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>, None)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (img, mask=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MultiResolutionDetector(
  (model): tensorflow_BlobDoGSingle, sigma1=1.0, sigma2=1.6)
  (nms): tensorflow_N...tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08))
  (aff): tensorflow_PassLAF()
)
img = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.32229692, 0.5976798 , 0.05366158, ..., 0.3217589...],
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>
mask = None

    def call(self, img, mask=None):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
    
        tensorflow_KORNIA_CHECK_SHAPE(img, ["1", "C", "H", "W"])
        responses, lafs = self.detect(img, mask)
        lafs = self.aff(lafs, img)
>       lafs = self.ori(lafs, img)

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:829: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFOrienter(patch_size=19, angle_detector=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08))
args = (<tf.Tensor: shape=(1, 902, 2, 3), dtype=float32, numpy=
array([[[[ 15.558011,   0.      ,  47.381218],
         [ -0....,
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55dc9aac1ed0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFOrienter(patch_size=19, angle_detector=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 902, 2, 3), dtype=float32, numpy=
array([[[[ 15.558011,   0.      ,  47.381218],
         [ -0....,
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFOrienter(patch_size=19, angle_detector=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 902, 2, 3), dtype=float32, numpy=
array([[[[ 15.558011,   0.      ,  47.381218],
         [ -0....,
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (laf, img)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFOrienter(patch_size=19, angle_detector=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08))
laf = <tf.Tensor: shape=(1, 902, 2, 3), dtype=float32, numpy=
array([[[[ 15.558011,   0.      ,  47.381218],
         [ -0. ...9 ]],

        [[ 90.83871 ,   0.      , 189.93549 ],
         [ -0.      ,  90.83871 ,  74.32258 ]]]], dtype=float32)>
img = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.32229692, 0.5976798 , 0.05366158, ..., 0.3217589...],
         [0.7185138 , 0.99620855, 0.79834414, ..., 0.9647866 ,
          0.68987864, 0.7810205 ]]]], dtype=float32)>

    def call(self, laf, img):
        from ..core.check import tensorflow_KORNIA_CHECK_LAF
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from .laf import tensorflow_extract_patches_from_pyramid
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_as_frnt_
        from .laf import tensorflow_get_laf_orientation
        from .laf import tensorflow_set_laf_orientation
        from ..geometry.conversions import tensorflow_rad2deg
    
        tensorflow_KORNIA_CHECK_LAF(laf)
        tensorflow_KORNIA_CHECK_SHAPE(img, ["B", "C", "H", "W"])
        if tensorflow_size_frnt_(laf, 0) != tensorflow_size_frnt_(img, 0):
            raise ValueError(
                f"Batch size of laf and img should be the same. Got {tensorflow_size_frnt_(img, 0)}, {tensorflow_size_frnt_(laf, 0)}"
            )
        B, N = tensorflow_shape_frnt_(laf)[:2][0], tensorflow_shape_frnt_(laf)[:2][1]
        patches: typing.Any = tensorflow_view_frnt_(
            tensorflow_extract_patches_from_pyramid(img, laf, self.patch_size),
            -1,
            1,
            self.patch_size,
            self.patch_size,
        )
        angles_radians: typing.Any = tensorflow_view_frnt_(
>           self.angle_detector(patches), B, N
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:919: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)
args = (<tf.Tensor: shape=(902, 1, 19, 19), dtype=float32, numpy=
array([[[[0.86472845, 0.85434455, 0.25630528, ..., 0.378841...
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55dcf3816660, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08), v = None, buffers = None
args = (<tf.Tensor: shape=(902, 1, 19, 19), dtype=float32, numpy=
array([[[[0.86472845, 0.85434455, 0.25630528, ..., 0.378841...
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08), v = None, buffers = None
args = (<tf.Tensor: shape=(902, 1, 19, 19), dtype=float32, numpy=
array([[[[0.86472845, 0.85434455, 0.25630528, ..., 0.378841...
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (patch)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)
patch = <tf.Tensor: shape=(902, 1, 19, 19), dtype=float32, numpy=
array([[[[0.86472845, 0.85434455, 0.25630528, ..., 0.3788416...],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ]]]], dtype=float32)>

    def call(self, patch):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_atan2_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_floor_frnt,
        )
        from ...ivy.functional.frontends.torch.nn.functional.pooling_functions import (
            tensorflow_adaptive_avg_pool2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ..constants import pi
    
        tensorflow_KORNIA_CHECK_SHAPE(patch, ["B", "1", "H", "W"])
        _, CH, W, H = tensorflow_size_frnt_(patch)
        if W != self.patch_size or H != self.patch_size or CH != 1:
            raise TypeError(
                f"input shape should be must be [Bx1x{self.patch_size}x{self.patch_size}]. Got {tensorflow_size_frnt_(patch)}"
            )
        self.weighting = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.weighting, patch.dtype), patch.device
        )
        self.angular_smooth = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.angular_smooth, patch.dtype), patch.device
        )
        grads: typing.Any = self.gradient(patch)
        gx: typing.Any = grads[:, :, 0]
        gy: typing.Any = grads[:, :, 1]
        mag: typing.Any = (
            tensorflow_sqrt_frnt(gx * gx + gy * gy + self.eps) * self.weighting
        )
        ori: typing.Any = tensorflow_atan2_frnt(gy, gx + self.eps) + 2.0 * pi
        o_big = float(self.num_ang_bins) * (ori + 1.0 * pi) / (2.0 * pi)
        bo0_big = tensorflow_floor_frnt(o_big)
        wo1_big = o_big - bo0_big
        bo0_big = bo0_big % self.num_ang_bins
        bo1_big = (bo0_big + 1) % self.num_ang_bins
        wo0_big = (1.0 - wo1_big) * mag
        wo1_big = wo1_big * mag
        ang_bins_list = []
        for i in range(0, self.num_ang_bins):
            ang_bins_i = tensorflow_adaptive_avg_pool2d_frnt(
                tensorflow_to_frnt_(bo0_big == i, patch.dtype) * wo0_big
                + tensorflow_to_frnt_(bo1_big == i, patch.dtype) * wo1_big,
                (1, 1),
            )
            ang_bins_list.append(ang_bins_i)
        ang_bins = tensorflow_view_frnt_(
            tensorflow_cat_frnt(ang_bins_list, 1), -1, 1, self.num_ang_bins
        )
>       ang_bins = tensorflow_view_frnt_(
            self.angular_smooth(ang_bins), -1, self.num_ang_bins
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:500: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(902, 5, 40), dtype=float32, numpy=
array([[[8.87346005e-07, 9.63566890e-07, 4.80268170e-07, ...,
 ...00000000e+00, 0.00000000e+00, ...,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>, -1, 36)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f9990cae8c0>
array_like = <tf.Tensor: shape=(902, 5, 40), dtype=float32, numpy=
array([[[8.87346005e-07, 9.63566890e-07, 4.80268170e-07, ...,
  ...0e+00, 0.00000000e+00, 0.00000000e+00, ...,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(902, 5, 40), dtype=float32, numpy=
array([[[8.87346005e-07, 9.63566890e-07, 4.80268170e-07, ...,
  ...0e+00, 0.00000000e+00, 0.00000000e+00, ...,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>
size = None, args = (-1, 36), tensorflow_exists_bknd = <function tensorflow_exists_bknd at 0x7f9990cae7a0>, tensorflow_reshape_frnt = <function tensorflow_reshape_frnt at 0x7f9967b468c0>

    @tensorflow_handle_methods
    def tensorflow_view_frnt_(tensor, *args, size=None):
        from ...ivy.general import tensorflow_exists_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_reshape_frnt
    
        """Reshape Tensor.
    
        possible arguments are either:
            - size
            - tuple of ints
            - list of ints
            - torch.Size object
            - ints
    
        Parameters
        ----------
        args:int arguments
        size: optional shape
    
        Returns reshaped tensor
        -------
        """
        if tensorflow_exists_bknd(size) and not args:
            shape_tup = size
        elif args and not tensorflow_exists_bknd(size):
            if (
                isinstance(args[0], (tuple, list, tuple, tf.TensorShape))
                or type(args[0]).__name__ == "Size"
            ) and len(args) == 1:
                shape_tup = args[0]
            else:
                shape_tup = args
        else:
            raise ValueError(
                "View only accepts as argument ints, tuple or list of ints or the keyword argument size."
            )
>       return tensorflow_reshape_frnt(tensor, shape_tup)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:282: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(902, 5, 40), dtype=float32, numpy=
array([[[8.87346005e-07, 9.63566890e-07, 4.80268170e-07, ...,
 ...000000e+00, 0.00000000e+00, ...,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>, (-1, 36))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f9990cae8c0>
array_like = <tf.Tensor: shape=(902, 5, 40), dtype=float32, numpy=
array([[[8.87346005e-07, 9.63566890e-07, 4.80268170e-07, ...,
  ...0e+00, 0.00000000e+00, 0.00000000e+00, ...,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(902, 5, 40), dtype=float32, numpy=
array([[[8.87346005e-07, 9.63566890e-07, 4.80268170e-07, ...,
  ...0e+00, 0.00000000e+00, 0.00000000e+00, ...,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>
shape = (-1, 36)

    @tensorflow_handle_methods
    def tensorflow_reshape_frnt(input, shape):
        from ...backends.tensorflow.manipulation import tensorflow_reshape
    
>       return tensorflow_reshape(input, shape)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(902, 5, 40), dtype=float32, numpy=
array([[[8.87346005e-07, 9.63566890e-07, 4.80268170e-07, ...,
 ...000000e+00, 0.00000000e+00, ...,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>, (-1, 36))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f9990cae8c0>
array_like = <tf.Tensor: shape=(902, 5, 40), dtype=float32, numpy=
array([[[8.87346005e-07, 9.63566890e-07, 4.80268170e-07, ...,
  ...0e+00, 0.00000000e+00, 0.00000000e+00, ...,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(902, 5, 40), dtype=float32, numpy=
array([[[8.87346005e-07, 9.63566890e-07, 4.80268170e-07, ...,
 ...000000e+00, 0.00000000e+00, ...,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>, (-1, 36)]
kwargs = {}, tensorflow_get_item = <function tensorflow_get_item at 0x7f99a2b4e9e0>, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f9990cae8c0>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f9990cae560>, tensorflow_asarray = <function tensorflow_asarray at 0x7f9990b3e290>, num_args = 2
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'shape', 'copy', 'order', 'allowzero', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...s 'bool'>, typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 1

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(902, 5, 40), dtype=float32, numpy=
array([[[8.87346005e-07, 9.63566890e-07, 4.80268170e-07, ...,
  ...0e+00, 0.00000000e+00, 0.00000000e+00, ...,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]], dtype=float32)>
shape = (-1, 36)

    @tensorflow_handle_methods
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_reshape(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Union[tf.TensorShape, Sequence[int]],
        *,
        copy: Optional[bool] = None,
        order: str = "C",
        allowzero: bool = True,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ....utils.assertions import tensorflow_check_elem_in_list
    
        tensorflow_check_elem_in_list(order, ["C", "F"])
        if not allowzero:
            shape = [
                (new_s if con else old_s)
                for new_s, con, old_s in zip(
                    shape, tensorflow.constant(shape) != 0, x.shape
                )
            ]
        if order == "F":
            return tensorflow__reshape_fortran_tf(x, shape)
>       return tensorflow.reshape(x, shape)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PatchDominantGradientOrientation.call().
E       
E       [1m{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 180400 values, but the requested shape requires a multiple of 36 [Op:Reshape][0m
E       
E       Arguments received by tensorflow_PatchDominantGradientOrientation.call():
E         â€¢ patch=tf.Tensor(shape=(902, 1, 19, 19), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/manipulation.py:201: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SIFTFeature
___________________________________________________________________________ test_SIFTFeatureScaleSpace[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SIFTFeatureScaleSpace(target_framework, mode, backend_compile):
        print("kornia.feature.SIFTFeatureScaleSpace")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSIFTFeatureScaleSpace = ivy.transpile(kornia.feature.SIFTFeatureScaleSpace, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.SIFTFeatureScaleSpace(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSIFTFeatureScaleSpace(num_features=5000)(transpiled_x)

kornia/test_feature.py:1194: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_py...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.809181...
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f9978b6fa40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_p...,
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_py...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.809181...
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_p...,
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_py...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.809181...
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.8091812...],
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (img, mask=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_p..._bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0...,
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_py...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
img = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.8091812...],
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>
mask = None

    def call(self, img, mask=None):
        from .laf import tensorflow_scale_laf
    
>       lafs, responses = self.detector(img, mask)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:461: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...ctor=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), aff=tensorflow_PassLAF())
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.809181...     [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>, None)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f9978b01260, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...ctor=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), aff=tensorflow_PassLAF())
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.809181...     [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>, None)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...ctor=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), aff=tensorflow_PassLAF())
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.809181...     [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>, None)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (img, mask=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...ctor=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), aff=tensorflow_PassLAF())
img = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.8091812...],
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>
mask = None

    def call(self, img, mask=None):
>       responses, lafs = self.detect(img, self.num_features, mask)

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:274: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...ctor=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), aff=tensorflow_PassLAF())
img = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.8091812...],
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>
num_feats = 5000, mask = None

    def detect(self, img, num_feats, mask=None):
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.comparison_ops import (
            tensorflow_topk_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from .laf import tensorflow_laf_is_inside_image
        from ..core._backend import eye
        from ..core._backend import concatenate
    
        dev: typing.Any = img.device
        dtype: typing.Any = img.dtype
        sigmas: typing.Any
>       sp, sigmas, _ = self.scale_pyr(img)

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:146: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=32, extra_levels=3, border=15, sigma_step=1.2599210498948732, double_image=True)
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.809181...
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f99904fc3e0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ls.py', lineno=117, function='error_handler', code_context=['            return fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=32, extra_levels=3, border=15, sigma_step=1.2599210498948732, double_image=True), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.809181...
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=32, extra_levels=3, border=15, sigma_step=1.2599210498948732, double_image=True), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.809181...
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=32, extra_levels=3, border=15, sigma_step=1.2599210498948732, double_image=True)
x = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.8091812...],
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>

    def call(self, x):
        from ...filters.gaussian import tensorflow_gaussian_blur2d
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from ...core._backend import ones
        from ...core._backend import stack
    
        bs, _, _, _ = tensorflow_size_frnt_(x)
>       cur_level, cur_sigma, pixel_distance = self.get_first_level(x)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:115: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=32, extra_levels=3, border=15, sigma_step=1.2599210498948732, double_image=True)
input = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.8091812...],
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>

    def get_first_level(self, input):
        from ...filters.gaussian import tensorflow_gaussian_blur2d
    
        pixel_distance = 1.0
        cur_sigma = 0.5
        if self.double_image:
>           x = tensorflow_upscale_double(input)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.8785066 , 0.27548462, 0.13213086, ..., 0.8091812...],
         [0.10007203, 0.9082184 , 0.90634435, ..., 0.34744722,
          0.41316915, 0.20078838]]]], dtype=float32)>

    def tensorflow_upscale_double(x):
        from ...core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...core._backend import zeros
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(x)
        tensorflow_KORNIA_CHECK_SHAPE(x, ["*", "H", "W"])
        double_shape = tensorflow_shape_frnt_(x)[:-2] + (
            tensorflow_shape_frnt_(x)[-2] * 2,
            tensorflow_shape_frnt_(x)[-1] * 2,
        )
        upscaled = zeros(double_shape, device=x.device, dtype=x.dtype)
        upscaled = tensorflow_set_item_bknd(
            upscaled, (..., slice(None, None, 2), slice(None, None, 2)), x
        )
>       upscaled[..., ::2, 1::2] = tensorflow_set_item_bknd(
            upscaled[..., ::2, 1::2],
            (..., slice(None, -1, None)),
            (upscaled[..., ::2, ::2][..., :-1] + upscaled[..., ::2, 2::2]) / 2,
        )
E       TypeError: Exception encountered when calling tensorflow_ScalePyramid.call().
E       
E       [1m'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment[0m
E       
E       Arguments received by tensorflow_ScalePyramid.call():
E         â€¢ x=tf.Tensor(shape=(1, 1, 256, 256), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:569: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SIFTFeatureScaleSpace
_____________________________________________________________________________ test_GFTTAffNetHardNet[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_GFTTAffNetHardNet(target_framework, mode, backend_compile):
        print("kornia.feature.GFTTAffNetHardNet")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledGFTTAffNetHardNet = ivy.transpile(kornia.feature.GFTTAffNetHardNet, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.GFTTAffNetHardNet(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledGFTTAffNetHardNet(num_features=5000)(transpiled_x)

kornia/test_feature.py:1211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_GFTTAffNetHardNet object at 0x7f99a31ae290>, num_features = 5000, upright = False, device = 'cpu'
config = {'nms_size': 15, 'pyramid_levels': 4, 's_mult': 22.0, 'scale_factor_levels': 1.4142135623730951, ...}

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        config=tensorflow_get_default_detector_config(),
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .scale_space_detector import tensorflow_MultiResolutionDetector
        from .responses import tensorflow_CornerGFTT
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .affine_shape import tensorflow_LAFAffNetShapeEstimator
    
        detector = tensorflow_to_frnt_(
            tensorflow_MultiResolutionDetector(
                tensorflow_CornerGFTT(),
                num_features,
                config,
                ori_module=tensorflow_PassLAF()
                if upright
                else tensorflow_LAFOrienter(19),
>               aff_module=tensorflow_LAFAffNetShapeEstimator(True).eval(),
            ),
            device,
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:934: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
pretrained = True, preserve_orientation = True

    @tensorflow_store_config_info
    def __init__(self, pretrained=False, preserve_orientation=True):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ...torch.nn.modules.activation import tensorflow_Tanh
        from ...torch.nn.modules.pooling import tensorflow_AdaptiveAvgPool2d
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            preserve_orientation=preserve_orientation,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=32,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.25),
            KerasConv2D(
                in_channels=64,
                filters=3,
                kernel_size=8,
                strides=1,
                padding=0,
                use_bias=True,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            tensorflow_Tanh(),
            tensorflow_AdaptiveAvgPool2d(1),
        )
        self.patch_size = 32
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["affnet"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:208: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.GFTTAffNetHardNet
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/ducha-aiki/affnet/raw/master/pretrained/AffNet.pth" to /root/.cache/torch/hub/checkpoints/AffNet.pth

  0%|          | 0.00/332k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332k/332k [00:00<00:00, 16.7MB/s]
____________________________________________________________________________ test_KeyNetAffNetHardNet[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KeyNetAffNetHardNet(target_framework, mode, backend_compile):
        print("kornia.feature.KeyNetAffNetHardNet")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetAffNetHardNet = ivy.transpile(kornia.feature.KeyNetAffNetHardNet, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.KeyNetAffNetHardNet(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledKeyNetAffNetHardNet(num_features=5000)(transpiled_x)

kornia/test_feature.py:1228: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_KeyNetAffNetHardNet object at 0x7f99a3032500>, num_features = 5000, upright = False, device = 'cpu'
scale_laf = 1.0

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        scale_laf=1.0,
    ):
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .orientation import tensorflow_OriNet
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .keynet import tensorflow_KeyNetDetector
        from .affine_shape import tensorflow_LAFAffNetShapeEstimator
    
        ori_module = (
            tensorflow_PassLAF()
            if upright
>           else tensorflow_LAFOrienter(angle_detector=tensorflow_OriNet(True))
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:963: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
pretrained = True, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, pretrained=False, eps=1e-08):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ...torch.nn.modules.activation import tensorflow_Tanh
        from ...torch.nn.modules.pooling import tensorflow_AdaptiveAvgPool2d
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=32,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.25),
            KerasConv2D(
                in_channels=64,
                filters=2,
                kernel_size=8,
                strides=1,
                padding=1,
                use_bias=True,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            tensorflow_Tanh(),
            tensorflow_AdaptiveAvgPool2d(1),
        )
        self.eps = eps
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["orinet"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:1040: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.KeyNetAffNetHardNet
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/ducha-aiki/affnet/raw/master/pretrained/OriNet.pth" to /root/.cache/torch/hub/checkpoints/OriNet.pth

  0%|          | 0.00/316k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 316k/316k [00:00<00:00, 15.9MB/s]
Downloading: "https://github.com/axelBarroso/Key.Net-Pytorch/raw/main/model/weights/keynet_pytorch.pth" to /root/.cache/torch/hub/checkpoints/keynet_pytorch.pth

  0%|          | 0.00/78.0k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78.0k/78.0k [00:00<00:00, 9.23MB/s]
_______________________________________________________________________________ test_KeyNetHardNet[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KeyNetHardNet(target_framework, mode, backend_compile):
        print("kornia.feature.KeyNetHardNet")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetHardNet = ivy.transpile(kornia.feature.KeyNetHardNet, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.KeyNetHardNet(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledKeyNetHardNet(num_features=5000)(transpiled_x)

kornia/test_feature.py:1245: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_KeyNetHardNet object at 0x7f99a2d21b10>, num_features = 5000, upright = False, device = 'cpu'
scale_laf = 1.0

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        scale_laf=1.0,
    ):
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .orientation import tensorflow_OriNet
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .keynet import tensorflow_KeyNetDetector
    
        ori_module = (
            tensorflow_PassLAF()
            if upright
>           else tensorflow_LAFOrienter(angle_detector=tensorflow_OriNet(True))
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:962: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
pretrained = True, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, pretrained=False, eps=1e-08):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ...torch.nn.modules.activation import tensorflow_Tanh
        from ...torch.nn.modules.pooling import tensorflow_AdaptiveAvgPool2d
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=32,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.25),
            KerasConv2D(
                in_channels=64,
                filters=2,
                kernel_size=8,
                strides=1,
                padding=1,
                use_bias=True,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            tensorflow_Tanh(),
            tensorflow_AdaptiveAvgPool2d(1),
        )
        self.eps = eps
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["orinet"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:1040: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.KeyNetHardNet
____________________________________________________________________________ test_LocalFeatureMatcher[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LocalFeatureMatcher(target_framework, mode, backend_compile):
        print("kornia.feature.LocalFeatureMatcher")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledGFTTAffNetHardNet = ivy.transpile(kornia.feature.GFTTAffNetHardNet, source="torch", target=target_framework)
        TranspiledDescriptorMatcher = ivy.transpile(kornia.feature.DescriptorMatcher, source="torch", target=target_framework)
        TranspiledLocalFeatureMatcher = ivy.transpile(kornia.feature.LocalFeatureMatcher, source="torch", target=target_framework)
    
        data = {
            "image0": torch.rand(1, 1, 320, 200),
            "image1": torch.rand(1, 1, 128, 128),
        }
        torch_local_feature = kornia.feature.GFTTAffNetHardNet(10)
        torch_matcher = kornia.feature.DescriptorMatcher('snn', 0.8)
        torch_out = kornia.feature.LocalFeatureMatcher(torch_local_feature, torch_matcher)(data)
    
>       transpiled_local_feature = TranspiledGFTTAffNetHardNet(10)

kornia/test_feature.py:1309: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_GFTTAffNetHardNet object at 0x7f99627389a0>, num_features = 10, upright = False, device = 'cpu'
config = {'nms_size': 15, 'pyramid_levels': 4, 's_mult': 22.0, 'scale_factor_levels': 1.4142135623730951, ...}

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        config=tensorflow_get_default_detector_config(),
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .scale_space_detector import tensorflow_MultiResolutionDetector
        from .responses import tensorflow_CornerGFTT
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .affine_shape import tensorflow_LAFAffNetShapeEstimator
    
        detector = tensorflow_to_frnt_(
            tensorflow_MultiResolutionDetector(
                tensorflow_CornerGFTT(),
                num_features,
                config,
                ori_module=tensorflow_PassLAF()
                if upright
                else tensorflow_LAFOrienter(19),
>               aff_module=tensorflow_LAFAffNetShapeEstimator(True).eval(),
            ),
            device,
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:935: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
pretrained = True, preserve_orientation = True

    @tensorflow_store_config_info
    def __init__(self, pretrained=False, preserve_orientation=True):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ...torch.nn.modules.activation import tensorflow_Tanh
        from ...torch.nn.modules.pooling import tensorflow_AdaptiveAvgPool2d
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            preserve_orientation=preserve_orientation,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=32,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.25),
            KerasConv2D(
                in_channels=64,
                filters=3,
                kernel_size=8,
                strides=1,
                padding=0,
                use_bias=True,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            tensorflow_Tanh(),
            tensorflow_AdaptiveAvgPool2d(1),
        )
        self.patch_size = 32
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["affnet"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:208: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LocalFeatureMatcher
_____________________________________________________________________________ test_LightGlueMatcher[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LightGlueMatcher(target_framework, mode, backend_compile):
        print("kornia.feature.LightGlueMatcher")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLightGlueMatcher = ivy.transpile(kornia.feature.LightGlueMatcher, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(2, 128),
            torch.rand(5, 128),
            torch.rand(1, 2, 2, 3),
            torch.rand(1, 5, 2, 3),
        )
        torch_out = kornia.feature.LightGlueMatcher('disk')(*torch_args)
    
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
>       transpiled_out = TranspiledLightGlueMatcher('disk')(*transpiled_args)

kornia/test_feature.py:1334: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlueMatcher(), feature_name = 'disk', params = {}

    def __init__(self, feature_name="disk", params={}):
        from .lightglue import tensorflow_LightGlue
    
        feature_name_: typing.Any = feature_name.lower()
        super().__init__(feature_name_)
        self.feature_name = feature_name_
        self.params = params
>       self.matcher = tensorflow_LightGlue(self.feature_name, **params)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:1493: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): KerasDense()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), args = ('disk',), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): KerasDense()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), features = 'disk', conf_ = {}
tensorflow_KORNIA_CHECK = <function tensorflow_KORNIA_CHECK at 0x7f999820edd0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f9967c60430>
tensorflow_Identity = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.linear.tensorflow_Identity'>
ModuleList = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.container.tensorflow_ModuleList'>
KerasDense = <class 'Translated_Outputs.tensorflow_outputs.tensorflow__stateful_layers.KerasDense'>
conf = namespace(name='lightglue', input_dim=128, descriptor_dim=256, add_scale_ori=False, add_laf=False, scale_coef=1.0, n_l...=4, flash=True, mp=False, depth_confidence=0.95, width_confidence=0.99, filter_threshold=0.1, weights='disk_lightglue')
k = 'input_dim'

    @tensorflow_store_config_info
    def __init__(self, features="superpoint", **conf_):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...torch.nn.modules.linear import tensorflow_Identity
        from ..core._backend import ModuleList
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            features=features,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.conf = conf = SimpleNamespace(**{**self.default_conf, **conf_})
        if features is not None:
            tensorflow_KORNIA_CHECK(
                features in list(self.features.keys()), "Features keys are wrong"
            )
            for k, v in tensorflow_get_item(self.features, features).items():
                setattr(conf, k, v)
        tensorflow_KORNIA_CHECK(not (self.conf.add_scale_ori and self.conf.add_laf))
        if conf.input_dim != conf.descriptor_dim:
            self.input_proj = KerasDense(
                in_features=conf.input_dim, units=conf.descriptor_dim, use_bias=True
            )
        else:
            self.input_proj = tensorflow_Identity()
        head_dim = conf.descriptor_dim // conf.num_heads
        self.posenc = tensorflow_LearnableFourierPositionalEncoding(
            2 + 2 * conf.add_scale_ori + 4 * conf.add_laf, head_dim, head_dim
        )
        h, n, d = conf.num_heads, conf.n_layers, conf.descriptor_dim
        ag__result_list_0 = []
        for _ in range(n):
>           res = tensorflow_TransformerLayer(d, h, conf.flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:3173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @tensorflow_store_config_info
    def __init__(self, *args, **kwargs):
        self.super___init__(
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.self_attn = tensorflow_SelfBlock(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:2224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), embed_dim = 256, num_heads = 4, flash = True, bias = True

    @tensorflow_store_config_info
    def __init__(self, embed_dim, num_heads, flash=False, bias=True):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.normalization import tensorflow_LayerNorm
        from ...torch.nn.modules.activation import tensorflow_GELU
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            embed_dim,
            num_heads,
            flash=flash,
            bias=bias,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        tensorflow_KORNIA_CHECK(
            self.embed_dim % num_heads == 0,
            "Embed dimension should be dividable by num_heads",
        )
        self.head_dim = self.embed_dim // num_heads
        self.Wqkv = KerasDense(
            in_features=embed_dim, units=3 * embed_dim, use_bias=bias
        )
>       self.inner_attn = tensorflow_Attention(flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:1362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), allow_flash = True

    @tensorflow_store_config_info
    def __init__(self, allow_flash):
        from ...torch.backends.cuda.__init__ import tensorflow_enable_flash_sdp
    
        self.super___init__(
            allow_flash,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if allow_flash and not FLASH_AVAILABLE:
            warnings.warn(
                "FlashAttention is not available. For optimal speed, consider installing torch >= 2.0 or flash-attn.",
                stacklevel=2,
            )
        self.enable_flash = allow_flash and FLASH_AVAILABLE
>       self.has_sdp = hasattr(torch.nn.functional, "scaled_dot_product_attention")
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:911: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LightGlueMatcher
Loaded LightGlue model
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/cvg/LightGlue/releases/download/v0.1_arxiv/disk_lightglue.pth" to /root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth

  0%|          | 0.00/45.4M [00:00<?, ?B/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20.4M/45.4M [00:00<00:00, 212MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45.4M/45.4M [00:00<00:00, 248MB/s]
_________________________________________________________________________________ test_LightGlue[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LightGlue(target_framework, mode, backend_compile):
        print("kornia.feature.LightGlue")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLightGlue = ivy.transpile(kornia.feature.LightGlue, source="torch", target=target_framework)
    
        data = {
            "image0": {
                "keypoints": torch.rand(1, 100, 2),
                "descriptors": torch.rand(1, 100, 256),
                "image_size": torch.tensor([[640, 480]]),
            },
            "image1": {
                "keypoints": torch.rand(1, 120, 2),
                "descriptors": torch.rand(1, 120, 256),
                "image_size": torch.tensor([[640, 480]]),
            }
        }
        torch_out = kornia.feature.LightGlue(features='superpoint')(data)
    
        transpiled_data = _nest_torch_tensor_to_new_framework(data, target_framework)
>       transpiled_out = TranspiledLightGlue(features='superpoint')(transpiled_data)

kornia/test_feature.py:1362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): tensorflow_Identity()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), args = ()
kwargs = {'features': 'superpoint'}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): tensorflow_Identity()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), features = 'superpoint', conf_ = {}
tensorflow_KORNIA_CHECK = <function tensorflow_KORNIA_CHECK at 0x7f999820edd0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f9967c60430>
tensorflow_Identity = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.linear.tensorflow_Identity'>
ModuleList = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.container.tensorflow_ModuleList'>
KerasDense = <class 'Translated_Outputs.tensorflow_outputs.tensorflow__stateful_layers.KerasDense'>
conf = namespace(name='lightglue', input_dim=256, descriptor_dim=256, add_scale_ori=False, add_laf=False, scale_coef=1.0, n_l...ash=True, mp=False, depth_confidence=0.95, width_confidence=0.99, filter_threshold=0.1, weights='superpoint_lightglue')
k = 'input_dim'

    @tensorflow_store_config_info
    def __init__(self, features="superpoint", **conf_):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...torch.nn.modules.linear import tensorflow_Identity
        from ..core._backend import ModuleList
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            features=features,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.conf = conf = SimpleNamespace(**{**self.default_conf, **conf_})
        if features is not None:
            tensorflow_KORNIA_CHECK(
                features in list(self.features.keys()), "Features keys are wrong"
            )
            for k, v in tensorflow_get_item(self.features, features).items():
                setattr(conf, k, v)
        tensorflow_KORNIA_CHECK(not (self.conf.add_scale_ori and self.conf.add_laf))
        if conf.input_dim != conf.descriptor_dim:
            self.input_proj = KerasDense(
                in_features=conf.input_dim, units=conf.descriptor_dim, use_bias=True
            )
        else:
            self.input_proj = tensorflow_Identity()
        head_dim = conf.descriptor_dim // conf.num_heads
        self.posenc = tensorflow_LearnableFourierPositionalEncoding(
            2 + 2 * conf.add_scale_ori + 4 * conf.add_laf, head_dim, head_dim
        )
        h, n, d = conf.num_heads, conf.n_layers, conf.descriptor_dim
        ag__result_list_0 = []
        for _ in range(n):
>           res = tensorflow_TransformerLayer(d, h, conf.flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:3173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @tensorflow_store_config_info
    def __init__(self, *args, **kwargs):
        self.super___init__(
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.self_attn = tensorflow_SelfBlock(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:2224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), embed_dim = 256, num_heads = 4, flash = True, bias = True

    @tensorflow_store_config_info
    def __init__(self, embed_dim, num_heads, flash=False, bias=True):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.normalization import tensorflow_LayerNorm
        from ...torch.nn.modules.activation import tensorflow_GELU
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            embed_dim,
            num_heads,
            flash=flash,
            bias=bias,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        tensorflow_KORNIA_CHECK(
            self.embed_dim % num_heads == 0,
            "Embed dimension should be dividable by num_heads",
        )
        self.head_dim = self.embed_dim // num_heads
        self.Wqkv = KerasDense(
            in_features=embed_dim, units=3 * embed_dim, use_bias=bias
        )
>       self.inner_attn = tensorflow_Attention(flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:1362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), allow_flash = True

    @tensorflow_store_config_info
    def __init__(self, allow_flash):
        from ...torch.backends.cuda.__init__ import tensorflow_enable_flash_sdp
    
        self.super___init__(
            allow_flash,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if allow_flash and not FLASH_AVAILABLE:
            warnings.warn(
                "FlashAttention is not available. For optimal speed, consider installing torch >= 2.0 or flash-attn.",
                stacklevel=2,
            )
        self.enable_flash = allow_flash and FLASH_AVAILABLE
>       self.has_sdp = hasattr(torch.nn.functional, "scaled_dot_product_attention")
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:911: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LightGlue
Loaded LightGlue model
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/cvg/LightGlue/releases/download/v0.1_arxiv/superpoint_lightglue.pth" to /root/.cache/torch/hub/checkpoints/superpoint_lightglue_v0-1_arxiv-pth

  0%|          | 0.00/45.3M [00:00<?, ?B/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22.9M/45.3M [00:00<00:00, 239MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45.3M/45.3M [00:00<00:00, 258MB/s]
___________________________________________________________________________________ test_LoFTR[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LoFTR(target_framework, mode, backend_compile):
        print("kornia.feature.LoFTR")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledLoFTR = ivy.transpile(kornia.feature.LoFTR, source="torch", target=target_framework)

kornia/test_feature.py:1373: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.feature.loftr.loftr.LoFTR'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, line 77)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LoFTR
_________________________________________________________________________ test_PatchAffineShapeEstimator[tensorflow-s2s-False] _________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_PatchAffineShapeEstimator(target_framework, mode, backend_compile):
        print("kornia.feature.PatchAffineShapeEstimator")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledPatchAffineShapeEstimator = ivy.transpile(kornia.feature.PatchAffineShapeEstimator, source="torch", target=target_framework)
    
        patch = torch.rand(1, 1, 19, 19)
        torch_out = kornia.feature.PatchAffineShapeEstimator()(patch)
    
        transpiled_patch = _nest_torch_tensor_to_new_framework(patch, target_framework)
>       transpiled_out = TranspiledPatchAffineShapeEstimator()(transpiled_patch)

kornia/test_feature.py:1415: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_PatchAffineShapeEstimator' object has no attribute 'eps'") raised in repr()] tensorflow_PatchAffineShapeEstimator object at 0x7f9961e12d70>, patch_size = 19
eps = 1e-10

    def __init__(self, patch_size=19, eps=1e-10):
        from ..filters.sobel import tensorflow_sobel
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size: typing.Any = patch_size
>       self.gradient: typing.Any = tensorflow_sobel.SpatialGradient("sobel", 1)
E       AttributeError: 'function' object has no attribute 'SpatialGradient'

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:53: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.PatchAffineShapeEstimator
__________________________________________________________________________ test_LAFAffineShapeEstimator[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LAFAffineShapeEstimator(target_framework, mode, backend_compile):
        print("kornia.feature.LAFAffineShapeEstimator")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLAFAffineShapeEstimator = ivy.transpile(kornia.feature.LAFAffineShapeEstimator, source="torch", target=target_framework)
    
        laf = torch.rand(1, 2, 2, 3)
        img = torch.rand(1, 1, 32, 32)
        torch_out = kornia.feature.LAFAffineShapeEstimator()(laf, img)
    
        transpiled_laf = _nest_torch_tensor_to_new_framework(laf, target_framework)
        transpiled_img = _nest_torch_tensor_to_new_framework(img, target_framework)
>       transpiled_out = TranspiledLAFAffineShapeEstimator()(transpiled_laf, transpiled_img)

kornia/test_feature.py:1434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFAffineShapeEstimator' object has no attribute 'affine_shape_detector'") raised in repr()] tensorflow_LAFAffineShapeEstimator object at 0x7f99a1cf8490>
patch_size = 32, affine_shape_detector = None, preserve_orientation = True

    def __init__(
        self, patch_size=32, affine_shape_detector=None, preserve_orientation=True
    ):
        self.super___init__(
            patch_size=patch_size,
            affine_shape_detector=affine_shape_detector,
            preserve_orientation=preserve_orientation,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.affine_shape_detector = (
            affine_shape_detector
>           or tensorflow_PatchAffineShapeEstimator(self.patch_size)
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:484: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_PatchAffineShapeEstimator' object has no attribute 'eps'") raised in repr()] tensorflow_PatchAffineShapeEstimator object at 0x7f9967aa64d0>, patch_size = 32
eps = 1e-10

    def __init__(self, patch_size=19, eps=1e-10):
        from ..filters.sobel import tensorflow_sobel
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size: typing.Any = patch_size
>       self.gradient: typing.Any = tensorflow_sobel.SpatialGradient("sobel", 1)
E       AttributeError: 'function' object has no attribute 'SpatialGradient'

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:53: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LAFAffineShapeEstimator
________________________________________________________________________________ test_LAFOrienter[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LAFOrienter(target_framework, mode, backend_compile):
        print("kornia.feature.LAFOrienter")
    
        if backend_compile:
            pytest.skip()
    
        import os
        flag = os.environ.get("APPLY_TRANSPOSE_OPTIMIZATION")
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = "false"
    
        TranspiledLAFOrienter = ivy.transpile(kornia.feature.LAFOrienter, source="torch", target=target_framework)
    
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = flag
    
        laf = torch.rand(1, 2, 2, 3)
        img = torch.rand(1, 1, 32, 32)
        torch_out = kornia.feature.LAFOrienter()(laf, img)
    
        transpiled_laf = _nest_torch_tensor_to_new_framework(laf, target_framework)
        transpiled_img = _nest_torch_tensor_to_new_framework(img, target_framework)
>       transpiled_out = TranspiledLAFOrienter()(transpiled_laf, transpiled_img)

kornia/test_feature.py:1459: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFOrienter(patch_size=32, angle_detector=tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08))
args = (<tf.Tensor: shape=(1, 2, 2, 3), dtype=float32, numpy=
array([[[[0.03711855, 0.5869228 , 0.85112315],
         [0.5842...,
         [0.11222917, 0.0644232 , 0.27760035, ..., 0.75578547,
          0.95210606, 0.31589442]]]], dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f99620a17d0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LAFOrienter(patch_size=32, angle_detector=tensorflow_PatchDominantGradientOrientation(patch_size=32, num_a...,
         [0.11222917, 0.0644232 , 0.27760035, ..., 0.75578547,
          0.95210606, 0.31589442]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFOrienter(patch_size=32, angle_detector=tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08)), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 2, 2, 3), dtype=float32, numpy=
array([[[[0.03711855, 0.5869228 , 0.85112315],
         [0.5842...,
         [0.11222917, 0.0644232 , 0.27760035, ..., 0.75578547,
          0.95210606, 0.31589442]]]], dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LAFOrienter(patch_size=32, angle_detector=tensorflow_PatchDominantGradientOrientation(patch_size=32, num_a...,
         [0.11222917, 0.0644232 , 0.27760035, ..., 0.75578547,
          0.95210606, 0.31589442]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFOrienter(patch_size=32, angle_detector=tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08)), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 2, 2, 3), dtype=float32, numpy=
array([[[[0.03711855, 0.5869228 , 0.85112315],
         [0.5842...,
         [0.11222917, 0.0644232 , 0.27760035, ..., 0.75578547,
          0.95210606, 0.31589442]]]], dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 2, 2, 3), dtype=float32, numpy=
array([[[[0.03711855, 0.5869228 , 0.85112315],
         [0.58421...18]],

        [[0.58262247, 0.47598112, 0.7163451 ],
         [0.4707259 , 0.56775856, 0.13895965]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (laf, img)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LAFOrienter(patch_size=32, angle_detector=tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08)),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.4402929 , 0.3536573 , 0.59489644, ..., 0.9...8]],

        [[0.58262247, 0.47598112, 0.7163451 ],
         [0.4707259 , 0.56775856, 0.13895965]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFOrienter(patch_size=32, angle_detector=tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08))
laf = <tf.Tensor: shape=(1, 2, 2, 3), dtype=float32, numpy=
array([[[[0.03711855, 0.5869228 , 0.85112315],
         [0.58421...18]],

        [[0.58262247, 0.47598112, 0.7163451 ],
         [0.4707259 , 0.56775856, 0.13895965]]]], dtype=float32)>
img = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.4402929 , 0.3536573 , 0.59489644, ..., 0.98044384,...],
         [0.11222917, 0.0644232 , 0.27760035, ..., 0.75578547,
          0.95210606, 0.31589442]]]], dtype=float32)>

    def call(self, laf, img):
        from ..core.check import tensorflow_KORNIA_CHECK_LAF
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from .laf import tensorflow_extract_patches_from_pyramid
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_as_frnt_
        from .laf import tensorflow_get_laf_orientation
        from .laf import tensorflow_set_laf_orientation
        from ..geometry.conversions import tensorflow_rad2deg
    
        tensorflow_KORNIA_CHECK_LAF(laf)
        tensorflow_KORNIA_CHECK_SHAPE(img, ["B", "C", "H", "W"])
        if tensorflow_size_frnt_(laf, 0) != tensorflow_size_frnt_(img, 0):
            raise ValueError(
                f"Batch size of laf and img should be the same. Got {tensorflow_size_frnt_(img, 0)}, {tensorflow_size_frnt_(laf, 0)}"
            )
        B, N = tensorflow_shape_frnt_(laf)[:2][0], tensorflow_shape_frnt_(laf)[:2][1]
        patches: typing.Any = tensorflow_view_frnt_(
            tensorflow_extract_patches_from_pyramid(img, laf, self.patch_size),
            -1,
            1,
            self.patch_size,
            self.patch_size,
        )
        angles_radians: typing.Any = tensorflow_view_frnt_(
>           self.angle_detector(patches), B, N
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:1478: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08)
args = (<tf.Tensor: shape=(2, 1, 32, 32), dtype=float32, numpy=
array([[[[0.4402929, 0.4402929, 0.       , ..., 0.       , 0....    ],
         [0.       , 0.       , 0.       , ..., 0.       , 0.       ,
          0.       ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55dce548f130, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 32, 32), dtype=float32, numpy=
array([[[[0.4402929, 0.4402929, 0.       , ..., 0.       , 0....    ],
         [0.       , 0.       , 0.       , ..., 0.       , 0.       ,
          0.       ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 32, 32), dtype=float32, numpy=
array([[[[0.4402929, 0.4402929, 0.       , ..., 0.       , 0....    ],
         [0.       , 0.       , 0.       , ..., 0.       , 0.       ,
          0.       ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (patch)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08)
patch = <tf.Tensor: shape=(2, 1, 32, 32), dtype=float32, numpy=
array([[[[0.4402929, 0.4402929, 0.       , ..., 0.       , 0. ...      ],
         [0.       , 0.       , 0.       , ..., 0.       , 0.       ,
          0.       ]]]], dtype=float32)>

    def call(self, patch):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_atan2_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_floor_frnt,
        )
        from ...ivy.functional.frontends.torch.nn.functional.pooling_functions import (
            tensorflow_adaptive_avg_pool2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ..constants import pi
    
        tensorflow_KORNIA_CHECK_SHAPE(patch, ["B", "1", "H", "W"])
        _, CH, W, H = tensorflow_size_frnt_(patch)
        if W != self.patch_size or H != self.patch_size or CH != 1:
            raise TypeError(
                f"input shape should be must be [Bx1x{self.patch_size}x{self.patch_size}]. Got {tensorflow_size_frnt_(patch)}"
            )
        self.weighting = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.weighting, patch.dtype), patch.device
        )
        self.angular_smooth = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.angular_smooth, patch.dtype), patch.device
        )
        grads: typing.Any = self.gradient(patch)
        gx: typing.Any = grads[:, :, 0]
        gy: typing.Any = grads[:, :, 1]
        mag: typing.Any = (
            tensorflow_sqrt_frnt(gx * gx + gy * gy + self.eps) * self.weighting
        )
        ori: typing.Any = tensorflow_atan2_frnt(gy, gx + self.eps) + 2.0 * pi
        o_big = float(self.num_ang_bins) * (ori + 1.0 * pi) / (2.0 * pi)
        bo0_big = tensorflow_floor_frnt(o_big)
        wo1_big = o_big - bo0_big
        bo0_big = bo0_big % self.num_ang_bins
        bo1_big = (bo0_big + 1) % self.num_ang_bins
        wo0_big = (1.0 - wo1_big) * mag
        wo1_big = wo1_big * mag
        ang_bins_list = []
        for i in range(0, self.num_ang_bins):
            ang_bins_i = tensorflow_adaptive_avg_pool2d_frnt(
                tensorflow_to_frnt_(bo0_big == i, patch.dtype) * wo0_big
                + tensorflow_to_frnt_(bo1_big == i, patch.dtype) * wo1_big,
                (1, 1),
            )
            ang_bins_list.append(ang_bins_i)
        ang_bins = tensorflow_view_frnt_(
            tensorflow_cat_frnt(ang_bins_list, 1), -1, 1, self.num_ang_bins
        )
>       ang_bins = tensorflow_view_frnt_(
            self.angular_smooth(ang_bins), -1, self.num_ang_bins
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:502: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 5, 40), dtype=float32, numpy=
array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.060215...0000000e+00,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],
      dtype=float32)>, -1, 36)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99642c96c0>
array_like = <tf.Tensor: shape=(2, 5, 40), dtype=float32, numpy=
array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.0602153...e+00, 0.00000000e+00,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],
      dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(2, 5, 40), dtype=float32, numpy=
array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.0602153...e+00, 0.00000000e+00,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],
      dtype=float32)>
size = None, args = (-1, 36), tensorflow_exists_bknd = <function tensorflow_exists_bknd at 0x7f99642c9750>, tensorflow_reshape_frnt = <function tensorflow_reshape_frnt at 0x7f99646ecdc0>

    @tensorflow_handle_methods
    def tensorflow_view_frnt_(tensor, *args, size=None):
        from ...ivy.general import tensorflow_exists_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_reshape_frnt
    
        """Reshape Tensor.
    
        possible arguments are either:
            - size
            - tuple of ints
            - list of ints
            - torch.Size object
            - ints
    
        Parameters
        ----------
        args:int arguments
        size: optional shape
    
        Returns reshaped tensor
        -------
        """
        if tensorflow_exists_bknd(size) and not args:
            shape_tup = size
        elif args and not tensorflow_exists_bknd(size):
            if (
                isinstance(args[0], (tuple, list, tuple, tf.TensorShape))
                or type(args[0]).__name__ == "Size"
            ) and len(args) == 1:
                shape_tup = args[0]
            else:
                shape_tup = args
        else:
            raise ValueError(
                "View only accepts as argument ints, tuple or list of ints or the keyword argument size."
            )
>       return tensorflow_reshape_frnt(tensor, shape_tup)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:282: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 5, 40), dtype=float32, numpy=
array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.060215...00000e+00,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],
      dtype=float32)>, (-1, 36))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99642c96c0>
array_like = <tf.Tensor: shape=(2, 5, 40), dtype=float32, numpy=
array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.0602153...e+00, 0.00000000e+00,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],
      dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 5, 40), dtype=float32, numpy=
array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.0602153...e+00, 0.00000000e+00,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],
      dtype=float32)>
shape = (-1, 36)

    @tensorflow_handle_methods
    def tensorflow_reshape_frnt(input, shape):
        from ...backends.tensorflow.manipulation import tensorflow_reshape
    
>       return tensorflow_reshape(input, shape)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 5, 40), dtype=float32, numpy=
array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.060215...00000e+00,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],
      dtype=float32)>, (-1, 36))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99642c96c0>
array_like = <tf.Tensor: shape=(2, 5, 40), dtype=float32, numpy=
array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.0602153...e+00, 0.00000000e+00,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],
      dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(2, 5, 40), dtype=float32, numpy=
array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.060215...00000e+00,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],
      dtype=float32)>, (-1, 36)]
kwargs = {}, tensorflow_get_item = <function tensorflow_get_item at 0x7f99628b0f70>, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99642c96c0>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f99642c9a20>, tensorflow_asarray = <function tensorflow_asarray at 0x7f996454b010>, num_args = 2
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'shape', 'copy', 'order', 'allowzero', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...s 'bool'>, typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 1

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 5, 40), dtype=float32, numpy=
array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.0602153...e+00, 0.00000000e+00,
         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]],
      dtype=float32)>
shape = (-1, 36)

    @tensorflow_handle_methods
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_reshape(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Union[tf.TensorShape, Sequence[int]],
        *,
        copy: Optional[bool] = None,
        order: str = "C",
        allowzero: bool = True,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ....utils.assertions import tensorflow_check_elem_in_list
    
        tensorflow_check_elem_in_list(order, ["C", "F"])
        if not allowzero:
            shape = [
                (new_s if con else old_s)
                for new_s, con, old_s in zip(
                    shape, tensorflow.constant(shape) != 0, x.shape
                )
            ]
        if order == "F":
            return tensorflow__reshape_fortran_tf(x, shape)
>       return tensorflow.reshape(x, shape)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PatchDominantGradientOrientation.call().
E       
E       [1m{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 400 values, but the requested shape requires a multiple of 36 [Op:Reshape][0m
E       
E       Arguments received by tensorflow_PatchDominantGradientOrientation.call():
E         â€¢ patch=tf.Tensor(shape=(2, 1, 32, 32), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/manipulation.py:201: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LAFOrienter
_____________________________________________________________________ test_PatchDominantGradientOrientation[tensorflow-s2s-False] ______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_PatchDominantGradientOrientation(target_framework, mode, backend_compile):
        print("kornia.feature.PatchDominantGradientOrientation")
    
        if backend_compile:
            pytest.skip()
    
        import os
        flag = os.environ.get("APPLY_TRANSPOSE_OPTIMIZATION")
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = "false"
    
        TranspiledPatchDominantGradientOrientation = ivy.transpile(kornia.feature.PatchDominantGradientOrientation, source="torch", target=target_framework)
    
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = flag
    
        patch = torch.rand(10, 1, 32, 32)
        torch_out = kornia.feature.PatchDominantGradientOrientation()(patch)
    
        transpiled_patch = _nest_torch_tensor_to_new_framework(patch, target_framework)
>       transpiled_out = TranspiledPatchDominantGradientOrientation()(transpiled_patch)

kornia/test_feature.py:1482: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08)
args = (<tf.Tensor: shape=(10, 1, 32, 32), dtype=float32, numpy=
array([[[[6.04547381e-01, 1.05123162e-01, 9.93052602e-01, .....3720281e-01, 6.32161200e-01, ...,
          7.49025941e-02, 4.83747125e-02, 1.27079487e-01]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55dcee3e5c00, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08), <tf.Tensor: shape=(10, 1, 32,...93720281e-01, 6.32161200e-01, ...,
          7.49025941e-02, 4.83747125e-02, 1.27079487e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 1, 32, 32), dtype=float32, numpy=
array([[[[6.04547381e-01, 1.05123162e-01, 9.93052602e-01, .....3720281e-01, 6.32161200e-01, ...,
          7.49025941e-02, 4.83747125e-02, 1.27079487e-01]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08), <tf.Tensor: shape=(10, 1, 32,...93720281e-01, 6.32161200e-01, ...,
          7.49025941e-02, 4.83747125e-02, 1.27079487e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 1, 32, 32), dtype=float32, numpy=
array([[[[6.04547381e-01, 1.05123162e-01, 9.93052602e-01, .....3720281e-01, 6.32161200e-01, ...,
          7.49025941e-02, 4.83747125e-02, 1.27079487e-01]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(10, 1, 32, 32), dtype=float32, numpy=
array([[[[6.04547381e-01, 1.05123162e-01, 9.93052602e-01, .......93720281e-01, 6.32161200e-01, ...,
          7.49025941e-02, 4.83747125e-02, 1.27079487e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (patch)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08),)
kwargs = {'patch': <tf.Tensor: shape=(10, 1, 32, 32), dtype=float32, numpy=
array([[[[6.04547381e-01, 1.05123162e-01, 9.9305260...93720281e-01, 6.32161200e-01, ...,
          7.49025941e-02, 4.83747125e-02, 1.27079487e-01]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08)
patch = <tf.Tensor: shape=(10, 1, 32, 32), dtype=float32, numpy=
array([[[[6.04547381e-01, 1.05123162e-01, 9.93052602e-01, .......93720281e-01, 6.32161200e-01, ...,
          7.49025941e-02, 4.83747125e-02, 1.27079487e-01]]]],
      dtype=float32)>

    def call(self, patch):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_atan2_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_floor_frnt,
        )
        from ...ivy.functional.frontends.torch.nn.functional.pooling_functions import (
            tensorflow_adaptive_avg_pool2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ..constants import pi
    
        tensorflow_KORNIA_CHECK_SHAPE(patch, ["B", "1", "H", "W"])
        _, CH, W, H = tensorflow_size_frnt_(patch)
        if W != self.patch_size or H != self.patch_size or CH != 1:
            raise TypeError(
                f"input shape should be must be [Bx1x{self.patch_size}x{self.patch_size}]. Got {tensorflow_size_frnt_(patch)}"
            )
        self.weighting = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.weighting, patch.dtype), patch.device
        )
        self.angular_smooth = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.angular_smooth, patch.dtype), patch.device
        )
        grads: typing.Any = self.gradient(patch)
        gx: typing.Any = grads[:, :, 0]
        gy: typing.Any = grads[:, :, 1]
        mag: typing.Any = (
            tensorflow_sqrt_frnt(gx * gx + gy * gy + self.eps) * self.weighting
        )
        ori: typing.Any = tensorflow_atan2_frnt(gy, gx + self.eps) + 2.0 * pi
        o_big = float(self.num_ang_bins) * (ori + 1.0 * pi) / (2.0 * pi)
        bo0_big = tensorflow_floor_frnt(o_big)
        wo1_big = o_big - bo0_big
        bo0_big = bo0_big % self.num_ang_bins
        bo1_big = (bo0_big + 1) % self.num_ang_bins
        wo0_big = (1.0 - wo1_big) * mag
        wo1_big = wo1_big * mag
        ang_bins_list = []
        for i in range(0, self.num_ang_bins):
            ang_bins_i = tensorflow_adaptive_avg_pool2d_frnt(
                tensorflow_to_frnt_(bo0_big == i, patch.dtype) * wo0_big
                + tensorflow_to_frnt_(bo1_big == i, patch.dtype) * wo1_big,
                (1, 1),
            )
            ang_bins_list.append(ang_bins_i)
        ang_bins = tensorflow_view_frnt_(
            tensorflow_cat_frnt(ang_bins_list, 1), -1, 1, self.num_ang_bins
        )
>       ang_bins = tensorflow_view_frnt_(
            self.angular_smooth(ang_bins), -1, self.num_ang_bins
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:502: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(10, 5, 40), dtype=float32, numpy=
array([[[5.0973051e-07, 3.8859594e-07, 2.7668318e-07, ...,
     ...7, 5.2576252e-07, 3.4516560e-07, ...,
         5.2576252e-07, 3.4516560e-07, 4.0755089e-07]]], dtype=float32)>, -1, 36)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99642c96c0>
array_like = <tf.Tensor: shape=(10, 5, 40), dtype=float32, numpy=
array([[[5.0973051e-07, 3.8859594e-07, 2.7668318e-07, ...,
      ...970095e-07, 5.2576252e-07, 3.4516560e-07, ...,
         5.2576252e-07, 3.4516560e-07, 4.0755089e-07]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(10, 5, 40), dtype=float32, numpy=
array([[[5.0973051e-07, 3.8859594e-07, 2.7668318e-07, ...,
      ...970095e-07, 5.2576252e-07, 3.4516560e-07, ...,
         5.2576252e-07, 3.4516560e-07, 4.0755089e-07]]], dtype=float32)>
size = None, args = (-1, 36), tensorflow_exists_bknd = <function tensorflow_exists_bknd at 0x7f99642c9750>, tensorflow_reshape_frnt = <function tensorflow_reshape_frnt at 0x7f99646ecdc0>

    @tensorflow_handle_methods
    def tensorflow_view_frnt_(tensor, *args, size=None):
        from ...ivy.general import tensorflow_exists_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_reshape_frnt
    
        """Reshape Tensor.
    
        possible arguments are either:
            - size
            - tuple of ints
            - list of ints
            - torch.Size object
            - ints
    
        Parameters
        ----------
        args:int arguments
        size: optional shape
    
        Returns reshaped tensor
        -------
        """
        if tensorflow_exists_bknd(size) and not args:
            shape_tup = size
        elif args and not tensorflow_exists_bknd(size):
            if (
                isinstance(args[0], (tuple, list, tuple, tf.TensorShape))
                or type(args[0]).__name__ == "Size"
            ) and len(args) == 1:
                shape_tup = args[0]
            else:
                shape_tup = args
        else:
            raise ValueError(
                "View only accepts as argument ints, tuple or list of ints or the keyword argument size."
            )
>       return tensorflow_reshape_frnt(tensor, shape_tup)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:282: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(10, 5, 40), dtype=float32, numpy=
array([[[5.0973051e-07, 3.8859594e-07, 2.7668318e-07, ...,
     ... 5.2576252e-07, 3.4516560e-07, ...,
         5.2576252e-07, 3.4516560e-07, 4.0755089e-07]]], dtype=float32)>, (-1, 36))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99642c96c0>
array_like = <tf.Tensor: shape=(10, 5, 40), dtype=float32, numpy=
array([[[5.0973051e-07, 3.8859594e-07, 2.7668318e-07, ...,
      ...970095e-07, 5.2576252e-07, 3.4516560e-07, ...,
         5.2576252e-07, 3.4516560e-07, 4.0755089e-07]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(10, 5, 40), dtype=float32, numpy=
array([[[5.0973051e-07, 3.8859594e-07, 2.7668318e-07, ...,
      ...970095e-07, 5.2576252e-07, 3.4516560e-07, ...,
         5.2576252e-07, 3.4516560e-07, 4.0755089e-07]]], dtype=float32)>
shape = (-1, 36)

    @tensorflow_handle_methods
    def tensorflow_reshape_frnt(input, shape):
        from ...backends.tensorflow.manipulation import tensorflow_reshape
    
>       return tensorflow_reshape(input, shape)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(10, 5, 40), dtype=float32, numpy=
array([[[5.0973051e-07, 3.8859594e-07, 2.7668318e-07, ...,
     ... 5.2576252e-07, 3.4516560e-07, ...,
         5.2576252e-07, 3.4516560e-07, 4.0755089e-07]]], dtype=float32)>, (-1, 36))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99642c96c0>
array_like = <tf.Tensor: shape=(10, 5, 40), dtype=float32, numpy=
array([[[5.0973051e-07, 3.8859594e-07, 2.7668318e-07, ...,
      ...970095e-07, 5.2576252e-07, 3.4516560e-07, ...,
         5.2576252e-07, 3.4516560e-07, 4.0755089e-07]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(10, 5, 40), dtype=float32, numpy=
array([[[5.0973051e-07, 3.8859594e-07, 2.7668318e-07, ...,
     ... 5.2576252e-07, 3.4516560e-07, ...,
         5.2576252e-07, 3.4516560e-07, 4.0755089e-07]]], dtype=float32)>, (-1, 36)]
kwargs = {}, tensorflow_get_item = <function tensorflow_get_item at 0x7f99628b0f70>, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f99642c96c0>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f99642c9a20>, tensorflow_asarray = <function tensorflow_asarray at 0x7f996454b010>, num_args = 2
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'shape', 'copy', 'order', 'allowzero', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...s 'bool'>, typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 1

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(10, 5, 40), dtype=float32, numpy=
array([[[5.0973051e-07, 3.8859594e-07, 2.7668318e-07, ...,
      ...970095e-07, 5.2576252e-07, 3.4516560e-07, ...,
         5.2576252e-07, 3.4516560e-07, 4.0755089e-07]]], dtype=float32)>
shape = (-1, 36)

    @tensorflow_handle_methods
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_reshape(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Union[tf.TensorShape, Sequence[int]],
        *,
        copy: Optional[bool] = None,
        order: str = "C",
        allowzero: bool = True,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ....utils.assertions import tensorflow_check_elem_in_list
    
        tensorflow_check_elem_in_list(order, ["C", "F"])
        if not allowzero:
            shape = [
                (new_s if con else old_s)
                for new_s, con, old_s in zip(
                    shape, tensorflow.constant(shape) != 0, x.shape
                )
            ]
        if order == "F":
            return tensorflow__reshape_fortran_tf(x, shape)
>       return tensorflow.reshape(x, shape)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PatchDominantGradientOrientation.call().
E       
E       [1m{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 2000 values, but the requested shape requires a multiple of 36 [Op:Reshape][0m
E       
E       Arguments received by tensorflow_PatchDominantGradientOrientation.call():
E         â€¢ patch=tf.Tensor(shape=(10, 1, 32, 32), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/manipulation.py:201: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.PatchDominantGradientOrientation
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_feature.py::test_get_laf_descriptors[tensorflow-s2s-False] - TypeError: tensor([[[[0.8998, 0.6329],
FAILED kornia/test_feature.py::test_extract_patches_from_pyramid[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__DepthwiseConv2dNati...
FAILED kornia/test_feature.py::test_laf_is_inside_image[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of bool is not in the list of allowe...
FAILED kornia/test_feature.py::test_DenseSIFTDescriptor[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_DenseSIFTDescriptor.call().
FAILED kornia/test_feature.py::test_SIFTDescriptor[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_SIFTDescriptor.call().
FAILED kornia/test_feature.py::test_MKDDescriptor[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_VonMisesKernel.call().
FAILED kornia/test_feature.py::test_HardNet8[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_HardNet8.call().
FAILED kornia/test_feature.py::test_HyNet[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_mix...
FAILED kornia/test_feature.py::test_SOSNet[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_mi...
FAILED kornia/test_feature.py::test_MultiResolutionDetector[tensorflow-s2s-False] - ValueError: operands could not be broadcast together with shapes (1,3,2,3) (1,2,2,3)
FAILED kornia/test_feature.py::test_ScaleSpaceDetector[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_ScaleSpace...
FAILED kornia/test_feature.py::test_KeyNetDetector[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_feature.py::test_LAFDescriptor[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_SOLD2[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SOLD2.call().
FAILED kornia/test_feature.py::test_LocalFeature[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_SOLD2_detector[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SOLD2_detector...
FAILED kornia/test_feature.py::test_DeDoDe[tensorflow-s2s-False] - KeyError: 'num_features'
FAILED kornia/test_feature.py::test_DISK[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PReLU.call().
FAILED kornia/test_feature.py::test_SIFTFeature[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PatchDominantGrad...
FAILED kornia/test_feature.py::test_SIFTFeatureScaleSpace[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_ScalePyramid.call().
FAILED kornia/test_feature.py::test_GFTTAffNetHardNet[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_KeyNetAffNetHardNet[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_KeyNetHardNet[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_LocalFeatureMatcher[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_LightGlueMatcher[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_LightGlue[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_LoFTR[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, line 77)
FAILED kornia/test_feature.py::test_PatchAffineShapeEstimator[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'SpatialGradient'
FAILED kornia/test_feature.py::test_LAFAffineShapeEstimator[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'SpatialGradient'
FAILED kornia/test_feature.py::test_LAFOrienter[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PatchDominantGrad...
FAILED kornia/test_feature.py::test_PatchDominantGradientOrientation[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorf...
============================================================================== 31 failed, 44 passed in 8352.12s (2:19:12) ==============================================================================

