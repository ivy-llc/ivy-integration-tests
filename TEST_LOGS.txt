========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 13 items

kornia/test_metrics.py ...F.........                                                                                                                                                             [100%]

=============================================================================================== FAILURES ===============================================================================================
__________________________________________________________________________ test_mean_average_precision[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_mean_average_precision(target_framework, mode, backend_compile):
        trace_args = (
            [torch.tensor([[100, 50, 150, 100.]])],
            [torch.tensor([1.])],
            [torch.tensor([0.7])],
            [torch.tensor([[100, 50, 150, 100.]])],
            [torch.tensor([1.])],
            2,
        )
        trace_kwargs = {}
        test_args = (
            [torch.tensor([[50, 25, 75, 50], [100, 50, 150, 100.]])],
            [torch.tensor([1, 2.])],
            [torch.tensor([0.6, 0.8])],
            [torch.tensor([[50, 25, 75, 50], [100, 50, 150, 100.]])],
            [torch.tensor([1, 2.])],
            3,
        )
        kornia.metrics.mean_average_precision(*trace_args)
        kornia.metrics.mean_average_precision(*test_args)
        test_kwargs = {}
    
        # NOTE: this test fails due to the use of dynamic control flow; skipping
>       _test_function(
            kornia.metrics.mean_average_precision,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
            skip=True,
        )

kornia/test_metrics.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function mean_average_precision at 0x7f8834030940>
trace_args = ([<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shap....,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>], 2)
trace_kwargs = {}
test_args = ([tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], [tensor([0.6000, 0.8000])], [tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], 3)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = True, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function mean_average_precision at 0x7f8834030940>
trace_args = ([<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shap....,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>], 2)
trace_kwargs = {}
test_args = ([tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], [tensor([0.6000, 0.8000])], [tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], 3)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pred_boxes = [<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>]
pred_labels = [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>], pred_scores = [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.7], dtype=float32)>]
gt_boxes = [<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>], gt_labels = [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]
n_classes = 2, threshold = 0.5

    def tensorflow_mean_average_precision(
        pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels, n_classes, threshold=0.5
    ):
        from ..core._backend import concatenate
        from ..core._backend import tensor
        from ..core._backend import zeros
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.comparison_ops import tensorflow_sort_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .mean_iou import tensorflow_mean_iou
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_max_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_cumsum_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_tolist_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_arange_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
    
        if (
            not len(pred_boxes)
            == len(pred_labels)
            == len(pred_scores)
            == len(gt_boxes)
            == len(gt_labels)
        ):
            raise AssertionError
        gt_images = []
        for i, labels in enumerate(gt_labels):
            gt_images.extend([i] * tensorflow_size_frnt_(labels, 0))
        _gt_boxes = concatenate(gt_boxes, 0)
        _gt_labels = concatenate(gt_labels, 0)
        _gt_images = tensor(gt_images, device=_gt_boxes.device, dtype=tf.int64)
        if (
            not tensorflow_size_frnt_(_gt_images, 0)
            == tensorflow_size_frnt_(_gt_boxes, 0)
            == tensorflow_size_frnt_(_gt_labels, 0)
        ):
            raise AssertionError
        pred_images = []
        for i, labels in enumerate(pred_labels):
            pred_images.extend([i] * tensorflow_size_frnt_(labels, 0))
        _pred_boxes = concatenate(pred_boxes, 0)
        _pred_labels = concatenate(pred_labels, 0)
        _pred_scores = concatenate(pred_scores, 0)
        _pred_images = tensor(pred_images, device=_pred_boxes.device, dtype=tf.int64)
        if (
            not tensorflow_size_frnt_(_pred_images, 0)
            == tensorflow_size_frnt_(_pred_boxes, 0)
            == tensorflow_size_frnt_(_pred_labels, 0)
            == tensorflow_size_frnt_(_pred_scores, 0)
        ):
            raise AssertionError
        average_precisions = zeros(
            n_classes - 1, device=_pred_boxes.device, dtype=_pred_boxes.dtype
        )
        for c in range(1, n_classes):
            gt_class_images = tensorflow_get_item(_gt_images, _gt_labels == c)
            gt_class_boxes = tensorflow_get_item(_gt_boxes, _gt_labels == c)
            gt_class_boxes_detected = zeros(
                tensorflow_size_frnt_(gt_class_images, 0),
                dtype=tf.uint8,
                device=gt_class_images.device,
            )
            pred_class_images = tensorflow_get_item(_pred_images, _pred_labels == c)
            pred_class_boxes = tensorflow_get_item(_pred_boxes, _pred_labels == c)
            pred_class_scores = tensorflow_get_item(_pred_scores, _pred_labels == c)
            n_class_detections = tensorflow_size_frnt_(pred_class_boxes, 0)
            if n_class_detections == 0:
                continue
            pred_class_scores, sort_ind = tensorflow_sort_frnt(
                pred_class_scores, dim=0, descending=True
            )
            pred_class_images = tensorflow_get_item(pred_class_images, sort_ind)
            pred_class_boxes = tensorflow_get_item(pred_class_boxes, sort_ind)
            gt_positives = zeros(
                (n_class_detections,),
                dtype=pred_class_boxes.dtype,
                device=pred_class_boxes.device,
            )
            false_positives = zeros(
                (n_class_detections,),
                dtype=pred_class_boxes.dtype,
                device=pred_class_boxes.device,
            )
            for d in range(n_class_detections):
                this_detection_box = tensorflow_unsqueeze_frnt_(
                    tensorflow_get_item(pred_class_boxes, d), 0
                )
                this_image = tensorflow_get_item(pred_class_images, d)
                object_boxes = tensorflow_get_item(
                    gt_class_boxes, gt_class_images == this_image
                )
                if tensorflow_size_frnt_(object_boxes, 0) == 0:
                    false_positives = tensorflow_set_item_bknd(false_positives, d, 1)
                    continue
>               overlaps = tensorflow_mean_iou.mean_iou_bbox(
                    this_detection_box, object_boxes
                )
E               AttributeError: 'function' object has no attribute 'mean_iou_bbox'

Translated_Outputs/tensorflow_outputs/kornia/metrics/mean_average_precision.py:131: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.metrics.mean_average_precision.mean_average_precision
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_metrics.py::test_mean_average_precision[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'mean_iou_bbox'
=============================================================================== 1 failed, 12 passed in 201.75s (0:03:21) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 57 items

kornia/geometry/test_transform.py ........................F..................F.......F..FFF                                                                                                      [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_upscale_double[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_upscale_double(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 4, 4),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(5, 3, 8, 8),
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.transform.upscale_double,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_transform.py:612: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function upscale_double at 0x7f03924863b0>
trace_args = (tensor([[[[0.4403, 0.7081, 0.3237, 0.2451],
          [0.1986, 0.4206, 0.6514, 0.4532],
          [0.0702, 0.9736, 0...., 0.1378, 0.3760, 0.2376],
          [0.5680, 0.3074, 0.0309, 0.8828],
          [0.2329, 0.0731, 0.2807, 0.4261]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[0.6445, 0.2945, 0.6661, 0.9429, 0.4069, 0.3118, 0.6852, 0.8256],
          [0.4873, 0.2801, 0.5706, 0.7575...4483, 0.3399, 0.3483, 0.7488, 0.3070],
          [0.8652, 0.1362, 0.2253, 0.1749, 0.6309, 0.7688, 0.9727, 0.4167]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function upscale_double at 0x7f03924863b0>
trace_args = (tensor([[[[0.4403, 0.7081, 0.3237, 0.2451],
          [0.1986, 0.4206, 0.6514, 0.4532],
          [0.0702, 0.9736, 0...., 0.1378, 0.3760, 0.2376],
          [0.5680, 0.3074, 0.0309, 0.8828],
          [0.2329, 0.0731, 0.2807, 0.4261]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[0.6445, 0.2945, 0.6661, 0.9429, 0.4069, 0.3118, 0.6852, 0.8256],
          [0.4873, 0.2801, 0.5706, 0.7575...4483, 0.3399, 0.3483, 0.7488, 0.3070],
          [0.8652, 0.1362, 0.2253, 0.1749, 0.6309, 0.7688, 0.9727, 0.4167]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.44030982, 0.7080565 , 0.32368505, 0.2451024 ],
     ....30740798, 0.03093898, 0.8828092 ],
         [0.23287654, 0.07306498, 0.28067774, 0.4260642 ]]]],
      dtype=float32)>

    def tensorflow_upscale_double(x):
        from ...core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...core._backend import zeros
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(x)
        tensorflow_KORNIA_CHECK_SHAPE(x, ["*", "H", "W"])
        double_shape = tensorflow_shape_frnt_(x)[:-2] + (
            tensorflow_shape_frnt_(x)[-2] * 2,
            tensorflow_shape_frnt_(x)[-1] * 2,
        )
        upscaled = zeros(double_shape, device=x.device, dtype=x.dtype)
        upscaled = tensorflow_set_item_bknd(
            upscaled, (..., slice(None, None, 2), slice(None, None, 2)), x
        )
>       upscaled[..., ::2, 1::2] = tensorflow_set_item_bknd(
            upscaled[..., ::2, 1::2],
            (..., slice(None, -1, None)),
            (upscaled[..., ::2, ::2][..., :-1] + upscaled[..., ::2, 2::2]) / 2,
        )
E       TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:177: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.pyramid.upscale_double
___________________________________________________________________________________ test_Shear[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Shear(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Shear")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledShear = ivy.transpile(kornia.geometry.transform.Shear, source="torch", target=target_framework)
    
        x = torch.rand(2, 3, 4, 4)
        shear = torch.tensor([[0.5, 0.0], [0.0, 0.5]])
        torch_out = kornia.geometry.transform.Shear(shear)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_shear = _nest_torch_tensor_to_new_framework(shear, target_framework)
>       transpiled_out = TranspiledShear(transpiled_shear)(transpiled_x)

kornia/geometry/test_transform.py:1156: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear()
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.21385074, 0.31908303, 0.8361167 , 0.95324516],
    ...2577006, 0.46994692, 0.41851968],
         [0.0060637 , 0.8549858 , 0.614465  , 0.9634386 ]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55d07283f1b0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Shear(), <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.21385074, 0.31908303, 0.8361167...72577006, 0.46994692, 0.41851968],
         [0.0060637 , 0.8549858 , 0.614465  , 0.9634386 ]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.21385074, 0.31908303, 0.8361167 , 0.95324516],
    ...2577006, 0.46994692, 0.41851968],
         [0.0060637 , 0.8549858 , 0.614465  , 0.9634386 ]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Shear(), <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.21385074, 0.31908303, 0.8361167...72577006, 0.46994692, 0.41851968],
         [0.0060637 , 0.8549858 , 0.614465  , 0.9634386 ]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.21385074, 0.31908303, 0.8361167 , 0.95324516],
    ...2577006, 0.46994692, 0.41851968],
         [0.0060637 , 0.8549858 , 0.614465  , 0.9634386 ]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.21385074, 0.31908303, 0.8361167 , 0.95324516],
     ....72577006, 0.46994692, 0.41851968],
         [0.0060637 , 0.8549858 , 0.614465  , 0.9634386 ]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Shear(),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.21385074, 0.31908303, 0.8361167 , 0.953245...72577006, 0.46994692, 0.41851968],
         [0.0060637 , 0.8549858 , 0.614465  , 0.9634386 ]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear()
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.21385074, 0.31908303, 0.8361167 , 0.95324516],
     ....72577006, 0.46994692, 0.41851968],
         [0.0060637 , 0.8549858 , 0.614465  , 0.9634386 ]]]],
      dtype=float32)>

    def call(self, input):
>       return shear(
            input, self.shear, self.mode, self.padding_mode, self.align_corners
        )
E       NameError: Exception encountered when calling tensorflow_Shear.call().
E       
E       [1mname 'shear' is not defined[0m
E       
E       Arguments received by tensorflow_Shear.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 4, 4), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/affwarp.py:1527: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Shear
__________________________________________________________________________________ test_Rescale[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Rescale(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Rescale")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledRescale = ivy.transpile(kornia.geometry.transform.Rescale, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 4, 4)
        torch_out = kornia.geometry.transform.Rescale((2.0, 3.0))(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_out = TranspiledRescale((2.0, 3.0))(transpiled_x)
    
>       _to_numpy_and_allclose(torch_out, transpiled_out)

kornia/geometry/test_transform.py:1294: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0.2403, 0.2156, 0.1909, 0.1662, 0.2158, 0.4143, 0.6127, 0.8111,
           0.8012, 0.6871, 0.5731, 0.4590],...        [0.0269, 0.0554, 0.0839, 0.1123, 0.1645, 0.2641, 0.3637, 0.4633,
           0.5760, 0.6954, 0.8148, 0.9341]]]])
transpiled_x = <tf.Tensor: shape=(1, 3, 8, 12), dtype=float32, numpy=
array([[[[0.240318  , 0.240318  , 0.21011421, 0.17991044, 0.149...      0.2530188 , 0.37474418, 0.4964695 , 0.6423483 , 0.78822696,
          0.9341056 , 0.9341056 ]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0.240318  , 0.21560583, 0.19089364, 0.16618145, 0.21584748,
          0.41426983, 0.6126923 , 0.81111467, 0....       0.26408473, 0.36367825, 0.46327168, 0.57603973, 0.695395  ,
          0.8147503 , 0.9341056 ]]]], dtype=float32)
y = array([[[[0.240318  , 0.240318  , 0.21011421, 0.17991044, 0.14970666,
          0.39222294, 0.63473916, 0.8772554 , 0....       0.2530188 , 0.37474418, 0.4964695 , 0.6423483 , 0.78822696,
          0.9341056 , 0.9341056 ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Rescale
________________________________________________________________________________ test_Homography[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Homography(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Homography")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHomography = ivy.transpile(
            kornia.geometry.transform.image_registrator.Homography, source="torch", target=target_framework
        )
    
        torch_out = kornia.geometry.transform.image_registrator.Homography()()
>       transpiled_out = TranspiledHomography()()

kornia/geometry/test_transform.py:1354: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>), args = (), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55d07139dc20, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>),), kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>), v = None, buffers = None
args = (), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>),)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>), v = None, buffers = None
args = (), kwargs = {}, first_arr = None, replace_v = False, replace_buffers = False, call_signature = <Signature ()>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>),), kwargs = {}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <keras.src.layers.layer.CallSpec object at 0x7f038a3d3bb0>, signature = <Signature ()>, args = (), kwargs = {}

    def __init__(self, signature, args, kwargs):
        # `training` and `mask` are special kwargs that are always available in
        # a layer, if user specifies them in their call without adding to spec,
        # we remove them to be able to bind variables. User is not using
        # `training` anyway so we can ignore.
        # TODO: If necessary use workaround for `mask`
        if "training" in kwargs and "training" not in signature.parameters:
            kwargs.pop("training")
            bound_args = signature.bind(*args, **kwargs)
        else:
            bound_args = signature.bind(*args, **kwargs)
        self.user_arguments_dict = {
            k: v for k, v in bound_args.arguments.items()
        }
        bound_args.apply_defaults()
        arg_dict = {}
        arg_names = []
        tensor_arg_dict = {}
        tensor_args = []
        tensor_arg_names = []
        nested_tensor_arg_names = []
        for name, value in bound_args.arguments.items():
            arg_dict[name] = value
            arg_names.append(name)
            if is_backend_tensor_or_symbolic(value):
                tensor_args.append(value)
                tensor_arg_names.append(name)
                tensor_arg_dict[name] = value
            elif tree.is_nested(value) and len(value) > 0:
                flat_values = tree.flatten(value)
                if all(
                    is_backend_tensor_or_symbolic(x, allow_none=True)
                    for x in flat_values
                ):
                    tensor_args.append(value)
                    tensor_arg_names.append(name)
                    tensor_arg_dict[name] = value
                    nested_tensor_arg_names.append(name)
                elif any(is_backend_tensor_or_symbolic(x) for x in flat_values):
                    raise ValueError(
                        "In a nested call() argument, "
                        "you cannot mix tensors and non-tensors. "
                        "Received invalid mixed argument: "
                        f"{name}={value}"
                    )
        self.arguments_dict = arg_dict
        self.argument_names = arg_names
        self.tensor_arguments_dict = tensor_arg_dict
        self.tensor_arguments_names = tensor_arg_names
        self.nested_tensor_argument_names = nested_tensor_arg_names
>       self.first_arg = arg_dict[arg_names[0]]
E       IndexError: list index out of range

/opt/fw/tensorflow/keras/src/layers/layer.py:1614: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Homography
_____________________________________________________________________________ test_ImageRegistrator[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ImageRegistrator(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.ImageRegistrator")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledImageRegistrator = ivy.transpile(
            kornia.geometry.transform.ImageRegistrator, source="torch", target=target_framework
        )

kornia/geometry/test_transform.py:1365: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.transform.image_registrator.ImageRegistrator'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geometry.transform.image_registrator: name 'Args' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.ImageRegistrator
________________________________________________________________________________ test_Similarity[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Similarity(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Similarity")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledSimilarity = ivy.transpile(
            kornia.geometry.transform.image_registrator.Similarity, source="torch", target=target_framework
        )

kornia/geometry/test_transform.py:1386: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.transform.image_registrator.Similarity'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geometry.transform.image_registrator: name 'Args' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Similarity
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_transform.py::test_upscale_double[tensorflow-s2s-False] - TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment
FAILED kornia/geometry/test_transform.py::test_Shear[tensorflow-s2s-False] - NameError: Exception encountered when calling tensorflow_Shear.call().
FAILED kornia/geometry/test_transform.py::test_Rescale[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_transform.py::test_Homography[tensorflow-s2s-False] - IndexError: list index out of range
FAILED kornia/geometry/test_transform.py::test_ImageRegistrator[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geom...
FAILED kornia/geometry/test_transform.py::test_Similarity[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geometry.t...
=============================================================================== 6 failed, 51 passed in 938.82s (0:15:38) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 39 items

kornia/test_enhance.py ..............F........................                                                                                                                                   [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_equalize_clahe[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_equalize_clahe(target_framework, mode, backend_compile):
        trace_args = (torch.rand(1, 10, 20),)
        trace_kwargs = {
            "clip_limit": 40.0,
            "grid_size": (8, 8),
            "slow_and_differentiable": False,
        }
        test_args = (torch.rand(2, 3, 10, 20),)
        test_kwargs = {
            "clip_limit": 20.0,
            "grid_size": (4, 4),
            "slow_and_differentiable": False,
        }
>       _test_function(
            kornia.enhance.equalize_clahe,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
            skip=True,
        )

kornia/test_enhance.py:363: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function equalize_clahe at 0x7fb193ed5360>
trace_args = (tensor([[[6.0038e-01, 4.5429e-01, 5.1182e-01, 6.2706e-01, 5.2452e-01,
          7.3965e-01, 4.4075e-01, 2.0884e-01, 8...952e-01, 2.9950e-03, 2.8209e-01, 7.9494e-03,
          2.8380e-01, 6.0278e-01, 2.9011e-01, 2.0662e-01, 9.9198e-01]]]),)
trace_kwargs = {'clip_limit': 40.0, 'grid_size': (8, 8), 'slow_and_differentiable': False}
test_args = (tensor([[[[0.8477, 0.9959, 0.9820,  ..., 0.7594, 0.9683, 0.0907],
          [0.2684, 0.4449, 0.2262,  ..., 0.5204, 0...., 0.4669, 0.2089,  ..., 0.1870, 0.3201, 0.1903],
          [0.9276, 0.3768, 0.3545,  ..., 0.1707, 0.4485, 0.1049]]]]),)
test_kwargs = {'clip_limit': 20.0, 'grid_size': (4, 4), 'slow_and_differentiable': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = True
deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function equalize_clahe at 0x7fb193ed5360>
trace_args = (tensor([[[6.0038e-01, 4.5429e-01, 5.1182e-01, 6.2706e-01, 5.2452e-01,
          7.3965e-01, 4.4075e-01, 2.0884e-01, 8...952e-01, 2.9950e-03, 2.8209e-01, 7.9494e-03,
          2.8380e-01, 6.0278e-01, 2.9011e-01, 2.0662e-01, 9.9198e-01]]]),)
trace_kwargs = {'clip_limit': 40.0, 'grid_size': (8, 8), 'slow_and_differentiable': False}
test_args = (tensor([[[[0.8477, 0.9959, 0.9820,  ..., 0.7594, 0.9683, 0.0907],
          [0.2684, 0.4449, 0.2262,  ..., 0.5204, 0...., 0.4669, 0.2089,  ..., 0.1870, 0.3201, 0.1903],
          [0.9276, 0.3768, 0.3545,  ..., 0.1707, 0.4485, 0.1049]]]]),)
test_kwargs = {'clip_limit': 20.0, 'grid_size': (4, 4), 'slow_and_differentiable': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 10, 20), dtype=float32, numpy=
array([[[[6.0038441e-01, 4.5429230e-01, 5.1181680e-01, 6.27058...523e-03, 2.8380442e-01,
          6.0277754e-01, 2.9010576e-01, 2.0661831e-01, 9.9197525e-01]]]],
      dtype=float32)>
args = (), kwargs = {'clip_limit': 40.0, 'grid_size': (8, 8), 'slow_and_differentiable': False}, tensorflow_numel_frnt_ = <function tensorflow_numel_frnt_ at 0x7fb18cc36680>
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7fb18cc349d0>, tensorflow_view_frnt_ = <function tensorflow_view_frnt_ at 0x7fb18cc36830>
input_shape = ivy.frontends.torch.Size([1, 10, 20])

    @wraps(f)
    def _wrapper(input, *args, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_numel_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
        if tensorflow_numel_frnt_(input) == 0:
            raise ValueError("Invalid input tensor, it is empty.")
        input_shape = tensorflow_shape_frnt_(input)
        input = tensorflow__to_bchw(input)
>       output = f(input, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/utils/image.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 10, 20), dtype=float32, numpy=
array([[[[6.0038441e-01, 4.5429230e-01, 5.1181680e-01, 6.27058...523e-03, 2.8380442e-01,
          6.0277754e-01, 2.9010576e-01, 2.0661831e-01, 9.9197525e-01]]]],
      dtype=float32)>
clip_limit = 40.0, grid_size = (8, 8), slow_and_differentiable = False

    @tensorflow_perform_keep_shape_image
    def tensorflow_equalize_clahe(
        input, clip_limit=40.0, grid_size=(8, 8), slow_and_differentiable=False
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_as_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
    
        if not isinstance(clip_limit, (float,)):
            raise TypeError(f"Input clip_limit type is not float. Got {type(clip_limit)}")
        if not isinstance(grid_size, (tuple,)):
            raise TypeError(f"Input grid_size type is not Tuple. Got {type(grid_size)}")
        if len(grid_size) != 2:
            raise TypeError(
                f"Input grid_size is not a Tuple with 2 elements. Got {len(grid_size)}"
            )
        if isinstance(grid_size[0], (float,)) or isinstance(grid_size[1], (float,)):
            raise TypeError("Input grid_size type is not valid, must be a Tuple[int, int].")
        if grid_size[0] <= 0 or grid_size[1] <= 0:
            raise ValueError(f"Input grid_size elements must be positive. Got {grid_size}")
        imgs: typing.Any = input
>       hist_tiles, img_padded = tensorflow__compute_tiles(imgs, grid_size, True)

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:513: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

imgs = <tf.Tensor: shape=(1, 1, 10, 20), dtype=float32, numpy=
array([[[[6.0038441e-01, 4.5429230e-01, 5.1181680e-01, 6.27058...523e-03, 2.8380442e-01,
          6.0277754e-01, 2.9010576e-01, 2.0661831e-01, 9.9197525e-01]]]],
      dtype=float32)>
grid_size = (8, 8), even_tile_size = True

    def tensorflow__compute_tiles(imgs, grid_size, even_tile_size=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_pad_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unfold_frnt_
    
        batch: typing.Any = imgs
        h, w = tensorflow_shape_frnt_(batch)[-2:][0], tensorflow_shape_frnt_(batch)[-2:][1]
        kernel_vert: typing.Any = math.ceil(h / grid_size[0])
        kernel_horz: typing.Any = math.ceil(w / grid_size[1])
        if even_tile_size:
            kernel_vert = kernel_vert + (1 if kernel_vert % 2 else 0)
            kernel_horz = kernel_horz + (1 if kernel_horz % 2 else 0)
        pad_vert = kernel_vert * grid_size[0] - h
        pad_horz = kernel_horz * grid_size[1] - w
        if (
            pad_vert > tensorflow_shape_frnt_(batch)[-2]
            or pad_horz > tensorflow_shape_frnt_(batch)[-1]
        ):
            raise ValueError(
                "Cannot compute tiles on the image according to the given grid size"
            )
        if pad_vert > 0 or pad_horz > 0:
            batch = tensorflow_pad_frnt(batch, [0, pad_horz, 0, pad_vert], mode="reflect")
        c: typing.Any = tensorflow_shape_frnt_(batch)[-3]
        tiles: typing.Any = tensorflow_contiguous_frnt_(
            tensorflow_squeeze_frnt_(
                tensorflow_unfold_frnt_(
>                   tensorflow_unfold_frnt_(
                        tensorflow_unfold_frnt_(batch, 1, c, c), 2, kernel_vert, kernel_vert
                    ),
                    3,
                    kernel_horz,
                    kernel_horz,
                ),
                1,
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 1, 32, 16), dtype=float32, numpy=
array([[[[[6.0038441e-01, 3.1267416e-01, 8.5136586e-01, 6....537106e-01,
           2.7152294e-01, 8.3371133e-01, 1.3953865e-01, 9.7528064e-01]]]]],
      dtype=float32)>, 2, 2, 2)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fb18ccbdc60>
array_like = <tf.Tensor: shape=(1, 1, 1, 32, 16), dtype=float32, numpy=
array([[[[[6.0038441e-01, 3.1267416e-01, 8.5136586e-01, 6.6...7e-01, 4.7537106e-01,
           2.7152294e-01, 8.3371133e-01, 1.3953865e-01, 9.7528064e-01]]]]],
      dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 1, 1, 32, 16), dtype=float32, numpy=
array([[[[[6.0038441e-01, 3.1267416e-01, 8.5136586e-01, 6.6...7e-01, 4.7537106e-01,
           2.7152294e-01, 8.3371133e-01, 1.3953865e-01, 9.7528064e-01]]]]],
      dtype=float32)>
dimension = 2, size = 2, step = 2

    @tensorflow_handle_methods
    def tensorflow_unfold_frnt_(tensor, dimension, size, step):
        from ...backends.tensorflow.general import tensorflow_get_item
        from ...ivy.general import tensorflow_set_item_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_stack_frnt
    
        slices = []
        self_shape = tuple(tensorflow_shape_frnt_(tensor))
        for i in range(0, tensorflow_get_item(self_shape, dimension) - size + 1, step):
            slicing = [slice(None)] * len(tensorflow_shape_frnt_(tensor))
            slicing = tensorflow_set_item_bknd(slicing, dimension, slice(i, i + size))
            slices.append(tensorflow_get_item(tensor, tuple(slicing)))
>       stacked = tensorflow_stack_frnt(slices, dim=dimension)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:423: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = [], dim = 2

    def tensorflow_stack_frnt(tensors, dim=0, *, out=None):
        from ...backends.tensorflow.manipulation import tensorflow_stack
    
>       return tensorflow_stack(tensors, axis=dim, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ([],), kwargs = {'axis': 2, 'out': None}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fb18ccbdc60>, array_like = []

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
>           array_like = array_like[0]
E           IndexError: list index out of range

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:46: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.enhance.equalization.equalize_clahe
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_enhance.py::test_equalize_clahe[tensorflow-s2s-False] - IndexError: list index out of range
=============================================================================== 1 failed, 38 passed in 629.69s (0:10:29) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 5 items

kornia/geometry/test_solvers.py .....                                                                                                                                                            [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 5 passed in 108.30s (0:01:48) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 26 items

kornia/geometry/test_epipolar.py ...FFF....................                                                                                                                                      [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________ test_decompose_essential_matrix[tensorflow-s2s-False] _________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_decompose_essential_matrix(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(3, 3),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(3, 3),
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.epipolar.decompose_essential_matrix,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
            # NOTE: numerical instability in svd()/lu() leads to logits not being allclose
            deterministic=False,
        )

kornia/geometry/test_epipolar.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function decompose_essential_matrix at 0x7fa229c83c70>, trace_args = (tensor([[0.9927, 0.3452, 0.8326],
        [0.0489, 0.4345, 0.0388],
        [0.3729, 0.8858, 0.5120]]),), trace_kwargs = {}
test_args = (tensor([[0.0246, 0.5310, 0.0114],
        [0.3207, 0.7847, 0.7075],
        [0.0489, 0.9021, 0.6777]]),), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, mode = 's2s', skip = False, deterministic = False

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function decompose_essential_matrix at 0x7fa229c83c70>, trace_args = (tensor([[0.9927, 0.3452, 0.8326],
        [0.0489, 0.4345, 0.0388],
        [0.3729, 0.8858, 0.5120]]),), trace_kwargs = {}
test_args = (tensor([[0.0246, 0.5310, 0.0114],
        [0.3207, 0.7847, 0.7075],
        [0.0489, 0.9021, 0.6777]]),), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, deterministic = False

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
            _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)
    
        # test it works with the test_args as input
        orig_out = fn(*test_args, **test_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(test_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(test_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
>           _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[-0.1169,  0.5116,  0.8512],
         [-0.3777, -0.8156,  0.4383],
         [ 0.9185, -0.2703,  0.2886]]]), ...0e-04],
         [ 4.0287e-01,  4.0039e-01, -8.2303e-01]]]), tensor([[ 0.3793],
        [ 0.5865],
        [-0.7156]]))
transpiled_x = (<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[-5.8340943e-01, -5.8052844e-01, -5.6799644e-01],
        ...shape=(3, 1), dtype=float32, numpy=
array([[-0.37929344],
       [-0.58649963],
       [ 0.7156497 ]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_shape_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_shape_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[-0.1169239 ,  0.5115691 ,  0.8512497 ],
        [-0.37770784, -0.81562686,  0.43828052],
        [ 0.9185127..., -8.2303119e-01]]], dtype=float32), array([[ 0.37929395],
       [ 0.5864996 ],
       [-0.7156497 ]], dtype=float32))
y = (array([[[-5.8340943e-01, -5.8052844e-01, -5.6799644e-01],
        [-7.0521349e-01,  7.0899493e-01, -2.8788086e-04],
 ...875,  0.28858986]]], dtype=float32), array([[-0.37929344],
       [-0.58649963],
       [ 0.7156497 ]], dtype=float32))
tolerance = 0.001

    def _check_shape_allclose(x, y, tolerance=1e-3):
        """
        Checks that all array shapes are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x.shape, y.shape, atol=tolerance), "numpy array shapes are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7fa223b0c040>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[-0.1169239 ,  0.5115691 ,  0.8512497 ],
        [-0.37770784, -0.81562686,  0.43828052],
        [ 0.9185127 , -0.27027822,  0.2885898 ]]], dtype=float32)
y = array([[[-5.8340943e-01, -5.8052844e-01, -5.6799644e-01],
        [-7.0521349e-01,  7.0899493e-01, -2.8788086e-04],
        [ 4.0287349e-01,  4.0039116e-01, -8.2303113e-01]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.epipolar.essential.decompose_essential_matrix
___________________________________________________________________________ test_motion_from_essential[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_motion_from_essential(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(3, 3),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(3, 3),
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.epipolar.motion_from_essential,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_epipolar.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function motion_from_essential at 0x7fa229c83e20>, trace_args = (tensor([[0.8610, 0.1336, 0.1613],
        [0.1671, 0.0955, 0.9052],
        [0.8871, 0.7981, 0.0999]]),), trace_kwargs = {}
test_args = (tensor([[0.2582, 0.0645, 0.4777],
        [0.7876, 0.9424, 0.8625],
        [0.3633, 0.6982, 0.2948]]),), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function motion_from_essential at 0x7fa229c83e20>, trace_args = (tensor([[0.8610, 0.1336, 0.1613],
        [0.1671, 0.0955, 0.9052],
        [0.8871, 0.7981, 0.0999]]),), trace_kwargs = {}
test_args = (tensor([[0.2582, 0.0645, 0.4777],
        [0.7876, 0.9424, 0.8625],
        [0.3633, 0.6982, 0.2948]]),), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
            _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)
    
        # test it works with the test_args as input
        orig_out = fn(*test_args, **test_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(test_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(test_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[[-0.8410, -0.5406, -0.0223],
          [ 0.5091, -0.8046,  0.3056],
          [-0.1832,  0.2457,  0.9519]],...   [[ 0.5528],
         [-0.5148],
         [ 0.6553]],

        [[-0.5528],
         [ 0.5148],
         [-0.6553]]]))
transpiled_x = (<tf.Tensor: shape=(1, 4, 3, 3), dtype=float32, numpy=
array([[[[-0.09544168,  0.84613377,  0.5243559 ],
         [ 0....0097],
        [-0.655286  ]],

       [[ 0.5527931 ],
        [-0.51480097],
        [ 0.655286  ]]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[-0.84100276, -0.5405694 , -0.02233711],
         [ 0.50907487, -0.8046331 ,  0.3056276 ],
         [-0.1831...0.5148009],
        [ 0.6552862]],

       [[-0.5527929],
        [ 0.5148009],
        [-0.6552862]]], dtype=float32))
y = (array([[[[-0.09544168,  0.84613377,  0.5243559 ],
         [ 0.36301014,  0.5200699 , -0.77314365],
         [-0.9268...80097],
        [-0.655286  ]],

       [[ 0.5527931 ],
        [-0.51480097],
        [ 0.655286  ]]], dtype=float32))
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7fa223b11200>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[-0.84100276, -0.5405694 , -0.02233711],
         [ 0.50907487, -0.8046331 ,  0.3056276 ],
         [-0.18318...,
         [ 0.36301023,  0.52006954, -0.77314365],
         [-0.9268842 ,  0.1165565 , -0.3567914 ]]]], dtype=float32)
y = array([[[[-0.09544168,  0.84613377,  0.5243559 ],
         [ 0.36301014,  0.5200699 , -0.77314365],
         [-0.92688...,
         [ 0.509075  , -0.8046333 ,  0.30562744],
         [-0.1831858 ,  0.2456624 ,  0.9518891 ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.epipolar.essential.motion_from_essential
___________________________________________________________________ test_motion_from_essential_choose_solution[tensorflow-s2s-False] ___________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_motion_from_essential_choose_solution(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(3, 3),
            torch.rand(3, 3),
            torch.rand(3, 3),
            torch.rand(8, 2),
            torch.rand(8, 2),
        )
        trace_kwargs = {'mask': torch.ones(8).bool()}
        test_args = (
            torch.rand(3, 3),
            torch.rand(3, 3),
            torch.rand(3, 3),
            torch.rand(8, 2),
            torch.rand(8, 2),
        )
        test_kwargs = {'mask': torch.ones(8).bool()}
>       _test_function(
            kornia.geometry.epipolar.motion_from_essential_choose_solution,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_epipolar.py:152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function motion_from_essential_choose_solution at 0x7fa229c83eb0>
trace_args = (tensor([[0.6816, 0.0974, 0.6930],
        [0.8897, 0.3062, 0.8788],
        [0.3128, 0.3025, 0.6695]]), tensor([[0.57...924, 0.9495],
        [0.1271, 0.9816],
        [0.6926, 0.8940],
        [0.8701, 0.1983],
        [0.2778, 0.2281]]))
trace_kwargs = {'mask': <tf.Tensor: shape=(8,), dtype=bool, numpy=array([ True,  True,  True,  True,  True,  True,  True,  True])>}
test_args = (tensor([[0.9264, 0.2370, 0.8429],
        [0.1209, 0.1125, 0.6984],
        [0.1189, 0.0508, 0.4092]]), tensor([[0.56...405, 0.1019],
        [0.4128, 0.8853],
        [0.0196, 0.4953],
        [0.2792, 0.5227],
        [0.5455, 0.8061]]))
test_kwargs = {'mask': <tf.Tensor: shape=(8,), dtype=bool, numpy=array([ True,  True,  True,  True,  True,  True,  True,  True])>}, target = 'tensorflow', backend_compile = False, tolerance = 0.001
mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function motion_from_essential_choose_solution at 0x7fa229c83eb0>
trace_args = (tensor([[0.6816, 0.0974, 0.6930],
        [0.8897, 0.3062, 0.8788],
        [0.3128, 0.3025, 0.6695]]), tensor([[0.57...924, 0.9495],
        [0.1271, 0.9816],
        [0.6926, 0.8940],
        [0.8701, 0.1983],
        [0.2778, 0.2281]]))
trace_kwargs = {'mask': <tf.Tensor: shape=(8,), dtype=bool, numpy=array([ True,  True,  True,  True,  True,  True,  True,  True])>}
test_args = (tensor([[0.9264, 0.2370, 0.8429],
        [0.1209, 0.1125, 0.6984],
        [0.1189, 0.0508, 0.4092]]), tensor([[0.56...405, 0.1019],
        [0.4128, 0.8853],
        [0.0196, 0.4953],
        [0.2792, 0.5227],
        [0.5455, 0.8061]]))
test_kwargs = {'mask': <tf.Tensor: shape=(8,), dtype=bool, numpy=array([ True,  True,  True,  True,  True,  True,  True,  True])>}, target = 'tensorflow', backend_compile = False, tolerance = 0.001
deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
            _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)
    
        # test it works with the test_args as input
        orig_out = fn(*test_args, **test_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(test_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(test_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[-0.3753,  0.1138,  0.9199],
        [-0.8728,  0.2905, -0.3921],
        [-0.3119, -0.9501, -0.0097]]), tens...3923],
        [ 0.2328,  0.0448,  0.0232],
        [ 0.1903,  0.2009,  0.1386],
        [ 0.4881,  0.0528,  0.0167]]))
transpiled_x = (<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[-0.37533557,  0.1138389 ,  0.9198721 ],
       [-0.8728403 , ...021],
       [-0.1902562 , -0.20093776, -0.13862441],
       [-0.48809946, -0.05278678, -0.01668871]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[-0.37533525,  0.11383902,  0.9198718 ],
       [-0.8728402 ,  0.29053068, -0.39209983],
       [-0.31188732, ...2981],
       [ 0.19025634,  0.20093803,  0.13862431],
       [ 0.48809898,  0.0527873 ,  0.01668859]], dtype=float32))
y = (array([[-0.37533557,  0.1138389 ,  0.9198721 ],
       [-0.8728403 ,  0.29053062, -0.39209986],
       [-0.3118871 , ...3021],
       [-0.1902562 , -0.20093776, -0.13862441],
       [-0.48809946, -0.05278678, -0.01668871]], dtype=float32))
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7fa22191ba80>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[-0.05192956],
       [-0.45499194],
       [ 0.8889802 ]], dtype=float32), y = array([[ 0.05192955],
       [ 0.45499197],
       [-0.88898015]], dtype=float32), tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.epipolar.essential.motion_from_essential_choose_solution
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_epipolar.py::test_decompose_essential_matrix[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_epipolar.py::test_motion_from_essential[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_epipolar.py::test_motion_from_essential_choose_solution[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
=============================================================================== 3 failed, 23 passed in 393.88s (0:06:33) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 16 items

kornia/test_utils.py .F....F...FFFF..                                                                                                                                                            [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_draw_rectangle[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_draw_rectangle(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(2, 3, 10, 12),
            torch.tensor([[[0, 0, 4, 4]], [[4, 4, 10, 10]]]),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(3, 3, 10, 12),
            torch.tensor([[[0, 0, 4, 4]], [[4, 4, 10, 10]], [[2, 2, 6, 6]]]),
        )
        test_kwargs = {}
>       _test_function(
            kornia.utils.draw_rectangle,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_utils.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function draw_rectangle at 0x7f191b46da20>
trace_args = (tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7549, 0.6857, 0.9397,
           0.2701, 0.8195, 0.2423, 0.4280]...000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.5651]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]]]))
trace_kwargs = {}
test_args = (tensor([[[[0.8973, 0.5846, 0.4991,  ..., 0.7642, 0.2921, 0.2635],
          [0.2385, 0.0294, 0.7603,  ..., 0.8776, 0....., 0.0856, 0.8361, 0.2529]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]],

        [[ 2,  2,  6,  6]]]))
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function draw_rectangle at 0x7f191b46da20>
trace_args = (tensor([[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7549, 0.6857, 0.9397,
           0.2701, 0.8195, 0.2423, 0.4280]...000, 0.0000,
           0.0000, 0.0000, 0.0000, 0.5651]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]]]))
trace_kwargs = {}
test_args = (tensor([[[[0.8973, 0.5846, 0.4991,  ..., 0.7642, 0.2921, 0.2635],
          [0.2385, 0.0294, 0.7603,  ..., 0.8776, 0....., 0.0856, 0.8361, 0.2529]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]],

        [[ 2,  2,  6,  6]]]))
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.  ...      0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.5651096 ]]]], dtype=float32)>
rectangle = <tf.Tensor: shape=(2, 1, 4), dtype=int64, numpy=
array([[[ 0,  0,  4,  4]],

       [[ 4,  4, 10,  9]]])>
color = <tf.Tensor: shape=(2, 1, 3), dtype=float32, numpy=
array([[[0., 0., 0.]],

       [[0., 0., 0.]]], dtype=float32)>, fill = False

    def tensorflow_draw_rectangle(image, rectangle, color=None, fill=None):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_long_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_clamp_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_tensor_frnt
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
        batch, c, h, w = tensorflow_shape_frnt_(image)
        batch_rect, num_rectangle, num_points = tensorflow_shape_frnt_(rectangle)
        if batch != batch_rect:
            raise AssertionError("Image batch and rectangle batch must be equal")
        if num_points != 4:
            raise AssertionError("Number of points in rectangle must be 4")
        rectangle = tensorflow_clone_frnt_(tensorflow_long_frnt_(rectangle))
        rectangle = tensorflow_set_item_bknd(
            rectangle,
            (slice(None, None, None), slice(None, None, None), slice(1, None, 2)),
            tensorflow_clamp_frnt(rectangle[:, :, 1::2], 0, h - 1),
        )
        rectangle = tensorflow_set_item_bknd(
            rectangle,
            (slice(None, None, None), slice(None, None, None), slice(None, None, 2)),
            tensorflow_clamp_frnt(rectangle[:, :, ::2], 0, w - 1),
        )
        if color is None:
            color = tensorflow_expand_frnt_(
                tensorflow_tensor_frnt([0.0] * c), batch, num_rectangle, c
            )
        if fill is None:
            fill = False
        if len(tensorflow_shape_frnt_(color)) == 1:
            color = tensorflow_expand_frnt_(color, batch, num_rectangle, c)
        b, n, color_channels = tensorflow_shape_frnt_(color)
        if color_channels == 1 and c == 3:
            color = tensorflow_expand_frnt_(color, batch, num_rectangle, c)
        for b in range(batch):
            for n in range(num_rectangle):
                if fill:
                    image = tensorflow_set_item_bknd(
                        image,
                        (
                            b,
                            slice(None, None, None),
                            slice(
                                int(rectangle[b, n, 1]), int(rectangle[b, n, 3] + 1), None
                            ),
                            slice(
                                int(rectangle[b, n, 0]), int(rectangle[b, n, 2] + 1), None
                            ),
                        ),
                        tensorflow_get_item(
                            color, (b, n, slice(None, None, None), None, None)
                        ),
                    )
                else:
                    image = tensorflow_set_item_bknd(
                        image,
                        (
                            b,
                            slice(None, None, None),
                            slice(
                                int(rectangle[b, n, 1]), int(rectangle[b, n, 3] + 1), None
                            ),
                            rectangle[b, n, 0],
                        ),
                        tensorflow_get_item(color, (b, n, slice(None, None, None), None)),
                    )
>                   image = tensorflow_set_item_bknd(
                        image,
                        (
                            b,
                            slice(None, None, None),
                            slice(
                                int(rectangle[b, n, 1]), int(rectangle[b, n, 3] + 1), None
                            ),
                            rectangle[b, n, 2],
                        ),
                        tensorflow_get_item(color, (b, n, slice(None, None, None), None)),
                    )

Translated_Outputs/tensorflow_outputs/kornia/utils/draw.py:275: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inp = <tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.  ...      0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.5651096 ]]]], dtype=float32)>
query = (0, slice(None, None, None), slice(0, 5, None), <tf.Tensor: shape=(), dtype=int64, numpy=4>)
val = <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.],
       [0.],
       [0.]], dtype=float32)>, kwargs = {}

    @functools.wraps(fn)
    def wrapper(inp, query, val, **kwargs):
        try:
            inp.__setitem__(query, val)
            res = inp
        except IndexError:
            raise
        except Exception:
>           res = fn(inp, query, val, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0. ...64, numpy=4>), <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.],
       [0.],
       [0.]], dtype=float32)>)
kwargs = {}, handle_mixed_in_backend = True

    @functools.wraps(fn)
    def _handle_partial_mixed_function(*args, **kwargs):
        handle_mixed_in_backend = False
        if not hasattr(fn, "partial_mixed_handler"):
            handle_mixed_in_backend = True
        else:
            compos = getattr(fn, "compos")
            condition = getattr(fn, "partial_mixed_handler")
        if handle_mixed_in_backend or condition(*args, **kwargs):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0.  ...      0.        , 0.        , 0.        , 0.        , 0.        ,
          0.        , 0.5651096 ]]]], dtype=float32)>
query = (0, slice(None, None, None), slice(0, 5, None), <tf.Tensor: shape=(), dtype=int64, numpy=4>)
val = <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.],
       [0.],
       [0.]], dtype=float32)>

    @tensorflow_handle_set_item
    @tensorflow_handle_partial_mixed_function
    def tensorflow_set_item_bknd(
        x: Union[tensorflow.Tensor, tf.Tensor],
        query: Union[tensorflow.Tensor, tf.Tensor, Tuple],
        val: Union[tensorflow.Tensor, tf.Tensor],
        /,
        *,
        copy: Optional[bool] = False,
    ):
        from ..backends.tensorflow.general import tensorflow_shape
        from ..backends.tensorflow.creation import tensorflow_copy_array
        from ..backends.tensorflow.creation import tensorflow_asarray
        from .data_type import tensorflow_is_bool_dtype_bknd
        from ..backends.tensorflow.manipulation import tensorflow_tile
        from ..backends.tensorflow.searching import tensorflow_nonzero
        from ...data_classes.array.data_type import tensorflow_astype_bknd_
        from ..backends.tensorflow.general import tensorflow_scatter_nd
    
        if copy:
            x = tensorflow_copy_array(x)
        if not tensorflow_is_array_bknd(val):
            val = tensorflow_asarray(val)
        if 0 in x.shape or 0 in val.shape:
            return x
        if tensorflow_is_array_bknd(query) and tensorflow_is_bool_dtype_bknd(query):
            if not len(query.shape):
                query = tensorflow_tile(query, (x.shape[0],))
            indices = tensorflow_nonzero(query, as_tuple=False)
        else:
            indices, target_shape, _ = tensorflow__parse_query_bknd(
                query, tensorflow_shape(x, as_array=True), scatter=True
            )
            if indices is None:
                return x
        val = tensorflow_astype_bknd_(val, x.dtype)
>       ret = tensorflow_scatter_nd(indices, val, reduction="replace", out=x)

Translated_Outputs/tensorflow_outputs/ivy/functional/ivy/general.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

indices = <tf.Tensor: shape=(3, 5, 4), dtype=int64, numpy=
array([[[0, 4, 0, 0],
        [0, 4, 0, 1],
        [0, 4, 0, 2],
   ... 4]],

       [[0, 4, 2, 0],
        [0, 4, 2, 1],
        [0, 4, 2, 2],
        [0, 4, 2, 3],
        [0, 4, 2, 4]]])>
updates = <tf.Tensor: shape=(3, 5), dtype=float32, numpy=
array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)>, shape = None

    def tensorflow_scatter_nd(
        indices: Union[tensorflow.Tensor, tensorflow.Variable],
        updates: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Optional[Union[tf.TensorShape, Sequence[int]]] = None,
        *,
        reduction: str = "sum",
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.general import tensorflow_exists_bknd
        from ...ivy.data_type import tensorflow_promote_types_bknd
        from .data_type import tensorflow_as_native_dtype
        from ...ivy.general import tensorflow__broadcast_to_bknd
        from ....utils.assertions import tensorflow_check_equal
        from .elementwise import tensorflow_multiply
    
        updates_dtype = updates.dtype
        if tensorflow_exists_bknd(out):
            dtype = tensorflow_promote_types_bknd(out.dtype, updates_dtype)
        updates = tensorflow.cast(
            updates,
            tensorflow_as_native_dtype(dtype)
            if tensorflow_exists_bknd(out)
            else updates_dtype,
        )
        expected_shape = (
            list(tensorflow.shape(indices)[:-1])
            + list(out.shape[tensorflow.shape(indices)[-1] :])
            if tensorflow_exists_bknd(out)
            else list(tensorflow.shape(indices)[:-1])
            + list(shape[tensorflow.shape(indices)[-1] :])
        )
        updates = tensorflow__broadcast_to_bknd(updates, expected_shape)
        if len(updates.shape) == 0:
            indices = tensorflow.expand_dims(indices, 0)
            updates = tensorflow.expand_dims(updates, 0)
        target = out
        target_given = tensorflow_exists_bknd(target)
        if tensorflow_exists_bknd(shape) and target_given:
            tensorflow_check_equal(tuple(target.shape), tuple(shape), as_array=False)
        if not target_given:
            shape = list(shape) if tensorflow_exists_bknd(shape) else list(out.shape)
            target = tensorflow.zeros(shape, dtype=updates.dtype)
        if reduction == "sum":
            res = tensorflow.tensor_scatter_nd_add(target, indices, updates)
        elif reduction == "min":
            res = tensorflow.tensor_scatter_nd_min(target, indices, updates)
        elif reduction == "max":
            res = tensorflow.tensor_scatter_nd_max(target, indices, updates)
        elif reduction == "mul":
            updates = tensorflow_multiply(tensorflow_gather_nd(target, indices), updates)
            res = tensorflow.tensor_scatter_nd_update(target, indices, updates)
        elif reduction == "replace":
>           res = tensorflow.tensor_scatter_nd_update(target, indices, updates)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/general.py:274: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , 0.        , 0. ...loat32, numpy=
array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)>)
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__TensorScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,0] = [0, 4, 0, 0] does not index into shape [2,3,10,12] [Op:TensorScatterUpdate] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.draw.draw_rectangle
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-14 04:18:35.660686: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at scatter_nd_op.cc:218 : INVALID_ARGUMENT: indices[0,0] = [0, 4, 0, 0] does not index into shape [2,3,10,12]
______________________________________________________________________________ test_tensor_to_image[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_tensor_to_image(target_framework, mode, backend_compile):
        print("kornia.utils.tensor_to_image")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_func = ivy.transpile(kornia.utils.tensor_to_image, source="torch", target=target_framework)
    
        tensor = torch.ones(1, 3, 3)
        transpiled_tensor = _nest_torch_tensor_to_new_framework(tensor, target_framework)
        torch_image = kornia.utils.tensor_to_image(tensor)
>       transpiled_image = transpiled_func(transpiled_tensor)

kornia/test_utils.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]]], dtype=float32)>, keepdim = False, force_contiguous = False

    def tensorflow_tensor_to_image(tensor, keepdim=False, force_contiguous=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_detach_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_numpy_frnt_
    
        if not isinstance(tensor, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(tensor)}")
        if (
            len(tensorflow_shape_frnt_(tensor)) > 4
            or len(tensorflow_shape_frnt_(tensor)) < 2
        ):
            raise ValueError("Input size must be a two, three or four dimensional tensor")
        input_shape = tensorflow_shape_frnt_(tensor)
        image = tensorflow_detach_frnt_(tensor.cpu())
        if len(input_shape) == 2:
            pass
        elif len(input_shape) == 3:
            if input_shape[0] == 1:
                image = tensorflow_squeeze_frnt_(image)
            else:
                image = tensorflow_permute_frnt_(image, 1, 2, 0)
        elif len(input_shape) == 4:
            image = tensorflow_permute_frnt_(image, 0, 2, 3, 1)
            if input_shape[0] == 1 and not keepdim:
                image = tensorflow_squeeze_frnt_(image, 0)
            if input_shape[1] == 1:
                image = tensorflow_squeeze_frnt_(image, -1)
        else:
            raise ValueError(f"Cannot process tensor with shape {input_shape}")
        if force_contiguous:
            image = tensorflow_contiguous_frnt_(image)
>       return tensorflow_numpy_frnt_(image)

Translated_Outputs/tensorflow_outputs/kornia/utils/image.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>,), kwargs = {}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f190fbae4d0>
array_like = <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>

    @tensorflow_handle_methods
    def tensorflow_numpy_frnt_(tensor):
>       return np_frontend_array(tensor)
E       NameError: name 'np_frontend_array' is not defined

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:381: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.tensor_to_image
____________________________________________________________________________ test_save_pointcloud_ply[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_save_pointcloud_ply(target_framework, mode, backend_compile):
        print("kornia.utils.save_pointcloud_ply")
    
        if backend_compile:
            pytest.skip()
    
        pointcloud = torch.rand(100, 3)
        transpiled_pointcloud = _nest_torch_tensor_to_new_framework(pointcloud, target_framework)
    
        with tempfile.NamedTemporaryFile(suffix=".ply") as temp_file:
            filename = temp_file.name
    
            transpiled_save_pointcloud = ivy.transpile(kornia.utils.save_pointcloud_ply, source="torch", target=target_framework)
            transpiled_load_pointcloud = ivy.transpile(kornia.utils.load_pointcloud_ply, source="torch", target=target_framework)
    
            # Save and load pointcloud to ensure both steps work correctly
            transpiled_save_pointcloud(filename, transpiled_pointcloud)
>           loaded_pointcloud = transpiled_load_pointcloud(filename)

kornia/test_utils.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

filename = '/tmp/tmpqi50g2oo.ply', header_size = 8

    def tensorflow_load_pointcloud_ply(filename, header_size=8):
        from ...genericpath import tensorflow_isfile
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_split_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_tensor_frnt
    
        if not isinstance(filename, (str,)) and filename[-3:] == ".ply":
            raise TypeError(
                f"Input filename must be a string in with the .ply  extension. Got {filename}"
            )
>       if not tensorflow_isfile(filename):

Translated_Outputs/tensorflow_outputs/kornia/utils/pointcloud_io.py:91: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

path = '/tmp/tmpqi50g2oo.ply'

    def tensorflow_isfile(path):
        try:
            st = os.stat(path)
        except (OSError, ValueError):
            return False
>       return stat.S_ISREG(st.st_mode)
E       NameError: name 'stat' is not defined

Translated_Outputs/tensorflow_outputs/genericpath.py:35: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.save_pointcloud_ply
_______________________________________________________________________ test_get_cuda_device_if_available[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_cuda_device_if_available(target_framework, mode, backend_compile):
        print("kornia.utils.get_cuda_device_if_available")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_get_cuda_device_if_available = ivy.transpile(kornia.utils.get_cuda_device_if_available, source="torch", target=target_framework)
    
        torch_device = kornia.utils.get_cuda_device_if_available()
>       transpiled_device = transpiled_get_cuda_device_if_available()

kornia/test_utils.py:283: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

index = 0

    def tensorflow_get_cuda_device_if_available(index=0):
        from ...ivy.functional.frontends.torch.__init__ import tensorflow_device_frnt
    
>       if cuda.is_available():
E       NameError: name 'cuda' is not defined

Translated_Outputs/tensorflow_outputs/kornia/utils/helpers.py:31: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.get_cuda_device_if_available
________________________________________________________________________ test_get_mps_device_if_available[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_mps_device_if_available(target_framework, mode, backend_compile):
        print("kornia.utils.get_mps_device_if_available")
    
        if backend_compile:
            pytest.skip()
    
>       transpiled_get_mps_device_if_available = ivy.transpile(kornia.utils.get_mps_device_if_available, source="torch", target=target_framework)

kornia/test_utils.py:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <function get_mps_device_if_available at 0x7f191b46e440>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Unsupported type: None

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.get_mps_device_if_available
____________________________________________________________________ test_get_cuda_or_mps_device_if_available[tensorflow-s2s-False] ____________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_cuda_or_mps_device_if_available(target_framework, mode, backend_compile):
        print("kornia.utils.get_cuda_or_mps_device_if_available")
    
        if backend_compile:
            pytest.skip()
    
>       transpiled_get_cuda_or_mps_device_if_available = ivy.transpile(kornia.utils.get_cuda_or_mps_device_if_available, source="torch", target=target_framework)

kornia/test_utils.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <function get_cuda_or_mps_device_if_available at 0x7f191b46e4d0>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Unsupported type: None

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.get_cuda_or_mps_device_if_available
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_utils.py::test_draw_rectangle[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__TensorScatterUpdate_device_/job:loc...
FAILED kornia/test_utils.py::test_tensor_to_image[tensorflow-s2s-False] - NameError: name 'np_frontend_array' is not defined
FAILED kornia/test_utils.py::test_save_pointcloud_ply[tensorflow-s2s-False] - NameError: name 'stat' is not defined
FAILED kornia/test_utils.py::test_get_cuda_device_if_available[tensorflow-s2s-False] - NameError: name 'cuda' is not defined
FAILED kornia/test_utils.py::test_get_mps_device_if_available[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Unsupported type: None
FAILED kornia/test_utils.py::test_get_cuda_or_mps_device_if_available[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Unsupported type: None
=============================================================================== 6 failed, 10 passed in 256.62s (0:04:16) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

kornia/geometry/test_quaternion.py .                                                                                                                                                             [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 1 passed in 116.55s (0:01:56) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 4 items

kornia/test_sensors.py ....                                                                                                                                                                      [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
===================================================================================== 4 passed in 88.12s (0:01:28) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/geometry/test_boxes.py FF                                                                                                                                                                 [100%]

=============================================================================================== FAILURES ===============================================================================================
___________________________________________________________________________________ test_Boxes[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Boxes(target_framework, mode, backend_compile):
        print("kornia.geometry.boxes.Boxes")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledBoxes = ivy.transpile(kornia.geometry.boxes.Boxes, source="torch", target=target_framework)
    
        torch_args = (
            torch.as_tensor([[0, 3, 1, 4], [5, 1, 8, 4]]),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        # test .from_tensor
        torch_boxes = kornia.geometry.boxes.Boxes.from_tensor(*torch_args, mode="xyxy")
        transpiled_boxes = TranspiledBoxes.from_tensor(*transpiled_args, mode="xyxy")
        _check_boxes_same(torch_boxes, transpiled_boxes)
    
        # test .compute_area
        torch_area = torch_boxes.compute_area()
        transpiled_area = transpiled_boxes.compute_area()
        _to_numpy_and_allclose(torch_area, transpiled_area)
    
        # test .get_boxes_shape
        torch_heights, torch_widths = torch_boxes.get_boxes_shape()
        transpiled_heights, transpiled_widths = transpiled_boxes.get_boxes_shape()
        _to_numpy_and_allclose(torch_heights, transpiled_heights)
        _to_numpy_and_allclose(torch_widths, transpiled_widths)
    
        # test .merge
        torch_x = torch.as_tensor([[6, 6, 10, 10], [6, 6, 10, 10]])
        transpiled_x = _nest_torch_tensor_to_new_framework(torch_x, target_framework)
        merge_boxes = kornia.geometry.boxes.Boxes.from_tensor(torch_x, mode="xyxy")
        transpiled_merge_boxes = TranspiledBoxes.from_tensor(transpiled_x, mode="xyxy")
        torch_merged_boxes = torch_boxes.merge(merge_boxes)
        transpiled_merged_boxes = transpiled_boxes.merge(transpiled_merge_boxes)
        _check_boxes_same(torch_merged_boxes, transpiled_merged_boxes)
    
        # test .to_mask
        height, width = 10, 10
        torch_mask = torch_boxes.to_mask(height, width)
        transpiled_mask = transpiled_boxes.to_mask(height, width)
>       _to_numpy_and_allclose(torch_mask, transpiled_mask)

kornia/geometry/test_boxes.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0....., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])
transpiled_x = <tf.Tensor: shape=(2, 10, 10), dtype=float32, numpy=
array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0....,
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0...],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)
y = array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0...],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.boxes.Boxes
__________________________________________________________________________________ test_Boxes3D[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Boxes3D(target_framework, mode, backend_compile):
        print("kornia.geometry.boxes.Boxes3D")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledBoxes3D = ivy.transpile(kornia.geometry.boxes.Boxes3D, source="torch", target=target_framework)
    
        torch_args = (
            torch.as_tensor([[0, 3, 6, 1, 4, 8], [5, 1, 3, 8, 4, 9]]),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        # test .from_tensor
        torch_boxes3d = kornia.geometry.boxes.Boxes3D.from_tensor(*torch_args, mode="xyzxyz")
        transpiled_boxes3d = TranspiledBoxes3D.from_tensor(*transpiled_args, mode="xyzxyz")
        _check_boxes_same(torch_boxes3d, transpiled_boxes3d)
    
        # test .get_boxes_shape
        torch_depths, torch_heights, torch_widths = torch_boxes3d.get_boxes_shape()
        transpiled_depths, transpiled_heights, transpiled_widths = transpiled_boxes3d.get_boxes_shape()
        _to_numpy_and_allclose(torch_depths, transpiled_depths)
        _to_numpy_and_allclose(torch_heights, transpiled_heights)
        _to_numpy_and_allclose(torch_widths, transpiled_widths)
    
        # test .to_mask
        depth, height, width = 10, 10, 10
        torch_mask = torch_boxes3d.to_mask(depth, height, width)
        transpiled_mask = transpiled_boxes3d.to_mask(depth, height, width)
>       _to_numpy_and_allclose(torch_mask, transpiled_mask)

kornia/geometry/test_boxes.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0... [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
transpiled_x = <tf.Tensor: shape=(2, 10, 10, 10), dtype=float32, numpy=
array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0.,...., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]...0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)
y = array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]...0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.boxes.Boxes3D
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_boxes.py::test_Boxes[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_boxes.py::test_Boxes3D[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
==================================================================================== 2 failed in 165.60s (0:02:45) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/test_image.py ....FFFF                                                                                                                                                                    [100%]

=============================================================================================== FAILURES ===============================================================================================
___________________________________________________________________________________ test_Image[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image(target_framework, mode, backend_compile):
        print("kornia.image.Image")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImageSize = ivy.transpile(kornia.image.ImageSize, source="torch", target=target_framework)
        TranspiledPixelFormat = ivy.transpile(kornia.image.PixelFormat, source="torch", target=target_framework)
        TranspiledChannelsOrder = ivy.transpile(kornia.image.ChannelsOrder, source="torch", target=target_framework)
        TranspiledImageLayout = ivy.transpile(kornia.image.ImageLayout, source="torch", target=target_framework)
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        torch_data = torch.randint(0, 255, (3, 4, 5), dtype=torch.uint8)
        transpiled_data = _array_to_new_backend(torch_data, target_framework)
    
        # torch
        pixel_format = kornia.image.PixelFormat(
            color_space=ColorSpace.rgb,
            bit_depth=8,
        )
        layout = kornia.image.ImageLayout(
            image_size=kornia.image.ImageSize(4, 5),
            channels=3,
            channels_order=kornia.image.ChannelsOrder.CHANNELS_FIRST,
        )
        torch_img = kornia.image.Image(torch_data, pixel_format, layout)
    
        # transpiled
        pixel_format = TranspiledPixelFormat(
            color_space=ColorSpace.rgb,
            bit_depth=8,
        )
        layout = TranspiledImageLayout(
            image_size=TranspiledImageSize(4, 5),
            channels=3,
            channels_order=TranspiledChannelsOrder.CHANNELS_FIRST,
        )
>       transpiled_img = TranspiledImage(transpiled_data, pixel_format, layout)

kornia/test_image.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_Image' object has no attribute '_data'") raised in repr()] tensorflow_Image object at 0x7ff608cd3d30>
data = <tf.Tensor: shape=(3, 4, 5), dtype=uint8, numpy=
array([[[175, 216,   3,  91,  95],
        [ 91,  58,  30, 169, 196],...       [171, 159, 178,  64, 164],
        [ 71, 198, 186, 219,  84],
        [241,   4, 105, 224, 103]]], dtype=uint8)>
pixel_format = tensorflow_PixelFormat(color_space=<ColorSpace.rgb: 0>, bit_depth=8)
layout = tensorflow_ImageLayout(image_size=tensorflow_ImageSize(height=4, width=5), channels=3, channels_order=<tensorflow_ChannelsOrder.CHANNELS_FIRST: 0>)

    def __init__(self, data, pixel_format, layout):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
    
        if layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            shape = [
                str(layout.channels),
                str(layout.image_size.height),
                str(layout.image_size.width),
            ]
        elif layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            shape = [
                str(layout.image_size.height),
                str(layout.image_size.width),
                str(layout.channels),
            ]
        else:
            raise NotImplementedError(
                f"Layout {layout.channels_order} not implemented."
            )
        tensorflow_KORNIA_CHECK_SHAPE(data, shape)
        tensorflow_KORNIA_CHECK(
>           data.element_size() == pixel_format.bit_depth // 8, "Invalid bit depth."
        )

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Tensor: shape=(3, 4, 5), dtype=uint8, numpy=
array([[[175, 216,   3,  91,  95],
        [ 91,  58,  30, 169, 196],...       [171, 159, 178,  64, 164],
        [ 71, 198, 186, 219,  84],
        [241,   4, 105, 224, 103]]], dtype=uint8)>
name = 'element_size'

    def __getattr__(self, name):
      if name in {"T", "astype", "ravel", "transpose", "reshape", "clip", "size",
                  "tolist", "data"}:
        # TODO(wangpeng): Export the enable_numpy_behavior knob
        raise AttributeError(
            f"{type(self).__name__} object has no attribute '{name}'. " + """
          If you are looking for numpy-related methods, please run the following:
          tf.experimental.numpy.experimental_enable_numpy_behavior()
        """)
>     self.__getattribute__(name)
E     AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'

/opt/fw/tensorflow/tensorflow/python/framework/tensor.py:260: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image
_____________________________________________________________________________ test_Image_from_numpy[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image_from_numpy(target_framework, mode, backend_compile):
        print("kornia.image.Image.from_numpy")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        data = np.ones((4, 5, 3), dtype=np.uint8)
>       img = TranspiledImage.from_numpy(data, color_space=ColorSpace.rgb)

kornia/test_image.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'Translated_Outputs.tensorflow_outputs.kornia.image.image.tensorflow_Image'>
data = array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]],

       [[1, 1, 1],
  ... 1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)
color_space = <ColorSpace.rgb: 0>, channels_order = <tensorflow_ChannelsOrder.CHANNELS_LAST: 1>

    @classmethod
    def from_numpy(
        cls,
        data,
        color_space=tensorflow_ColorSpace.RGB,
        channels_order=tensorflow_ChannelsOrder.CHANNELS_LAST,
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .base import tensorflow_ImageSize
        from .base import tensorflow_PixelFormat
        from ...ivy.data_classes.array.array import tensorflow_itemsize_bknd_
        from .base import tensorflow_ImageLayout
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_from_numpy_frnt,
        )
    
        if channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[0],
                width=tensorflow_shape_frnt_(data)[1],
            )
            channels = tensorflow_shape_frnt_(data)[2]
        elif channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[1],
                width=tensorflow_shape_frnt_(data)[2],
            )
            channels = tensorflow_shape_frnt_(data)[0]
        else:
            raise ValueError(
                "channels_order must be either `CHANNELS_LAST` or `CHANNELS_FIRST`"
            )
        pixel_format = tensorflow_PixelFormat(
            color_space=color_space, bit_depth=tensorflow_itemsize_bknd_(data) * 8
        )
        layout = tensorflow_ImageLayout(
            image_size=image_size, channels=channels, channels_order=channels_order
        )
>       return cls(tensorflow_from_numpy_frnt(data), pixel_format, layout)

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_Image' object has no attribute '_data'") raised in repr()] tensorflow_Image object at 0x7ff60a1a7580>
data = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
pixel_format = tensorflow_PixelFormat(color_space=<ColorSpace.rgb: 0>, bit_depth=8)
layout = tensorflow_ImageLayout(image_size=tensorflow_ImageSize(height=4, width=5), channels=3, channels_order=<tensorflow_ChannelsOrder.CHANNELS_LAST: 1>)

    def __init__(self, data, pixel_format, layout):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
    
        if layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            shape = [
                str(layout.channels),
                str(layout.image_size.height),
                str(layout.image_size.width),
            ]
        elif layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            shape = [
                str(layout.image_size.height),
                str(layout.image_size.width),
                str(layout.channels),
            ]
        else:
            raise NotImplementedError(
                f"Layout {layout.channels_order} not implemented."
            )
        tensorflow_KORNIA_CHECK_SHAPE(data, shape)
        tensorflow_KORNIA_CHECK(
>           data.element_size() == pixel_format.bit_depth // 8, "Invalid bit depth."
        )

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
name = 'element_size'

    def __getattr__(self, name):
      if name in {"T", "astype", "ravel", "transpose", "reshape", "clip", "size",
                  "tolist", "data"}:
        # TODO(wangpeng): Export the enable_numpy_behavior knob
        raise AttributeError(
            f"{type(self).__name__} object has no attribute '{name}'. " + """
          If you are looking for numpy-related methods, please run the following:
          tf.experimental.numpy.experimental_enable_numpy_behavior()
        """)
>     self.__getattribute__(name)
E     AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'

/opt/fw/tensorflow/tensorflow/python/framework/tensor.py:260: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image.from_numpy
_____________________________________________________________________________ test_Image_from_dlpack[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image_from_dlpack(target_framework, mode, backend_compile):
        print("kornia.image.Image.from_dlpack")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        x = np.ones((4, 5, 3))
>       img = TranspiledImage.from_dlpack(x.__dlpack__())

kornia/test_image.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'Translated_Outputs.tensorflow_outputs.kornia.image.image.tensorflow_Image'>, data = <capsule object "dltensor" at 0x7ff6092e3450>

    @classmethod
    def from_dlpack(cls, data):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .base import tensorflow_ImageSize
        from .base import tensorflow_PixelFormat
        from .base import tensorflow_ImageLayout
        from ...ivy.data_classes.array.creation import tensorflow_from_dlpack_bknd_
    
>       _data: typing.Any = tensorflow_from_dlpack_bknd_(torch.utils.dlpack, data)
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:188: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image.from_dlpack
______________________________________________________________________________ test_Image_to_numpy[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image_to_numpy(target_framework, mode, backend_compile):
        print("kornia.image.Image.to_numpy")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        data = np.ones((4, 5, 3), dtype=np.uint8)
>       img = TranspiledImage.from_numpy(data, color_space=ColorSpace.rgb)

kornia/test_image.py:160: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'Translated_Outputs.tensorflow_outputs.kornia.image.image.tensorflow_Image'>
data = array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]],

       [[1, 1, 1],
  ... 1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)
color_space = <ColorSpace.rgb: 0>, channels_order = <tensorflow_ChannelsOrder.CHANNELS_LAST: 1>

    @classmethod
    def from_numpy(
        cls,
        data,
        color_space=tensorflow_ColorSpace.RGB,
        channels_order=tensorflow_ChannelsOrder.CHANNELS_LAST,
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .base import tensorflow_ImageSize
        from .base import tensorflow_PixelFormat
        from ...ivy.data_classes.array.array import tensorflow_itemsize_bknd_
        from .base import tensorflow_ImageLayout
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_from_numpy_frnt,
        )
    
        if channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[0],
                width=tensorflow_shape_frnt_(data)[1],
            )
            channels = tensorflow_shape_frnt_(data)[2]
        elif channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[1],
                width=tensorflow_shape_frnt_(data)[2],
            )
            channels = tensorflow_shape_frnt_(data)[0]
        else:
            raise ValueError(
                "channels_order must be either `CHANNELS_LAST` or `CHANNELS_FIRST`"
            )
        pixel_format = tensorflow_PixelFormat(
            color_space=color_space, bit_depth=tensorflow_itemsize_bknd_(data) * 8
        )
        layout = tensorflow_ImageLayout(
            image_size=image_size, channels=channels, channels_order=channels_order
        )
>       return cls(tensorflow_from_numpy_frnt(data), pixel_format, layout)

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_Image' object has no attribute '_data'") raised in repr()] tensorflow_Image object at 0x7ff6093b3b20>
data = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
pixel_format = tensorflow_PixelFormat(color_space=<ColorSpace.rgb: 0>, bit_depth=8)
layout = tensorflow_ImageLayout(image_size=tensorflow_ImageSize(height=4, width=5), channels=3, channels_order=<tensorflow_ChannelsOrder.CHANNELS_LAST: 1>)

    def __init__(self, data, pixel_format, layout):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
    
        if layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            shape = [
                str(layout.channels),
                str(layout.image_size.height),
                str(layout.image_size.width),
            ]
        elif layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            shape = [
                str(layout.image_size.height),
                str(layout.image_size.width),
                str(layout.channels),
            ]
        else:
            raise NotImplementedError(
                f"Layout {layout.channels_order} not implemented."
            )
        tensorflow_KORNIA_CHECK_SHAPE(data, shape)
        tensorflow_KORNIA_CHECK(
>           data.element_size() == pixel_format.bit_depth // 8, "Invalid bit depth."
        )

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
name = 'element_size'

    def __getattr__(self, name):
      if name in {"T", "astype", "ravel", "transpose", "reshape", "clip", "size",
                  "tolist", "data"}:
        # TODO(wangpeng): Export the enable_numpy_behavior knob
        raise AttributeError(
            f"{type(self).__name__} object has no attribute '{name}'. " + """
          If you are looking for numpy-related methods, please run the following:
          tf.experimental.numpy.experimental_enable_numpy_behavior()
        """)
>     self.__getattribute__(name)
E     AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'

/opt/fw/tensorflow/tensorflow/python/framework/tensor.py:260: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image.to_numpy
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_image.py::test_Image[tensorflow-s2s-False] - AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'
FAILED kornia/test_image.py::test_Image_from_numpy[tensorflow-s2s-False] - AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'
FAILED kornia/test_image.py::test_Image_from_dlpack[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_image.py::test_Image_to_numpy[tensorflow-s2s-False] - AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'
=============================================================================== 4 failed, 4 passed in 186.58s (0:03:06) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/geometry/test_vector.py ..                                                                                                                                                                [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
===================================================================================== 2 passed in 84.67s (0:01:24) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 5 items

kornia/test_x.py sssss                                                                                                                                                                           [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 5 skipped in 110.05s (0:01:50) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 3 items

kornia/augmentation/test_auto.py FFF                                                                                                                                                             [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________________ test_AutoAugment[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_AutoAugment(target_framework, mode, backend_compile):
        print("kornia.augmentation.auto.AutoAugment")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledAutoAugment = ivy.transpile(
            kornia.augmentation.auto.AutoAugment,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_auto.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.auto.autoaugment.autoaugment.AutoAugment'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.auto.AutoAugment
________________________________________________________________________________ test_RandAugment[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandAugment(target_framework, mode, backend_compile):
        print("kornia.augmentation.auto.RandAugment")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledRandAugment = ivy.transpile(
            kornia.augmentation.auto.RandAugment,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_auto.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.auto.rand_augment.rand_augment.RandAugment'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.auto.RandAugment
______________________________________________________________________________ test_TrivialAugment[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_TrivialAugment(target_framework, mode, backend_compile):
        print("kornia.augmentation.auto.TrivialAugment")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledTrivialAugment = ivy.transpile(
            kornia.augmentation.auto.TrivialAugment,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_auto.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.auto.trivial_augment.trivial_augment.TrivialAugment'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.auto.TrivialAugment
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/augmentation/test_auto.py::test_AutoAugment[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix....
FAILED kornia/augmentation/test_auto.py::test_RandAugment[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix....
FAILED kornia/augmentation/test_auto.py::test_TrivialAugment[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.m...
==================================================================================== 3 failed in 312.21s (0:05:12) =====================================================================================


========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

transformers/test_vision.py .                                                                                                                                                                    [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 1 passed in 1016.38s (0:16:56) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

kornia/test_tracking.py F                                                                                                                                                                        [100%]

=============================================================================================== FAILURES ===============================================================================================
_____________________________________________________________________________ test_HomographyTracker[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HomographyTracker(target_framework, mode, backend_compile):
        print("kornia.tracking.HomographyTracker")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledHomographyTracker = ivy.transpile(kornia.tracking.HomographyTracker, source="torch", target=target_framework)

kornia/test_tracking.py:30: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.tracking.planar_tracker.HomographyTracker'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, line 77)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.tracking.HomographyTracker
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_tracking.py::test_HomographyTracker[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>...
==================================================================================== 1 failed in 680.21s (0:11:20) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 38 items

kornia/geometry/test_conversions.py ......................................                                                                                                                       [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 38 passed in 288.45s (0:04:48) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/test_nerf.py FF.FFFFF                                                                                                                                                                     [100%]

=============================================================================================== FAILURES ===============================================================================================
_________________________________________________________________________________ test_NerfModel[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_NerfModel(target_framework, mode, backend_compile):
        print("kornia.nerf.nerf_model.NerfModel")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledNerfModel = ivy.transpile(nerf_model.NerfModel, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(5, 3),
            torch.rand(5, 3),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_nerf = nerf_model.NerfModel(num_ray_points=32)
>       transpiled_nerf = TranspiledNerfModel(num_ray_points=32)

kornia/test_nerf.py:34: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_NerfModel(
  (_renderer): tensorflow_IrregularRenderer()
  (_pos_encoder): tensorflow_PositionalEncoder()
 ...ense()
  (_fc2): tensorflow_Sequential(
    (0): KerasDense()
    (1): tensorflow_ReLU()
  )
  (_sigma): KerasDense()
)
num_ray_points = 32, irregular_ray_sampling = True, num_pos_freqs = 10, num_dir_freqs = 4, num_units = 2, num_unit_layers = 4, num_hidden = 128, log_space_encoding = True

    def __init__(
        self,
        num_ray_points,
        irregular_ray_sampling=True,
        num_pos_freqs=10,
        num_dir_freqs=4,
        num_units=2,
        num_unit_layers=4,
        num_hidden=128,
        log_space_encoding=True,
    ):
        from .volume_renderer import tensorflow_IrregularRenderer
        from .volume_renderer import tensorflow_RegularRenderer
        from .positional_encoder import tensorflow_PositionalEncoder
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.init import tensorflow_xavier_uniform_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_data_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
        from ...torch.nn.modules.activation import tensorflow_Sigmoid
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            num_ray_points,
            irregular_ray_sampling=irregular_ray_sampling,
            num_pos_freqs=num_pos_freqs,
            num_dir_freqs=num_dir_freqs,
            num_units=num_units,
            num_unit_layers=num_unit_layers,
            num_hidden=num_hidden,
            log_space_encoding=log_space_encoding,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self._num_ray_points = num_ray_points
        self._irregular_ray_sampling = irregular_ray_sampling
        self._renderer = (
            tensorflow_IrregularRenderer()
            if self._irregular_ray_sampling
            else tensorflow_RegularRenderer()
        )
        self._pos_encoder = tensorflow_PositionalEncoder(
            3, num_pos_freqs, log_space=log_space_encoding
        )
        self._dir_encoder = tensorflow_PositionalEncoder(
            3, num_dir_freqs, log_space=log_space_encoding
        )
        self._mlp = tensorflow_MLP(
            self._pos_encoder.num_encoded_dims, num_units, num_unit_layers, num_hidden
        )
        self._fc1 = KerasDense(in_features=num_hidden, units=num_hidden, use_bias=True)
        self._fc2 = tensorflow_Sequential(
            KerasDense(
                in_features=num_hidden + self._dir_encoder.num_encoded_dims,
                units=num_hidden // 2,
                use_bias=True,
            ),
            tensorflow_ReLU(),
        )
        self._sigma = KerasDense(in_features=num_hidden, units=1, use_bias=True)
        self._sigma.weight = tensorflow_xavier_uniform_(
            tensorflow_data_frnt_(self._sigma.weight)
        )
>       self._sigma.bias.data = tensorflow_float_frnt_(tensorflow_tensor_frnt([0.1]))
E       AttributeError: can't set attribute

Translated_Outputs/tensorflow_outputs/kornia/nerf/nerf_model.py:507: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.nerf_model.NerfModel
_____________________________________________________________________________ test_NerfModelRenderer[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_NerfModelRenderer(target_framework, mode, backend_compile):
        print("kornia.nerf.nerf_model.NerfModelRenderer")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)
        TranspiledNerfModel = ivy.transpile(nerf_model.NerfModel, source="torch", target=target_framework)
        TranspiledNerfModelRenderer = ivy.transpile(nerf_model.NerfModelRenderer, source="torch", target=target_framework)
    
        torch_nerf_model = nerf_model.NerfModel(num_ray_points=32)
>       transpiled_nerf_model = TranspiledNerfModel(num_ray_points=32)

kornia/test_nerf.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_NerfModel(
  (_renderer): tensorflow_IrregularRenderer()
  (_pos_encoder): tensorflow_PositionalEncoder()
 ...ense()
  (_fc2): tensorflow_Sequential(
    (0): KerasDense()
    (1): tensorflow_ReLU()
  )
  (_sigma): KerasDense()
)
num_ray_points = 32, irregular_ray_sampling = True, num_pos_freqs = 10, num_dir_freqs = 4, num_units = 2, num_unit_layers = 4, num_hidden = 128, log_space_encoding = True

    def __init__(
        self,
        num_ray_points,
        irregular_ray_sampling=True,
        num_pos_freqs=10,
        num_dir_freqs=4,
        num_units=2,
        num_unit_layers=4,
        num_hidden=128,
        log_space_encoding=True,
    ):
        from .volume_renderer import tensorflow_IrregularRenderer
        from .volume_renderer import tensorflow_RegularRenderer
        from .positional_encoder import tensorflow_PositionalEncoder
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.init import tensorflow_xavier_uniform_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_data_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
        from ...torch.nn.modules.activation import tensorflow_Sigmoid
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            num_ray_points,
            irregular_ray_sampling=irregular_ray_sampling,
            num_pos_freqs=num_pos_freqs,
            num_dir_freqs=num_dir_freqs,
            num_units=num_units,
            num_unit_layers=num_unit_layers,
            num_hidden=num_hidden,
            log_space_encoding=log_space_encoding,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self._num_ray_points = num_ray_points
        self._irregular_ray_sampling = irregular_ray_sampling
        self._renderer = (
            tensorflow_IrregularRenderer()
            if self._irregular_ray_sampling
            else tensorflow_RegularRenderer()
        )
        self._pos_encoder = tensorflow_PositionalEncoder(
            3, num_pos_freqs, log_space=log_space_encoding
        )
        self._dir_encoder = tensorflow_PositionalEncoder(
            3, num_dir_freqs, log_space=log_space_encoding
        )
        self._mlp = tensorflow_MLP(
            self._pos_encoder.num_encoded_dims, num_units, num_unit_layers, num_hidden
        )
        self._fc1 = KerasDense(in_features=num_hidden, units=num_hidden, use_bias=True)
        self._fc2 = tensorflow_Sequential(
            KerasDense(
                in_features=num_hidden + self._dir_encoder.num_encoded_dims,
                units=num_hidden // 2,
                use_bias=True,
            ),
            tensorflow_ReLU(),
        )
        self._sigma = KerasDense(in_features=num_hidden, units=1, use_bias=True)
        self._sigma.weight = tensorflow_xavier_uniform_(
            tensorflow_data_frnt_(self._sigma.weight)
        )
>       self._sigma.bias.data = tensorflow_float_frnt_(tensorflow_tensor_frnt([0.1]))
E       AttributeError: can't set attribute

Translated_Outputs/tensorflow_outputs/kornia/nerf/nerf_model.py:507: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.nerf_model.NerfModelRenderer
________________________________________________________________________________ test_NerfSolver[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_NerfSolver(target_framework, mode, backend_compile):
        print("kornia.nerf.nerf_solver.NerfSolver")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)
>       TranspiledNerfSolver = ivy.transpile(nerf_solver.NerfSolver, source="torch", target=target_framework)

kornia/test_nerf.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.nerf.nerf_solver.NerfSolver'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.nerf_solver.NerfSolver
_____________________________________________________________________________ test_IrregularRenderer[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_IrregularRenderer(target_framework, mode, backend_compile):
        print("kornia.nerf.volume_renderer.IrregularRenderer")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledIrregularRenderer = ivy.transpile(volume_renderer.IrregularRenderer, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(5, 32, 3),
            torch.rand(5, 32, 1),
            torch.rand(5, 32, 3),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_renderer = volume_renderer.IrregularRenderer(shift=1)
        transpiled_renderer = TranspiledIrregularRenderer(shift=1)
    
        torch_out = torch_renderer(*torch_args)
>       transpiled_out = transpiled_renderer(*transpiled_args)

kornia/test_nerf.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer()
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.02243781, 0.7489689 , 0.72849673],
        [0.4713288 ...7646  ],
        [0.27759886, 0.43039072, 0.4141689 ],
        [0.9172771 , 0.7436104 , 0.22631794]]], dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f7083a37300, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_IrregularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.02243781, 0.7489689 , ...7646  ],
        [0.27759886, 0.43039072, 0.4141689 ],
        [0.9172771 , 0.7436104 , 0.22631794]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.02243781, 0.7489689 , 0.72849673],
        [0.4713288 ...7646  ],
        [0.27759886, 0.43039072, 0.4141689 ],
        [0.9172771 , 0.7436104 , 0.22631794]]], dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_IrregularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.02243781, 0.7489689 , ...7646  ],
        [0.27759886, 0.43039072, 0.4141689 ],
        [0.9172771 , 0.7436104 , 0.22631794]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.02243781, 0.7489689 , 0.72849673],
        [0.4713288 ...7646  ],
        [0.27759886, 0.43039072, 0.4141689 ],
        [0.9172771 , 0.7436104 , 0.22631794]]], dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.02243781, 0.7489689 , 0.72849673],
        [0.4713288 ,...0193878],
        [0.0499199 , 0.04994112, 0.45472693],
        [0.94946283, 0.02115643, 0.6599906 ]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (rgbs, densities, points_3d)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_IrregularRenderer(),)
kwargs = {'densities': <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[0.3066141 ],
        [0.9337965 ],
        ...193878],
        [0.0499199 , 0.04994112, 0.45472693],
        [0.94946283, 0.02115643, 0.6599906 ]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer()
rgbs = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.02243781, 0.7489689 , 0.72849673],
        [0.4713288 ,...0193878],
        [0.0499199 , 0.04994112, 0.45472693],
        [0.94946283, 0.02115643, 0.6599906 ]]], dtype=float32)>
densities = <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[0.3066141 ],
        [0.9337965 ],
        [0.65803933],
...[0.9535489 ],
        [0.49709624],
        [0.14642662],
        [0.16758305],
        [0.5172693 ]]], dtype=float32)>
points_3d = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.97005886, 0.04118413, 0.6392498 ],
        [0.92608106,...57646  ],
        [0.27759886, 0.43039072, 0.4141689 ],
        [0.9172771 , 0.7436104 , 0.22631794]]], dtype=float32)>

    def call(self, rgbs, densities, points_3d):
        from .samplers import tensorflow_calc_ray_t_vals
        from ...ivy.functional.frontends.torch.tensor import tensorflow_fill__frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_empty_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_exp_frnt
    
>       t_vals = tensorflow_calc_ray_t_vals(points_3d)

Translated_Outputs/tensorflow_outputs/kornia/nerf/volume_renderer.py:443: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

points_3d = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.97005886, 0.04118413, 0.6392498 ],
        [0.92608106,...57646  ],
        [0.27759886, 0.43039072, 0.4141689 ],
        [0.9172771 , 0.7436104 , 0.22631794]]], dtype=float32)>

    def tensorflow_calc_ray_t_vals(points_3d):
        from ...ivy.functional.frontends.torch.linalg import tensorflow_norm_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
    
>       t_vals = tensorflow_norm_frnt(
            points_3d - tensorflow_unsqueeze_frnt_(points_3d[..., 0, :], -2), dim=-1
        )

Translated_Outputs/tensorflow_outputs/kornia/nerf/samplers.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[ 0.        ,  0.        ,  0.        ],
        [-0.04397...3],
        [-0.3027649 , -0.060215  , -0.20401281],
        [ 0.33691335,  0.25300467, -0.39186376]]], dtype=float32)>
ord = None, dim = -1, keepdim = False

    def tensorflow_norm_frnt(
        input, ord=None, dim=None, keepdim=False, *, dtype=None, out=None
    ):
        from .tensor import tensorflow_ndim_frnt_
        from ...backends.tensorflow.linear_algebra import tensorflow_vector_norm
        from ...backends.tensorflow.linear_algebra import tensorflow_matrix_norm
        from ...backends.tensorflow.manipulation import tensorflow_flatten
        from ...backends.tensorflow.data_type import tensorflow_astype
    
        if dim is None and ord is not None:
            if tensorflow_ndim_frnt_(input) == 1:
                ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)
            else:
                ret = tensorflow_matrix_norm(input, keepdims=keepdim, ord=ord)
        elif dim is None and ord is None:
            input = tensorflow_flatten(input)
            ret = tensorflow_vector_norm(input, axis=0, keepdims=keepdim, ord=2)
        elif isinstance(dim, (int,)):
>           ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/linalg.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[ 0.        ,  0.        ,  0.        ],
        [-0.0439...],
        [-0.3027649 , -0.060215  , -0.20401281],
        [ 0.33691335,  0.25300467, -0.39186376]]], dtype=float32)>]
kwargs = {'axis': -1, 'keepdims': False, 'ord': None}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f708318b0a0>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f708318b760>, tensorflow_asarray = <function tensorflow_asarray at 0x7f70831067a0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f70831040d0>, num_args = 1
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'axis', 'keepdims', 'ord', 'dtype', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[int, ...es.DType], typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 0

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[ 0.        ,  0.        ,  0.        ],
        [-0.04397...3],
        [-0.3027649 , -0.060215  , -0.20401281],
        [ 0.33691335,  0.25300467, -0.39186376]]], dtype=float32)>

    @tensorflow_handle_array_like_without_promotion
    def tensorflow_vector_norm(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        axis: Optional[Union[int, Sequence[int]]] = None,
        keepdims: bool = False,
        ord: Union[int, float, Literal[inf, -inf]] = 2,
        dtype: Optional[tf.DType] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.constants import inf
    
        if dtype and x.dtype != dtype:
            x = tensorflow.cast(x, dtype)
        abs_x = tensorflow.abs(x)
        if ord == 0:
            return tensorflow.reduce_sum(
                tensorflow.cast(x != 0, abs_x.dtype), axis=axis, keepdims=keepdims
            )
        elif ord == inf:
            return tensorflow.reduce_max(abs_x, axis=axis, keepdims=keepdims)
        elif ord == -inf:
            return tensorflow.reduce_min(abs_x, axis=axis, keepdims=keepdims)
        else:
>           return tensorflow.reduce_sum(abs_x**ord, axis=axis, keepdims=keepdims) ** (
                1.0 / ord
            )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/linear_algebra.py:200: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.        , 0.        , 0.        ],
        [0.0439778 ...],
        [0.3027649 , 0.060215  , 0.20401281],
        [0.33691335, 0.25300467, 0.39186376]]], dtype=float32)>, None)
kwargs = {}, arg = None

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       ValueError: Exception encountered when calling tensorflow_IrregularRenderer.call().
E       
E       [1mAttempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.[0m
E       
E       Arguments received by tensorflow_IrregularRenderer.call():
E         â€¢ rgbs=tf.Tensor(shape=(5, 32, 3), dtype=float32)
E         â€¢ densities=tf.Tensor(shape=(5, 32, 1), dtype=float32)
E         â€¢ points_3d=tf.Tensor(shape=(5, 32, 3), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.volume_renderer.IrregularRenderer
______________________________________________________________________________ test_RegularRenderer[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RegularRenderer(target_framework, mode, backend_compile):
        print("kornia.nerf.volume_renderer.RegularRenderer")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledRegularRenderer = ivy.transpile(volume_renderer.RegularRenderer, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(5, 32, 3),
            torch.rand(5, 32, 1),
            torch.rand(5, 32, 3),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_renderer = volume_renderer.RegularRenderer(shift=1)
        transpiled_renderer = TranspiledRegularRenderer(shift=1)
    
        torch_out = torch_renderer(*torch_args)
>       transpiled_out = transpiled_renderer(*transpiled_args)

kornia/test_nerf.py:179: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer()
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9073085 , 0.4123749 , 0.6755465 ],
        [0.16822547...2730e-01, 9.67414916e-01, 9.67681825e-01],
        [9.60185468e-01, 5.18891394e-01, 1.03143454e-02]]], dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x5650c07b6e00, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RegularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9073085 , 0.4123749 , 0....2730e-01, 9.67414916e-01, 9.67681825e-01],
        [9.60185468e-01, 5.18891394e-01, 1.03143454e-02]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9073085 , 0.4123749 , 0.6755465 ],
        [0.16822547...2730e-01, 9.67414916e-01, 9.67681825e-01],
        [9.60185468e-01, 5.18891394e-01, 1.03143454e-02]]], dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RegularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9073085 , 0.4123749 , 0....2730e-01, 9.67414916e-01, 9.67681825e-01],
        [9.60185468e-01, 5.18891394e-01, 1.03143454e-02]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9073085 , 0.4123749 , 0.6755465 ],
        [0.16822547...2730e-01, 9.67414916e-01, 9.67681825e-01],
        [9.60185468e-01, 5.18891394e-01, 1.03143454e-02]]], dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9073085 , 0.4123749 , 0.6755465 ],
        [0.16822547,...923839 ],
        [0.1225602 , 0.861789  , 0.27510566],
        [0.977179  , 0.6468071 , 0.2874992 ]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (rgbs, densities, points_3d)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RegularRenderer(),)
kwargs = {'densities': <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[7.74466991e-03],
        [7.98890412e-01],
...23839 ],
        [0.1225602 , 0.861789  , 0.27510566],
        [0.977179  , 0.6468071 , 0.2874992 ]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer()
rgbs = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.9073085 , 0.4123749 , 0.6755465 ],
        [0.16822547,...923839 ],
        [0.1225602 , 0.861789  , 0.27510566],
        [0.977179  , 0.6468071 , 0.2874992 ]]], dtype=float32)>
densities = <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[7.74466991e-03],
        [7.98890412e-01],
        [3.529...      [4.18312013e-01],
        [5.53436816e-01],
        [3.24428320e-01],
        [5.26440024e-01]]], dtype=float32)>
points_3d = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[5.19406557e-01, 8.61300766e-01, 7.60685384e-01],
        ...22730e-01, 9.67414916e-01, 9.67681825e-01],
        [9.60185468e-01, 5.18891394e-01, 1.03143454e-02]]], dtype=float32)>

    def call(self, rgbs, densities, points_3d):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.linalg import tensorflow_norm_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_exp_frnt
    
        tensorflow_KORNIA_CHECK_SHAPE(rgbs, ["*", "N", "3"])
        tensorflow_KORNIA_CHECK_SHAPE(densities, ["*", "N"])
        tensorflow_KORNIA_CHECK_SHAPE(points_3d, ["*", "N", "3"])
        num_ray_points: typing.Any = tensorflow_shape_frnt_(points_3d)[-2]
        points_3d = tensorflow_reshape_frnt_(points_3d, -1, num_ray_points, 3)
        delta_3d = points_3d[0, 1, :] - points_3d[0, 0, :]
>       delta = tensorflow_norm_frnt(delta_3d, dim=-1)

Translated_Outputs/tensorflow_outputs/kornia/nerf/volume_renderer.py:472: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.3134575 , -0.06188077,  0.23825729], dtype=float32)>, ord = None, dim = -1, keepdim = False

    def tensorflow_norm_frnt(
        input, ord=None, dim=None, keepdim=False, *, dtype=None, out=None
    ):
        from .tensor import tensorflow_ndim_frnt_
        from ...backends.tensorflow.linear_algebra import tensorflow_vector_norm
        from ...backends.tensorflow.linear_algebra import tensorflow_matrix_norm
        from ...backends.tensorflow.manipulation import tensorflow_flatten
        from ...backends.tensorflow.data_type import tensorflow_astype
    
        if dim is None and ord is not None:
            if tensorflow_ndim_frnt_(input) == 1:
                ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)
            else:
                ret = tensorflow_matrix_norm(input, keepdims=keepdim, ord=ord)
        elif dim is None and ord is None:
            input = tensorflow_flatten(input)
            ret = tensorflow_vector_norm(input, axis=0, keepdims=keepdim, ord=2)
        elif isinstance(dim, (int,)):
>           ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/linalg.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.3134575 , -0.06188077,  0.23825729], dtype=float32)>], kwargs = {'axis': -1, 'keepdims': False, 'ord': None}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f708318b0a0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f708318b760>
tensorflow_asarray = <function tensorflow_asarray at 0x7f70831067a0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f70831040d0>, num_args = 1
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'axis', 'keepdims', 'ord', 'dtype', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[int, ...es.DType], typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 0

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.3134575 , -0.06188077,  0.23825729], dtype=float32)>

    @tensorflow_handle_array_like_without_promotion
    def tensorflow_vector_norm(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        axis: Optional[Union[int, Sequence[int]]] = None,
        keepdims: bool = False,
        ord: Union[int, float, Literal[inf, -inf]] = 2,
        dtype: Optional[tf.DType] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.constants import inf
    
        if dtype and x.dtype != dtype:
            x = tensorflow.cast(x, dtype)
        abs_x = tensorflow.abs(x)
        if ord == 0:
            return tensorflow.reduce_sum(
                tensorflow.cast(x != 0, abs_x.dtype), axis=axis, keepdims=keepdims
            )
        elif ord == inf:
            return tensorflow.reduce_max(abs_x, axis=axis, keepdims=keepdims)
        elif ord == -inf:
            return tensorflow.reduce_min(abs_x, axis=axis, keepdims=keepdims)
        else:
>           return tensorflow.reduce_sum(abs_x**ord, axis=axis, keepdims=keepdims) ** (
                1.0 / ord
            )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/linear_algebra.py:200: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.3134575 , 0.06188077, 0.23825729], dtype=float32)>, None), kwargs = {}, arg = None

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       ValueError: Exception encountered when calling tensorflow_RegularRenderer.call().
E       
E       [1mAttempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.[0m
E       
E       Arguments received by tensorflow_RegularRenderer.call():
E         â€¢ rgbs=tf.Tensor(shape=(5, 32, 3), dtype=float32)
E         â€¢ densities=tf.Tensor(shape=(5, 32, 1), dtype=float32)
E         â€¢ points_3d=tf.Tensor(shape=(5, 32, 3), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.volume_renderer.RegularRenderer
________________________________________________________________________________ test_RaySampler[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RaySampler(target_framework, mode, backend_compile):
        print("kornia.nerf.samplers.RaySampler")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)

kornia/test_nerf.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.camera.pinhole.PinholeCamera'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.samplers.RaySampler
_____________________________________________________________________________ test_RandomRaySampler[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRaySampler(target_framework, mode, backend_compile):
        print("kornia.nerf.samplers.RandomRaySampler")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)

kornia/test_nerf.py:224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.camera.pinhole.PinholeCamera'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.samplers.RandomRaySampler
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_nerf.py::test_NerfModel[tensorflow-s2s-False] - AttributeError: can't set attribute
FAILED kornia/test_nerf.py::test_NerfModelRenderer[tensorflow-s2s-False] - AttributeError: can't set attribute
FAILED kornia/test_nerf.py::test_NerfSolver[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor'...
FAILED kornia/test_nerf.py::test_IrregularRenderer[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_IrregularRenderer.call().
FAILED kornia/test_nerf.py::test_RegularRenderer[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_RegularRenderer.call().
FAILED kornia/test_nerf.py::test_RaySampler[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor'...
FAILED kornia/test_nerf.py::test_RandomRaySampler[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'T...
=============================================================================== 7 failed, 1 passed in 243.38s (0:04:03) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 6 items

kornia/augmentation/test_container.py FFFFFF                                                                                                                                                     [100%]

=============================================================================================== FAILURES ===============================================================================================
__________________________________________________________________________ test_AugmentationSequential[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_AugmentationSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.AugmentationSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledAugmentationSequential = ivy.transpile(
            kornia.augmentation.container.AugmentationSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.augment.AugmentationSequential'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.AugmentationSequential
______________________________________________________________________ test_ManyToManyAugmentationDispather[tensorflow-s2s-False] ______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ManyToManyAugmentationDispather(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.ManyToManyAugmentationDispather")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledManyToManyAugmentationDispather = ivy.transpile(
            kornia.augmentation.container.ManyToManyAugmentationDispather,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:81: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.dispatcher.ManyToManyAugmentationDispather'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.ManyToManyAugmentationDispather
______________________________________________________________________ test_ManyToOneAugmentationDispather[tensorflow-s2s-False] _______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ManyToOneAugmentationDispather(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.ManyToOneAugmentationDispather")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledManyToOneAugmentationDispather = ivy.transpile(
            kornia.augmentation.container.ManyToOneAugmentationDispather,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.dispatcher.ManyToOneAugmentationDispather'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.ManyToOneAugmentationDispather
______________________________________________________________________________ test_ImageSequential[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ImageSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.ImageSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledImageSequential = ivy.transpile(
            kornia.augmentation.container.ImageSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.image.ImageSequential'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.ImageSequential
______________________________________________________________________________ test_PatchSequential[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_PatchSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.PatchSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledPatchSequential = ivy.transpile(
            kornia.augmentation.container.PatchSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.patch.PatchSequential'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.PatchSequential
______________________________________________________________________________ test_VideoSequential[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_VideoSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.VideoSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledVideoSequential = ivy.transpile(
            kornia.augmentation.container.VideoSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:376: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.video.VideoSequential'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.VideoSequential
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/augmentation/test_container.py::test_AugmentationSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augme...
FAILED kornia/augmentation/test_container.py::test_ManyToManyAugmentationDispather[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kor...
FAILED kornia/augmentation/test_container.py::test_ManyToOneAugmentationDispather[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.korn...
FAILED kornia/augmentation/test_container.py::test_ImageSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation...
FAILED kornia/augmentation/test_container.py::test_PatchSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation...
FAILED kornia/augmentation/test_container.py::test_VideoSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation...
==================================================================================== 6 failed in 324.66s (0:05:24) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 15 items

kornia/test_feature3.py .F......FFFFFFF                                                                                                                                                          [100%]

=============================================================================================== FAILURES ===============================================================================================
__________________________________________________________________________________ test_SOSNet[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SOSNet(target_framework, mode, backend_compile):
        print("kornia.feature.SOSNet")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledSOSNet = ivy.transpile(kornia.feature.SOSNet, source="torch", target=target_framework)

kornia/test_feature3.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.feature.sosnet.SOSNet'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_mixed_handler = local_response_norm.partial_mixed_handler = lambda x, size, **kwargs: size % 2 != 0

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SOSNet
__________________________________________________________________________ test_MultiResolutionDetector[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_MultiResolutionDetector(target_framework, mode, backend_compile):
        print("kornia.feature.MultiResolutionDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledMultiResolutionDetector = ivy.transpile(kornia.feature.MultiResolutionDetector, source="torch", target=target_framework)
        TranspiledKeyNet = ivy.transpile(kornia.feature.KeyNet, source="torch", target=target_framework)
    
        model = kornia.feature.KeyNet()
        transpiled_model = TranspiledKeyNet()
    
        x = torch.rand(1, 1, 32, 32) * 10.
        torch_out = kornia.feature.MultiResolutionDetector(model)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_out = TranspiledMultiResolutionDetector(transpiled_model)(transpiled_x)
    
>       _to_numpy_and_shape_allclose(torch_out, transpiled_out)

kornia/test_feature3.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[[15.6444,  0.0000, 13.5111],
          [-0.0000, 15.6444, 15.6444]],

         [[15.6444,  0.0000, 20.6222]....0000, 15.0000],
          [-0.0000, 22.0000, 15.0000]]]]), tensor([[0.0528, 0.0493, 0.0409]], grad_fn=<CatBackward0>))
transpiled_x = (<tf.Tensor: shape=(1, 2, 2, 3), dtype=float32, numpy=
array([[[[15.644444,  0.      , 14.933333],
         [-0.      ...]]], dtype=float32)>, <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.27493322, 0.17156972]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_shape_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_shape_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[15.644444,  0.      , 13.511111],
         [-0.      , 15.644444, 15.644444]],

        [[15.644444,  0.   ...    [-0.      , 22.      , 15.      ]]]], dtype=float32), array([[0.05280389, 0.04925548, 0.04094767]], dtype=float32))
y = (array([[[[15.644444,  0.      , 14.933333],
         [-0.      , 15.644444, 16.355556]],

        [[22.      ,  0.   ...    ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32), array([[0.27493322, 0.17156972]], dtype=float32))
tolerance = 0.001

    def _check_shape_allclose(x, y, tolerance=1e-3):
        """
        Checks that all array shapes are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x.shape, y.shape, atol=tolerance), "numpy array shapes are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7fe50f3b2340>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[15.644444,  0.      , 13.511111],
         [-0.      , 15.644444, 15.644444]],

        [[15.644444,  0.    ...14.222222]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
y = array([[[[15.644444,  0.      , 14.933333],
         [-0.      , 15.644444, 16.355556]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"

helpers.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[15.644444,  0.      , 13.511111],
         [-0.      , 15.644444, 15.644444]],

        [[15.644444,  0.    ...14.222222]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
b = array([[[[15.644444,  0.      , 14.933333],
         [-0.      , 15.644444, 16.355556]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
rtol = 1e-05, atol = 0.001, equal_nan = False

    @array_function_dispatch(_allclose_dispatcher)
    def allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
        """
        Returns True if two arrays are element-wise equal within a tolerance.
    
        The tolerance values are positive, typically very small numbers.  The
        relative difference (`rtol` * abs(`b`)) and the absolute difference
        `atol` are added together to compare against the absolute difference
        between `a` and `b`.
    
        NaNs are treated as equal if they are in the same place and if
        ``equal_nan=True``.  Infs are treated as equal if they are in the same
        place and of the same sign in both arrays.
    
        Parameters
        ----------
        a, b : array_like
            Input arrays to compare.
        rtol : float
            The relative tolerance parameter (see Notes).
        atol : float
            The absolute tolerance parameter (see Notes).
        equal_nan : bool
            Whether to compare NaN's as equal.  If True, NaN's in `a` will be
            considered equal to NaN's in `b` in the output array.
    
            .. versionadded:: 1.10.0
    
        Returns
        -------
        allclose : bool
            Returns True if the two arrays are equal within the given
            tolerance; False otherwise.
    
        See Also
        --------
        isclose, all, any, equal
    
        Notes
        -----
        If the following equation is element-wise True, then allclose returns
        True.
    
         absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
    
        The above equation is not symmetric in `a` and `b`, so that
        ``allclose(a, b)`` might be different from ``allclose(b, a)`` in
        some rare cases.
    
        The comparison of `a` and `b` uses standard broadcasting, which
        means that `a` and `b` need not have the same shape in order for
        ``allclose(a, b)`` to evaluate to True.  The same is true for
        `equal` but not `array_equal`.
    
        `allclose` is not defined for non-numeric data types.
        `bool` is considered a numeric data-type for this purpose.
    
        Examples
        --------
        >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])
        False
        >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])
        True
        >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])
        False
        >>> np.allclose([1.0, np.nan], [1.0, np.nan])
        False
        >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
        True
    
        """
>       res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))

/opt/fw/mxnet/numpy/core/numeric.py:2241: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[15.644444,  0.      , 13.511111],
         [-0.      , 15.644444, 15.644444]],

        [[15.644444,  0.    ...14.222222]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
b = array([[[[15.644444,  0.      , 14.933333],
         [-0.      , 15.644444, 16.355556]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
rtol = 1e-05, atol = 0.001, equal_nan = False

    @array_function_dispatch(_isclose_dispatcher)
    def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
        """
        Returns a boolean array where two arrays are element-wise equal within a
        tolerance.
    
        The tolerance values are positive, typically very small numbers.  The
        relative difference (`rtol` * abs(`b`)) and the absolute difference
        `atol` are added together to compare against the absolute difference
        between `a` and `b`.
    
        .. warning:: The default `atol` is not appropriate for comparing numbers
                     that are much smaller than one (see Notes).
    
        Parameters
        ----------
        a, b : array_like
            Input arrays to compare.
        rtol : float
            The relative tolerance parameter (see Notes).
        atol : float
            The absolute tolerance parameter (see Notes).
        equal_nan : bool
            Whether to compare NaN's as equal.  If True, NaN's in `a` will be
            considered equal to NaN's in `b` in the output array.
    
        Returns
        -------
        y : array_like
            Returns a boolean array of where `a` and `b` are equal within the
            given tolerance. If both `a` and `b` are scalars, returns a single
            boolean value.
    
        See Also
        --------
        allclose
        math.isclose
    
        Notes
        -----
        .. versionadded:: 1.7.0
    
        For finite values, isclose uses the following equation to test whether
        two floating point values are equivalent.
    
         absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
    
        Unlike the built-in `math.isclose`, the above equation is not symmetric
        in `a` and `b` -- it assumes `b` is the reference value -- so that
        `isclose(a, b)` might be different from `isclose(b, a)`. Furthermore,
        the default value of atol is not zero, and is used to determine what
        small values should be considered close to zero. The default value is
        appropriate for expected values of order unity: if the expected values
        are significantly smaller than one, it can result in false positives.
        `atol` should be carefully selected for the use case at hand. A zero value
        for `atol` will result in `False` if either `a` or `b` is zero.
    
        `isclose` is not defined for non-numeric data types.
        `bool` is considered a numeric data-type for this purpose.
    
        Examples
        --------
        >>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])
        array([ True, False])
        >>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])
        array([ True, True])
        >>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])
        array([False,  True])
        >>> np.isclose([1.0, np.nan], [1.0, np.nan])
        array([ True, False])
        >>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
        array([ True, True])
        >>> np.isclose([1e-8, 1e-7], [0.0, 0.0])
        array([ True, False])
        >>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)
        array([False, False])
        >>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])
        array([ True,  True])
        >>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)
        array([False,  True])
        """
        def within_tol(x, y, atol, rtol):
            with errstate(invalid='ignore'), _no_nep50_warning():
                return less_equal(abs(x-y), atol + rtol * abs(y))
    
        x = asanyarray(a)
        y = asanyarray(b)
    
        # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
        # This will cause casting of x later. Also, make sure to allow subclasses
        # (e.g., for numpy.ma).
        # NOTE: We explicitly allow timedelta, which used to work. This could
        #       possibly be deprecated. See also gh-18286.
        #       timedelta works if `atol` is an integer or also a timedelta.
        #       Although, the default tolerances are unlikely to be useful
        if y.dtype.kind != "m":
            dt = multiarray.result_type(y, 1.)
            y = asanyarray(y, dtype=dt)
    
        xfin = isfinite(x)
        yfin = isfinite(y)
        if all(xfin) and all(yfin):
>           return within_tol(x, y, atol, rtol)

/opt/fw/mxnet/numpy/core/numeric.py:2351: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[15.644444,  0.      , 13.511111],
         [-0.      , 15.644444, 15.644444]],

        [[15.644444,  0.    ...14.222222]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
y = array([[[[15.644444,  0.      , 14.933333],
         [-0.      , 15.644444, 16.355556]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
atol = 0.001, rtol = 1e-05

    def within_tol(x, y, atol, rtol):
        with errstate(invalid='ignore'), _no_nep50_warning():
>           return less_equal(abs(x-y), atol + rtol * abs(y))
E           ValueError: operands could not be broadcast together with shapes (1,3,2,3) (1,2,2,3)

/opt/fw/mxnet/numpy/core/numeric.py:2332: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.MultiResolutionDetector
____________________________________________________________________________ test_ScaleSpaceDetector[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ScaleSpaceDetector(target_framework, mode, backend_compile):
        print("kornia.feature.ScaleSpaceDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledScaleSpaceDetector = ivy.transpile(kornia.feature.ScaleSpaceDetector, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 32, 32) * 10.
        torch_out = kornia.feature.ScaleSpaceDetector()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledScaleSpaceDetector()(transpiled_x)

kornia/test_feature3.py:186: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.23702264, 1.1609942 , 3.9926064 , ..., 4.611556  ...
         [9.923333  , 6.595364  , 0.70981324, ..., 2.3261456 ,
          9.395992  , 4.8350863 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x561fdaa61570, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...,
         [9.923333  , 6.595364  , 0.70981324, ..., 2.3261456 ,
          9.395992  , 4.8350863 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.23702264, 1.1609942 , 3.9926064 , ..., 4.611556  ...
         [9.923333  , 6.595364  , 0.70981324, ..., 2.3261456 ,
          9.395992  , 4.8350863 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...,
         [9.923333  , 6.595364  , 0.70981324, ..., 2.3261456 ,
          9.395992  , 4.8350863 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.23702264, 1.1609942 , 3.9926064 , ..., 4.611556  ...
         [9.923333  , 6.595364  , 0.70981324, ..., 2.3261456 ,
          9.395992  , 4.8350863 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.23702264, 1.1609942 , 3.9926064 , ..., 4.611556  ,...],
         [9.923333  , 6.595364  , 0.70981324, ..., 2.3261456 ,
          9.395992  , 4.8350863 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (img, mask=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...es=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF()),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.23702264, 1.1609942 , 3.9926064 , ..., 4.6...,
         [9.923333  , 6.595364  , 0.70981324, ..., 2.3261456 ,
          9.395992  , 4.8350863 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
img = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.23702264, 1.1609942 , 3.9926064 , ..., 4.611556  ,...],
         [9.923333  , 6.595364  , 0.70981324, ..., 2.3261456 ,
          9.395992  , 4.8350863 ]]]], dtype=float32)>
mask = None

    def call(self, img, mask=None):
>       responses, lafs = self.detect(img, self.num_features, mask)

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:274: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
img = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.23702264, 1.1609942 , 3.9926064 , ..., 4.611556  ,...],
         [9.923333  , 6.595364  , 0.70981324, ..., 2.3261456 ,
          9.395992  , 4.8350863 ]]]], dtype=float32)>
num_feats = 500, mask = None

    def detect(self, img, num_feats, mask=None):
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.comparison_ops import (
            tensorflow_topk_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from .laf import tensorflow_laf_is_inside_image
        from ..core._backend import eye
        from ..core._backend import concatenate
    
        dev: typing.Any = img.device
        dtype: typing.Any = img.dtype
        sigmas: typing.Any
        sp, sigmas, _ = self.scale_pyr(img)
        all_responses: typing.Any = []
        all_lafs: typing.Any = []
        px_size = 0.5 if self.scale_pyr.double_image else 1.0
        for oct_idx, octave in enumerate(sp):
            sigmas_oct = tensorflow_get_item(sigmas, oct_idx)
            B, CH, L, H, W = tensorflow_size_frnt_(octave)
            if self.scale_space_response:
                oct_resp = self.resp(octave, tensorflow_view_frnt_(sigmas_oct, -1))
            else:
                oct_resp = tensorflow_view_frnt_(
                    self.resp(
                        tensorflow_reshape_frnt_(
                            tensorflow_permute_frnt_(octave, 0, 2, 1, 3, 4),
                            B * L,
                            CH,
                            H,
                            W,
                        ),
                        tensorflow_view_frnt_(sigmas_oct, -1),
                    ),
                    B,
                    L,
                    CH,
                    H,
                    W,
                )
                oct_resp = tensorflow_permute_frnt_(oct_resp, 0, 2, 1, 3, 4)
                if (
                    isinstance(
                        self.scale_pyr.extra_levels,
                        (tensorflow.Tensor, tensorflow.Variable),
                    )
                    and self.scale_pyr.extra_levels % 2 != 0
                ):
                    oct_resp = oct_resp[:, :, :-1]
            if mask is not None:
                oct_mask: typing.Any = tensorflow__create_octave_mask(
                    mask, tensorflow_shape_frnt_(oct_resp)
                )
                oct_resp = oct_mask * oct_resp
            coord_max: typing.Any
            response_max: typing.Any
            coord_max, response_max = self.nms(oct_resp)
            if self.minima_are_also_good:
                coord_min, response_min = self.nms(-oct_resp)
                take_min_mask = tensorflow_to_frnt_(
                    response_min > response_max, response_max.dtype
                )
                response_max = (
                    response_min * take_min_mask + (1 - take_min_mask) * response_max
                )
                coord_max = (
                    coord_min * tensorflow_unsqueeze_frnt_(take_min_mask, 2)
                    + (1 - tensorflow_unsqueeze_frnt_(take_min_mask, 2)) * coord_max
                )
            responses_flatten = tensorflow_view_frnt_(
                response_max, tensorflow_size_frnt_(response_max, 0), -1
            )
            max_coords_flatten = tensorflow_permute_frnt_(
                tensorflow_view_frnt_(
                    coord_max, tensorflow_size_frnt_(response_max, 0), 3, -1
                ),
                0,
                2,
                1,
            )
            if tensorflow_size_frnt_(responses_flatten, 1) > num_feats:
                resp_flat_best, idxs = tensorflow_topk_frnt(
                    responses_flatten, k=num_feats, dim=1
                )
                max_coords_best = tensorflow_gather_frnt(
                    max_coords_flatten,
                    1,
                    tensorflow_repeat_frnt_(
                        tensorflow_unsqueeze_frnt_(idxs, -1), 1, 1, 3
                    ),
                )
            else:
                resp_flat_best = responses_flatten
                max_coords_best = max_coords_flatten
            B, N = tensorflow_size_frnt_(resp_flat_best)
            if isinstance(
                self.scale_pyr.n_levels, (tensorflow.Tensor, tensorflow.Variable)
            ):
                num_levels = int(tensorflow_item_frnt_(self.scale_pyr.n_levels))
            elif isinstance(self.scale_pyr.n_levels, (int,)):
                num_levels = self.scale_pyr.n_levels
            else:
                raise TypeError(
                    f"Expected the scale pyramid module to have `n_levels` as a Tensor or int.Gotcha {type(self.scale_pyr.n_levels)}"
                )
            max_coords_best = tensorflow__scale_index_to_scale(
                max_coords_best, sigmas_oct, num_levels
            )
            rotmat = tensorflow_view_frnt_(eye(2, dtype=dtype, device=dev), 1, 1, 2, 2)
            current_lafs = concatenate(
                [
                    self.mr_size
                    * tensorflow_view_frnt_(max_coords_best[:, :, 0], B, N, 1, 1)
                    * rotmat,
                    tensorflow_view_frnt_(max_coords_best[:, :, 1:3], B, N, 2, 1),
                ],
                3,
            )
>           good_mask = tensorflow_laf_is_inside_image(current_lafs, octave[:, 0])

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:251: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

laf = <tf.Tensor: shape=(1, 500, 2, 3), dtype=float32, numpy=
array([[[[12.137152 ,  0.       , 16.98125  ],
         [ 0.  ...36]],

        [[27.181969 ,  0.       , 30.49623  ],
         [ 0.       , 27.181969 ,  8.011764 ]]]], dtype=float32)>
images = <tf.Tensor: shape=(1, 6, 32, 32), dtype=float32, numpy=
array([[[[4.34874  , 4.5107946, 4.8571286, ..., 3.5373724, 3.4...380867],
         [5.0885024, 5.093371 , 5.107723 , ..., 5.221513 , 5.2359314,
          5.2410455]]]], dtype=float32)>
border = 0

    def tensorflow_laf_is_inside_image(laf, images, border=0):
        from ..core.check import tensorflow_KORNIA_CHECK_LAF
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
    
        tensorflow_KORNIA_CHECK_LAF(laf)
        _, _, h, w = tensorflow_size_frnt_(images)
        pts = tensorflow_laf_to_boundary_points(laf, 12)
        good_lafs_mask = (
>           (pts[..., 0] >= border)
            * (pts[..., 0] <= w - border)
            * (pts[..., 1] >= border)
            * (pts[..., 1] <= h - border)
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/laf.py:145: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 500, 12), dtype=bool, numpy=
array([[[ True,  True,  True, ...,  True,  True,  True],
        [...,
        [ True,  True, False, ...,  True,  True,  True],
        [ True,  True, False, ...,  True,  True,  True]]])>)
kwargs = {}
arg = <tf.Tensor: shape=(1, 500, 12), dtype=bool, numpy=
array([[[ True,  True,  True, ...,  True,  True,  True],
        [ ...],
        [ True,  True, False, ...,  True,  True,  True],
        [ True,  True, False, ...,  True,  True,  True]]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_ScaleSpaceDetector.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_ScaleSpaceDetector.call():
E         â€¢ img=tf.Tensor(shape=(1, 1, 32, 32), dtype=float32)
E         â€¢ mask=None

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.ScaleSpaceDetector
______________________________________________________________________________ test_KeyNetDetector[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KeyNetDetector(target_framework, mode, backend_compile):
        print("kornia.feature.KeyNetDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetDetector = ivy.transpile(kornia.feature.KeyNetDetector, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 32, 32)
        torch_out = kornia.feature.KeyNetDetector()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_out = TranspiledKeyNetDetector()(transpiled_x)
    
>       _to_numpy_and_shape_allclose(torch_out, transpiled_out)

kornia/test_feature3.py:205: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[[15.6444,  0.0000, 17.0667],
          [-0.0000, 15.6444, 10.6667]],

         [[15.6444,  0.0000, 19.2000]....0000, 16.0000],
          [-0.0000, 22.0000, 15.0000]]]]), tensor([[0.0231, 0.0225, 0.0211]], grad_fn=<CatBackward0>))
transpiled_x = (<tf.Tensor: shape=(1, 2, 2, 3), dtype=float32, numpy=
array([[[[15.644444,  0.      , 14.222222],
         [-0.      ...]]], dtype=float32)>, <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.03442707, 0.02047981]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_shape_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_shape_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[15.644444,  0.      , 17.066668],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.   ...    [-0.      , 22.      , 15.      ]]]], dtype=float32), array([[0.02305724, 0.02254741, 0.02107917]], dtype=float32))
y = (array([[[[15.644444,  0.      , 14.222222],
         [-0.      , 15.644444, 14.222222]],

        [[22.      ,  0.   ...    ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32), array([[0.03442707, 0.02047981]], dtype=float32))
tolerance = 0.001

    def _check_shape_allclose(x, y, tolerance=1e-3):
        """
        Checks that all array shapes are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x.shape, y.shape, atol=tolerance), "numpy array shapes are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7fe51557a340>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[15.644444,  0.      , 17.066668],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.    ...17.777779]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
y = array([[[[15.644444,  0.      , 14.222222],
         [-0.      , 15.644444, 14.222222]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"

helpers.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[15.644444,  0.      , 17.066668],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.    ...17.777779]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
b = array([[[[15.644444,  0.      , 14.222222],
         [-0.      , 15.644444, 14.222222]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
rtol = 1e-05, atol = 0.001, equal_nan = False

    @array_function_dispatch(_allclose_dispatcher)
    def allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
        """
        Returns True if two arrays are element-wise equal within a tolerance.
    
        The tolerance values are positive, typically very small numbers.  The
        relative difference (`rtol` * abs(`b`)) and the absolute difference
        `atol` are added together to compare against the absolute difference
        between `a` and `b`.
    
        NaNs are treated as equal if they are in the same place and if
        ``equal_nan=True``.  Infs are treated as equal if they are in the same
        place and of the same sign in both arrays.
    
        Parameters
        ----------
        a, b : array_like
            Input arrays to compare.
        rtol : float
            The relative tolerance parameter (see Notes).
        atol : float
            The absolute tolerance parameter (see Notes).
        equal_nan : bool
            Whether to compare NaN's as equal.  If True, NaN's in `a` will be
            considered equal to NaN's in `b` in the output array.
    
            .. versionadded:: 1.10.0
    
        Returns
        -------
        allclose : bool
            Returns True if the two arrays are equal within the given
            tolerance; False otherwise.
    
        See Also
        --------
        isclose, all, any, equal
    
        Notes
        -----
        If the following equation is element-wise True, then allclose returns
        True.
    
         absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
    
        The above equation is not symmetric in `a` and `b`, so that
        ``allclose(a, b)`` might be different from ``allclose(b, a)`` in
        some rare cases.
    
        The comparison of `a` and `b` uses standard broadcasting, which
        means that `a` and `b` need not have the same shape in order for
        ``allclose(a, b)`` to evaluate to True.  The same is true for
        `equal` but not `array_equal`.
    
        `allclose` is not defined for non-numeric data types.
        `bool` is considered a numeric data-type for this purpose.
    
        Examples
        --------
        >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])
        False
        >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])
        True
        >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])
        False
        >>> np.allclose([1.0, np.nan], [1.0, np.nan])
        False
        >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
        True
    
        """
>       res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))

/opt/fw/mxnet/numpy/core/numeric.py:2241: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([[[[15.644444,  0.      , 17.066668],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.    ...17.777779]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
b = array([[[[15.644444,  0.      , 14.222222],
         [-0.      , 15.644444, 14.222222]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
rtol = 1e-05, atol = 0.001, equal_nan = False

    @array_function_dispatch(_isclose_dispatcher)
    def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
        """
        Returns a boolean array where two arrays are element-wise equal within a
        tolerance.
    
        The tolerance values are positive, typically very small numbers.  The
        relative difference (`rtol` * abs(`b`)) and the absolute difference
        `atol` are added together to compare against the absolute difference
        between `a` and `b`.
    
        .. warning:: The default `atol` is not appropriate for comparing numbers
                     that are much smaller than one (see Notes).
    
        Parameters
        ----------
        a, b : array_like
            Input arrays to compare.
        rtol : float
            The relative tolerance parameter (see Notes).
        atol : float
            The absolute tolerance parameter (see Notes).
        equal_nan : bool
            Whether to compare NaN's as equal.  If True, NaN's in `a` will be
            considered equal to NaN's in `b` in the output array.
    
        Returns
        -------
        y : array_like
            Returns a boolean array of where `a` and `b` are equal within the
            given tolerance. If both `a` and `b` are scalars, returns a single
            boolean value.
    
        See Also
        --------
        allclose
        math.isclose
    
        Notes
        -----
        .. versionadded:: 1.7.0
    
        For finite values, isclose uses the following equation to test whether
        two floating point values are equivalent.
    
         absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
    
        Unlike the built-in `math.isclose`, the above equation is not symmetric
        in `a` and `b` -- it assumes `b` is the reference value -- so that
        `isclose(a, b)` might be different from `isclose(b, a)`. Furthermore,
        the default value of atol is not zero, and is used to determine what
        small values should be considered close to zero. The default value is
        appropriate for expected values of order unity: if the expected values
        are significantly smaller than one, it can result in false positives.
        `atol` should be carefully selected for the use case at hand. A zero value
        for `atol` will result in `False` if either `a` or `b` is zero.
    
        `isclose` is not defined for non-numeric data types.
        `bool` is considered a numeric data-type for this purpose.
    
        Examples
        --------
        >>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])
        array([ True, False])
        >>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])
        array([ True, True])
        >>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])
        array([False,  True])
        >>> np.isclose([1.0, np.nan], [1.0, np.nan])
        array([ True, False])
        >>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
        array([ True, True])
        >>> np.isclose([1e-8, 1e-7], [0.0, 0.0])
        array([ True, False])
        >>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)
        array([False, False])
        >>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])
        array([ True,  True])
        >>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)
        array([False,  True])
        """
        def within_tol(x, y, atol, rtol):
            with errstate(invalid='ignore'), _no_nep50_warning():
                return less_equal(abs(x-y), atol + rtol * abs(y))
    
        x = asanyarray(a)
        y = asanyarray(b)
    
        # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
        # This will cause casting of x later. Also, make sure to allow subclasses
        # (e.g., for numpy.ma).
        # NOTE: We explicitly allow timedelta, which used to work. This could
        #       possibly be deprecated. See also gh-18286.
        #       timedelta works if `atol` is an integer or also a timedelta.
        #       Although, the default tolerances are unlikely to be useful
        if y.dtype.kind != "m":
            dt = multiarray.result_type(y, 1.)
            y = asanyarray(y, dtype=dt)
    
        xfin = isfinite(x)
        yfin = isfinite(y)
        if all(xfin) and all(yfin):
>           return within_tol(x, y, atol, rtol)

/opt/fw/mxnet/numpy/core/numeric.py:2351: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[15.644444,  0.      , 17.066668],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.    ...17.777779]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
y = array([[[[15.644444,  0.      , 14.222222],
         [-0.      , 15.644444, 14.222222]],

        [[22.      ,  0.      , 16.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
atol = 0.001, rtol = 1e-05

    def within_tol(x, y, atol, rtol):
        with errstate(invalid='ignore'), _no_nep50_warning():
>           return less_equal(abs(x-y), atol + rtol * abs(y))
E           ValueError: operands could not be broadcast together with shapes (1,3,2,3) (1,2,2,3)

/opt/fw/mxnet/numpy/core/numeric.py:2332: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.KeyNetDetector
_______________________________________________________________________________ test_LAFDescriptor[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LAFDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.LAFDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLAFDescriptor = ivy.transpile(kornia.feature.LAFDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 64, 64)
        lafs = torch.rand(1, 2, 2, 3)
        torch_out = kornia.feature.LAFDescriptor()(x, lafs)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_lafs = _nest_torch_tensor_to_new_framework(lafs, target_framework)
>       transpiled_out = TranspiledLAFDescriptor()(transpiled_x, transpiled_lafs)

kornia/test_feature3.py:222: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFDescriptor' object has no attribute 'descriptor'") raised in repr()] tensorflow_LAFDescriptor object at 0x7fe514895120>, patch_descriptor_module = None
patch_size = 32, grayscale_descriptor = True

    def __init__(
        self, patch_descriptor_module=None, patch_size=32, grayscale_descriptor=True
    ):
        from .hardnet import tensorflow_HardNet
    
        self.super___init__(
            patch_descriptor_module=patch_descriptor_module,
            patch_size=patch_size,
            grayscale_descriptor=grayscale_descriptor,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if patch_descriptor_module is None:
>           patch_descriptor_module = tensorflow_HardNet(True)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): t...)
    (17): tensorflow_ReLU()
    (18): tensorflow_Dropout()
    (19): KerasConv2D()
    (20): KerasBatchNorm2D()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): t...)
    (17): tensorflow_ReLU()
    (18): tensorflow_Dropout()
    (19): KerasConv2D()
    (20): KerasBatchNorm2D()
  )
)
pretrained = True

    @tensorflow_store_config_info
    def __init__(self, pretrained=False):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=128,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.3),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=8,
                strides=1,
                padding=0,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
        )
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["liberty_aug"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/hardnet.py:212: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LAFDescriptor
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /root/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth

  0%|          | 0.00/5.10M [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.10M/5.10M [00:00<00:00, 109MB/s]
___________________________________________________________________________________ test_SOLD2[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SOLD2(target_framework, mode, backend_compile):
        print("kornia.feature.SOLD2")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSOLD2 = ivy.transpile(kornia.feature.SOLD2, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 512, 512)
        torch_out = kornia.feature.SOLD2(pretrained=False)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSOLD2(pretrained=False)(transpiled_x)

kornia/test_feature3.py:239: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.13195753, 0.7988189 , 0.7870028 , ..., 0.561145...
         [0.25467992, 0.04954183, 0.5255058 , ..., 0.13527894,
          0.8461877 , 0.16530919]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x562007d44220, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tens...,
         [0.25467992, 0.04954183, 0.5255058 , ..., 0.13527894,
          0.8461877 , 0.16530919]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.13195753, 0.7988189 , 0.7870028 , ..., 0.561145...
         [0.25467992, 0.04954183, 0.5255058 , ..., 0.13527894,
          0.8461877 , 0.16530919]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tens...,
         [0.25467992, 0.04954183, 0.5255058 , ..., 0.13527894,
          0.8461877 , 0.16530919]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.13195753, 0.7988189 , 0.7870028 , ..., 0.561145...
         [0.25467992, 0.04954183, 0.5255058 , ..., 0.13527894,
          0.8461877 , 0.16530919]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (img)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tens...   (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.13195753, 0.7988189 , 0.7870028 , ..., 0...,
         [0.25467992, 0.04954183, 0.5255058 , ..., 0.13527894,
          0.8461877 , 0.16530919]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
img = <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.13195753, 0.7988189 , 0.7870028 , ..., 0.5611453...],
         [0.25467992, 0.04954183, 0.5255058 , ..., 0.13527894,
          0.8461877 , 0.16530919]]]], dtype=float32)>

    def call(self, img):
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .sold2_detector import tensorflow_prob_to_junctions
        from .sold2_detector import tensorflow_line_map_to_segments
    
        tensorflow_KORNIA_CHECK_SHAPE(img, ["B", "1", "H", "W"])
        outputs = {}
        net_outputs = self.model(img)
        outputs = tensorflow_set_item_bknd(
            outputs, "junction_heatmap", net_outputs["junctions"]
        )
        outputs = tensorflow_set_item_bknd(
            outputs, "line_heatmap", net_outputs["heatmap"]
        )
        outputs = tensorflow_set_item_bknd(
            outputs, "dense_desc", net_outputs["descriptors"]
        )
        lines = []
        for junc_prob, heatmap in zip(net_outputs["junctions"], net_outputs["heatmap"]):
            junctions = tensorflow_prob_to_junctions(
                junc_prob,
                self.grid_size,
                self.junc_detect_thresh,
                self.max_num_junctions,
            )
>           line_map, junctions, _ = self.line_detector.detect(junctions, heatmap)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2.py:108: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Translated_Outputs.tensorflow_outputs.kornia.feature.sold2.sold2_detector.tensorflow_LineSegmentDetectionModule object at 0x7fe50f78ba90>
junctions = <tf.Tensor: shape=(500, 2), dtype=float32, numpy=
array([[ 90., 455.],
       [ 82., 455.],
       [ 74., 455.],
     ...     [386., 151.],
       [ 26., 335.],
       [394., 463.],
       [178., 455.],
       [410., 391.]], dtype=float32)>
heatmap = <tf.Tensor: shape=(512, 512), dtype=float32, numpy=
array([[0.7621792 , 0.78582335, 0.65396684, ..., 0.6500897 , 0.695...665035],
       [0.8164852 , 0.6370089 , 0.5235733 , ..., 0.74669963, 0.67141616,
        0.6727509 ]], dtype=float32)>

    def detect(self, junctions, heatmap):
        from ...core._backend import zeros
        from ...core._backend import where
        from ...core._backend import concatenate
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .structures import tensorflow_HeatMapRefineCfg
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_triu_frnt,
        )
        from ....ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_clamp_frnt,
        )
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_sqrt_frnt,
        )
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
        from ....ivy.functional.frontends.torch.reduction_ops import (
            tensorflow_mean_frnt,
        )
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        tensorflow_KORNIA_CHECK_SHAPE(heatmap, ["H", "W"])
        H, W = tensorflow_shape_frnt_(heatmap)
        device = junctions.device
        if self.use_heatmap_refinement and isinstance(
            self.heatmap_refine_cfg, (tensorflow_HeatMapRefineCfg,)
        ):
            if self.heatmap_refine_cfg.mode == "global":
                heatmap = self.refine_heatmap(
                    heatmap,
                    self.heatmap_refine_cfg.ratio,
                    self.heatmap_refine_cfg.valid_thresh,
                )
            elif self.heatmap_refine_cfg.mode == "local":
                heatmap = self.refine_heatmap_local(
                    heatmap,
                    self.heatmap_refine_cfg.num_blocks,
                    self.heatmap_refine_cfg.overlap_ratio,
                    self.heatmap_refine_cfg.ratio,
                    self.heatmap_refine_cfg.valid_thresh,
                )
        num_junctions = len(junctions)
        line_map_pred = zeros(
            [num_junctions, num_junctions], device=device, dtype=tf.int32
        )
        if num_junctions < 2:
            return line_map_pred, junctions, heatmap
        candidate_map = tensorflow_triu_frnt(
            tensorflow_ones_frnt(
                [num_junctions, num_junctions], device=device, dtype=tf.int32
            ),
            diagonal=1,
        )
        if self.use_candidate_suppression:
>           candidate_map = self.candidate_suppression(junctions, candidate_map)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2_detector.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Translated_Outputs.tensorflow_outputs.kornia.feature.sold2.sold2_detector.tensorflow_LineSegmentDetectionModule object at 0x7fe50f78ba90>
junctions = <tf.Tensor: shape=(500, 2), dtype=float32, numpy=
array([[ 90., 455.],
       [ 82., 455.],
       [ 74., 455.],
     ...     [386., 151.],
       [ 26., 335.],
       [394., 463.],
       [178., 455.],
       [410., 391.]], dtype=float32)>
candidate_map = <tf.Tensor: shape=(500, 500), dtype=float32, numpy=
array([[0., 1., 1., ..., 1., 1., 1.],
       [0., 0., 1., ..., 1.,... 0., 0., ..., 0., 1., 1.],
       [0., 0., 0., ..., 0., 0., 1.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>

    def candidate_suppression(self, junctions, candidate_map):
        from ...core._backend import where
        from ...core._backend import sin
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_triu_frnt,
        )
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_unsqueeze_frnt,
        )
        from ....ivy.functional.frontends.torch.reduction_ops import (
            tensorflow_norm_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_einsum_frnt,
        )
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_acos_frnt,
        )
        from ....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_arange_frnt,
        )
    
        dist_tolerance = self.nms_dist_tolerance
        line_dist_map = (
            tensorflow_sum_frnt(
                (tensorflow_unsqueeze_frnt(junctions, dim=1) - junctions[None, ...])
                ** 2,
                dim=-1,
            )
            ** 0.5
        )
        seg_indexes = where(tensorflow_triu_frnt(candidate_map, diagonal=1))
        start_point_idxs = seg_indexes[0]
        end_point_idxs = seg_indexes[1]
        start_points = tensorflow_get_item(
            junctions, (start_point_idxs, slice(None, None, None))
        )
        end_points = tensorflow_get_item(
            junctions, (end_point_idxs, slice(None, None, None))
        )
        line_dists = tensorflow_get_item(
            line_dist_map, (start_point_idxs, end_point_idxs)
        )
        dir_vecs = (end_points - start_points) / tensorflow_norm_frnt(
            end_points - start_points, dim=-1
        )[..., None]
        cand_vecs = junctions[None, ...] - tensorflow_unsqueeze_frnt_(
            start_points, dim=1
        )
        cand_vecs_norm = tensorflow_norm_frnt(cand_vecs, dim=-1)
        proj = (
            tensorflow_einsum_frnt("bij,bjk->bik", cand_vecs, dir_vecs[..., None])
            / line_dists[..., None, None]
        )
>       proj_mask = (proj >= 0) * (proj <= 1)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2_detector.py:368: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(124750, 500, 1), dtype=bool, numpy=
array([[[ True],
        [ True],
        [ True],
        ......
       [[ True],
        [ True],
        [ True],
        ...,
        [ True],
        [ True],
        [False]]])>)
kwargs = {}
arg = <tf.Tensor: shape=(124750, 500, 1), dtype=bool, numpy=
array([[[ True],
        [ True],
        [False],
        ...,...

       [[ True],
        [ True],
        [ True],
        ...,
        [ True],
        [ True],
        [False]]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SOLD2.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_SOLD2.call():
E         â€¢ img=tf.Tensor(shape=(1, 1, 512, 512), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SOLD2
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-14 04:35:49.486842: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 499000000 exceeds 10% of free system memory.
2024-09-14 04:35:49.837437: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 499000000 exceeds 10% of free system memory.
2024-09-14 04:35:49.869741: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 499000000 exceeds 10% of free system memory.
_______________________________________________________________________________ test_LocalFeature[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LocalFeature(target_framework, mode, backend_compile):
        print("kornia.feature.LocalFeature")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetDetector = ivy.transpile(
            kornia.feature.KeyNetDetector, source="torch", target=target_framework
        )
        TranspiledLAFDescriptor = ivy.transpile(
            kornia.feature.LAFDescriptor, source="torch", target=target_framework
        )
        TranspiledLocalFeature = ivy.transpile(
            kornia.feature.LocalFeature, source="torch", target=target_framework
        )
    
        torch_detector = kornia.feature.KeyNetDetector()
        torch_descriptor = kornia.feature.LAFDescriptor()
        transpiled_detector = TranspiledKeyNetDetector()
>       transpiled_descriptor = TranspiledLAFDescriptor()

kornia/test_feature3.py:263: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFDescriptor' object has no attribute 'descriptor'") raised in repr()] tensorflow_LAFDescriptor object at 0x7fe50f87aa70>, patch_descriptor_module = None
patch_size = 32, grayscale_descriptor = True

    def __init__(
        self, patch_descriptor_module=None, patch_size=32, grayscale_descriptor=True
    ):
        from .hardnet import tensorflow_HardNet
    
        self.super___init__(
            patch_descriptor_module=patch_descriptor_module,
            patch_size=patch_size,
            grayscale_descriptor=grayscale_descriptor,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if patch_descriptor_module is None:
>           patch_descriptor_module = tensorflow_HardNet(True)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): t...)
    (17): tensorflow_ReLU()
    (18): tensorflow_Dropout()
    (19): KerasConv2D()
    (20): KerasBatchNorm2D()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): t...)
    (17): tensorflow_ReLU()
    (18): tensorflow_Dropout()
    (19): KerasConv2D()
    (20): KerasBatchNorm2D()
  )
)
pretrained = True

    @tensorflow_store_config_info
    def __init__(self, pretrained=False):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=128,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.3),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=8,
                strides=1,
                padding=0,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
        )
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["liberty_aug"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/hardnet.py:212: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LocalFeature
______________________________________________________________________________ test_SOLD2_detector[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SOLD2_detector(target_framework, mode, backend_compile):
        print("kornia.feature.SOLD2_detector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSOLD2Detector = ivy.transpile(kornia.feature.SOLD2_detector, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 512, 512)
        torch_out = kornia.feature.SOLD2_detector(pretrained=False)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSOLD2Detector(pretrained=False)(transpiled_x)

kornia/test_feature3.py:286: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.92079014, 0.9263203 , 0.39712209, ..., 0.191664...
         [0.46485823, 0.81092167, 0.02560818, ..., 0.55231017,
          0.07246202, 0.587014  ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7fe519db3870, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (n...,
         [0.46485823, 0.81092167, 0.02560818, ..., 0.55231017,
          0.07246202, 0.587014  ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.92079014, 0.9263203 , 0.39712209, ..., 0.191664...
         [0.46485823, 0.81092167, 0.02560818, ..., 0.55231017,
          0.07246202, 0.587014  ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (n...,
         [0.46485823, 0.81092167, 0.02560818, ..., 0.55231017,
          0.07246202, 0.587014  ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.92079014, 0.9263203 , 0.39712209, ..., 0.191664...
         [0.46485823, 0.81092167, 0.02560818, ..., 0.55231017,
          0.07246202, 0.587014  ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (img)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (n...ntDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.92079014, 0.9263203 , 0.39712209, ..., 0...,
         [0.46485823, 0.81092167, 0.02560818, ..., 0.55231017,
          0.07246202, 0.587014  ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
img = <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.92079014, 0.9263203 , 0.39712209, ..., 0.1916646...],
         [0.46485823, 0.81092167, 0.02560818, ..., 0.55231017,
          0.07246202, 0.587014  ]]]], dtype=float32)>

    def call(self, img):
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
    
        tensorflow_KORNIA_CHECK_SHAPE(img, ["B", "1", "H", "W"])
        outputs = {}
        net_outputs = self.model(img)
        outputs = tensorflow_set_item_bknd(
            outputs, "junction_heatmap", net_outputs["junctions"]
        )
        outputs = tensorflow_set_item_bknd(
            outputs, "line_heatmap", net_outputs["heatmap"]
        )
        lines = []
        for junc_prob, heatmap in zip(net_outputs["junctions"], net_outputs["heatmap"]):
            junctions = tensorflow_prob_to_junctions(
                junc_prob,
                self.grid_size,
                self.junc_detect_thresh,
                self.max_num_junctions,
            )
>           line_map, junctions, _ = self.line_detector.detect(junctions, heatmap)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2_detector.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Translated_Outputs.tensorflow_outputs.kornia.feature.sold2.sold2_detector.tensorflow_LineSegmentDetectionModule object at 0x7fe51600ded0>
junctions = <tf.Tensor: shape=(500, 2), dtype=float32, numpy=
array([[  6.,  81.],
       [107., 505.],
       [  6.,  73.],
     ...     [214.,  73.],
       [342., 425.],
       [ 86., 337.],
       [  6., 177.],
       [126., 433.]], dtype=float32)>
heatmap = <tf.Tensor: shape=(512, 512), dtype=float32, numpy=
array([[0.47786736, 0.385142  , 0.4194385 , ..., 0.76443374, 0.541...869485],
       [0.29725394, 0.6142195 , 0.43895382, ..., 0.8498055 , 0.56718236,
        0.5740768 ]], dtype=float32)>

    def detect(self, junctions, heatmap):
        from ...core._backend import zeros
        from ...core._backend import where
        from ...core._backend import concatenate
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .structures import tensorflow_HeatMapRefineCfg
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_triu_frnt,
        )
        from ....ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_clamp_frnt,
        )
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_sqrt_frnt,
        )
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
        from ....ivy.functional.frontends.torch.reduction_ops import (
            tensorflow_mean_frnt,
        )
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        tensorflow_KORNIA_CHECK_SHAPE(heatmap, ["H", "W"])
        H, W = tensorflow_shape_frnt_(heatmap)
        device = junctions.device
        if self.use_heatmap_refinement and isinstance(
            self.heatmap_refine_cfg, (tensorflow_HeatMapRefineCfg,)
        ):
            if self.heatmap_refine_cfg.mode == "global":
                heatmap = self.refine_heatmap(
                    heatmap,
                    self.heatmap_refine_cfg.ratio,
                    self.heatmap_refine_cfg.valid_thresh,
                )
            elif self.heatmap_refine_cfg.mode == "local":
                heatmap = self.refine_heatmap_local(
                    heatmap,
                    self.heatmap_refine_cfg.num_blocks,
                    self.heatmap_refine_cfg.overlap_ratio,
                    self.heatmap_refine_cfg.ratio,
                    self.heatmap_refine_cfg.valid_thresh,
                )
        num_junctions = len(junctions)
        line_map_pred = zeros(
            [num_junctions, num_junctions], device=device, dtype=tf.int32
        )
        if num_junctions < 2:
            return line_map_pred, junctions, heatmap
        candidate_map = tensorflow_triu_frnt(
            tensorflow_ones_frnt(
                [num_junctions, num_junctions], device=device, dtype=tf.int32
            ),
            diagonal=1,
        )
        if self.use_candidate_suppression:
>           candidate_map = self.candidate_suppression(junctions, candidate_map)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2_detector.py:571: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Translated_Outputs.tensorflow_outputs.kornia.feature.sold2.sold2_detector.tensorflow_LineSegmentDetectionModule object at 0x7fe51600ded0>
junctions = <tf.Tensor: shape=(500, 2), dtype=float32, numpy=
array([[  6.,  81.],
       [107., 505.],
       [  6.,  73.],
     ...     [214.,  73.],
       [342., 425.],
       [ 86., 337.],
       [  6., 177.],
       [126., 433.]], dtype=float32)>
candidate_map = <tf.Tensor: shape=(500, 500), dtype=float32, numpy=
array([[0., 1., 1., ..., 1., 1., 1.],
       [0., 0., 1., ..., 1.,... 0., 0., ..., 0., 1., 1.],
       [0., 0., 0., ..., 0., 0., 1.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>

    def candidate_suppression(self, junctions, candidate_map):
        from ...core._backend import where
        from ...core._backend import sin
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_triu_frnt,
        )
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_unsqueeze_frnt,
        )
        from ....ivy.functional.frontends.torch.reduction_ops import (
            tensorflow_norm_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ....ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_einsum_frnt,
        )
        from ....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_acos_frnt,
        )
        from ....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_arange_frnt,
        )
    
        dist_tolerance = self.nms_dist_tolerance
        line_dist_map = (
            tensorflow_sum_frnt(
                (tensorflow_unsqueeze_frnt(junctions, dim=1) - junctions[None, ...])
                ** 2,
                dim=-1,
            )
            ** 0.5
        )
        seg_indexes = where(tensorflow_triu_frnt(candidate_map, diagonal=1))
        start_point_idxs = seg_indexes[0]
        end_point_idxs = seg_indexes[1]
        start_points = tensorflow_get_item(
            junctions, (start_point_idxs, slice(None, None, None))
        )
        end_points = tensorflow_get_item(
            junctions, (end_point_idxs, slice(None, None, None))
        )
        line_dists = tensorflow_get_item(
            line_dist_map, (start_point_idxs, end_point_idxs)
        )
        dir_vecs = (end_points - start_points) / tensorflow_norm_frnt(
            end_points - start_points, dim=-1
        )[..., None]
        cand_vecs = junctions[None, ...] - tensorflow_unsqueeze_frnt_(
            start_points, dim=1
        )
        cand_vecs_norm = tensorflow_norm_frnt(cand_vecs, dim=-1)
        proj = (
            tensorflow_einsum_frnt("bij,bjk->bik", cand_vecs, dir_vecs[..., None])
            / line_dists[..., None, None]
        )
>       proj_mask = (proj >= 0) * (proj <= 1)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2_detector.py:821: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(124750, 500, 1), dtype=bool, numpy=
array([[[ True],
        [ True],
        [False],
        ......
       [[ True],
        [False],
        [ True],
        ...,
        [ True],
        [ True],
        [ True]]])>)
kwargs = {}
arg = <tf.Tensor: shape=(124750, 500, 1), dtype=bool, numpy=
array([[[ True],
        [ True],
        [ True],
        ...,...

       [[ True],
        [False],
        [ True],
        ...,
        [ True],
        [ True],
        [ True]]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SOLD2_detector.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_SOLD2_detector.call():
E         â€¢ img=tf.Tensor(shape=(1, 1, 512, 512), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SOLD2_detector
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-14 04:39:58.219627: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 499000000 exceeds 10% of free system memory.
2024-09-14 04:39:58.548098: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 499000000 exceeds 10% of free system memory.
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_feature3.py::test_SOSNet[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_m...
FAILED kornia/test_feature3.py::test_MultiResolutionDetector[tensorflow-s2s-False] - ValueError: operands could not be broadcast together with shapes (1,3,2,3) (1,2,2,3)
FAILED kornia/test_feature3.py::test_ScaleSpaceDetector[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_ScaleSpac...
FAILED kornia/test_feature3.py::test_KeyNetDetector[tensorflow-s2s-False] - ValueError: operands could not be broadcast together with shapes (1,3,2,3) (1,2,2,3)
FAILED kornia/test_feature3.py::test_LAFDescriptor[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature3.py::test_SOLD2[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SOLD2.call().
FAILED kornia/test_feature3.py::test_LocalFeature[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature3.py::test_SOLD2_detector[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_SOLD2_detecto...
=============================================================================== 8 failed, 7 passed in 1367.57s (0:22:47) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/test_io.py ..                                                                                                                                                                             [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
===================================================================================== 2 passed in 81.37s (0:01:21) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 5 items

kornia/geometry/test_calibration.py ....F                                                                                                                                                        [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________________ test_solve_pnp_dlt[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_solve_pnp_dlt(target_framework, mode, backend_compile):
        trace_args = (
            torch.tensor([[
                [5.0, -5.0, 0.0], [0.0, 0.0, 1.5],
                [2.5, 3.0, 6.0], [9.0, -2.0, 3.0],
                [-4.0, 5.0, 2.0], [-5.0, 5.0, 1.0]
            ]], dtype=torch.float64),
            torch.tensor([[
                [1409.1504, -800.936], [407.0207, -182.1229],
                [392.7021, 177.9428], [1016.838, -2.9416],
                [-63.1116, 142.9204], [-219.3874, 99.666]
            ]], dtype=torch.float64),
            torch.tensor([[
                [500.0, 0.0, 250.0],
                [0.0, 500.0, 250.0],
                [0.0, 0.0, 1.0]
            ]], dtype=torch.float64),
        )
        trace_kwargs = {'svd_eps': 1e-3}
        test_args = (
            torch.tensor([[
                [10.0, -10.0, 0.0], [0.0, 0.0, 3.0],
                [5.0, 6.0, 12.0], [18.0, -4.0, 6.0],
                [-8.0, 10.0, 4.0], [-10.0, 10.0, 2.0]
            ]], dtype=torch.float64),
            torch.tensor([[
                [2818.3008, -1601.872], [814.0414, -364.2458],
                [785.4042, 355.8856], [2033.676, -5.8832],
                [-126.2232, 285.8408], [-438.7748, 199.332]
            ]], dtype=torch.float64),
            torch.tensor([[
                [1000.0, 0.0, 500.0],
                [0.0, 1000.0, 500.0],
                [0.0, 0.0, 1.0]
            ]], dtype=torch.float64),
        )
        test_kwargs = {'svd_eps': 1e-3}
>       _test_function(
            kornia.geometry.calibration.solve_pnp_dlt,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_calibration.py:148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function solve_pnp_dlt at 0x7ffa4c51a8c0>
trace_args = (tensor([[[ 5.0000, -5.0000,  0.0000],
         [ 0.0000,  0.0000,  1.5000],
         [ 2.5000,  3.0000,  6.0000],
   ...loat64), tensor([[[500.,   0., 250.],
         [  0., 500., 250.],
         [  0.,   0.,   1.]]], dtype=torch.float64))
trace_kwargs = {'svd_eps': 0.001}
test_args = (tensor([[[ 10., -10.,   0.],
         [  0.,   0.,   3.],
         [  5.,   6.,  12.],
         [ 18.,  -4.,   6.],
 ...tensor([[[1000.,    0.,  500.],
         [   0., 1000.,  500.],
         [   0.,    0.,    1.]]], dtype=torch.float64))
test_kwargs = {'svd_eps': 0.001}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function solve_pnp_dlt at 0x7ffa4c51a8c0>
trace_args = (tensor([[[ 5.0000, -5.0000,  0.0000],
         [ 0.0000,  0.0000,  1.5000],
         [ 2.5000,  3.0000,  6.0000],
   ...loat64), tensor([[[500.,   0., 250.],
         [  0., 500., 250.],
         [  0.,   0.,   1.]]], dtype=torch.float64))
trace_kwargs = {'svd_eps': 0.001}
test_args = (tensor([[[ 10., -10.,   0.],
         [  0.,   0.,   3.],
         [  5.,   6.,  12.],
         [ 18.,  -4.,   6.],
 ...tensor([[[1000.,    0.,  500.],
         [   0., 1000.,  500.],
         [   0.,    0.,    1.]]], dtype=torch.float64))
test_kwargs = {'svd_eps': 0.001}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

world_points = <tf.Tensor: shape=(1, 6, 3), dtype=float64, numpy=
array([[[ 5. , -5. ,  0. ],
        [ 0. ,  0. ,  1.5],
        [ 2.5,  3. ,  6. ],
        [ 9. , -2. ,  3. ],
        [-4. ,  5. ,  2. ],
        [-5. ,  5. ,  1. ]]])>
img_points = <tf.Tensor: shape=(1, 6, 2), dtype=float64, numpy=
array([[[1409.1504, -800.936 ],
        [ 407.0207, -182.1229],
   ...92.7021,  177.9428],
        [1016.838 ,   -2.9416],
        [ -63.1116,  142.9204],
        [-219.3874,   99.666 ]]])>
intrinsics = <tf.Tensor: shape=(1, 3, 3), dtype=float64, numpy=
array([[[500.,   0., 250.],
        [  0., 500., 250.],
        [  0.,   0.,   1.]]])>, weights = None, svd_eps = 0.001

    def tensorflow_solve_pnp_dlt(
        world_points, img_points, intrinsics, weights=None, svd_eps=0.0001
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...utils.helpers import tensorflow__torch_linalg_svdvals
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_any_frnt
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_inverse_frnt,
        )
        from ..conversions import tensorflow_convert_points_to_homogeneous
        from ..linalg import tensorflow_transform_points
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_svd_frnt_base_count_1_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...utils.misc import tensorflow_eye_like
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_bmm_frnt,
        )
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_det_frnt,
        )
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_norm_frnt
        from ....ivy.functional.frontends.torch.linalg import tensorflow_qr_frnt
        from ....ivy.functional.frontends.torch.pointwise_ops import tensorflow_sign_frnt
        from ....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...core._backend import zeros
        from ...core._backend import ones_like
        from ...core._backend import where
    
        if not isinstance(world_points, (tensorflow.Tensor, tensorflow.Variable)):
            raise AssertionError(
                f"world_points is not an instance of torch.Tensor. Type of world_points is {type(world_points)}"
            )
        if not isinstance(img_points, (tensorflow.Tensor, tensorflow.Variable)):
            raise AssertionError(
                f"img_points is not an instance of torch.Tensor. Type of img_points is {type(img_points)}"
            )
        if not isinstance(intrinsics, (tensorflow.Tensor, tensorflow.Variable)):
            raise AssertionError(
                f"intrinsics is not an instance of torch.Tensor. Type of intrinsics is {type(intrinsics)}"
            )
        if weights is not None and not isinstance(
            weights, (tensorflow.Tensor, tensorflow.Variable)
        ):
            raise AssertionError(
                f"If weights is not None, then weights should be an instance of torch.Tensor. Type of weights is {type(weights)}"
            )
        if not isinstance(svd_eps, (float,)):
            raise AssertionError(f"Type of svd_eps is not float. Got {type(svd_eps)}")
        accepted_dtypes = tf.float32, tf.float64
        if world_points.dtype not in accepted_dtypes:
            raise AssertionError(
                f"world_points must have one of the following dtypes {accepted_dtypes}. Currently it has {world_points.dtype}."
            )
        if img_points.dtype not in accepted_dtypes:
            raise AssertionError(
                f"img_points must have one of the following dtypes {accepted_dtypes}. Currently it has {img_points.dtype}."
            )
        if intrinsics.dtype not in accepted_dtypes:
            raise AssertionError(
                f"intrinsics must have one of the following dtypes {accepted_dtypes}. Currently it has {intrinsics.dtype}."
            )
        if (
            len(tensorflow_shape_frnt_(world_points)) != 3
            or tensorflow_shape_frnt_(world_points)[2] != 3
        ):
            raise AssertionError(
                f"world_points must be of shape (B, N, 3). Got shape {tensorflow_shape_frnt_(world_points)}."
            )
        if (
            len(tensorflow_shape_frnt_(img_points)) != 3
            or tensorflow_shape_frnt_(img_points)[2] != 2
        ):
            raise AssertionError(
                f"img_points must be of shape (B, N, 2). Got shape {tensorflow_shape_frnt_(img_points)}."
            )
        if len(tensorflow_shape_frnt_(intrinsics)) != 3 or tensorflow_shape_frnt_(
            intrinsics
        )[1:] != (3, 3):
            raise AssertionError(
                f"intrinsics must be of shape (B, 3, 3). Got shape {tensorflow_shape_frnt_(intrinsics)}."
            )
        if tensorflow_shape_frnt_(world_points)[1] != tensorflow_shape_frnt_(img_points)[1]:
            raise AssertionError(
                "world_points and img_points must have equal number of points."
            )
        if (
            tensorflow_shape_frnt_(world_points)[0] != tensorflow_shape_frnt_(img_points)[0]
            or tensorflow_shape_frnt_(world_points)[0]
            != tensorflow_shape_frnt_(intrinsics)[0]
        ):
            raise AssertionError(
                "world_points, img_points and intrinsics must have the same batch size."
            )
        if tensorflow_shape_frnt_(world_points)[1] < 6:
            raise AssertionError(
                f"At least 6 points are required to use this function. Got {tensorflow_shape_frnt_(world_points)[1]} points."
            )
        B, N = (
            tensorflow_shape_frnt_(world_points)[:2][0],
            tensorflow_shape_frnt_(world_points)[:2][1],
        )
        world_points_norm, world_transform_norm = (
            tensorflow__mean_isotropic_scale_normalize(world_points)
        )
        s = tensorflow__torch_linalg_svdvals(world_points_norm)
        if tensorflow_any_frnt(s[:, -1] < svd_eps):
            raise AssertionError(
                f"The last singular value of one/more of the elements of the batch is smaller than {svd_eps}. This function cannot be used if all world_points (of any element of the batch) lie on a line or if all world_points (of any element of the batch) lie on a plane."
            )
        intrinsics_inv = tensorflow_inverse_frnt(intrinsics)
        world_points_norm_h = tensorflow_convert_points_to_homogeneous(world_points_norm)
        img_points_inv = tensorflow_transform_points(intrinsics_inv, img_points)
        img_points_norm, img_transform_norm = tensorflow__mean_isotropic_scale_normalize(
            img_points_inv
        )
        inv_img_transform_norm = tensorflow_inverse_frnt(img_transform_norm)
        system = zeros((B, 2 * N, 12), dtype=world_points.dtype, device=world_points.device)
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(0, None, 2), slice(0, 4, None)),
            world_points_norm_h,
        )
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(1, None, 2), slice(4, 8, None)),
            world_points_norm_h,
        )
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(0, None, 2), slice(8, 12, None)),
            world_points_norm_h * -1 * img_points_norm[..., 0:1],
        )
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(1, None, 2), slice(8, 12, None)),
            world_points_norm_h * -1 * img_points_norm[..., 1:2],
        )
        _, _, v = tensorflow_svd_frnt_base_count_1_frnt(system)
        solution = v[..., -1]
        solution = tensorflow_reshape_frnt_(solution, B, 3, 4)
        solution_4x4 = tensorflow_eye_like(4, solution)
        solution_4x4 = tensorflow_set_item_bknd(
            solution_4x4,
            (slice(None, None, None), slice(None, 3, None), slice(None, None, None)),
            solution,
        )
        intermediate = tensorflow_bmm_frnt(solution_4x4, world_transform_norm)
        solution = tensorflow_bmm_frnt(inv_img_transform_norm, intermediate[:, :3, :])
        det = tensorflow_det_frnt(solution[:, :3, :3])
        ones = ones_like(det)
        sign_fix = where(det < 0, ones * -1, ones)
        solution = solution * sign_fix[:, None, None]
>       norm_col = tensorflow_norm_frnt(input=solution[:, :3, 0], p=2, dim=1)

Translated_Outputs/tensorflow_outputs/kornia/geometry/calibration/pnp.py:237: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (), kwargs = {'dim': 1, 'input': <tf.Tensor: shape=(1, 3), dtype=float64, numpy=array([[-0.02017233,  0.02388227,  0.05749283]])>, 'p': 2}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7ffa449d9900>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
>       array_like = args[0]
E       IndexError: tuple index out of range

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:44: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.calibration.pnp.solve_pnp_dlt
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_calibration.py::test_solve_pnp_dlt[tensorflow-s2s-False] - IndexError: tuple index out of range
=============================================================================== 1 failed, 4 passed in 170.81s (0:02:50) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 15 items

kornia/geometry/test_subpix.py ..F.........F..                                                                                                                                                   [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________ test_conv_quad_interp3d[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_conv_quad_interp3d(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(20, 16, 3, 50, 32),
        )
        trace_kwargs = {
            'strict_maxima_bonus': 10.0,
            'eps': 1e-7,
        }
        test_args = (
            torch.rand(10, 16, 5, 50, 32),
        )
        test_kwargs = {
            'strict_maxima_bonus': 5.0,
            'eps': 1e-7,
        }
>       _test_function(
            kornia.geometry.subpix.conv_quad_interp3d,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_subpix.py:109: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function conv_quad_interp3d at 0x7f1638708b80>
trace_args = (tensor([[[[[2.1179e-01, 3.4118e-02, 1.8035e-01,  ..., 6.0546e-01,
            1.3323e-01, 8.3207e-01],
           [4....7136e-01],
           [7.2789e-01, 5.0977e-01, 5.3149e-01,  ..., 3.4693e-01,
            9.3333e-01, 9.4194e-01]]]]]),)
trace_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 10.0}
test_args = (tensor([[[[[6.1379e-01, 2.9811e-01, 3.7542e-01,  ..., 6.7567e-01,
            3.8934e-01, 5.2573e-01],
           [3....7717e-01],
           [8.0677e-01, 2.2544e-01, 5.9680e-01,  ..., 7.8894e-01,
            8.4294e-01, 2.6935e-01]]]]]),)
test_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 5.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function conv_quad_interp3d at 0x7f1638708b80>
trace_args = (tensor([[[[[2.1179e-01, 3.4118e-02, 1.8035e-01,  ..., 6.0546e-01,
            1.3323e-01, 8.3207e-01],
           [4....7136e-01],
           [7.2789e-01, 5.0977e-01, 5.3149e-01,  ..., 3.4693e-01,
            9.3333e-01, 9.4194e-01]]]]]),)
trace_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 10.0}
test_args = (tensor([[[[[6.1379e-01, 2.9811e-01, 3.7542e-01,  ..., 6.7567e-01,
            3.8934e-01, 5.2573e-01],
           [3....7717e-01],
           [8.0677e-01, 2.2544e-01, 5.9680e-01,  ..., 7.8894e-01,
            8.4294e-01, 2.6935e-01]]]]]),)
test_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 5.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(20, 16, 3, 50, 32), dtype=float32, numpy=
array([[[[[2.11790144e-01, 3.41182947e-02, 1.80349290e-01...9770155e-01, 5.31485558e-01, ...,
           3.46933901e-01, 9.33333218e-01, 9.41937506e-01]]]]],
      dtype=float32)>
strict_maxima_bonus = 10.0, eps = 1e-07

    def tensorflow_conv_quad_interp3d(input, strict_maxima_bonus=10.0, eps=1e-07):
        from ....ivy.functional.frontends.torch.tensor_functions import (
            tensorflow_is_tensor_frnt_,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...utils.grid import tensorflow_create_meshgrid3d
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...filters.sobel import tensorflow_sobel
        from ....ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...utils._compat import tensorflow_torch_version_ge
        from ....ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from .nms import tensorflow_nms3d
        from ...utils.helpers import tensorflow_safe_solve_with_mask
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import (
            tensorflow_masked_scatter_frnt_,
        )
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_masked_fill__frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_expand_as_frnt_
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_bmm_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...core._backend import stack
        from ...core._backend import rand
        from ...core._backend import zeros_like
        from ...core._backend import where
    
        if not tensorflow_is_tensor_frnt_(input):
            raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
        if not len(tensorflow_shape_frnt_(input)) == 5:
            raise ValueError(
                f"Invalid input shape, we expect BxCxDxHxW. Got: {tensorflow_shape_frnt_(input)}"
            )
        B, CH, D, H, W = tensorflow_shape_frnt_(input)
        grid_global: typing.Any = tensorflow_permute_frnt_(
            tensorflow_create_meshgrid3d(D, H, W, False, device=input.device), 0, 4, 1, 2, 3
        )
        grid_global = tensorflow_to_frnt_(grid_global, input.dtype)
>       b: typing.Any = tensorflow_sobel.spatial_gradient3d(input, order=1, mode="diff")
E       AttributeError: 'function' object has no attribute 'spatial_gradient3d'

Translated_Outputs/tensorflow_outputs/kornia/geometry/subpix/spatial_soft_argmax.py:434: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.subpix.spatial_soft_argmax.conv_quad_interp3d
_____________________________________________________________________________ test_ConvQuadInterp3d[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ConvQuadInterp3d(target_framework, mode, backend_compile):
        print("kornia.geometry.subpix.ConvQuadInterp3d")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledConvQuadInterp3d = ivy.transpile(
            kornia.geometry.subpix.ConvQuadInterp3d, source="torch", target=target_framework
        )
    
        conv_quad_interp3d = kornia.geometry.subpix.ConvQuadInterp3d()
        transpiled_conv_quad_interp3d = TranspiledConvQuadInterp3d()
    
        heatmap = torch.randn(1, 1, 3, 5, 5, requires_grad=True)
        transpiled_heatmap = _nest_torch_tensor_to_new_framework(heatmap, target_framework)
    
        torch_output = conv_quad_interp3d(heatmap)
>       transpiled_output = transpiled_conv_quad_interp3d(transpiled_heatmap)

kornia/geometry/test_subpix.py:371: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0)
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.24975836, -0.5798478 , -0.6344185 ,  0.3029212...075084 ],
          [-0.6182113 ,  1.6843088 , -1.7310512 , -0.23528723,
           -0.27671027]]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f16311efde0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array...6075084 ],
          [-0.6182113 ,  1.6843088 , -1.7310512 , -0.23528723,
           -0.27671027]]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.24975836, -0.5798478 , -0.6344185 ,  0.3029212...075084 ],
          [-0.6182113 ,  1.6843088 , -1.7310512 , -0.23528723,
           -0.27671027]]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array...6075084 ],
          [-0.6182113 ,  1.6843088 , -1.7310512 , -0.23528723,
           -0.27671027]]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.24975836, -0.5798478 , -0.6344185 ,  0.3029212...075084 ],
          [-0.6182113 ,  1.6843088 , -1.7310512 , -0.23528723,
           -0.27671027]]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.24975836, -0.5798478 , -0.6344185 ,  0.30292127....6075084 ],
          [-0.6182113 ,  1.6843088 , -1.7310512 , -0.23528723,
           -0.27671027]]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0),)
kwargs = {'x': <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.24975836, -0.5798478 , -0.6344185 ,  0.30...6075084 ],
          [-0.6182113 ,  1.6843088 , -1.7310512 , -0.23528723,
           -0.27671027]]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0)
x = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.24975836, -0.5798478 , -0.6344185 ,  0.30292127....6075084 ],
          [-0.6182113 ,  1.6843088 , -1.7310512 , -0.23528723,
           -0.27671027]]]]], dtype=float32)>

    def call(self, x):
>       return tensorflow_conv_quad_interp3d(x, self.strict_maxima_bonus, self.eps)

Translated_Outputs/tensorflow_outputs/kornia/geometry/subpix/spatial_soft_argmax.py:1667: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.24975836, -0.5798478 , -0.6344185 ,  0.30292127....6075084 ],
          [-0.6182113 ,  1.6843088 , -1.7310512 , -0.23528723,
           -0.27671027]]]]], dtype=float32)>
strict_maxima_bonus = 10.0, eps = 1e-07

    def tensorflow_conv_quad_interp3d(input, strict_maxima_bonus=10.0, eps=1e-07):
        from ....ivy.functional.frontends.torch.tensor_functions import (
            tensorflow_is_tensor_frnt_,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...utils.grid import tensorflow_create_meshgrid3d
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...filters.sobel import tensorflow_sobel
        from ....ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...utils._compat import tensorflow_torch_version_ge
        from ....ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from .nms import tensorflow_nms3d
        from ...utils.helpers import tensorflow_safe_solve_with_mask
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import (
            tensorflow_masked_scatter_frnt_,
        )
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_masked_fill__frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_expand_as_frnt_
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_bmm_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...core._backend import stack
        from ...core._backend import rand
        from ...core._backend import zeros_like
        from ...core._backend import where
    
        if not tensorflow_is_tensor_frnt_(input):
            raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
        if not len(tensorflow_shape_frnt_(input)) == 5:
            raise ValueError(
                f"Invalid input shape, we expect BxCxDxHxW. Got: {tensorflow_shape_frnt_(input)}"
            )
        B, CH, D, H, W = tensorflow_shape_frnt_(input)
        grid_global: typing.Any = tensorflow_permute_frnt_(
            tensorflow_create_meshgrid3d(D, H, W, False, device=input.device), 0, 4, 1, 2, 3
        )
        grid_global = tensorflow_to_frnt_(grid_global, input.dtype)
>       b: typing.Any = tensorflow_sobel.spatial_gradient3d(input, order=1, mode="diff")
E       AttributeError: Exception encountered when calling tensorflow_ConvQuadInterp3d.call().
E       
E       [1m'function' object has no attribute 'spatial_gradient3d'[0m
E       
E       Arguments received by tensorflow_ConvQuadInterp3d.call():
E         â€¢ x=tf.Tensor(shape=(1, 1, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/geometry/subpix/spatial_soft_argmax.py:1584: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.subpix.ConvQuadInterp3d
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_subpix.py::test_conv_quad_interp3d[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'spatial_gradient3d'
FAILED kornia/geometry/test_subpix.py::test_ConvQuadInterp3d[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_ConvQuadInterp3d.call().
=============================================================================== 2 failed, 13 passed in 298.47s (0:04:58) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/geometry/test_homography.py .......F                                                                                                                                                      [100%]

=============================================================================================== FAILURES ===============================================================================================
_________________________________________________________________________ test_symmetric_transfer_error[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_symmetric_transfer_error(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 4, 2),
            torch.rand(1, 4, 2),
            torch.rand(1, 3, 3),
        )
        trace_kwargs = {'squared': True, 'eps': 1e-8}
        test_args = (
            torch.rand(5, 4, 2),
            torch.rand(5, 4, 2),
            torch.rand(5, 3, 3),
        )
        test_kwargs = {'squared': True, 'eps': 1e-7}
>       _test_function(
            kornia.geometry.homography.symmetric_transfer_error,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_homography.py:206: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function symmetric_transfer_error at 0x7f544d6c0e50>
trace_args = (tensor([[[0.9771, 0.1563],
         [0.6962, 0.4101],
         [0.6845, 0.7695],
         [0.8009, 0.6634]]]), tensor...0.1366]]]), tensor([[[0.7304, 0.8693, 0.6648],
         [0.4639, 0.8154, 0.0312],
         [0.6434, 0.9630, 0.0497]]]))
trace_kwargs = {'eps': 1e-08, 'squared': True}
test_args = (tensor([[[0.8196, 0.0078],
         [0.7137, 0.1470],
         [0.9083, 0.4534],
         [0.2551, 0.2751]],

       ... 0.2584]],

        [[0.6586, 0.2652, 0.3840],
         [0.7384, 0.1559, 0.3272],
         [0.9540, 0.2143, 0.7587]]]))
test_kwargs = {'eps': 1e-07, 'squared': True}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function symmetric_transfer_error at 0x7f544d6c0e50>
trace_args = (tensor([[[0.9771, 0.1563],
         [0.6962, 0.4101],
         [0.6845, 0.7695],
         [0.8009, 0.6634]]]), tensor...0.1366]]]), tensor([[[0.7304, 0.8693, 0.6648],
         [0.4639, 0.8154, 0.0312],
         [0.6434, 0.9630, 0.0497]]]))
trace_kwargs = {'eps': 1e-08, 'squared': True}
test_args = (tensor([[[0.8196, 0.0078],
         [0.7137, 0.1470],
         [0.9083, 0.4534],
         [0.2551, 0.2751]],

       ... 0.2584]],

        [[0.6586, 0.2652, 0.3840],
         [0.7384, 0.1559, 0.3272],
         [0.9540, 0.2143, 0.7587]]]))
test_kwargs = {'eps': 1e-07, 'squared': True}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
            _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)
    
        # test it works with the test_args as input
        orig_out = fn(*test_args, **test_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(test_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(test_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[4.3906e+00, 1.0774e+02, 2.9895e+00, 3.1366e+00],
        [3.0385e+03, 6.2535e+00, 3.6503e+01, 7.6947e+00],
  ...],
        [4.6905e+00, 3.5227e+00, 5.0558e+00, 6.9752e+00],
        [3.9241e+03, 2.2867e+00, 3.5617e+01, 3.0300e+00]])
transpiled_x = <tf.Tensor: shape=(5, 4), dtype=float32, numpy=
array([[4.3906355e+00, 1.0773528e+02, 2.9895065e+00, 3.1365645e+00],
 ...557947e+00, 6.9751959e+00],
       [3.9241973e+03, 2.2867138e+00, 3.5617184e+01, 3.0300395e+00]],
      dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[4.3906250e+00, 1.0773542e+02, 2.9895053e+00, 3.1365612e+00],
       [3.0384910e+03, 6.2534990e+00, 3.6502991e+...0557947e+00, 6.9751954e+00],
       [3.9241484e+03, 2.2867105e+00, 3.5617222e+01, 3.0300395e+00]],
      dtype=float32)
y = array([[4.3906355e+00, 1.0773528e+02, 2.9895065e+00, 3.1365645e+00],
       [3.0385112e+03, 6.2534981e+00, 3.6503063e+...0557947e+00, 6.9751959e+00],
       [3.9241973e+03, 2.2867138e+00, 3.5617184e+01, 3.0300395e+00]],
      dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.homography.symmetric_transfer_error
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_homography.py::test_symmetric_transfer_error[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
=============================================================================== 1 failed, 7 passed in 167.90s (0:02:47) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 10 items

kornia/test_feature5.py F.FF..FFF.                                                                                                                                                               [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________________ test_LAFOrienter[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LAFOrienter(target_framework, mode, backend_compile):
        print("kornia.feature.LAFOrienter")
    
        if backend_compile:
            pytest.skip()
    
        import os
        flag = os.environ.get("APPLY_TRANSPOSE_OPTIMIZATION")
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = "false"
    
        TranspiledLAFOrienter = ivy.transpile(kornia.feature.LAFOrienter, source="torch", target=target_framework)
    
>       os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = flag

kornia/test_feature5.py:29: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = environ({'COLUMNS': '200', 'HOSTNAME': 'f04101356796', 'IVY_KEY': 'b137n3ADsDjns64kPbdfFUgDWwrWadHYOZjmu7mF-fY=', 'PWD...ivy-integration-tests/.ivy', 'USE_NATIVE_KERAS_LAYERS': 'true', 'PROFILE_S2S': 'false', 'CONV_BLOCK_DETECTED': 'true'})
key = b'APPLY_TRANSPOSE_OPTIMIZATION', value = None

    def __setitem__(self, key, value):
        key = self.encodekey(key)
>       value = self.encodevalue(value)

/opt/miniconda/envs/multienv/lib/python3.10/os.py:684: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

value = None

    def encode(value):
        if not isinstance(value, str):
>           raise TypeError("str expected, not %s" % type(value).__name__)
E           TypeError: str expected, not NoneType

/opt/miniconda/envs/multienv/lib/python3.10/os.py:756: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LAFOrienter
__________________________________________________________________________________ test_OriNet[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_OriNet(target_framework, mode, backend_compile):
        print("kornia.feature.OriNet")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledOriNet = ivy.transpile(kornia.feature.OriNet, source="torch", target=target_framework)
    
        patch = torch.rand(16, 1, 32, 32)
        torch_out = kornia.feature.OriNet()(patch)
    
        transpiled_patch = _nest_torch_tensor_to_new_framework(patch, target_framework)
>       transpiled_out = TranspiledOriNet()(transpiled_patch)

kornia/test_feature5.py:77: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.54350793, 0.6678106 , 0.9093085 , ..., 0.655184 ...
         [0.9386366 , 0.29566437, 0.18491662, ..., 0.88224995,
          0.06216604, 0.50544447]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06deac3870, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): t...,
         [0.9386366 , 0.29566437, 0.18491662, ..., 0.88224995,
          0.06216604, 0.50544447]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.54350793, 0.6678106 , 0.9093085 , ..., 0.655184 ...
         [0.9386366 , 0.29566437, 0.18491662, ..., 0.88224995,
          0.06216604, 0.50544447]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): t...,
         [0.9386366 , 0.29566437, 0.18491662, ..., 0.88224995,
          0.06216604, 0.50544447]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.54350793, 0.6678106 , 0.9093085 , ..., 0.655184 ...
         [0.9386366 , 0.29566437, 0.18491662, ..., 0.88224995,
          0.06216604, 0.50544447]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (patch)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): t...nsorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
),)
kwargs = {'patch': <tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.54350793, 0.6678106 , 0.9093085 , ..., ...,
         [0.9386366 , 0.29566437, 0.18491662, ..., 0.88224995,
          0.06216604, 0.50544447]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
patch = <tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.54350793, 0.6678106 , 0.9093085 , ..., 0.655184  ...],
         [0.9386366 , 0.29566437, 0.18491662, ..., 0.88224995,
          0.06216604, 0.50544447]]]], dtype=float32)>

    def call(self, patch):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_atan2_frnt,
        )
    
>       xy = tensorflow_view_frnt_(self.features(self._normalize_input(patch)), -1, 2)

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:696: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): KerasConv2D()
  (1): KerasBatchNorm2D()
  (2): tensorflow_ReLU()
  (3): KerasConv2D()
  ...)
  (18): tensorflow_Dropout()
  (19): KerasConv2D()
  (20): tensorflow_Tanh()
  (21): tensorflow_AdaptiveAvgPool2d()
)
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[ 0.18006402,  0.6134592 ,  1.4554685 , ...,  0.569...    [ 1.5245409 , -0.7474217 , -1.1387523 , ...,  1.3252969 ,
          -1.5724953 , -0.00615712]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55639220df80, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): KerasConv2D()
  (1): KerasBatchNorm2D()
  (2): tensorflow_ReLU()
  (3): KerasConv2D()
  ...)
  (18): tensorflow_Dropout()
  (19): KerasConv2D()
  (20): tensorflow_Tanh()
  (21): tensorflow_AdaptiveAvgPool2d()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[ 0.18006402,  0.6134592 ,  1.4554685 , ...,  0.569...    [ 1.5245409 , -0.7474217 , -1.1387523 , ...,  1.3252969 ,
          -1.5724953 , -0.00615712]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): KerasConv2D()
  (1): KerasBatchNorm2D()
  (2): tensorflow_ReLU()
  (3): KerasConv2D()
  ...)
  (18): tensorflow_Dropout()
  (19): KerasConv2D()
  (20): tensorflow_Tanh()
  (21): tensorflow_AdaptiveAvgPool2d()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[ 0.18006402,  0.6134592 ,  1.4554685 , ...,  0.569...    [ 1.5245409 , -0.7474217 , -1.1387523 , ...,  1.3252969 ,
          -1.5724953 , -0.00615712]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): KerasConv2D()
  (1): KerasBatchNorm2D()
  (2): tensorflow_ReLU()
  (3): KerasConv2D()
  ...)
  (18): tensorflow_Dropout()
  (19): KerasConv2D()
  (20): tensorflow_Tanh()
  (21): tensorflow_AdaptiveAvgPool2d()
)
input = <tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[ 0.18006402,  0.6134592 ,  1.4554685 , ...,  0.5694...      [ 1.5245409 , -0.7474217 , -1.1387523 , ...,  1.3252969 ,
          -1.5724953 , -0.00615712]]]], dtype=float32)>

    def call(self, input):
        for module in self:
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:181: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[ 0.18006402,  0.6134592 ,  1.4554685 , ...,  0.569...    [ 1.5245409 , -0.7474217 , -1.1387523 , ...,  1.3252969 ,
          -1.5724953 , -0.00615712]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06dfee03e0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[ 0.18006402,  0.6134592 ,  1.4554685 , ...,  0.569...    [ 1.5245409 , -0.7474217 , -1.1387523 , ...,  1.3252969 ,
          -1.5724953 , -0.00615712]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    def __call__(self, *args, **kwargs):
        if not self.built:
>           res = super().__call__(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:433: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
input = <tf.Tensor: shape=(16, 3, 34, 32), dtype=float32, numpy=
array([[[[ 0.        ,  0.        ,  0.        , ...,  0.    ...      [ 0.        ,  0.        ,  0.        , ...,  0.        ,
           0.        ,  0.        ]]]], dtype=float32)>
training = False

    def call(self, input, training=False):
        if self._padding != 0:
            padding_mode = (
                "constant" if self.padding_mode == "zeros" else self.padding_mode
            )
            # handle Pytorch-style padding
            input = torch_pad(
                input, self._reversed_padding_repeated_twice, mode=padding_mode
            )
>       return super().call(input)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:488: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def tf___conv_xla():
        with ag__.FunctionScope('_conv_xla', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:
            do_return = False
            retval_ = ag__.UndefinedReturnValue()
            try:
                do_return = True
>               retval_ = ag__.converted_call(ag__.ld(_conv), (), None, fscope)
E               ValueError: Exception encountered when calling KerasConv2D.call().
E               
E               [1min user code:
E               
E                   File "/opt/fw/tensorflow/keras/src/backend/tensorflow/nn.py", line 244, in _conv_xla  *
E                       return _conv()
E                   File "/opt/fw/tensorflow/keras/src/backend/tensorflow/nn.py", line 231, in _conv  **
E                       return tf.nn.convolution(
E               
E                   ValueError: Depth of output (16) is not a multiple of the number of groups (3) for '{{node convolution}} = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], explicit_paddings=[], padding="VALID", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](convolution/input, convolution/ReadVariableOp)' with input shapes: [16,3,34,32], [3,3,1,16].
E               [0m
E               
E               Arguments received by KerasConv2D.call():
E                 â€¢ input=tf.Tensor(shape=(16, 1, 32, 32), dtype=float32)
E                 â€¢ training=False

/tmp/__autograph_generated_filegfslv79p.py:13: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.OriNet
__________________________________________________________________________ test_LAFAffNetShapeEstimator[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LAFAffNetShapeEstimator(target_framework, mode, backend_compile):
        print("kornia.feature.LAFAffNetShapeEstimator")
    
        if backend_compile:
            pytest.skip()
    
        import os
        flag = os.environ.get("APPLY_TRANSPOSE_OPTIMIZATION")
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = "false"
    
        TranspiledLAFAffNetShapeEstimator = ivy.transpile(kornia.feature.LAFAffNetShapeEstimator, source="torch", target=target_framework)
    
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = flag
    
        laf = torch.rand(10, 2, 2, 3)
        img = torch.rand(10, 1, 32, 32)
        torch_out = kornia.feature.LAFAffNetShapeEstimator()(laf, img)
    
        transpiled_laf = _nest_torch_tensor_to_new_framework(laf, target_framework)
        transpiled_img = _nest_torch_tensor_to_new_framework(img, target_framework)
>       transpiled_out = TranspiledLAFAffNetShapeEstimator()(transpiled_laf, transpiled_img)

kornia/test_feature5.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (<tf.Tensor: shape=(10, 2, 2, 3), dtype=float32, numpy=
array([[[[0.82115984, 0.8257181 , 0.8651914 ],
         [0.755...10480595e-04, 2.39591122e-01, ...,
          6.72141254e-01, 9.35357273e-01, 3.91447425e-01]]]],
      dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x556392778a90, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNo...10480595e-04, 2.39591122e-01, ...,
          6.72141254e-01, 9.35357273e-01, 3.91447425e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(10, 2, 2, 3), dtype=float32, numpy=
array([[[[0.82115984, 0.8257181 , 0.8651914 ],
         [0.755...10480595e-04, 2.39591122e-01, ...,
          6.72141254e-01, 9.35357273e-01, 3.91447425e-01]]]],
      dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNo...10480595e-04, 2.39591122e-01, ...,
          6.72141254e-01, 9.35357273e-01, 3.91447425e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(10, 2, 2, 3), dtype=float32, numpy=
array([[[[0.82115984, 0.8257181 , 0.8651914 ],
         [0.755...10480595e-04, 2.39591122e-01, ...,
          6.72141254e-01, 9.35357273e-01, 3.91447425e-01]]]],
      dtype=float32)>)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (laf, img)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNo...nsorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
),)
kwargs = {'img': <tf.Tensor: shape=(10, 1, 32, 32), dtype=float32, numpy=
array([[[[4.35113013e-01, 8.56320202e-01, 3.31948936e... ]],

        [[0.23980272, 0.04318184, 0.7565485 ],
         [0.7435255 , 0.96275544, 0.9875495 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
laf = <tf.Tensor: shape=(10, 2, 2, 3), dtype=float32, numpy=
array([[[[0.82115984, 0.8257181 , 0.8651914 ],
         [0.7554...6 ]],

        [[0.23980272, 0.04318184, 0.7565485 ],
         [0.7435255 , 0.96275544, 0.9875495 ]]]], dtype=float32)>
img = <tf.Tensor: shape=(10, 1, 32, 32), dtype=float32, numpy=
array([[[[4.35113013e-01, 8.56320202e-01, 3.31948936e-01, .......10480595e-04, 2.39591122e-01, ...,
          6.72141254e-01, 9.35357273e-01, 3.91447425e-01]]]],
      dtype=float32)>

    def call(self, laf, img):
        from ..core.check import tensorflow_KORNIA_CHECK_LAF
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from .laf import tensorflow_extract_patches_from_pyramid
        from .laf import tensorflow_make_upright
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from .laf import tensorflow_get_laf_scale
        from .laf import tensorflow_get_laf_orientation
        from .laf import tensorflow_scale_laf
        from .laf import tensorflow_set_laf_orientation
    
        tensorflow_KORNIA_CHECK_LAF(laf)
        tensorflow_KORNIA_CHECK_SHAPE(img, ["B", "1", "H", "W"])
        B, N = tensorflow_shape_frnt_(laf)[:2][0], tensorflow_shape_frnt_(laf)[:2][1]
        PS: typing.Any = self.patch_size
        patches: typing.Any = tensorflow_view_frnt_(
            tensorflow_extract_patches_from_pyramid(
                img, tensorflow_make_upright(laf), PS, True
            ),
            -1,
            1,
            PS,
            PS,
        )
>       xy = tensorflow_view_frnt_(self.features(self._normalize_input(patches)), -1, 3)

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:258: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): KerasConv2D()
  (1): KerasBatchNorm2D()
  (2): tensorflow_ReLU()
  (3): KerasConv2D()
  ...)
  (18): tensorflow_Dropout()
  (19): KerasConv2D()
  (20): tensorflow_Tanh()
  (21): tensorflow_AdaptiveAvgPool2d()
)
args = (<tf.Tensor: shape=(20, 1, 32, 32), dtype=float32, numpy=
array([[[[22.593105  , 22.593105  , -0.04421351, ..., -0.044...    [ 0.        ,  0.        ,  0.        , ...,  0.        ,
           0.        ,  0.        ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x556392c33050, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): KerasConv2D()
  (1): KerasBatchNorm2D()
  (2): tensorflow_ReLU()
  (3): KerasConv2D()
  ...)
  (18): tensorflow_Dropout()
  (19): KerasConv2D()
  (20): tensorflow_Tanh()
  (21): tensorflow_AdaptiveAvgPool2d()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(20, 1, 32, 32), dtype=float32, numpy=
array([[[[22.593105  , 22.593105  , -0.04421351, ..., -0.044...    [ 0.        ,  0.        ,  0.        , ...,  0.        ,
           0.        ,  0.        ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): KerasConv2D()
  (1): KerasBatchNorm2D()
  (2): tensorflow_ReLU()
  (3): KerasConv2D()
  ...)
  (18): tensorflow_Dropout()
  (19): KerasConv2D()
  (20): tensorflow_Tanh()
  (21): tensorflow_AdaptiveAvgPool2d()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(20, 1, 32, 32), dtype=float32, numpy=
array([[[[22.593105  , 22.593105  , -0.04421351, ..., -0.044...    [ 0.        ,  0.        ,  0.        , ...,  0.        ,
           0.        ,  0.        ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): KerasConv2D()
  (1): KerasBatchNorm2D()
  (2): tensorflow_ReLU()
  (3): KerasConv2D()
  ...)
  (18): tensorflow_Dropout()
  (19): KerasConv2D()
  (20): tensorflow_Tanh()
  (21): tensorflow_AdaptiveAvgPool2d()
)
input = <tf.Tensor: shape=(20, 1, 32, 32), dtype=float32, numpy=
array([[[[22.593105  , 22.593105  , -0.04421351, ..., -0.0442...      [ 0.        ,  0.        ,  0.        , ...,  0.        ,
           0.        ,  0.        ]]]], dtype=float32)>

    def call(self, input):
        for module in self:
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:181: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
args = (<tf.Tensor: shape=(20, 1, 32, 32), dtype=float32, numpy=
array([[[[22.593105  , 22.593105  , -0.04421351, ..., -0.044...    [ 0.        ,  0.        ,  0.        , ...,  0.        ,
           0.        ,  0.        ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06de309c40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
args = (<tf.Tensor: shape=(20, 1, 32, 32), dtype=float32, numpy=
array([[[[22.593105  , 22.593105  , -0.04421351, ..., -0.044...    [ 0.        ,  0.        ,  0.        , ...,  0.        ,
           0.        ,  0.        ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    def __call__(self, *args, **kwargs):
        if not self.built:
>           res = super().__call__(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:433: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
input = <tf.Tensor: shape=(20, 3, 34, 32), dtype=float32, numpy=
array([[[[ 0.        ,  0.        ,  0.        , ...,  0.    ...      [ 0.        ,  0.        ,  0.        , ...,  0.        ,
           0.        ,  0.        ]]]], dtype=float32)>
training = False

    def call(self, input, training=False):
        if self._padding != 0:
            padding_mode = (
                "constant" if self.padding_mode == "zeros" else self.padding_mode
            )
            # handle Pytorch-style padding
            input = torch_pad(
                input, self._reversed_padding_repeated_twice, mode=padding_mode
            )
>       return super().call(input)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:488: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def tf___conv_xla():
        with ag__.FunctionScope('_conv_xla', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:
            do_return = False
            retval_ = ag__.UndefinedReturnValue()
            try:
                do_return = True
>               retval_ = ag__.converted_call(ag__.ld(_conv), (), None, fscope)
E               ValueError: Exception encountered when calling KerasConv2D.call().
E               
E               [1min user code:
E               
E                   File "/opt/fw/tensorflow/keras/src/backend/tensorflow/nn.py", line 244, in _conv_xla  *
E                       return _conv()
E                   File "/opt/fw/tensorflow/keras/src/backend/tensorflow/nn.py", line 231, in _conv  **
E                       return tf.nn.convolution(
E               
E                   ValueError: Depth of output (16) is not a multiple of the number of groups (3) for '{{node convolution}} = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], explicit_paddings=[], padding="VALID", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](convolution/input, convolution/ReadVariableOp)' with input shapes: [20,3,34,32], [3,3,1,16].
E               [0m
E               
E               Arguments received by KerasConv2D.call():
E                 â€¢ input=tf.Tensor(shape=(20, 1, 32, 32), dtype=float32)
E                 â€¢ training=False

/tmp/__autograph_generated_filegfslv79p.py:13: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LAFAffNetShapeEstimator
___________________________________________________________________________________ test_DeFMO[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DeFMO(target_framework, mode, backend_compile):
        print("kornia.feature.DeFMO")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDeFMO = ivy.transpile(kornia.feature.DeFMO, source="torch", target=target_framework)
    
        x = torch.rand(2, 6, 240, 320)
        torch_out = kornia.feature.DeFMO()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDeFMO()(transpiled_x)

kornia/test_feature5.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DeFMO(
  (encoder): tensorflow_EncoderDeFMO(
    (net): tensorflow_Sequential(
      (0): tensorflow_Sequen...tensorflow_PixelShuffle()
      (11): KerasConv2D()
      (12): tensorflow_ReLU()
      (13): KerasConv2D()
    )
  )
)
args = (<tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 , ..., 0.463167...
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06de437a40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DeFMO(
  (encoder): tensorflow_EncoderDeFMO(
    (net): tensorflow_Sequential(
      (0): tensorflow_Seque...,
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DeFMO(
  (encoder): tensorflow_EncoderDeFMO(
    (net): tensorflow_Sequential(
      (0): tensorflow_Sequen...tensorflow_PixelShuffle()
      (11): KerasConv2D()
      (12): tensorflow_ReLU()
      (13): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 , ..., 0.463167...
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DeFMO(
  (encoder): tensorflow_EncoderDeFMO(
    (net): tensorflow_Sequential(
      (0): tensorflow_Seque...,
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DeFMO(
  (encoder): tensorflow_EncoderDeFMO(
    (net): tensorflow_Sequential(
      (0): tensorflow_Sequen...tensorflow_PixelShuffle()
      (11): KerasConv2D()
      (12): tensorflow_ReLU()
      (13): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 , ..., 0.463167...
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input_data)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DeFMO(
  (encoder): tensorflow_EncoderDeFMO(
    (net): tensorflow_Sequential(
      (0): tensorflow_Seque...nsorflow_PixelShuffle()
      (11): KerasConv2D()
      (12): tensorflow_ReLU()
      (13): KerasConv2D()
    )
  )
),)
kwargs = {'input_data': <tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 ,...,
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DeFMO(
  (encoder): tensorflow_EncoderDeFMO(
    (net): tensorflow_Sequential(
      (0): tensorflow_Sequen...tensorflow_PixelShuffle()
      (11): KerasConv2D()
      (12): tensorflow_ReLU()
      (13): KerasConv2D()
    )
  )
)
input_data = <tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 , ..., 0.4631678...],
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>

    def call(self, input_data):
>       latent = self.encoder(input_data)

Translated_Outputs/tensorflow_outputs/kornia/feature/defmo.py:1973: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_EncoderDeFMO(
  (net): tensorflow_Sequential(
    (0): tensorflow_Sequential(
      (0): KerasConv2D()
    ...3): KerasConv2D()
          (bn3): KerasBatchNorm2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
args = (<tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 , ..., 0.463167...
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06de36af20, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_EncoderDeFMO(
  (net): tensorflow_Sequential(
    (0): tensorflow_Sequential(
      (0): KerasConv2D()
    ...3): KerasConv2D()
          (bn3): KerasBatchNorm2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 , ..., 0.463167...
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_EncoderDeFMO(
  (net): tensorflow_Sequential(
    (0): tensorflow_Sequential(
      (0): KerasConv2D()
    ...3): KerasConv2D()
          (bn3): KerasBatchNorm2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 , ..., 0.463167...
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input_data)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_EncoderDeFMO(
  (net): tensorflow_Sequential(
    (0): tensorflow_Sequential(
      (0): KerasConv2D()
    ...3): KerasConv2D()
          (bn3): KerasBatchNorm2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
input_data = <tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 , ..., 0.4631678...],
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>

    def call(self, input_data):
>       return self.net(input_data)

Translated_Outputs/tensorflow_outputs/kornia/feature/defmo.py:1083: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): tenso...
        (conv3): KerasConv2D()
        (bn3): KerasBatchNorm2D()
        (relu): tensorflow_ReLU()
      )
    )
  )
)
args = (<tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 , ..., 0.463167...
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55639361b890, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): tenso...
        (conv3): KerasConv2D()
        (bn3): KerasBatchNorm2D()
        (relu): tensorflow_ReLU()
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 , ..., 0.463167...
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): tenso...
        (conv3): KerasConv2D()
        (bn3): KerasBatchNorm2D()
        (relu): tensorflow_ReLU()
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 6, 240, 320), dtype=float32, numpy=
array([[[[0.6377418 , 0.07689083, 0.5349883 , ..., 0.463167...
         [0.29605693, 0.14272374, 0.49762285, ..., 0.36763322,
          0.24097478, 0.07756633]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): tenso...
        (conv3): KerasConv2D()
        (bn3): KerasBatchNorm2D()
        (relu): tensorflow_ReLU()
      )
    )
  )
)
input = <tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.078921...],
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>

    def call(self, input):
        for module in self:
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:181: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Sequential(
    (0): tensorflow_Bottleneck(
      (conv1): KerasConv2D()
    ...sBatchNorm2D()
      (conv3): KerasConv2D()
      (bn3): KerasBatchNorm2D()
      (relu): tensorflow_ReLU()
    )
  )
)
args = (<tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.07892...
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x5563ff3e9ba0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Sequential(
    (0): tensorflow_Bottleneck(
      (conv1): KerasConv2D()
    ...sBatchNorm2D()
      (conv3): KerasConv2D()
      (bn3): KerasBatchNorm2D()
      (relu): tensorflow_ReLU()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.07892...
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Sequential(
    (0): tensorflow_Bottleneck(
      (conv1): KerasConv2D()
    ...sBatchNorm2D()
      (conv3): KerasConv2D()
      (bn3): KerasBatchNorm2D()
      (relu): tensorflow_ReLU()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.07892...
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Sequential(
    (0): tensorflow_Bottleneck(
      (conv1): KerasConv2D()
    ...sBatchNorm2D()
      (conv3): KerasConv2D()
      (bn3): KerasBatchNorm2D()
      (relu): tensorflow_ReLU()
    )
  )
)
input = <tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.078921...],
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>

    def call(self, input):
        for module in self:
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:181: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
    (con... (bn2): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (relu): tensorflow_ReLU()
  )
)
args = (<tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.07892...
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06de3bdb60, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
    (con... (bn2): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (relu): tensorflow_ReLU()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.07892...
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
    (con... (bn2): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (relu): tensorflow_ReLU()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.07892...
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
    (con... (bn2): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (relu): tensorflow_ReLU()
  )
)
input = <tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.078921...],
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>

    def call(self, input):
        for module in self:
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:181: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn2): KerasBat...elu): tensorflow_ReLU()
  (downsample): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
  )
)
args = (<tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.07892...
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06de3997d0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn2): KerasBat...elu): tensorflow_ReLU()
  (downsample): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.07892...
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn2): KerasBat...elu): tensorflow_ReLU()
  (downsample): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.07892...
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn2): KerasBat...elu): tensorflow_ReLU()
  (downsample): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
  )
)
x = <tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.078921...],
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>

    def call(self, x):
        identity = x
>       out = self.conv1(x)

Translated_Outputs/tensorflow_outputs/kornia/feature/defmo.py:142: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
args = (<tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.07892...
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06de399f10, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
args = (<tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.07892...
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    def __call__(self, *args, **kwargs):
        if not self.built:
>           res = super().__call__(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:433: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
input = <tf.Tensor: shape=(2, 64, 120, 157), dtype=float32, numpy=
array([[[[0.18584184, 0.19560124, 0.20551199, ..., 0.078921...],
         [0.10489538, 0.03040081, 0.        , ..., 0.10016613,
          0.11076012, 0.09658428]]]], dtype=float32)>
training = False

    def call(self, input, training=False):
        if self._padding != 0:
            padding_mode = (
                "constant" if self.padding_mode == "zeros" else self.padding_mode
            )
            # handle Pytorch-style padding
            input = torch_pad(
                input, self._reversed_padding_repeated_twice, mode=padding_mode
            )
>       return super().call(input)
E       tensorflow.python.framework.errors_impl.UnimplementedError: Exception encountered when calling KerasConv2D.call().
E       
E       [1m{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:CPU:0}} The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW [Op:Conv2D][0m
E       
E       Arguments received by KerasConv2D.call():
E         â€¢ input=tf.Tensor(shape=(2, 64, 120, 157), dtype=float32)
E         â€¢ training=False

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:488: UnimplementedError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.DeFMO
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1726288306.318553      34 service.cc:146] XLA service 0x556394094830 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1726288306.318618      34 service.cc:154]   StreamExecutor device (0): Host, Default Version
I0000 00:00:1726288306.346959      34 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
__________________________________________________________________________________ test_DeDoDe[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DeDoDe(target_framework, mode, backend_compile):
        print("kornia.feature.DeDoDe")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDeDoDe = ivy.transpile(kornia.feature.DeDoDe, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 256, 256)
        torch_out = kornia.feature.DeDoDe(amp_dtype=torch.float32)(x)
    
        ivy.set_backend(target_framework)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDeDoDe(amp_dtype=ivy.as_native_dtype("float32"))(transpiled_x)

kornia/test_feature5.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DeDoDe(), detector_model = 'L', descriptor_model = 'G', amp_dtype = tf.float32

    def __init__(self, detector_model="L", descriptor_model="G", amp_dtype=tf.float16):
        from .dedode_models import tensorflow_get_detector
        from .dedode_models import tensorflow_get_descriptor
        from ...enhance.normalize import tensorflow_normalize
        from ....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
    
        self.super___init__(
            detector_model=detector_model,
            descriptor_model=descriptor_model,
            amp_dtype=amp_dtype,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.detector: typing.Any = tensorflow_get_detector(detector_model, amp_dtype)

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/dedode.py:68: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kind = 'L', amp_dtype = tf.float32

    def tensorflow_get_detector(kind="L", amp_dtype=tf.float16):
        if kind == "L":
>           return tensorflow_dedode_detector_L(amp_dtype)

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/dedode_models.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

amp_dtype = tf.float32

    def tensorflow_dedode_detector_L(amp_dtype=tf.float16):
        from ....torch.nn.modules.container import tensorflow_ModuleDict
        from .decoder import tensorflow_ConvRefiner
        from .encoder import tensorflow_VGG19
        from .decoder import tensorflow_Decoder
        from .detector import tensorflow_DeDoDeDetector
    
        NUM_PROTOTYPES = 1
        residual = True
        hidden_blocks = 8
        amp = True
        conv_refiner = tensorflow_ModuleDict(
            {
>               "8": tensorflow_ConvRefiner(
                    512,
                    512,
                    256 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
                "4": tensorflow_ConvRefiner(
                    256 + 256,
                    256,
                    128 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
                "2": tensorflow_ConvRefiner(
                    128 + 128,
                    128,
                    64 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
                "1": tensorflow_ConvRefiner(
                    64 + 64,
                    64,
                    1 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
            }
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/dedode_models.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvRefiner(), args = (512, 512, 257), kwargs = {'amp': True, 'amp_dtype': tf.float32, 'hidden_blocks': 8, 'residual': True}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvRefiner(), in_dim = 512, hidden_dim = 512, out_dim = 257, dw = True, kernel_size = 5, hidden_blocks = 8, amp = True, residual = True, amp_dtype = tf.float32

    @tensorflow_store_config_info
    def __init__(
        self,
        in_dim=6,
        hidden_dim=16,
        out_dim=2,
        dw=True,
        kernel_size=5,
        hidden_blocks=5,
        amp=True,
        residual=False,
        amp_dtype=tf.float16,
    ):
        from ....torch.nn.modules.container import tensorflow_Sequential
        from ....tensorflow__stateful_layers import KerasConv2D
    
        self.super___init__(
            in_dim=in_dim,
            hidden_dim=hidden_dim,
            out_dim=out_dim,
            dw=dw,
            kernel_size=kernel_size,
            hidden_blocks=hidden_blocks,
            amp=amp,
            residual=residual,
            amp_dtype=amp_dtype,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.block1 = self.create_block(in_dim, hidden_dim, dw=False, kernel_size=1)

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/decoder.py:466: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvRefiner(), in_dim = 512, out_dim = 512, dw = False, kernel_size = 1, bias = True
norm_type = <class 'Translated_Outputs.tensorflow_outputs.tensorflow__stateful_layers.KerasBatchNorm2D'>

    def create_block(
        self,
        in_dim,
        out_dim,
        dw=True,
        kernel_size=5,
        bias=True,
        norm_type=KerasBatchNorm2D,
    ):
        from ....torch.nn.modules.container import tensorflow_Sequential
        from ....torch.nn.modules.activation import tensorflow_ReLU
        from ....tensorflow__stateful_layers import KerasConv2D
        from ....tensorflow__stateful_layers import resolve_convolution
        from ....tensorflow__stateful_layers import KerasBatchNorm2D
    
        num_groups = 1 if not dw else in_dim
        if dw:
            if out_dim % in_dim != 0:
                raise Exception("outdim must be divisible by indim for depthwise")
        conv1 = resolve_convolution(
            in_channels=in_dim,
            filters=out_dim,
            kernel_size=kernel_size,
            strides=1,
            padding=kernel_size // 2,
            use_bias=bias,
            dilation_rate=1,
            groups=num_groups,
            padding_mode="zeros",
            data_format="channels_first",
        )
        norm = (
>           norm_type(out_dim)
            if norm_type is KerasBatchNorm2D
            else norm_type(num_channels=out_dim)
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/decoder.py:524: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasBatchNorm2D(), args = (512,), kwargs = {}

    def __init__(self, *args, **kwargs):
        self._previous_frame_info = None
    
        # pytorch layer attributes
>       self.num_features = kwargs.pop("num_features")
E       KeyError: 'num_features'

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:580: KeyError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.DeDoDe
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth" to /root/.cache/torch/hub/checkpoints/dinov2_vitl14_pretrain.pth

  0%|          | 0.00/1.13G [00:00<?, ?B/s]
  0%|          | 1.38M/1.13G [00:00<01:53, 10.7MB/s]
  0%|          | 2.75M/1.13G [00:00<01:40, 12.1MB/s]
  1%|          | 6.75M/1.13G [00:00<00:49, 24.4MB/s]
  1%|          | 11.1M/1.13G [00:00<00:38, 31.5MB/s]
  1%|â–         | 16.2M/1.13G [00:00<00:30, 38.9MB/s]
  2%|â–         | 22.8M/1.13G [00:00<00:24, 48.4MB/s]
  3%|â–Ž         | 30.4M/1.13G [00:00<00:20, 57.6MB/s]
  3%|â–Ž         | 39.9M/1.13G [00:00<00:16, 70.0MB/s]
  4%|â–         | 51.1M/1.13G [00:00<00:13, 84.5MB/s]
  6%|â–Œ         | 64.9M/1.13G [00:01<00:11, 102MB/s] 
  7%|â–‹         | 81.8M/1.13G [00:01<00:09, 124MB/s]
  9%|â–Š         | 101M/1.13G [00:01<00:07, 146MB/s] 
 10%|â–ˆ         | 119M/1.13G [00:01<00:06, 158MB/s]
 12%|â–ˆâ–        | 138M/1.13G [00:01<00:06, 171MB/s]
 14%|â–ˆâ–Ž        | 157M/1.13G [00:01<00:05, 180MB/s]
 15%|â–ˆâ–Œ        | 176M/1.13G [00:01<00:05, 184MB/s]
 17%|â–ˆâ–‹        | 195M/1.13G [00:01<00:05, 188MB/s]
 18%|â–ˆâ–Š        | 213M/1.13G [00:01<00:05, 187MB/s]
 20%|â–ˆâ–‰        | 232M/1.13G [00:01<00:05, 190MB/s]
 22%|â–ˆâ–ˆâ–       | 250M/1.13G [00:02<00:05, 185MB/s]
 23%|â–ˆâ–ˆâ–Ž       | 269M/1.13G [00:02<00:04, 189MB/s]
 25%|â–ˆâ–ˆâ–       | 289M/1.13G [00:02<00:04, 194MB/s]
 27%|â–ˆâ–ˆâ–‹       | 309M/1.13G [00:02<00:04, 199MB/s]
 28%|â–ˆâ–ˆâ–Š       | 329M/1.13G [00:02<00:04, 202MB/s]
 30%|â–ˆâ–ˆâ–ˆ       | 349M/1.13G [00:02<00:04, 203MB/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 369M/1.13G [00:02<00:04, 205MB/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 388M/1.13G [00:02<00:03, 204MB/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 409M/1.13G [00:02<00:03, 207MB/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 428M/1.13G [00:02<00:03, 198MB/s]
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 448M/1.13G [00:03<00:03, 198MB/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 466M/1.13G [00:03<00:03, 195MB/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 485M/1.13G [00:03<00:03, 196MB/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 505M/1.13G [00:03<00:03, 198MB/s]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 525M/1.13G [00:03<00:03, 202MB/s]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 545M/1.13G [00:03<00:03, 204MB/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 565M/1.13G [00:03<00:03, 207MB/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 585M/1.13G [00:03<00:02, 207MB/s]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 605M/1.13G [00:03<00:02, 204MB/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 625M/1.13G [00:04<00:02, 206MB/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 645M/1.13G [00:04<00:02, 207MB/s]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 665M/1.13G [00:04<00:02, 209MB/s]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 685M/1.13G [00:04<00:02, 209MB/s]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 705M/1.13G [00:04<00:02, 209MB/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 725M/1.13G [00:04<00:02, 210MB/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 746M/1.13G [00:04<00:02, 210MB/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 766M/1.13G [00:04<00:01, 210MB/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 786M/1.13G [00:04<00:01, 209MB/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 806M/1.13G [00:04<00:01, 209MB/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 826M/1.13G [00:05<00:01, 208MB/s]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 846M/1.13G [00:05<00:01, 208MB/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 866M/1.13G [00:05<00:01, 206MB/s]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 886M/1.13G [00:05<00:01, 206MB/s]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 905M/1.13G [00:05<00:01, 201MB/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 925M/1.13G [00:05<00:01, 203MB/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 945M/1.13G [00:05<00:01, 205MB/s]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 966M/1.13G [00:05<00:00, 207MB/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 985M/1.13G [00:05<00:00, 207MB/s]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 0.98G/1.13G [00:05<00:00, 191MB/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 1.00G/1.13G [00:06<00:00, 195MB/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1.02G/1.13G [00:06<00:00, 197MB/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.04G/1.13G [00:06<00:00, 200MB/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1.06G/1.13G [00:06<00:00, 203MB/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1.08G/1.13G [00:06<00:00, 204MB/s]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1.10G/1.13G [00:06<00:00, 205MB/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1.12G/1.13G [00:06<00:00, 207MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.13G/1.13G [00:06<00:00, 180MB/s]
___________________________________________________________________________________ test_DISK[tensorflow-s2s-False] ____________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DISK(target_framework, mode, backend_compile):
        print("kornia.feature.DISK")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDISK = ivy.transpile(kornia.feature.DISK, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 256, 256)
        torch_out = kornia.feature.DISK()(x)[0]
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDISK()(transpiled_x)

kornia/test_feature5.py:189: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06de437300, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDow...,
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDow...,
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.0619599...],
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (images, n=None, window_size=5, score_threshold=0.0, pad_if_not_divisible=False)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDow... tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
),)
kwargs = {'images': <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ......,
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
images = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.0619599...],
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>
n = None, window_size = 5, score_threshold = 0.0, pad_if_not_divisible = False

    def call(
        self,
        images,
        n=None,
        window_size=5,
        score_threshold=0.0,
        pad_if_not_divisible=False,
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_pad_frnt,
        )
        from .detector import tensorflow_heatmap_to_keypoints
    
        B = tensorflow_shape_frnt_(images)[0]
        if pad_if_not_divisible:
            h, w = (
                tensorflow_shape_frnt_(images)[2:][0],
                tensorflow_shape_frnt_(images)[2:][1],
            )
            pd_h = 16 - h % 16 if h % 16 > 0 else 0
            pd_w = 16 - w % 16 if w % 16 > 0 else 0
            images = tensorflow_pad_frnt(images, (0, pd_w, 0, pd_h), value=0.0)
>       heatmaps, descriptors = self.heatmap_and_dense_descriptors(images)

Translated_Outputs/tensorflow_outputs/kornia/feature/disk/disk.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
images = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.0619599...],
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>

    def heatmap_and_dense_descriptors(self, images):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
>       unet_output = self.unet(images)

Translated_Outputs/tensorflow_outputs/kornia/feature/disk/disk.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06e33ecec0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ls.py', lineno=117, function='error_handler', code_context=['            return fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (inp)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
inp = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.0619599...],
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>

    def call(self, inp):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
    
        if tensorflow_size_frnt_(inp, 1) != self.in_features:
            fmt = "Expected {} feature channels in input, got {}"
            msg = fmt.format(self.in_features, tensorflow_size_frnt_(inp, 1))
            raise ValueError(msg)
        input_size_divisor = 2 ** len(self.up)
        if (
            tensorflow_size_frnt_(inp, 2) % input_size_divisor != 0
            or tensorflow_size_frnt_(inp, 3) % input_size_divisor != 0
        ):
            raise ValueError(
                f"Input image shape must be divisible by {input_size_divisor} (got {tensorflow_size_frnt_(inp)}). This is not inherent to DISK, but to the U-Net architecture used in pretrained models. Please pad if necessary."
            )
        features = [inp]
        for layer in self.path_down:
>           features.append(layer(features[-1]))

Translated_Outputs/tensorflow_outputs/kornia/feature/disk/_unets/unet.py:91: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_Sequential()
  (1): tensorflow_Conv(
    (0): tensorflow_Sequential()
    (1): tensorflow_Sequential()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06e33ee650, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_Sequential()
  (1): tensorflow_Conv(
    (0): tensorflow_Sequential()
    (1): tensorflow_Sequential()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_Sequential()
  (1): tensorflow_Conv(
    (0): tensorflow_Sequential()
    (1): tensorflow_Sequential()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_Sequential()
  (1): tensorflow_Conv(
    (0): tensorflow_Sequential()
    (1): tensorflow_Sequential()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
input = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.0619599...],
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>

    def call(self, input):
        for module in self:
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_Sequential()
  (1): tensorflow_Sequential()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
)
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f06e33ef130, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_Sequential()
  (1): tensorflow_Sequential()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_Sequential()
  (1): tensorflow_Sequential()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_Sequential()
  (1): tensorflow_Sequential()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
)
input = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.0619599...],
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>

    def call(self, input):
        for module in self:
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x5563925d5640, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.2746362 , 0.808376  , 0.23146343, ..., 0.061959...
         [0.43736613, 0.9383005 , 0.69958556, ..., 0.8657289 ,
          0.7251562 , 0.7599752 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    def __call__(self, *args, **kwargs):
        if not self.built:
>           res = super().__call__(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:433: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D()
input = <tf.Tensor: shape=(1, 7, 260, 256), dtype=float32, numpy=
array([[[[0.        , 0.        , 0.        , ..., 0.       ...],
         [0.        , 0.        , 0.        , ..., 0.        ,
          0.        , 0.        ]]]], dtype=float32)>
training = False

    def call(self, input, training=False):
        if self._padding != 0:
            padding_mode = (
                "constant" if self.padding_mode == "zeros" else self.padding_mode
            )
            # handle Pytorch-style padding
            input = torch_pad(
                input, self._reversed_padding_repeated_twice, mode=padding_mode
            )
>       return super().call(input)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:488: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def tf___conv_xla():
        with ag__.FunctionScope('_conv_xla', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:
            do_return = False
            retval_ = ag__.UndefinedReturnValue()
            try:
                do_return = True
>               retval_ = ag__.converted_call(ag__.ld(_conv), (), None, fscope)
E               ValueError: Exception encountered when calling KerasConv2D.call().
E               
E               [1min user code:
E               
E                   File "/opt/fw/tensorflow/keras/src/backend/tensorflow/nn.py", line 244, in _conv_xla  *
E                       return _conv()
E                   File "/opt/fw/tensorflow/keras/src/backend/tensorflow/nn.py", line 231, in _conv  **
E                       return tf.nn.convolution(
E               
E                   ValueError: Depth of input (7) is not a multiple of input depth of filter (3) for '{{node convolution}} = Conv2D[T=DT_FLOAT, data_format="NCHW", dilations=[1, 1, 1, 1], explicit_paddings=[], padding="VALID", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](convolution/input, convolution/ReadVariableOp)' with input shapes: [1,7,260,256], [5,5,3,16].
E               [0m
E               
E               Arguments received by KerasConv2D.call():
E                 â€¢ input=tf.Tensor(shape=(1, 3, 256, 256), dtype=float32)
E                 â€¢ training=False

/tmp/__autograph_generated_filegfslv79p.py:13: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.DISK
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_feature5.py::test_LAFOrienter[tensorflow-s2s-False] - TypeError: str expected, not NoneType
FAILED kornia/test_feature5.py::test_OriNet[tensorflow-s2s-False] - ValueError: Exception encountered when calling KerasConv2D.call().
FAILED kornia/test_feature5.py::test_LAFAffNetShapeEstimator[tensorflow-s2s-False] - ValueError: Exception encountered when calling KerasConv2D.call().
FAILED kornia/test_feature5.py::test_DeFMO[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.UnimplementedError: Exception encountered when calling KerasConv2D.call().
FAILED kornia/test_feature5.py::test_DeDoDe[tensorflow-s2s-False] - KeyError: 'num_features'
FAILED kornia/test_feature5.py::test_DISK[tensorflow-s2s-False] - ValueError: Exception encountered when calling KerasConv2D.call().
=============================================================================== 6 failed, 4 passed in 1176.06s (0:19:36) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/geometry/test_bbox.py ..F.....                                                                                                                                                            [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________________ test_bbox_to_mask[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_bbox_to_mask(target_framework, mode, backend_compile):
        trace_args = (
            torch.tensor([[[1., 1.], [3., 1.], [3., 2.], [1., 2.]]]),
            5,
            5,
        )
        trace_kwargs = {}
        test_args = (
            torch.tensor([[[2., 2.], [4., 2.], [4., 3.], [2., 3.]]]),
            6,
            6,
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.bbox.bbox_to_mask,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_bbox.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function bbox_to_mask at 0x7fa18bbd9ea0>, trace_args = (tensor([[[1., 1.],
         [3., 1.],
         [3., 2.],
         [1., 2.]]]), 5, 5), trace_kwargs = {}
test_args = (tensor([[[2., 2.],
         [4., 2.],
         [4., 3.],
         [2., 3.]]]), 6, 6), test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s'
skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function bbox_to_mask at 0x7fa18bbd9ea0>, trace_args = (tensor([[[1., 1.],
         [3., 1.],
         [3., 2.],
         [1., 2.]]]), 5, 5), trace_kwargs = {}
test_args = (tensor([[[2., 2.],
         [4., 2.],
         [4., 3.],
         [2., 3.]]]), 6, 6), test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001
deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[0., 0., 0., 0., 0.],
         [0., 1., 1., 1., 0.],
         [0., 1., 1., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
transpiled_x = <tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=
array([[[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[0., 0., 0., 0., 0.],
        [0., 1., 1., 1., 0.],
        [0., 1., 1., 1., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]]], dtype=float32)
y = array([[[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]]], dtype=float32), tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.bbox.bbox_to_mask
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_bbox.py::test_bbox_to_mask[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
=============================================================================== 1 failed, 7 passed in 152.42s (0:02:32) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 44 items

kornia/test_filters.py .........FF..................F..F.........F.                                                                                                                              [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________________ test_unsharp_mask[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_unsharp_mask(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 5, 5),
            (3, 3),
            (1.5, 1.5),
        )
        trace_kwargs = {'border_type': 'reflect'}
        test_args = (
            torch.rand(5, 3, 5, 5),
            (5, 5),
            (2.0, 2.0),
        )
        test_kwargs = {'border_type': 'reflect'}
>       _test_function(
            kornia.filters.unsharp_mask,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unsharp_mask at 0x7f35e829dd80>
trace_args = (tensor([[[[0.2198, 0.1721, 0.0647, 0.0014, 0.6687],
          [0.0913, 0.4546, 0.8862, 0.5749, 0.4167],
          [0....  [0.8226, 0.1947, 0.2869, 0.2923, 0.6456],
          [0.5779, 0.7898, 0.3450, 0.1892, 0.2346]]]]), (3, 3), (1.5, 1.5))
trace_kwargs = {'border_type': 'reflect'}
test_args = (tensor([[[[0.7612, 0.6362, 0.1749, 0.2362, 0.2302],
          [0.3313, 0.7869, 0.4593, 0.6869, 0.7735],
          [0....  [0.8259, 0.4960, 0.5369, 0.5312, 0.9901],
          [0.9156, 0.7641, 0.5064, 0.2804, 0.9162]]]]), (5, 5), (2.0, 2.0))
test_kwargs = {'border_type': 'reflect'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unsharp_mask at 0x7f35e829dd80>
trace_args = (tensor([[[[0.2198, 0.1721, 0.0647, 0.0014, 0.6687],
          [0.0913, 0.4546, 0.8862, 0.5749, 0.4167],
          [0....  [0.8226, 0.1947, 0.2869, 0.2923, 0.6456],
          [0.5779, 0.7898, 0.3450, 0.1892, 0.2346]]]]), (3, 3), (1.5, 1.5))
trace_kwargs = {'border_type': 'reflect'}
test_args = (tensor([[[[0.7612, 0.6362, 0.1749, 0.2362, 0.2302],
          [0.3313, 0.7869, 0.4593, 0.6869, 0.7735],
          [0....  [0.8259, 0.4960, 0.5369, 0.5312, 0.9901],
          [0.9156, 0.7641, 0.5064, 0.2804, 0.9162]]]]), (5, 5), (2.0, 2.0))
test_kwargs = {'border_type': 'reflect'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.21984267, 0.17206764, 0.06471926, 0.00135082, 0.6687....29231155, 0.64555866],
         [0.5778954 , 0.78982776, 0.3449571 , 0.18923837, 0.23460114]]]],
      dtype=float32)>
kernel_size = (3, 3), sigma = (1.5, 1.5), border_type = 'reflect'

    def tensorflow_unsharp_mask(input, kernel_size, sigma, border_type="reflect"):
        from .kernels import tensorflow_gaussian
    
>       data_blur: typing.Any = tensorflow_gaussian.gaussian_blur2d(
            input, kernel_size, sigma, border_type
        )
E       AttributeError: 'function' object has no attribute 'gaussian_blur2d'

Translated_Outputs/tensorflow_outputs/kornia/filters/unsharp.py:33: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.unsharp.unsharp_mask
___________________________________________________________________________________ test_canny[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_canny(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 4, 4),
        )
        trace_kwargs = {
            'low_threshold': 0.1,
            'high_threshold': 0.2,
            'kernel_size': (5, 5),
            'sigma': (1, 1),
            'hysteresis': True,
            'eps': 1e-6,
        }
        test_args = (
            torch.rand(5, 3, 4, 4),
        )
        test_kwargs = {
            'low_threshold': 0.2,
            'high_threshold': 0.3,
            'kernel_size': (5, 5),
            'sigma': (1, 1),
            'hysteresis': True,
            'eps': 1e-6,
        }
>       _test_function(
            kornia.filters.canny,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:302: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function canny at 0x7f35e87f08b0>
trace_args = (tensor([[[[0.2069, 0.7390, 0.3397, 0.5481],
          [0.2417, 0.6969, 0.7212, 0.0642],
          [0.1585, 0.0865, 0...., 0.2742, 0.2922, 0.1880],
          [0.6712, 0.6003, 0.8214, 0.7549],
          [0.1361, 0.4862, 0.4191, 0.3924]]]]),)
trace_kwargs = {'eps': 1e-06, 'high_threshold': 0.2, 'hysteresis': True, 'kernel_size': (5, 5), ...}
test_args = (tensor([[[[0.3758, 0.8670, 0.3350, 0.5817],
          [0.1053, 0.2867, 0.3250, 0.4556],
          [0.9749, 0.0486, 0...., 0.0720, 0.3963, 0.4283],
          [0.7442, 0.7711, 0.7289, 0.3776],
          [0.0601, 0.0073, 0.4816, 0.1161]]]]),)
test_kwargs = {'eps': 1e-06, 'high_threshold': 0.3, 'hysteresis': True, 'kernel_size': (5, 5), ...}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False
deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function canny at 0x7f35e87f08b0>
trace_args = (tensor([[[[0.2069, 0.7390, 0.3397, 0.5481],
          [0.2417, 0.6969, 0.7212, 0.0642],
          [0.1585, 0.0865, 0...., 0.2742, 0.2922, 0.1880],
          [0.6712, 0.6003, 0.8214, 0.7549],
          [0.1361, 0.4862, 0.4191, 0.3924]]]]),)
trace_kwargs = {'eps': 1e-06, 'high_threshold': 0.2, 'hysteresis': True, 'kernel_size': (5, 5), ...}
test_args = (tensor([[[[0.3758, 0.8670, 0.3350, 0.5817],
          [0.1053, 0.2867, 0.3250, 0.4556],
          [0.9749, 0.0486, 0...., 0.0720, 0.3963, 0.4283],
          [0.7442, 0.7711, 0.7289, 0.3776],
          [0.0601, 0.0073, 0.4816, 0.1161]]]]),)
test_kwargs = {'eps': 1e-06, 'high_threshold': 0.3, 'hysteresis': True, 'kernel_size': (5, 5), ...}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[0.4829843 , 0.29541534, 0.4776603 , 0.35976318],
     ....16385694, 0.3898612 , 0.6179515 ],
         [0.71805114, 0.14427143, 0.7010676 , 0.41629064]]]],
      dtype=float32)>
low_threshold = 0.1, high_threshold = 0.2, kernel_size = (5, 5), sigma = (1, 1), hysteresis = True, eps = 1e-06

    def tensorflow_canny(
        input,
        low_threshold=0.1,
        high_threshold=0.2,
        kernel_size=(5, 5),
        sigma=(1, 1),
        hysteresis=True,
        eps=1e-06,
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..color.gray import tensorflow_rgb_to_grayscale
        from .kernels import tensorflow_gaussian
        from .sobel import tensorflow_sobel
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_atan2_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_round_frnt_
        from .kernels import tensorflow_get_canny_nms_kernel
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_long_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.non_linear_activation_functions import (
            tensorflow_threshold_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from .kernels import tensorflow_get_hysteresis_kernel
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK(
            low_threshold <= high_threshold,
            f"Invalid input thresholds. low_threshold should be smaller than the high_threshold. Got: {low_threshold}>{high_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < low_threshold < 1,
            f"Invalid low threshold. Should be in range (0, 1). Got: {low_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < high_threshold < 1,
            f"Invalid high threshold. Should be in range (0, 1). Got: {high_threshold}",
        )
        device = input.device
        dtype = input.dtype
        if tensorflow_shape_frnt_(input)[1] == 3:
            input = tensorflow_rgb_to_grayscale(input)
>       blurred: typing.Any = tensorflow_gaussian.gaussian_blur2d(input, kernel_size, sigma)
E       AttributeError: 'function' object has no attribute 'gaussian_blur2d'

Translated_Outputs/tensorflow_outputs/kornia/filters/canny.py:91: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.canny.canny
___________________________________________________________________________________ test_Canny[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Canny(target_framework, mode, backend_compile):
        print("kornia.filters.Canny")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledCanny = ivy.transpile(kornia.filters.Canny, source="torch", target=target_framework)
    
        x = torch.rand(5, 3, 4, 4)
        torch_out_magnitude, torch_out_edges = kornia.filters.Canny()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out_magnitude, transpiled_out_edges = TranspiledCanny()(transpiled_x)

kornia/test_filters.py:713: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f35e09252d0>, built=True,...ze at 0x7f35e0a6c550>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
args = (<tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[6.20400608e-01, 9.90346372e-01, 7.53479302e-01,
     ...3443234e-01],
         [6.73047245e-01, 8.68991375e-01, 3.82602215e-01,
          3.52481008e-02]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f35e0762820, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f35e09252d0>, built=True...23443234e-01],
         [6.73047245e-01, 8.68991375e-01, 3.82602215e-01,
          3.52481008e-02]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f35e09252d0>, built=True,...ze at 0x7f35e0a6c550>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[6.20400608e-01, 9.90346372e-01, 7.53479302e-01,
     ...3443234e-01],
         [6.73047245e-01, 8.68991375e-01, 3.82602215e-01,
          3.52481008e-02]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f35e09252d0>, built=True...23443234e-01],
         [6.73047245e-01, 8.68991375e-01, 3.82602215e-01,
          3.52481008e-02]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f35e09252d0>, built=True,...ze at 0x7f35e0a6c550>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[6.20400608e-01, 9.90346372e-01, 7.53479302e-01,
     ...3443234e-01],
         [6.73047245e-01, 8.68991375e-01, 3.82602215e-01,
          3.52481008e-02]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[6.20400608e-01, 9.90346372e-01, 7.53479302e-01,
      ....23443234e-01],
         [6.73047245e-01, 8.68991375e-01, 3.82602215e-01,
          3.52481008e-02]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f35e09252d0>, built=True... at 0x7f35e0a6c550>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None),)
kwargs = {'input': <tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[6.20400608e-01, 9.90346372e-01, 7.53479302e-...23443234e-01],
         [6.73047245e-01, 8.68991375e-01, 3.82602215e-01,
          3.52481008e-02]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7f35e09252d0>, built=True,...ze at 0x7f35e0a6c550>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
input = <tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[6.20400608e-01, 9.90346372e-01, 7.53479302e-01,
      ....23443234e-01],
         [6.73047245e-01, 8.68991375e-01, 3.82602215e-01,
          3.52481008e-02]]]], dtype=float32)>

    def call(self, input):
>       return tensorflow_canny(
            input,
            self.low_threshold,
            self.high_threshold,
            self.kernel_size,
            self.sigma,
            self.hysteresis,
            self.eps,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/canny.py:205: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(5, 1, 4, 4), dtype=float32, numpy=
array([[[[0.36895287, 0.5247189 , 0.4974137 , 0.6922013 ],
     ....58020777, 0.6137005 , 0.6308948 ],
         [0.3247151 , 0.66740245, 0.5044694 , 0.421606  ]]]],
      dtype=float32)>
low_threshold = 0.1, high_threshold = 0.2, kernel_size = (5, 5), sigma = (1, 1), hysteresis = True, eps = 1e-06

    def tensorflow_canny(
        input,
        low_threshold=0.1,
        high_threshold=0.2,
        kernel_size=(5, 5),
        sigma=(1, 1),
        hysteresis=True,
        eps=1e-06,
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..color.gray import tensorflow_rgb_to_grayscale
        from .kernels import tensorflow_gaussian
        from .sobel import tensorflow_sobel
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_atan2_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_round_frnt_
        from .kernels import tensorflow_get_canny_nms_kernel
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_long_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.non_linear_activation_functions import (
            tensorflow_threshold_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from .kernels import tensorflow_get_hysteresis_kernel
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK(
            low_threshold <= high_threshold,
            f"Invalid input thresholds. low_threshold should be smaller than the high_threshold. Got: {low_threshold}>{high_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < low_threshold < 1,
            f"Invalid low threshold. Should be in range (0, 1). Got: {low_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < high_threshold < 1,
            f"Invalid high threshold. Should be in range (0, 1). Got: {high_threshold}",
        )
        device = input.device
        dtype = input.dtype
        if tensorflow_shape_frnt_(input)[1] == 3:
            input = tensorflow_rgb_to_grayscale(input)
>       blurred: typing.Any = tensorflow_gaussian.gaussian_blur2d(input, kernel_size, sigma)
E       AttributeError: Exception encountered when calling tensorflow_Canny.call().
E       
E       [1m'function' object has no attribute 'gaussian_blur2d'[0m
E       
E       Arguments received by tensorflow_Canny.call():
E         â€¢ input=tf.Tensor(shape=(5, 3, 4, 4), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/filters/canny.py:95: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.Canny
__________________________________________________________________________________ test_DexiNed[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DexiNed(target_framework, mode, backend_compile):
        print("kornia.filters.DexiNed")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDexiNed = ivy.transpile(kornia.filters.DexiNed, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 320, 320)
        torch_out = kornia.filters.DexiNed(pretrained=False)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDexiNed(pretrained=False)(transpiled_x)

kornia/test_filters.py:765: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
args = (<tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.28372395, 0.13851261, 0.4626372 , ..., 0.761826...
         [0.57806414, 0.84031737, 0.17719245, ..., 0.49846357,
          0.71935016, 0.9879124 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x5559cf2a0010, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...,
         [0.57806414, 0.84031737, 0.17719245, ..., 0.49846357,
          0.71935016, 0.9879124 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.28372395, 0.13851261, 0.4626372 , ..., 0.761826...
         [0.57806414, 0.84031737, 0.17719245, ..., 0.49846357,
          0.71935016, 0.9879124 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...,
         [0.57806414, 0.84031737, 0.17719245, ..., 0.49846357,
          0.71935016, 0.9879124 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.28372395, 0.13851261, 0.4626372 , ..., 0.761826...
         [0.57806414, 0.84031737, 0.17719245, ..., 0.49846357,
          0.71935016, 0.9879124 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
),)
kwargs = {'x': <tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.28372395, 0.13851261, 0.4626372 , ..., 0.7...,
         [0.57806414, 0.84031737, 0.17719245, ..., 0.49846357,
          0.71935016, 0.9879124 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
x = <tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.28372395, 0.13851261, 0.4626372 , ..., 0.7618260...],
         [0.57806414, 0.84031737, 0.17719245, ..., 0.49846357,
          0.71935016, 0.9879124 ]]]], dtype=float32)>

    def call(self, x):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..core._backend import concatenate
    
        block_1 = self.block_1(x)
        block_1_side = self.side_1(block_1)
        block_2 = self.block_2(block_1)
        block_2_down = self.maxpool(block_2)
>       block_2_add = block_2_down + block_1_side

Translated_Outputs/tensorflow_outputs/kornia/filters/dexined.py:1117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 64, 160, 80), dtype=float32, numpy=
array([[[[ 0.00818203,  0.00963775, -0.01853747, ..., -0.01...1247e-02, -5.08636422e-02, ...,
          -6.83029462e-03,  6.06686156e-03, -1.21889673e-01]]]],
      dtype=float32)>)
kwargs = {}
arg = <tf.Tensor: shape=(1, 128, 80, 80), dtype=float32, numpy=
array([[[[ 3.94996554e-02,  5.11048958e-02,  4.70647123e-03,...31247e-02, -5.08636422e-02, ...,
          -6.83029462e-03,  6.06686156e-03, -1.21889673e-01]]]],
      dtype=float32)>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_DexiNed.call().
E       
E       [1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1,64,160,80] vs. [1,128,80,80] [Op:AddV2] name: [0m
E       
E       Arguments received by tensorflow_DexiNed.call():
E         â€¢ x=tf.Tensor(shape=(1, 3, 320, 320), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.DexiNed
________________________________________________________________________________ test_UnsharpMask[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_UnsharpMask(target_framework, mode, backend_compile):
        print("kornia.filters.UnsharpMask")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledUnsharpMask = ivy.transpile(kornia.filters.UnsharpMask, source="torch", target=target_framework)
    
        x = torch.rand(2, 3, 5, 5)
        torch_out = kornia.filters.UnsharpMask((3, 3), (1.5, 1.5))(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledUnsharpMask((3, 3), (1.5, 1.5))(transpiled_x)

kornia/test_filters.py:939: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask()
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.3940215 , 0.20535165, 0.15971303, 0.89570826, 0.123...950625 , 0.4506204 ],
         [0.573995  , 0.4809916 , 0.9370751 , 0.13499379, 0.20032638]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f35d006d3a0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_UnsharpMask(), <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.3940215 , 0.20535165, 0.1...5950625 , 0.4506204 ],
         [0.573995  , 0.4809916 , 0.9370751 , 0.13499379, 0.20032638]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.3940215 , 0.20535165, 0.15971303, 0.89570826, 0.123...950625 , 0.4506204 ],
         [0.573995  , 0.4809916 , 0.9370751 , 0.13499379, 0.20032638]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_UnsharpMask(), <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.3940215 , 0.20535165, 0.1...5950625 , 0.4506204 ],
         [0.573995  , 0.4809916 , 0.9370751 , 0.13499379, 0.20032638]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.3940215 , 0.20535165, 0.15971303, 0.89570826, 0.123...950625 , 0.4506204 ],
         [0.573995  , 0.4809916 , 0.9370751 , 0.13499379, 0.20032638]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.3940215 , 0.20535165, 0.15971303, 0.89570826, 0.1237....5950625 , 0.4506204 ],
         [0.573995  , 0.4809916 , 0.9370751 , 0.13499379, 0.20032638]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_UnsharpMask(),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.3940215 , 0.20535165, 0.15971303, 0.895708...5950625 , 0.4506204 ],
         [0.573995  , 0.4809916 , 0.9370751 , 0.13499379, 0.20032638]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask()
input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.3940215 , 0.20535165, 0.15971303, 0.89570826, 0.1237....5950625 , 0.4506204 ],
         [0.573995  , 0.4809916 , 0.9370751 , 0.13499379, 0.20032638]]]],
      dtype=float32)>

    def call(self, input):
>       return tensorflow_unsharp_mask(
            input, self.kernel_size, self.sigma, self.border_type
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/unsharp.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[0.3940215 , 0.20535165, 0.15971303, 0.89570826, 0.1237....5950625 , 0.4506204 ],
         [0.573995  , 0.4809916 , 0.9370751 , 0.13499379, 0.20032638]]]],
      dtype=float32)>
kernel_size = (3, 3), sigma = (1.5, 1.5), border_type = 'reflect'

    def tensorflow_unsharp_mask(input, kernel_size, sigma, border_type="reflect"):
        from .kernels import tensorflow_gaussian
    
>       data_blur: typing.Any = tensorflow_gaussian.gaussian_blur2d(
            input, kernel_size, sigma, border_type
        )
E       AttributeError: Exception encountered when calling tensorflow_UnsharpMask.call().
E       
E       [1m'function' object has no attribute 'gaussian_blur2d'[0m
E       
E       Arguments received by tensorflow_UnsharpMask.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/filters/unsharp.py:37: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.UnsharpMask
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_filters.py::test_unsharp_mask[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'gaussian_blur2d'
FAILED kornia/test_filters.py::test_canny[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'gaussian_blur2d'
FAILED kornia/test_filters.py::test_Canny[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_Canny.call().
FAILED kornia/test_filters.py::test_DexiNed[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_DexiNed.call().
FAILED kornia/test_filters.py::test_UnsharpMask[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_UnsharpMask.call().
=============================================================================== 5 failed, 39 passed in 782.36s (0:13:02) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

kornia/geometry/test_ransac.py .                                                                                                                                                                 [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 1 passed in 203.58s (0:03:23) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 15 items

kornia/test_contrib.py F...FFF.F...F..                                                                                                                                                           [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_compute_padding[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_compute_padding(target_framework, mode, backend_compile):
        trace_args = (
            (4, 3),
            (3, 3),
        )
        trace_kwargs = {}
        test_args = (
            (8, 5),
            (4, 4),
        )
        test_kwargs = {}
>       _test_function(
            kornia.contrib.compute_padding,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_contrib.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function compute_padding at 0x7f0ff51efb50>, trace_args = ((4, 3), (3, 3)), trace_kwargs = {}, test_args = ((8, 5), (4, 4)), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function compute_padding at 0x7f0ff51efb50>, trace_args = ((4, 3), (3, 3)), trace_kwargs = {}, test_args = ((8, 5), (4, 4)), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

original_size = (4, 3), window_size = (3, 3), stride = (3, 3)

    def tensorflow_compute_padding(original_size, window_size, stride=None):
        from ...torch.nn.modules.utils import _pair
    
        original_size = cast(Tuple[int, int], _pair(original_size))
        window_size = cast(Tuple[int, int], _pair(window_size))
        if stride is None:
            stride = window_size
        stride = cast(Tuple[int, int], _pair(stride))
        remainder_vertical = (original_size[0] - window_size[0]) % stride[0]
        remainder_horizontal = (original_size[1] - window_size[1]) % stride[1]
        if remainder_vertical != 0:
            vertical_padding = stride[0] - remainder_vertical
        else:
            vertical_padding = 0
        if remainder_horizontal != 0:
            horizontal_padding = stride[1] - remainder_horizontal
        else:
            horizontal_padding = 0
        if vertical_padding % 2 == 0:
            top_padding = bottom_padding = vertical_padding // 2
        else:
            top_padding = vertical_padding // 2
            bottom_padding = ceil(vertical_padding / 2)
        if horizontal_padding % 2 == 0:
            left_padding = right_padding = horizontal_padding // 2
        else:
            left_padding = horizontal_padding // 2
            right_padding = ceil(horizontal_padding / 2)
        padding = (
            int(top_padding),
            int(bottom_padding),
            int(left_padding),
            int(right_padding),
        )
>       return cast(FullPadType, padding)
E       NameError: name 'FullPadType' is not defined

Translated_Outputs/tensorflow_outputs/kornia/contrib/extract_patches.py:66: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.extract_patches.compute_padding
______________________________________________________________________________ test_diamond_square[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_diamond_square(target_framework, mode, backend_compile):
        trace_args = ((1, 1, 8, 8),)
        trace_kwargs = {
            "roughness": 0.5,
            "random_scale": 1.0,
            "normalize_range": (0.0, 1.0),
            "random_fn": torch.ones,
        }
        test_args = ((5, 1, 8, 8),)
        test_kwargs = {
            "roughness": 0.7,
            "random_scale": 0.9,
            "normalize_range": (-1.0, 1.0),
            "random_fn": torch.ones,
        }
>       _test_function(
            kornia.contrib.diamond_square,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_contrib.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function diamond_square at 0x7f0ff51eef80>, trace_args = ((1, 1, 8, 8),)
trace_kwargs = {'normalize_range': (0.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f10105ab900>, 'random_scale': 1.0, 'roughness': 0.5}, test_args = ((5, 1, 8, 8),)
test_kwargs = {'normalize_range': (-1.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f10105ab900>, 'random_scale': 0.9, 'roughness': 0.7}, target = 'tensorflow'
backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function diamond_square at 0x7f0ff51eef80>, trace_args = ((1, 1, 8, 8),)
trace_kwargs = {'normalize_range': (0.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f10105ab900>, 'random_scale': 1.0, 'roughness': 0.5}, test_args = ((5, 1, 8, 8),)
test_kwargs = {'normalize_range': (-1.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f10105ab900>, 'random_scale': 0.9, 'roughness': 0.7}, target = 'tensorflow'
backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

output_size = (1, 1, 8, 8), roughness = <tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[0.5]]]], dtype=float32)>
random_scale = <tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[1.]]]], dtype=float32)>, random_fn = <built-in method ones of type object at 0x7f10105ab900>, normalize_range = (0.0, 1.0)
device = None, dtype = None

    def tensorflow_diamond_square(
        output_size,
        roughness=0.5,
        random_scale=1.0,
        random_fn=tensorflow_rand_frnt,
        normalize_range=None,
        device=None,
        dtype=None,
    ):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..enhance.normalize import tensorflow_normalize_min_max
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
    
        tensorflow_KORNIA_CHECK(len(output_size) == 4, "output_size must be (B,C,H,W)")
        if not isinstance(random_scale, (tensorflow.Tensor, tensorflow.Variable)):
            random_scale = tensorflow_to_frnt_(
                tensorflow.convert_to_tensor([[[[random_scale]]]]), device, dtype
            )
            random_scale = tensorflow_expand_frnt_(
                random_scale, [output_size[0] * output_size[1], 1, 1, 1]
            )
        else:
            tensorflow_KORNIA_CHECK_IS_TENSOR(random_scale)
            random_scale = tensorflow_view_frnt_(random_scale, -1, 1, 1, 1)
            random_scale = tensorflow_expand_frnt_(
                random_scale, [output_size[0], output_size[1], 1, 1]
            )
            random_scale = tensorflow_reshape_frnt_(random_scale, [-1, 1, 1, 1])
        if not isinstance(roughness, (tensorflow.Tensor, tensorflow.Variable)):
            roughness = tensorflow_to_frnt_(
                tensorflow.convert_to_tensor([[[[roughness]]]]), device, dtype
            )
            roughness = tensorflow_expand_frnt_(
                roughness, [output_size[0] * output_size[1], 1, 1, 1]
            )
        else:
            roughness = tensorflow_view_frnt_(roughness, -1, 1, 1, 1)
            roughness = tensorflow_expand_frnt_(
                roughness, [output_size[0], output_size[1], 1, 1]
            )
            roughness = tensorflow_reshape_frnt_(roughness, [-1, 1, 1, 1])
        width, height = output_size[-2:][0], output_size[-2:][1]
        num_samples: typing.Any = 1
        for x in output_size[:-2]:
            num_samples = num_samples * x
        p2_width: typing.Any = 2 ** math.ceil(math.log2(width - 1)) + 1
        p2_height: typing.Any = 2 ** math.ceil(math.log2(height - 1)) + 1
        recursion_depth: typing.Any = int(
            min(math.log2(p2_width - 1) - 1, math.log2(p2_height - 1) - 1)
        )
        seed_width: typing.Any = (p2_width - 1) // 2**recursion_depth + 1
        seed_height: typing.Any = (p2_height - 1) // 2**recursion_depth + 1
>       img: typing.Any = random_scale * tensorflow__diamond_square_seed(
            num_samples, seed_width, seed_height, random_fn, device, dtype
        )

Translated_Outputs/tensorflow_outputs/kornia/contrib/diamond_square.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[1.]]]], dtype=float32)>, tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]]))
kwargs = {}, arg = tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]])

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[1.]]]], dtype=float32)>, tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]]))
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arrays_and_dtypes = [<class 'numpy.float32'>, tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]])]

    def _result_type(*arrays_and_dtypes):
      """Returns the resulting type given a set of arrays."""
    
      def preprocess_float(x):
        if is_prefer_float32():
          if isinstance(x, float):
            return np.float32(x)
          elif isinstance(x, complex):
            return np.complex64(x)
        return x
    
      arrays_and_dtypes = [preprocess_float(x) for x in arrays_and_dtypes]
>     dtype = np.result_type(*arrays_and_dtypes)
E     TypeError: Cannot interpret 'tensor([[[[0.3333, 1.0000, 0.3333],
E               [1.0000, 0.3333, 1.0000],
E               [0.3333, 1.0000, 0.3333]]]])' as a data type

/opt/fw/tensorflow/tensorflow/python/ops/numpy_ops/np_dtypes.py:190: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.diamond_square.diamond_square
_______________________________________________________________________________ test_EdgeDetector[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_EdgeDetector(target_framework, mode, backend_compile):
        print("kornia.contrib.EdgeDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledEdgeDetector = ivy.transpile(kornia.contrib.EdgeDetector, source="torch", target=target_framework)
    
        torch_detector = kornia.contrib.EdgeDetector()
>       transpiled_detector = TranspiledEdgeDetector()

kornia/test_contrib.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_EdgeDetector()

    def __init__(self):
        from ..filters.dexined import tensorflow_DexiNed
    
        self.super___init__(
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.model = tensorflow_DexiNed(pretrained=True)

Translated_Outputs/tensorflow_outputs/kornia/contrib/edge_detection.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
args = (), kwargs = {'pretrained': True}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
pretrained = True

    @tensorflow_store_config_info
    def __init__(self, pretrained):
        from ...torch.nn.modules.pooling import tensorflow_MaxPool2d
    
        self.super___init__(
            pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.block_1 = tensorflow_DoubleConvBlock(3, 32, 64, stride=2)
        self.block_2 = tensorflow_DoubleConvBlock(64, 128, use_act=False)
        self.dblock_3 = tensorflow__DenseBlock(2, 128, 256)
        self.dblock_4 = tensorflow__DenseBlock(3, 256, 512)
        self.dblock_5 = tensorflow__DenseBlock(3, 512, 512)
        self.dblock_6 = tensorflow__DenseBlock(3, 512, 256)
        self.maxpool = tensorflow_MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.side_1 = tensorflow_SingleConvBlock(64, 128, 2)
        self.side_2 = tensorflow_SingleConvBlock(128, 256, 2)
        self.side_3 = tensorflow_SingleConvBlock(256, 512, 2)
        self.side_4 = tensorflow_SingleConvBlock(512, 512, 1)
        self.side_5 = tensorflow_SingleConvBlock(512, 256, 1)
        self.pre_dense_2 = tensorflow_SingleConvBlock(128, 256, 2)
        self.pre_dense_3 = tensorflow_SingleConvBlock(128, 256, 1)
        self.pre_dense_4 = tensorflow_SingleConvBlock(256, 512, 1)
        self.pre_dense_5 = tensorflow_SingleConvBlock(512, 512, 1)
        self.pre_dense_6 = tensorflow_SingleConvBlock(512, 256, 1)
        self.up_block_1 = tensorflow_UpConvBlock(64, 1)
        self.up_block_2 = tensorflow_UpConvBlock(128, 1)
        self.up_block_3 = tensorflow_UpConvBlock(256, 2)
        self.up_block_4 = tensorflow_UpConvBlock(512, 3)
        self.up_block_5 = tensorflow_UpConvBlock(512, 4)
        self.up_block_6 = tensorflow_UpConvBlock(256, 4)
        self.block_cat = tensorflow_SingleConvBlock(6, 1, stride=1, use_bs=False)
        if pretrained:
>           self.load_from_file(url)

Translated_Outputs/tensorflow_outputs/kornia/filters/dexined.py:1096: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
path_file = 'http://cmp.felk.cvut.cz/~mishkdmy/models/DexiNed_BIPED_10.pth'

    def load_from_file(self, path_file):
        from ..utils.helpers import tensorflow_map_location_to_cpu
    
>       pretrained_dict = torch.hub.load_state_dict_from_url(
            path_file, map_location=tensorflow_map_location_to_cpu
        )
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/filters/dexined.py:1103: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.EdgeDetector
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "http://cmp.felk.cvut.cz/~mishkdmy/models/DexiNed_BIPED_10.pth" to /root/.cache/torch/hub/checkpoints/DexiNed_BIPED_10.pth

  0%|          | 0.00/135M [00:00<?, ?B/s]
  0%|          | 128k/135M [00:00<09:01, 260kB/s]
  0%|          | 256k/135M [00:00<05:29, 427kB/s]
  0%|          | 512k/135M [00:00<02:59, 783kB/s]
  1%|          | 896k/135M [00:01<01:50, 1.27MB/s]
  1%|â–         | 1.75M/135M [00:01<00:54, 2.56MB/s]
  3%|â–Ž         | 3.50M/135M [00:01<00:26, 5.10MB/s]
  5%|â–         | 6.38M/135M [00:01<00:14, 8.97MB/s]
  7%|â–‹         | 9.38M/135M [00:01<00:11, 11.9MB/s]
  9%|â–‰         | 12.2M/135M [00:01<00:09, 13.7MB/s]
 11%|â–ˆ         | 14.9M/135M [00:02<00:08, 14.5MB/s]
 13%|â–ˆâ–Ž        | 17.9M/135M [00:02<00:07, 15.7MB/s]
 15%|â–ˆâ–Œ        | 20.8M/135M [00:02<00:07, 16.4MB/s]
 18%|â–ˆâ–Š        | 23.6M/135M [00:02<00:06, 16.7MB/s]
 20%|â–ˆâ–‰        | 26.6M/135M [00:02<00:06, 17.3MB/s]
 22%|â–ˆâ–ˆâ–       | 29.6M/135M [00:02<00:06, 17.7MB/s]
 24%|â–ˆâ–ˆâ–       | 32.6M/135M [00:03<00:05, 18.0MB/s]
 26%|â–ˆâ–ˆâ–‹       | 35.6M/135M [00:03<00:05, 18.2MB/s]
 29%|â–ˆâ–ˆâ–Š       | 38.6M/135M [00:03<00:05, 18.3MB/s]
 31%|â–ˆâ–ˆâ–ˆ       | 41.5M/135M [00:03<00:05, 18.2MB/s]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 44.4M/135M [00:03<00:05, 18.3MB/s]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 47.4M/135M [00:03<00:05, 18.2MB/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 50.1M/135M [00:04<00:04, 17.8MB/s]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 53.1M/135M [00:04<00:04, 18.0MB/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 56.1M/135M [00:04<00:04, 18.2MB/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 59.0M/135M [00:04<00:04, 18.1MB/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 61.9M/135M [00:04<00:04, 18.1MB/s]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 63.6M/135M [00:04<00:04, 15.1MB/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 66.6M/135M [00:05<00:04, 16.1MB/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 68.8M/135M [00:05<00:04, 15.3MB/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 71.8M/135M [00:05<00:04, 16.3MB/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 74.6M/135M [00:05<00:03, 16.8MB/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 77.6M/135M [00:05<00:03, 17.3MB/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 80.2M/135M [00:05<00:03, 17.0MB/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 83.1M/135M [00:06<00:03, 17.3MB/s]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 86.1M/135M [00:06<00:02, 17.7MB/s]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 89.0M/135M [00:06<00:02, 17.8MB/s]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 92.0M/135M [00:06<00:02, 18.0MB/s]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 94.9M/135M [00:06<00:02, 18.0MB/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 97.4M/135M [00:06<00:02, 17.2MB/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 100M/135M [00:07<00:02, 17.7MB/s] 
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 103M/135M [00:07<00:01, 17.9MB/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 106M/135M [00:07<00:01, 18.1MB/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 109M/135M [00:07<00:01, 18.3MB/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 112M/135M [00:07<00:01, 18.4MB/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 115M/135M [00:07<00:01, 17.8MB/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 118M/135M [00:08<00:00, 18.0MB/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 121M/135M [00:08<00:00, 18.2MB/s]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 124M/135M [00:08<00:00, 18.3MB/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 127M/135M [00:08<00:00, 18.3MB/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 130M/135M [00:08<00:00, 18.2MB/s]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 133M/135M [00:08<00:00, 18.1MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135M/135M [00:08<00:00, 15.7MB/s]
_______________________________________________________________________________ test_FaceDetector[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_FaceDetector(target_framework, mode, backend_compile):
        print("kornia.contrib.FaceDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledFaceDetector = ivy.transpile(kornia.contrib.FaceDetector, source="torch", target=target_framework)
    
        torch_detector = kornia.contrib.FaceDetector()
>       transpiled_detector = TranspiledFaceDetector()

kornia/test_contrib.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_FaceDetector(), top_k = 5000, confidence_threshold = 0.3, nms_threshold = 0.3, keep_top_k = 750

    def __init__(
        self, top_k=5000, confidence_threshold=0.3, nms_threshold=0.3, keep_top_k=750
    ):
        from ..geometry.bbox import tensorflow_nms
    
        self.super___init__(
            top_k=top_k,
            confidence_threshold=confidence_threshold,
            nms_threshold=nms_threshold,
            keep_top_k=keep_top_k,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.top_k = top_k
        self.confidence_threshold = confidence_threshold
        self.nms_threshold = nms_threshold
        self.keep_top_k = keep_top_k
        self.config = {
            "name": "YuFaceDetectNet",
            "min_sizes": [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],
            "steps": [8, 16, 32, 64],
            "variance": [0.1, 0.2],
            "clip": False,
        }
        self.min_sizes = [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]]
        self.steps = [8, 16, 32, 64]
        self.variance = [0.1, 0.2]
        self.clip = False
>       self.model = tensorflow_YuFaceDetectNet("test", pretrained=True)

Translated_Outputs/tensorflow_outputs/kornia/contrib/face_detection.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_YuFaceDetectNet(
  (model0): tensorflow_Conv_head(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...nv2): tensorflow_ConvDPUnit(
        (conv1): KerasConv2D()
        (conv2): KerasDepthWiseConv2D()
      )
    )
  )
)
args = ('test',), kwargs = {'pretrained': True}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_YuFaceDetectNet(
  (model0): tensorflow_Conv_head(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...nv2): tensorflow_ConvDPUnit(
        (conv1): KerasConv2D()
        (conv2): KerasDepthWiseConv2D()
      )
    )
  )
)
phase = 'test', pretrained = True

    @tensorflow_store_config_info
    def __init__(self, phase, pretrained):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.init import tensorflow_xavier_normal_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_data_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_fill__frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_normal__frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_zero__frnt_
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            phase,
            pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.phase = phase
        self.num_classes = 2
        self.model0 = tensorflow_Conv_head(3, 16, 16)
        self.model1 = tensorflow_Conv4layerBlock(16, 64)
        self.model2 = tensorflow_Conv4layerBlock(64, 64)
        self.model3 = tensorflow_Conv4layerBlock(64, 64)
        self.model4 = tensorflow_Conv4layerBlock(64, 64)
        self.model5 = tensorflow_Conv4layerBlock(64, 64)
        self.model6 = tensorflow_Conv4layerBlock(64, 64)
        self.head = tensorflow_Sequential(
            tensorflow_Conv4layerBlock(64, 3 * (14 + 2 + 1), False),
            tensorflow_Conv4layerBlock(64, 2 * (14 + 2 + 1), False),
            tensorflow_Conv4layerBlock(64, 2 * (14 + 2 + 1), False),
            tensorflow_Conv4layerBlock(64, 3 * (14 + 2 + 1), False),
        )
        if self.phase == "train":
            for m in self.modules():
                if isinstance(m, (KerasConv2D,)):
                    if m.bias is not None:
                        m.weight = tensorflow_xavier_normal_(
                            tensorflow_data_frnt_(m.weight)
                        )
                        m.bias = tensorflow_fill__frnt_(
                            tensorflow_data_frnt_(m.bias), 0.02
                        )
                    else:
                        m.weight = tensorflow_normal__frnt_(
                            tensorflow_data_frnt_(m.weight), 0, 0.01
                        )
                elif isinstance(m, (KerasBatchNorm2D,)):
                    m.weight = tensorflow_fill__frnt_(
                        tensorflow_data_frnt_(m.weight), 1
                    )
                    m.bias = tensorflow_zero__frnt_(tensorflow_data_frnt_(m.bias))
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                url, map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/contrib/face_detection.py:680: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.FaceDetector
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/kornia/data/raw/main/yunet_final.pth" to /root/.cache/torch/hub/checkpoints/yunet_final.pth

  0%|          | 0.00/396k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396k/396k [00:00<00:00, 80.9MB/s]
__________________________________________________________________________________ test_KMeans[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KMeans(target_framework, mode, backend_compile):
        print("kornia.contrib.KMeans")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKMeans = ivy.transpile(kornia.contrib.KMeans, source="torch", target=target_framework)
    
        torch_kmeans = kornia.contrib.KMeans(3, None, 10e-4, 100, 0)
        transpiled_kmeans = TranspiledKMeans(3, None, 10e-4, 100, 0)
    
        torch_x1 = torch.rand((1000, 5))
        torch_x2 = torch.rand((10, 5))
        transpiled_x1 = _array_to_new_backend(torch_x1, target_framework)
        transpiled_x2 = _array_to_new_backend(torch_x2, target_framework)
    
        torch_kmeans.fit(torch_x1)
        torch_predictions = torch_kmeans.predict(torch_x2)
    
>       transpiled_kmeans.fit(transpiled_x1)

kornia/test_contrib.py:245: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Translated_Outputs.tensorflow_outputs.kornia.contrib.kmeans.tensorflow_KMeans object at 0x7f0feea41480>
X = <tf.Tensor: shape=(1000, 5), dtype=float32, numpy=
array([[0.4962566 , 0.7682218 , 0.08847743, 0.13203049, 0.30742282]...5, 0.49248123, 0.4287302 ],
       [0.3786084 , 0.04217941, 0.28761953, 0.5166052 , 0.11317676]],
      dtype=float32)>

    def fit(self, X):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_argmin_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_nonzero_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_index_select_frnt,
        )
        from ...ivy.functional.frontends.torch.random_sampling import (
            tensorflow_randint_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
    
        tensorflow_KORNIA_CHECK_SHAPE(X, ["N", "D"])
        if self._cluster_centers is None:
            self._cluster_centers = self._initialise_cluster_centers(
                X, self.num_clusters
            )
        else:
            tensorflow_KORNIA_CHECK(
                tensorflow_shape_frnt_(X)[1]
                == tensorflow_shape_frnt_(self._cluster_centers)[1],
                f"Dimensions at position 1 of X and cluster_centers do not match.                 {tensorflow_shape_frnt_(X)[1]} != {tensorflow_shape_frnt_(self._cluster_centers)[1]}",
            )
        current_centers = self._cluster_centers
        previous_centers: typing.Any = None
        iteration: typing.Any = 0
        while True:
            distance: typing.Any = self._pairwise_euclidean_distance(X, current_centers)
            cluster_assignment = tensorflow_argmin_frnt_(distance, -1)
            previous_centers = tensorflow_clone_frnt_(current_centers)
            for index in range(self.num_clusters):
                selected = tensorflow_squeeze_frnt_(
                    tensorflow_nonzero_frnt(cluster_assignment == index)
                )
                selected = tensorflow_index_select_frnt(X, 0, selected)
                if tensorflow_shape_frnt_(selected)[0] == 0:
                    selected = X[tensorflow_randint_frnt(len(X), (1,), device=X.device)]
>               current_centers[index] = tensorflow_mean_frnt_(selected, dim=0)
E               TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment

Translated_Outputs/tensorflow_outputs/kornia/contrib/kmeans.py:143: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.KMeans
_______________________________________________________________________________ test_ImageStitcher[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ImageStitcher(target_framework, mode, backend_compile):
        print("kornia.contrib.ImageStitcher")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledLoFTR = ivy.transpile(kornia.feature.LoFTR, source="torch", target=target_framework)

kornia/test_contrib.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.feature.loftr.loftr.LoFTR'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, line 77)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.ImageStitcher
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_contrib.py::test_compute_padding[tensorflow-s2s-False] - NameError: name 'FullPadType' is not defined
FAILED kornia/test_contrib.py::test_diamond_square[tensorflow-s2s-False] - TypeError: Cannot interpret 'tensor([[[[0.3333, 1.0000, 0.3333],
FAILED kornia/test_contrib.py::test_EdgeDetector[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_contrib.py::test_FaceDetector[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_contrib.py::test_KMeans[tensorflow-s2s-False] - TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment
FAILED kornia/test_contrib.py::test_ImageStitcher[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, lin...
=============================================================================== 6 failed, 9 passed in 710.59s (0:11:50) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 7 items

kornia/test_morphology.py .......                                                                                                                                                                [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 7 passed in 120.57s (0:02:00) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/geometry/test_line.py ..                                                                                                                                                                  [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
===================================================================================== 2 passed in 97.90s (0:01:37) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 19 items

kornia/test_feature1.py .....F......F......                                                                                                                                                      [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________ test_get_laf_descriptors[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_laf_descriptors(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 1, 32, 32),
            torch.rand(1, 3, 2, 2),
            kornia.feature.HardNet8(pretrained=False),
        )
        trace_kwargs = {'patch_size': 32, 'grayscale_descriptor': True}
        test_args = (
            torch.rand(5, 1, 32, 32),
            torch.rand(5, 3, 2, 2),
            kornia.feature.HardNet8(pretrained=False),
        )
        test_kwargs = {'patch_size': 32, 'grayscale_descriptor': True}
>       _test_function(
            kornia.feature.get_laf_descriptors,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_feature1.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function get_laf_descriptors at 0x7f9b5d7e37f0>
trace_args = (tensor([[[[0.0457, 0.9563, 0.8476,  ..., 0.2586, 0.9607, 0.7503],
          [0.7376, 0.2787, 0.0472,  ..., 0.3390, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
trace_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}
test_args = (tensor([[[[0.7826, 0.8652, 0.0380,  ..., 0.8966, 0.6375, 0.1967],
          [0.0891, 0.1381, 0.3620,  ..., 0.0634, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
test_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function get_laf_descriptors at 0x7f9b5d7e37f0>
trace_args = (tensor([[[[0.0457, 0.9563, 0.8476,  ..., 0.2586, 0.9607, 0.7503],
          [0.7376, 0.2787, 0.0472,  ..., 0.3390, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
trace_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}
test_args = (tensor([[[[0.7826, 0.8652, 0.0380,  ..., 0.8966, 0.6375, 0.1967],
          [0.0891, 0.1381, 0.3620,  ..., 0.0634, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
test_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
>       orig_out = fn(*trace_args, **trace_kwargs)

helpers.py:253: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

img = tensor([[[[0.0457, 0.9563, 0.8476,  ..., 0.2586, 0.9607, 0.7503],
          [0.7376, 0.2787, 0.0472,  ..., 0.3390, 0.5...53, 0.7397, 0.6072,  ..., 0.1783, 0.2402, 0.6178],
          [0.6965, 0.2211, 0.5949,  ..., 0.7437, 0.4714, 0.6144]]]])
lafs = tensor([[[[0.7685, 0.4800],
          [0.5391, 0.0592]],

         [[0.3304, 0.8741],
          [0.1171, 0.7035]],

         [[0.5662, 0.9993],
          [0.2200, 0.6221]]]])
patch_descriptor = HardNet8(
  (features): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=Fal...e=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
)
patch_size = 32, grayscale_descriptor = True

    def get_laf_descriptors(
        img: Tensor, lafs: Tensor, patch_descriptor: Module, patch_size: int = 32, grayscale_descriptor: bool = True
    ) -> Tensor:
        r"""Function to get local descriptors, corresponding to LAFs (keypoints).
    
        Args:
            img: image features with shape :math:`(B,C,H,W)`.
            lafs: local affine frames :math:`(B,N,2,3)`.
            patch_descriptor: patch descriptor module, e.g. :class:`~kornia.feature.SIFTDescriptor`
                or :class:`~kornia.feature.HardNet`.
            patch_size: patch size in pixels, which descriptor expects.
            grayscale_descriptor: True if ``patch_descriptor`` expects single-channel image.
    
        Returns:
            Local descriptors of shape :math:`(B,N,D)` where :math:`D` is descriptor size.
        """
>       KORNIA_CHECK_LAF(lafs)

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/feature/integrated.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

laf = tensor([[[[0.7685, 0.4800],
          [0.5391, 0.0592]],

         [[0.3304, 0.8741],
          [0.1171, 0.7035]],

         [[0.5662, 0.9993],
          [0.2200, 0.6221]]]]), raises = True

    def KORNIA_CHECK_LAF(laf: Tensor, raises: bool = True) -> bool:
        """Check whether a Local Affine Frame (laf) has a valid shape.
    
        Args:
            laf: local affine frame tensor to evaluate.
            raises: bool indicating whether an exception should be raised upon failure.
    
        Raises:
            Exception: if the input laf does not have a shape :math:`(B,N,2,3)` and raises is True.
    
        Example:
            >>> lafs = torch.rand(2, 10, 2, 3)
            >>> KORNIA_CHECK_LAF(lafs)
            True
        """
>       return KORNIA_CHECK_SHAPE(laf, ["B", "N", "2", "3"], raises)

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/core/check.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = tensor([[[[0.7685, 0.4800],
          [0.5391, 0.0592]],

         [[0.3304, 0.8741],
          [0.1171, 0.7035]],

         [[0.5662, 0.9993],
          [0.2200, 0.6221]]]])
shape = ['B', 'N', '2', '3'], raises = True

    def KORNIA_CHECK_SHAPE(x: Tensor, shape: list[str], raises: bool = True) -> bool:
        """Check whether a tensor has a specified shape.
    
        The shape can be specified with a implicit or explicit list of strings.
        The guard also check whether the variable is a type `Tensor`.
    
        Args:
            x: the tensor to evaluate.
            shape: a list with strings with the expected shape.
            raises: bool indicating whether an exception should be raised upon failure.
    
        Raises:
            Exception: if the input tensor is has not the expected shape and raises is True.
    
        Example:
            >>> x = torch.rand(2, 3, 4, 4)
            >>> KORNIA_CHECK_SHAPE(x, ["B", "C", "H", "W"])  # implicit
            True
    
            >>> x = torch.rand(2, 3, 4, 4)
            >>> KORNIA_CHECK_SHAPE(x, ["2", "3", "H", "W"])  # explicit
            True
        """
        if "*" == shape[0]:
            shape_to_check = shape[1:]
            x_shape_to_check = x.shape[-len(shape) + 1 :]
        elif "*" == shape[-1]:
            shape_to_check = shape[:-1]
            x_shape_to_check = x.shape[: len(shape) - 1]
        else:
            shape_to_check = shape
            x_shape_to_check = x.shape
    
        if len(x_shape_to_check) != len(shape_to_check):
            if raises:
                raise TypeError(f"{x} shape must be [{shape}]. Got {x.shape}")
            else:
                return False
    
        for i in range(len(x_shape_to_check)):
            # The voodoo below is because torchscript does not like
            # that dim can be both int and str
            dim_: str = shape_to_check[i]
            if not dim_.isnumeric():
                continue
            dim = int(dim_)
            if x_shape_to_check[i] != dim:
                if raises:
>                   raise TypeError(f"{x} shape must be [{shape}]. Got {x.shape}")
E                   TypeError: tensor([[[[0.7685, 0.4800],
E                             [0.5391, 0.0592]],
E                   
E                            [[0.3304, 0.8741],
E                             [0.1171, 0.7035]],
E                   
E                            [[0.5662, 0.9993],
E                             [0.2200, 0.6221]]]]) shape must be [['B', 'N', '2', '3']]. Got torch.Size([1, 3, 2, 2])

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/core/check.py:80: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.integrated.get_laf_descriptors
_______________________________________________________________________ test_extract_patches_from_pyramid[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_extract_patches_from_pyramid(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 32, 32),
            torch.rand(1, 5, 2, 3),
        )
        trace_kwargs = {'PS': 32}
        test_args = (
            torch.rand(1, 3, 64, 64),  # TODO: changing the batch size of these causes the trace_graph test to fail
            torch.rand(1, 5, 2, 3),
        )
        test_kwargs = {'PS': 16}
>       _test_function(
            kornia.feature.extract_patches_from_pyramid,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_feature1.py:317: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function extract_patches_from_pyramid at 0x7f9b5d96e320>
trace_args = (tensor([[[[5.8766e-01, 3.9829e-01, 4.9448e-01,  ..., 7.2996e-01,
           1.9029e-01, 9.2391e-01],
          [7.856...111],
          [0.9169, 0.7982, 0.6292]],

         [[0.9223, 0.1821, 0.8999],
          [0.6746, 0.3986, 0.1790]]]]))
trace_kwargs = {'PS': 32}
test_args = (tensor([[[[0.4559, 0.1688, 0.8748,  ..., 0.7920, 0.2581, 0.8860],
          [0.9112, 0.5381, 0.6138,  ..., 0.5759, 0....228],
          [0.8989, 0.1955, 0.8607]],

         [[0.2930, 0.0680, 0.4430],
          [0.7899, 0.9217, 0.4545]]]]))
test_kwargs = {'PS': 16}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function extract_patches_from_pyramid at 0x7f9b5d96e320>
trace_args = (tensor([[[[5.8766e-01, 3.9829e-01, 4.9448e-01,  ..., 7.2996e-01,
           1.9029e-01, 9.2391e-01],
          [7.856...111],
          [0.9169, 0.7982, 0.6292]],

         [[0.9223, 0.1821, 0.8999],
          [0.6746, 0.3986, 0.1790]]]]))
trace_kwargs = {'PS': 32}
test_args = (tensor([[[[0.4559, 0.1688, 0.8748,  ..., 0.7920, 0.2581, 0.8860],
          [0.9112, 0.5381, 0.6138,  ..., 0.5759, 0....228],
          [0.8989, 0.1955, 0.8607]],

         [[0.2930, 0.0680, 0.4430],
          [0.7899, 0.9217, 0.4545]]]]))
test_kwargs = {'PS': 16}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[[0.5877, 0.5877, 0.5877,  ..., 0.5877, 0.5847, 0.5777],
           [0.5877, 0.5877, 0.5877,  ..., 0.5877, 0..., 0.3679, 0.3679,  ..., 0.5363, 0.5564, 0.5722],
           [0.3679, 0.3679, 0.3679,  ..., 0.5481, 0.5665, 0.5806]]]]])
transpiled_x = <tf.Tensor: shape=(1, 5, 3, 32, 32), dtype=float32, numpy=
array([[[[[0.5876589, 0.5876589, 0.5876589, ..., 0.       ,...   ],
          [0.       , 0.       , 0.       , ..., 0.       , 0.       ,
           0.       ]]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[[0.5876589 , 0.5876589 , 0.5876589 , ..., 0.5876589 ,
           0.58470726, 0.57771146],
          [0.58765...
          [0.36785066, 0.36785066, 0.36785066, ..., 0.5481269 ,
           0.5665468 , 0.5806184 ]]]]], dtype=float32)
y = array([[[[[0.5876589, 0.5876589, 0.5876589, ..., 0.       , 0.       ,
           0.       ],
          [0.       , 0....    ],
          [0.       , 0.       , 0.       , ..., 0.       , 0.       ,
           0.       ]]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.laf.extract_patches_from_pyramid
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_feature1.py::test_get_laf_descriptors[tensorflow-s2s-False] - TypeError: tensor([[[[0.7685, 0.4800],
FAILED kornia/test_feature1.py::test_extract_patches_from_pyramid[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
=============================================================================== 2 failed, 17 passed in 524.48s (0:08:44) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 69 items

kornia/test_color.py ...........F...........F......F.............F...........F.....F......                                                                                                       [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________________ test_rgb_to_hls[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_rgb_to_hls(target_framework, mode, backend_compile):
        # Note: We test this function with requires_grad=True,
        # because otherwise we simply get an empty_like tensor
        # with garbage values on each run leading to test failures
        trace_args = (
            torch.rand(1, 3, 4, 5).requires_grad_(True),
        )
        trace_kwargs = {'eps': 1e-8}
        test_args = (
            torch.rand(5, 3, 4, 5).requires_grad_(True),
        )
        test_kwargs = {'eps': 1e-8}
>       _test_function(
            kornia.color.rgb_to_hls,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_color.py:291: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_hls at 0x7f48a82f1ea0>
trace_args = (tensor([[[[0.1553, 0.2790, 0.0459, 0.8861, 0.6092],
          [0.7767, 0.3166, 0.8938, 0.3650, 0.9301],
          [0.... [0.1666, 0.8955, 0.8418, 0.6065, 0.7110],
          [0.2251, 0.6139, 0.2675, 0.6163, 0.2751]]]], requires_grad=True),)
trace_kwargs = {'eps': 1e-08}
test_args = (tensor([[[[3.5632e-01, 6.5937e-01, 3.6304e-01, 7.1654e-01, 7.0400e-01],
          [6.9504e-01, 3.5695e-01, 4.2703e-01...1, 6.6469e-01],
          [5.2317e-01, 3.2999e-01, 1.1261e-01, 8.0847e-02, 4.2477e-01]]]],
       requires_grad=True),)
test_kwargs = {'eps': 1e-08}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_hls at 0x7f48a82f1ea0>
trace_args = (tensor([[[[0.1553, 0.2790, 0.0459, 0.8861, 0.6092],
          [0.7767, 0.3166, 0.8938, 0.3650, 0.9301],
          [0.... [0.1666, 0.8955, 0.8418, 0.6065, 0.7110],
          [0.2251, 0.6139, 0.2675, 0.6163, 0.2751]]]], requires_grad=True),)
trace_kwargs = {'eps': 1e-08}
test_args = (tensor([[[[3.5632e-01, 6.5937e-01, 3.6304e-01, 7.1654e-01, 7.0400e-01],
          [6.9504e-01, 3.5695e-01, 4.2703e-01...1, 6.6469e-01],
          [5.2317e-01, 3.2999e-01, 1.1261e-01, 8.0847e-02, 4.2477e-01]]]],
       requires_grad=True),)
test_kwargs = {'eps': 1e-08}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(1, 3, 4, 5), dtype=float32, numpy=
array([[[[0.15530777, 0.2789616 , 0.04587364, 0.88605887, 0.6092....60653317, 0.7109579 ],
         [0.22507876, 0.61391425, 0.26750994, 0.6162563 , 0.27505213]]]],
      dtype=float32)>
eps = 1e-08

    def tensorflow_rgb_to_hls(image, eps=1e-08):
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.pointwise_ops import sub
        from ..core._backend import where
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_requires_grad_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_empty_like_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_add_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_mul_frnt
    
        if not isinstance(image, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(image)}")
        if len(tensorflow_shape_frnt_(image)) < 3 or tensorflow_shape_frnt_(image)[-3] != 3:
            raise ValueError(
                f"Input size must have a shape of (*, 3, H, W). Got {tensorflow_shape_frnt_(image)}"
            )
        _RGB2HSL_IDX = tensor(
            [[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype
        )
        _img_max: typing.Any = tensorflow_max_frnt_(image, -3)
        maxc = _img_max[0]
        imax = _img_max[1]
        minc: typing.Any = tensorflow_min_frnt_(image, -3)[0]
        if tensorflow_requires_grad_frnt_(image):
            l_ = maxc + minc
            s = maxc - minc
            h = l_
            image_hls = l_
        else:
            image_hls = tensorflow_empty_like_frnt(image)
            h, l_, s = (
                image_hls[..., 0, :, :],
                image_hls[..., 1, :, :],
                image_hls[..., 2, :, :],
            )
            tensorflow_add_frnt(maxc, minc, out=l_)
            sub(maxc, minc, out=s)
        im = image / tensorflow_unsqueeze_frnt_(s + eps, -3)
        s = s / (where(l_ < 1.0, l_, 2.0 - l_) + eps)
        l_ = l_ / 2
        r, g, b = im[..., 0, :, :], im[..., 1, :, :], im[..., 2, :, :]
        cond = imax[..., None, :, :] == _RGB2HSL_IDX
        if tensorflow_requires_grad_frnt_(image):
            h = (g - b) % 6 * cond[..., 0, :, :]
        else:
>           tensorflow_mul_frnt((g - b) % 6, cond[..., 0, :, :], out=h)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  4.,  0.],
        [ 2.,  4.,  4.,  2.,  4.],
        [ 1., -0.,  4.,  4.,  4.],
        [ 4.,  4.,  4.,  2.,  0.]]], dtype=float32)>
other = <tf.Tensor: shape=(1, 4, 5), dtype=bool, numpy=
array([[[False, False, False,  True,  True],
        [False, False,  True,  True,  True],
        [ True, False, False,  True, False],
        [ True, False, False, False, False]]])>

    def tensorflow_mul_frnt(input, other, *, out=None):
        from ...backends.tensorflow.elementwise import tensorflow_multiply
    
>       return tensorflow_multiply(input, other, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/pointwise_ops.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x1 = <tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  4.,  0.],
        [ 2.,  4.,  4.,  2.,  4.],
        [ 1., -0.,  4.,  4.,  4.],
        [ 4.,  4.,  4.,  2.,  0.]]], dtype=float32)>
x2 = <tf.Tensor: shape=(1, 4, 5), dtype=bool, numpy=
array([[[False, False, False,  True,  True],
        [False, False,  True,  True,  True],
        [ True, False, False,  True, False],
        [ True, False, False, False, False]]])>

    def tensorflow_multiply(
        x1: Union[float, tensorflow.Tensor, tensorflow.Variable],
        x2: Union[float, tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.data_type import tensorflow_default_dtype_bknd
        from ...ivy.general import tensorflow_is_array_bknd
        from .creation import tensorflow_asarray
    
        oirg_x1 = x1
        oirg_x2 = x2
        try:
            dtype = (
                x1.dtype
                if hasattr(x1, "dtype")
                else x2.dtype
                if hasattr(x2, "dtype")
                else tensorflow_default_dtype_bknd()
            )
            if not tensorflow_is_array_bknd(x1):
                x1 = tensorflow_asarray(x1, dtype=dtype)
            if not tensorflow_is_array_bknd(x2):
                x2 = tensorflow_asarray(x2, dtype=dtype)
        except:
            x1 = oirg_x1
            x2 = oirg_x2
>       return tensorflow.math.multiply(x1, x2)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/elementwise.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  4.,  0.],
        [ 2.,  4.,  4.,  2.,  4...se,  True,  True,  True],
        [ True, False, False,  True, False],
        [ True, False, False, False, False]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      if not ops.is_auto_dtype_conversion_enabled():
>       return op(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/ops/weak_tensor_ops.py:142: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  4.,  0.],
        [ 2.,  4.,  4.,  2.,  4...se,  True,  True,  True],
        [ True, False, False,  True, False],
        [ True, False, False, False, False]]])>)
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a bool tensor [Op:Mul] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.hls.rgb_to_hls
_______________________________________________________________________________ test_rgb_to_yuv420[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_rgb_to_yuv420(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 4, 6),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(5, 3, 4, 6),
        )
        test_kwargs = {}
>       _test_function(
            kornia.color.rgb_to_yuv420,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_color.py:559: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_yuv420 at 0x7f48a82f3be0>
trace_args = (tensor([[[[0.3845, 0.7670, 0.5128, 0.8301, 0.3014, 0.9526],
          [0.1610, 0.3975, 0.7657, 0.7926, 0.9823, 0.5083...     [0.6833, 0.4114, 0.1477, 0.5493, 0.7601, 0.8958],
          [0.3020, 0.7808, 0.8362, 0.2540, 0.9180, 0.2953]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[0.4270, 0.5835, 0.5549, 0.5059, 0.5714, 0.8424],
          [0.3978, 0.3042, 0.1570, 0.5083, 0.3176, 0.8546...     [0.9874, 0.7546, 0.2031, 0.5703, 0.0908, 0.2477],
          [0.4793, 0.7220, 0.3640, 0.6203, 0.2263, 0.3244]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_yuv420 at 0x7f48a82f3be0>
trace_args = (tensor([[[[0.3845, 0.7670, 0.5128, 0.8301, 0.3014, 0.9526],
          [0.1610, 0.3975, 0.7657, 0.7926, 0.9823, 0.5083...     [0.6833, 0.4114, 0.1477, 0.5493, 0.7601, 0.8958],
          [0.3020, 0.7808, 0.8362, 0.2540, 0.9180, 0.2953]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[0.4270, 0.5835, 0.5549, 0.5059, 0.5714, 0.8424],
          [0.3978, 0.3042, 0.1570, 0.5083, 0.3176, 0.8546...     [0.9874, 0.7546, 0.2031, 0.5703, 0.0908, 0.2477],
          [0.4793, 0.7220, 0.3640, 0.6203, 0.2263, 0.3244]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[[0.2662, 0.4878, 0.4688, 0.4604, 0.2585, 0.8437],
          [0.5289, 0.6881, 0.7786, 0.5806, 0.3765, 0.3296...       [ 0.0390, -0.0626,  0.0737]],

         [[-0.0572,  0.1344,  0.2054],
          [ 0.1134,  0.0981, -0.1115]]]]))
transpiled_x = (<tf.Tensor: shape=(1, 1, 4, 6), dtype=float32, numpy=
array([[[[0.2661672 , 0.48779663, 0.46881503, 0.46041054, 0.258...        [[ 0.16542992,  0.11922272,  0.19762948],
         [-0.06262618, -0.02355438, -0.01355778]]]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[0.2661672 , 0.48779663, 0.46881503, 0.46041054, 0.25847983,
          0.8437306 ],
         [0.52888   , 0....
        [[-0.05724249,  0.13440159,  0.205361  ],
         [ 0.11341552,  0.09811719, -0.11150901]]]], dtype=float32))
y = (array([[[[0.2661672 , 0.48779663, 0.46881503, 0.46041054, 0.25847983,
          0.8437306 ],
         [0.52888   , 0....
        [[ 0.16542992,  0.11922272,  0.19762948],
         [-0.06262618, -0.02355438, -0.01355778]]]], dtype=float32))
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7f48a1639340>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[ 0.03921103,  0.08301292,  0.02938745],
         [ 0.03897465, -0.06261332,  0.07366677]],

        [[-0.05724249,  0.13440159,  0.205361  ],
         [ 0.11341552,  0.09811719, -0.11150901]]]], dtype=float32)
y = array([[[[ 0.06221057, -0.00241335,  0.01972689],
         [-0.02848332,  0.12943244,  0.02116625]],

        [[ 0.16542992,  0.11922272,  0.19762948],
         [-0.06262618, -0.02355438, -0.01355778]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.yuv.rgb_to_yuv420
________________________________________________________________________________ test_raw_to_rgb[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_raw_to_rgb(target_framework, mode, backend_compile):
        print("kornia.color.raw_to_rgb")
    
        transpiled_raw_to_rgb = ivy.transpile(kornia.color.raw_to_rgb, source="torch", target=target_framework)
        TranspiledCFA = ivy.transpile(kornia.color.CFA, source="torch", target=target_framework)
    
        torch_x = torch.rand(5, 1, 4, 6)
        transpiled_x = _array_to_new_backend(torch_x, target_framework)
    
        torch_out = kornia.color.raw_to_rgb(torch_x, kornia.color.CFA.RG)
        transpiled_out = transpiled_raw_to_rgb(transpiled_x, TranspiledCFA.RG)
    
>       _to_numpy_and_allclose(torch_out, transpiled_out)

kornia/test_color.py:713: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0.0925, 0.0925, 0.2209, 0.3493, 0.2323, 0.1152],
          [0.0925, 0.0925, 0.2209, 0.3493, 0.2323, 0.1152]...       [0.6118, 0.3693, 0.1267, 0.2610, 0.3952, 0.3952],
          [0.6118, 0.3693, 0.1267, 0.2610, 0.3952, 0.3952]]]])
transpiled_x = <tf.Tensor: shape=(5, 3, 4, 6), dtype=float32, numpy=
array([[[[0.09251142, 0.09251142, 0.22090623, 0.33258006, 0.1988...21158],
         [0.6118294 , 0.43857652, 0.161372  , 0.2609665 , 0.39521158,
          0.39521158]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0.09251142, 0.09251142, 0.22090623, 0.34930104, 0.23225456,
          0.11520809],
         [0.09251142, 0.0...521158],
         [0.6118294 , 0.3692754 , 0.12672144, 0.2609665 , 0.39521158,
          0.39521158]]]], dtype=float32)
y = array([[[[0.09251142, 0.09251142, 0.22090623, 0.33258006, 0.19881268,
          0.11520809],
         [0.09251142, 0.0...521158],
         [0.6118294 , 0.43857652, 0.161372  , 0.2609665 , 0.39521158,
          0.39521158]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.raw_to_rgb
_________________________________________________________________________________ test_RgbToHls[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RgbToHls(target_framework, mode, backend_compile):
        args = (
            torch.rand(2, 3, 4, 5),
        )
>       _test_color_class(
            kornia.color.RgbToHls,
            args,
            target_framework,
            backend_compile,
            tolerance=1e-3,
        )

kornia/test_color.py:908: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'kornia.color.hls.RgbToHls'>
args = (tensor([[[[0.9960, 0.3913, 0.2845, 0.7800, 0.2617],
          [0.8252, 0.8357, 0.4411, 0.8220, 0.3886],
          [0...., 0.0456],
          [0.3748, 0.4614, 0.3083, 0.5602, 0.5486],
          [0.6211, 0.9663, 0.1621, 0.2829, 0.2612]]]]),)
target = 'tensorflow', backend_compile = False, tolerance = 0.001, init_args = ()

    def _test_color_class(
        cls,
        args,
        target,
        backend_compile=False,
        tolerance=1e-3,
        init_args=(),
    ):
        print(f"{cls.__module__}.{cls.__name__}")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(cls, source="torch", target=target)
    
        torch_obj = cls(*init_args)
        transpiled_obj = transpiled_cls(*init_args)
    
        torch_out = torch_obj(*args)
        transpile_args = _nest_torch_tensor_to_new_framework(args, target)
>       transpiled_out = transpiled_obj(*transpile_args)

kornia/test_color.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls()
args = (<tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.9959889 , 0.39130485, 0.2844991 , 0.77996343, 0.261...602352 , 0.54861397],
         [0.6210606 , 0.96632683, 0.16211861, 0.2828712 , 0.26118338]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f48a097b440, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RgbToHls(), <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.9959889 , 0.39130485, 0.2844...5602352 , 0.54861397],
         [0.6210606 , 0.96632683, 0.16211861, 0.2828712 , 0.26118338]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.9959889 , 0.39130485, 0.2844991 , 0.77996343, 0.261...602352 , 0.54861397],
         [0.6210606 , 0.96632683, 0.16211861, 0.2828712 , 0.26118338]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RgbToHls(), <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.9959889 , 0.39130485, 0.2844...5602352 , 0.54861397],
         [0.6210606 , 0.96632683, 0.16211861, 0.2828712 , 0.26118338]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.9959889 , 0.39130485, 0.2844991 , 0.77996343, 0.261...602352 , 0.54861397],
         [0.6210606 , 0.96632683, 0.16211861, 0.2828712 , 0.26118338]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.9959889 , 0.39130485, 0.2844991 , 0.77996343, 0.2617....5602352 , 0.54861397],
         [0.6210606 , 0.96632683, 0.16211861, 0.2828712 , 0.26118338]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (image)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RgbToHls(),)
kwargs = {'image': <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.9959889 , 0.39130485, 0.2844991 , 0.779963...5602352 , 0.54861397],
         [0.6210606 , 0.96632683, 0.16211861, 0.2828712 , 0.26118338]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls()
image = <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.9959889 , 0.39130485, 0.2844991 , 0.77996343, 0.2617....5602352 , 0.54861397],
         [0.6210606 , 0.96632683, 0.16211861, 0.2828712 , 0.26118338]]]],
      dtype=float32)>

    def call(self, image):
>       return tensorflow_rgb_to_hls(image)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.9959889 , 0.39130485, 0.2844991 , 0.77996343, 0.2617....5602352 , 0.54861397],
         [0.6210606 , 0.96632683, 0.16211861, 0.2828712 , 0.26118338]]]],
      dtype=float32)>
eps = 1e-08

    def tensorflow_rgb_to_hls(image, eps=1e-08):
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.pointwise_ops import sub
        from ..core._backend import where
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_requires_grad_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_empty_like_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_add_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_mul_frnt
    
        if not isinstance(image, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(image)}")
        if len(tensorflow_shape_frnt_(image)) < 3 or tensorflow_shape_frnt_(image)[-3] != 3:
            raise ValueError(
                f"Input size must have a shape of (*, 3, H, W). Got {tensorflow_shape_frnt_(image)}"
            )
        _RGB2HSL_IDX = tensor(
            [[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype
        )
        _img_max: typing.Any = tensorflow_max_frnt_(image, -3)
        maxc = _img_max[0]
        imax = _img_max[1]
        minc: typing.Any = tensorflow_min_frnt_(image, -3)[0]
        if tensorflow_requires_grad_frnt_(image):
            l_ = maxc + minc
            s = maxc - minc
            h = l_
            image_hls = l_
        else:
            image_hls = tensorflow_empty_like_frnt(image)
            h, l_, s = (
                image_hls[..., 0, :, :],
                image_hls[..., 1, :, :],
                image_hls[..., 2, :, :],
            )
            tensorflow_add_frnt(maxc, minc, out=l_)
            sub(maxc, minc, out=s)
        im = image / tensorflow_unsqueeze_frnt_(s + eps, -3)
        s = s / (where(l_ < 1.0, l_, 2.0 - l_) + eps)
        l_ = l_ / 2
        r, g, b = im[..., 0, :, :], im[..., 1, :, :], im[..., 2, :, :]
        cond = imax[..., None, :, :] == _RGB2HSL_IDX
        if tensorflow_requires_grad_frnt_(image):
            h = (g - b) % 6 * cond[..., 0, :, :]
        else:
>           tensorflow_mul_frnt((g - b) % 6, cond[..., 0, :, :], out=h)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:86: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 5), dtype=float32, numpy=
array([[[-0.,  2.,  2., -0.,  4.],
        [ 2.,  2., -0.,  2.,  0....     [ 4.,  0.,  2.,  0.,  0.],
        [ 2.,  2.,  2.,  2.,  0.],
        [-0.,  4., -0.,  2.,  4.]]], dtype=float32)>
other = <tf.Tensor: shape=(2, 4, 5), dtype=bool, numpy=
array([[[ True,  True, False,  True, False],
        [False,  True, Fa...lse,  True, False,  True],
        [ True,  True,  True, False, False],
        [ True, False,  True, False, False]]])>

    def tensorflow_mul_frnt(input, other, *, out=None):
        from ...backends.tensorflow.elementwise import tensorflow_multiply
    
>       return tensorflow_multiply(input, other, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/pointwise_ops.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x1 = <tf.Tensor: shape=(2, 4, 5), dtype=float32, numpy=
array([[[-0.,  2.,  2., -0.,  4.],
        [ 2.,  2., -0.,  2.,  0....     [ 4.,  0.,  2.,  0.,  0.],
        [ 2.,  2.,  2.,  2.,  0.],
        [-0.,  4., -0.,  2.,  4.]]], dtype=float32)>
x2 = <tf.Tensor: shape=(2, 4, 5), dtype=bool, numpy=
array([[[ True,  True, False,  True, False],
        [False,  True, Fa...lse,  True, False,  True],
        [ True,  True,  True, False, False],
        [ True, False,  True, False, False]]])>

    def tensorflow_multiply(
        x1: Union[float, tensorflow.Tensor, tensorflow.Variable],
        x2: Union[float, tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.data_type import tensorflow_default_dtype_bknd
        from ...ivy.general import tensorflow_is_array_bknd
        from .creation import tensorflow_asarray
    
        oirg_x1 = x1
        oirg_x2 = x2
        try:
            dtype = (
                x1.dtype
                if hasattr(x1, "dtype")
                else x2.dtype
                if hasattr(x2, "dtype")
                else tensorflow_default_dtype_bknd()
            )
            if not tensorflow_is_array_bknd(x1):
                x1 = tensorflow_asarray(x1, dtype=dtype)
            if not tensorflow_is_array_bknd(x2):
                x2 = tensorflow_asarray(x2, dtype=dtype)
        except:
            x1 = oirg_x1
            x2 = oirg_x2
>       return tensorflow.math.multiply(x1, x2)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RgbToHls.call().
E       
E       [1mcannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a bool tensor [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_RgbToHls.call():
E         â€¢ image=tf.Tensor(shape=(2, 3, 4, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/elementwise.py:290: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.hls.RgbToHls
________________________________________________________________________________ test_RgbToYuv420[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RgbToYuv420(target_framework, mode, backend_compile):
        args = (
            torch.rand(2, 3, 4, 6),
        )
>       _test_color_class(
            kornia.color.RgbToYuv420,
            args,
            target_framework,
            backend_compile,
            tolerance=1e-3,
        )

kornia/test_color.py:1064: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'kornia.color.yuv.RgbToYuv420'>
args = (tensor([[[[0.5452, 0.3903, 0.2707, 0.1878, 0.6662, 0.0041],
          [0.2997, 0.3271, 0.5075, 0.8471, 0.7320, 0.3333...     [0.4367, 0.9288, 0.2004, 0.2033, 0.4215, 0.8186],
          [0.7486, 0.0752, 0.5597, 0.3138, 0.7638, 0.8419]]]]),)
target = 'tensorflow', backend_compile = False, tolerance = 0.001, init_args = ()

    def _test_color_class(
        cls,
        args,
        target,
        backend_compile=False,
        tolerance=1e-3,
        init_args=(),
    ):
        print(f"{cls.__module__}.{cls.__name__}")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(cls, source="torch", target=target)
    
        torch_obj = cls(*init_args)
        transpiled_obj = transpiled_cls(*init_args)
    
        torch_out = torch_obj(*args)
        transpile_args = _nest_torch_tensor_to_new_framework(args, target)
        transpiled_out = transpiled_obj(*transpile_args)
    
        orig_np = _nest_array_to_numpy(torch_out)
        graph_np = _nest_array_to_numpy(transpiled_out)
    
>       _check_allclose(orig_np, graph_np, tolerance=tolerance)

kornia/test_color.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[0.75590134, 0.3043153 , 0.3627882 , 0.42039534, 0.6080194 ,
          0.18830657],
         [0.29083595, 0....
        [[-0.19296852, -0.06312598,  0.11411697],
         [ 0.07555892, -0.00697177, -0.08877163]]]], dtype=float32))
y = (array([[[[0.75590134, 0.3043153 , 0.3627882 , 0.42039534, 0.6080194 ,
          0.18830657],
         [0.29083595, 0....
        [[ 0.14423555, -0.11750033, -0.08155767],
         [-0.06871755, -0.04961096,  0.01098897]]]], dtype=float32))
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7f4898d4f800>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[-0.06066635,  0.07449484,  0.09529536],
         [-0.05634499, -0.08640562,  0.10985658]],

        [[-0.023...

        [[-0.19296852, -0.06312598,  0.11411697],
         [ 0.07555892, -0.00697177, -0.08877163]]]], dtype=float32)
y = array([[[[-0.11066963, -0.06510137,  0.22062634],
         [ 0.01432721,  0.02188577, -0.00483853]],

        [[-0.118...

        [[ 0.14423555, -0.11750033, -0.08155767],
         [-0.06871755, -0.04961096,  0.01098897]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.yuv.RgbToYuv420
_________________________________________________________________________________ test_RawToRgb[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RawToRgb(target_framework, mode, backend_compile):
        print("kornia.color.RawToRgb")
    
        transpiled_RawToRgb = ivy.transpile(kornia.color.RawToRgb, source="torch", target=target_framework)
        TranspiledCFA = ivy.transpile(kornia.color.CFA, source="torch", target=target_framework)
    
        torch_x = torch.rand(2, 1, 4, 6)
        transpiled_x = _array_to_new_backend(torch_x, target_framework)
    
        torch_out = kornia.color.RawToRgb(kornia.color.CFA.RG)(torch_x)
        transpiled_out = transpiled_RawToRgb(TranspiledCFA.RG)(transpiled_x)
    
>       _to_numpy_and_allclose(torch_out, transpiled_out)

kornia/test_color.py:1152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0.4553, 0.4553, 0.2568, 0.0582, 0.2755, 0.4928],
          [0.4553, 0.4553, 0.2568, 0.0582, 0.2755, 0.4928]...       [0.8807, 0.9363, 0.9920, 0.7321, 0.4722, 0.4722],
          [0.8807, 0.9363, 0.9920, 0.7321, 0.4722, 0.4722]]]])
transpiled_x = <tf.Tensor: shape=(2, 3, 4, 6), dtype=float32, numpy=
array([[[[0.45533615, 0.45533615, 0.2567755 , 0.08925939, 0.3376...1877 ],
         [0.88072014, 0.92044914, 0.9840156 , 0.7320745 , 0.4721877 ,
          0.4721877 ]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0.45533615, 0.45533615, 0.2567755 , 0.05821484, 0.27552605,
          0.49283725],
         [0.45533615, 0.4...21877 ],
         [0.88072014, 0.9363408 , 0.99196136, 0.73207456, 0.4721877 ,
          0.4721877 ]]]], dtype=float32)
y = array([[[[0.45533615, 0.45533615, 0.2567755 , 0.08925939, 0.33761504,
          0.49283725],
         [0.45533615, 0.4...21877 ],
         [0.88072014, 0.92044914, 0.9840156 , 0.7320745 , 0.4721877 ,
          0.4721877 ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.RawToRgb
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_color.py::test_rgb_to_hls[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a fl...
FAILED kornia/test_color.py::test_rgb_to_yuv420[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_color.py::test_raw_to_rgb[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_color.py::test_RgbToHls[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RgbToHls.call().
FAILED kornia/test_color.py::test_RgbToYuv420[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_color.py::test_RawToRgb[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
=============================================================================== 6 failed, 63 passed in 759.12s (0:12:39) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 4 items

kornia/geometry/test_liegroup.py ..F.                                                                                                                                                            [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________________ test_So2[tensorflow-s2s-False] ____________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_So2(target_framework, mode, backend_compile):
        print("kornia.geometry.liegroup.So2")
    
        if backend_compile:
            pytest.skip()
    
        real_part = torch.tensor([1.0], requires_grad=True)
        imaginary_part = torch.tensor([2.0], requires_grad=True)
        complex_number = torch.complex(real_part, imaginary_part)
        torch_so2 = kornia.geometry.liegroup.So2(complex_number)
    
        TranspiledSo2 = ivy.transpile(kornia.geometry.liegroup.So2, source="torch", target=target_framework)
        transpiled_complex_number = _nest_torch_tensor_to_new_framework(complex_number, target_framework)
        transpiled_so2 = TranspiledSo2(transpiled_complex_number)
    
        # Test .matrix()
        torch_matrix = torch_so2.matrix()
        transpiled_matrix = transpiled_so2.matrix()
        _to_numpy_and_allclose(torch_matrix, transpiled_matrix)
    
        # Test .inverse()
        torch_inverse = torch_so2.inverse()
        transpiled_inverse = transpiled_so2.inverse()
        _to_numpy_and_allclose(torch_inverse.z, transpiled_inverse.z)
    
        # Test .log()
        torch_log = torch_so2.log()
        transpiled_log = transpiled_so2.log()
        _to_numpy_and_allclose(torch_log, transpiled_log)
    
        # Test .__mul__()
        other_real_part = torch.tensor([0.5], requires_grad=True)
        other_imaginary_part = torch.tensor([0.5], requires_grad=True)
        other_complex_number = torch.complex(other_real_part, other_imaginary_part)
        other_torch_so2 = kornia.geometry.liegroup.So2(other_complex_number)
    
        transpiled_other_complex_number = _nest_torch_tensor_to_new_framework(other_complex_number, target_framework)
        transpiled_other_so2 = TranspiledSo2(transpiled_other_complex_number)
    
        torch_composed_so2 = torch_so2 * other_torch_so2
        transpiled_composed_so2 = transpiled_so2 * transpiled_other_so2
        _to_numpy_and_allclose(torch_composed_so2.z, transpiled_composed_so2.z)
    
        # Test .adjoint()
        torch_adjoint = torch_so2.adjoint()
>       transpiled_adjoint = transpiled_so2.adjoint()

kornia/geometry/test_liegroup.py:202: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Variable 'Variable:0' shape=(1,) dtype=complex64, numpy=array([1.+2.j], dtype=complex64)>

    def adjoint(self):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_real_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
>       batch_size = len(self.z) if len(tensorflow_shape_frnt_(self.z)) > 0 else None
E       TypeError: object of type 'ResourceVariable' has no len()

Translated_Outputs/tensorflow_outputs/kornia/geometry/liegroup/so2.py:195: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.liegroup.So2
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_liegroup.py::test_So2[tensorflow-s2s-False] - TypeError: object of type 'ResourceVariable' has no len()
=============================================================================== 1 failed, 3 passed in 234.76s (0:03:54) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 17 items

kornia/test_feature2.py .......F...FFF.FF                                                                                                                                                        [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________ test_laf_is_inside_image[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_laf_is_inside_image(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 5, 2, 3),
            torch.rand(1, 1, 32, 32),
        )
        trace_kwargs = {'border': 0}
        test_args = (
            torch.rand(2, 10, 2, 3),
            torch.rand(2, 1, 64, 64),
        )
        test_kwargs = {'border': 1}
>       _test_function(
            kornia.feature.laf_is_inside_image,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_feature2.py:192: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function laf_is_inside_image at 0x7fb39b1ce4d0>
trace_args = (tensor([[[[3.7965e-01, 8.7296e-01, 1.0459e-01],
          [6.3732e-01, 9.6301e-01, 9.3273e-01]],

         [[9.1455e-...0, 0.0758, 0.3358,  ..., 0.4017, 0.4259, 0.4541],
          [0.0519, 0.5031, 0.8875,  ..., 0.8184, 0.2516, 0.6876]]]]))
trace_kwargs = {'border': 0}
test_args = (tensor([[[[3.9236e-01, 8.5456e-01, 1.1386e-01],
          [3.7607e-01, 7.6610e-01, 9.7048e-01]],

         [[4.1606e-...8, 0.5401, 0.1830,  ..., 0.7885, 0.9251, 0.0149],
          [0.3715, 0.4789, 0.3442,  ..., 0.6115, 0.6039, 0.2714]]]]))
test_kwargs = {'border': 1}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function laf_is_inside_image at 0x7fb39b1ce4d0>
trace_args = (tensor([[[[3.7965e-01, 8.7296e-01, 1.0459e-01],
          [6.3732e-01, 9.6301e-01, 9.3273e-01]],

         [[9.1455e-...0, 0.0758, 0.3358,  ..., 0.4017, 0.4259, 0.4541],
          [0.0519, 0.5031, 0.8875,  ..., 0.8184, 0.2516, 0.6876]]]]))
trace_kwargs = {'border': 0}
test_args = (tensor([[[[3.9236e-01, 8.5456e-01, 1.1386e-01],
          [3.7607e-01, 7.6610e-01, 9.7048e-01]],

         [[4.1606e-...8, 0.5401, 0.1830,  ..., 0.7885, 0.9251, 0.0149],
          [0.3715, 0.4789, 0.3442,  ..., 0.6115, 0.6039, 0.2714]]]]))
test_kwargs = {'border': 1}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

laf = <tf.Tensor: shape=(1, 5, 2, 3), dtype=float32, numpy=
array([[[[3.79647970e-01, 8.72959256e-01, 1.04587793e-01],
     ...1, 5.90719998e-01, 1.29111111e-01],
         [5.56568325e-01, 3.20155263e-01, 5.52749395e-01]]]],
      dtype=float32)>
images = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.35688895, 0.4020174 , 0.6201612 , ..., 0.1078248 ,...],
         [0.05194503, 0.5031241 , 0.8874587 , ..., 0.81840885,
          0.25162107, 0.68763494]]]], dtype=float32)>
border = 0

    def tensorflow_laf_is_inside_image(laf, images, border=0):
        from ..core.check import tensorflow_KORNIA_CHECK_LAF
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
    
        tensorflow_KORNIA_CHECK_LAF(laf)
        _, _, h, w = tensorflow_size_frnt_(images)
        pts = tensorflow_laf_to_boundary_points(laf, 12)
        good_lafs_mask = (
>           (pts[..., 0] >= border)
            * (pts[..., 0] <= w - border)
            * (pts[..., 1] >= border)
            * (pts[..., 1] <= h - border)
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/laf.py:231: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 12), dtype=bool, numpy=
array([[[ True,  True,  True,  True,  True, False, False, False, Fal...rue,  True],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True]]])>)
kwargs = {}
arg = <tf.Tensor: shape=(1, 5, 12), dtype=bool, numpy=
array([[[ True,  True,  True,  True,  True,  True,  True,  True,  Tru...True,  True],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True]]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 12), dtype=bool, numpy=
array([[[ True,  True,  True,  True,  True, False, False, False, Fal...rue,  True],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True]]])>)
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E     	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.laf.laf_is_inside_image
____________________________________________________________________________ test_DenseSIFTDescriptor[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DenseSIFTDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.DenseSIFTDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDenseSIFTDescriptor = ivy.transpile(kornia.feature.DenseSIFTDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(2, 1, 200, 300)
        torch_out = kornia.feature.DenseSIFTDescriptor()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDenseSIFTDescriptor()(transpiled_x)

kornia/test_feature2.py:285: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2)
args = (<tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.19496197, 0.16508347, 0.23270667, ..., 0.779346...
         [0.679259  , 0.2465136 , 0.18834752, ..., 0.49562198,
          0.1940341 , 0.31457633]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7fb3947ea820, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipv...,
         [0.679259  , 0.2465136 , 0.18834752, ..., 0.49562198,
          0.1940341 , 0.31457633]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.19496197, 0.16508347, 0.23270667, ..., 0.779346...
         [0.679259  , 0.2465136 , 0.18834752, ..., 0.49562198,
          0.1940341 , 0.31457633]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipv...,
         [0.679259  , 0.2465136 , 0.18834752, ..., 0.49562198,
          0.1940341 , 0.31457633]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.19496197, 0.16508347, 0.23270667, ..., 0.779346...
         [0.679259  , 0.2465136 , 0.18834752, ..., 0.49562198,
          0.1940341 , 0.31457633]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.19496197, 0.16508347, 0.23270667, ..., 0.7793469...],
         [0.679259  , 0.2465136 , 0.18834752, ..., 0.49562198,
          0.1940341 , 0.31457633]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2),)
kwargs = {'input': <tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.19496197, 0.16508347, 0.23270667, ...,...,
         [0.679259  , 0.2465136 , 0.18834752, ..., 0.49562198,
          0.1940341 , 0.31457633]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2)
input = <tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.19496197, 0.16508347, 0.23270667, ..., 0.7793469...],
         [0.679259  , 0.2465136 , 0.18834752, ..., 0.49562198,
          0.1940341 , 0.31457633]]]], dtype=float32)>

    def call(self, input):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ..filters.sobel import tensorflow_spatial_gradient
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_atan2_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_floor_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clamp__frnt_
        from ..constants import pi
        from ..core._backend import concatenate
        from ..core._backend import normalize
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "1", "H", "W"])
        B, CH, W, H = tensorflow_size_frnt_(input)
>       self.bin_pooling_kernel = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.bin_pooling_kernel, input.dtype), input.device
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/siftdesc.py:169: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (KerasConv2D(), '/job:localhost/replica:0/task:0/device:CPU:0'), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fb37cdde3b0>, array_like = KerasConv2D()
pattern = '_bknd_|_bknd|_frnt_|_frnt', fn_name = 'to'

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
            return fn(*args, **kwargs)
        else:
            pattern = "_bknd_|_bknd|_frnt_|_frnt"
            fn_name = extract_function_name(re.sub(pattern, "", fn.__name__))
            try:
                new_fn = getattr(array_like, fn_name)
                if not callable(new_fn):
                    return new_fn
                return new_fn(*args[1:], **kwargs)
            except AttributeError:
>               return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:195: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = KerasConv2D(), args = ('/job:localhost/replica:0/task:0/device:CPU:0',), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fb37cdde3b0>
tensorflow_dev = <function tensorflow_dev at 0x7fb37cd9a3b0>, tensorflow_dtype = <function tensorflow_dtype at 0x7fb37cd9a290>
tensorflow_check_elem_in_list = <function tensorflow_check_elem_in_list at 0x7fb37cd30310>, tensorflow_as_ivy_dev = <function tensorflow_as_ivy_dev at 0x7fb37cd9a440>
tensorflow_asarray = <function tensorflow_asarray at 0x7fb37cd98700>, _all_ivy_dtypes_str = ('int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', ...)
device = '/job:localhost/replica:0/task:0/device:CPU:0', dtype = None, arg = '/job:localhost/replica:0/task:0/device:CPU:0'

    @tensorflow_handle_methods
    def tensorflow_to_frnt_(tensor, *args, **kwargs):
        from ...ivy.general import tensorflow_is_array_bknd
        from ...backends.tensorflow.device import tensorflow_dev
        from ...backends.tensorflow.data_type import tensorflow_dtype
        from ....utils.assertions import tensorflow_check_elem_in_list
        from ...backends.tensorflow.device import tensorflow_as_ivy_dev
        from ...backends.tensorflow.creation import tensorflow_asarray
        from ....__init__ import _all_ivy_dtypes_str
    
        device = None
        dtype = None
        for arg in args:
            if hasattr(arg, "ivy_array") or tensorflow_is_array_bknd(arg):
                device = tensorflow_dev(arg)
                dtype = tensorflow_dtype(arg)
            elif (
                isinstance(arg, (tf.DType,))
                or isinstance(arg, (str,))
                and hasattr(arg, "as_native_dtype")
                or arg in _all_ivy_dtypes_str
            ):
                dtype = arg
            elif isinstance(arg, (str, str, str)):
                if isinstance(arg, (str,)) and not isinstance(arg, (str, str)):
                    tensorflow_check_elem_in_list(
                        arg,
                        [
                            "cpu",
                            "cuda",
                            "mps",
                            "xpu",
                            "mkldnn",
                            "opengl",
                            "opencl",
                            "ideep",
                            "hip",
                            "ve",
                            "ort",
                            "mlc",
                            "xla",
                            "lazy",
                            "vulkan",
                            "meta",
                            "hpu",
                        ],
                    )
                device = arg
        if "device" in kwargs:
            device = kwargs["device"]
        if "dtype" in kwargs:
            dtype = kwargs["dtype"]
        if (dtype is None or tensor.dtype == dtype) and (
>           device is None or tensor.device == tensorflow_as_ivy_dev(device)
        ):

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:233: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D(), name = 'device'

    def __getattribute__(self, name):
        built = object.__getattribute__(self, "__dict__").get("built", False)
        if built:
            attr_map = {"weight": "kernel", "out_channels": "filters"}
        else:
            attr_map = {
                "out_channels": "filters",
            }
    
        new_name = attr_map[name] if name in attr_map else name
>       return super().__getattribute__(new_name)
E       AttributeError: Exception encountered when calling tensorflow_DenseSIFTDescriptor.call().
E       
E       [1m'KerasConv2D' object has no attribute 'device'[0m
E       
E       Arguments received by tensorflow_DenseSIFTDescriptor.call():
E         â€¢ input=tf.Tensor(shape=(2, 1, 200, 300), dtype=float32)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:465: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.DenseSIFTDescriptor
______________________________________________________________________________ test_SIFTDescriptor[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SIFTDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.SIFTDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSIFTDescriptor = ivy.transpile(kornia.feature.SIFTDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(23, 1, 41, 41)
        torch_out = kornia.feature.SIFTDescriptor()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSIFTDescriptor()(transpiled_x)

kornia/test_feature2.py:302: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2)
args = (<tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.68695366, 0.70636773, 0.48107606, ..., 0.4685844...
         [0.00884753, 0.4353963 , 0.7388469 , ..., 0.5998622 ,
          0.89203227, 0.46767694]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7fb37cdb3870, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), <tf.Tensor:...,
         [0.00884753, 0.4353963 , 0.7388469 , ..., 0.5998622 ,
          0.89203227, 0.46767694]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.68695366, 0.70636773, 0.48107606, ..., 0.4685844...
         [0.00884753, 0.4353963 , 0.7388469 , ..., 0.5998622 ,
          0.89203227, 0.46767694]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), <tf.Tensor:...,
         [0.00884753, 0.4353963 , 0.7388469 , ..., 0.5998622 ,
          0.89203227, 0.46767694]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.68695366, 0.70636773, 0.48107606, ..., 0.4685844...
         [0.00884753, 0.4353963 , 0.7388469 , ..., 0.5998622 ,
          0.89203227, 0.46767694]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.68695366, 0.70636773, 0.48107606, ..., 0.46858442...],
         [0.00884753, 0.4353963 , 0.7388469 , ..., 0.5998622 ,
          0.89203227, 0.46767694]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2),)
kwargs = {'input': <tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.68695366, 0.70636773, 0.48107606, ..., ...,
         [0.00884753, 0.4353963 , 0.7388469 , ..., 0.5998622 ,
          0.89203227, 0.46767694]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2)
input = <tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[0.68695366, 0.70636773, 0.48107606, ..., 0.46858442...],
         [0.00884753, 0.4353963 , 0.7388469 , ..., 0.5998622 ,
          0.89203227, 0.46767694]]]], dtype=float32)>

    def call(self, input):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ..filters.sobel import tensorflow_spatial_gradient
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_atan2_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_as_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_as_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_floor_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_clamp_frnt,
        )
        from ..constants import pi
        from ..core._backend import concatenate
        from ..core._backend import normalize
    
        tensorflow_KORNIA_CHECK_SHAPE(
            input, ["B", "1", f"{self.patch_size}", f"{self.patch_size}"]
        )
        B: typing.Any = tensorflow_shape_frnt_(input)[0]
>       self.pk = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.pk, input.dtype), input.device
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/siftdesc.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (KerasConv2D(), '/job:localhost/replica:0/task:0/device:CPU:0'), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fb37cd4af80>, array_like = KerasConv2D()
pattern = '_bknd_|_bknd|_frnt_|_frnt', fn_name = 'to'

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
            return fn(*args, **kwargs)
        else:
            pattern = "_bknd_|_bknd|_frnt_|_frnt"
            fn_name = extract_function_name(re.sub(pattern, "", fn.__name__))
            try:
                new_fn = getattr(array_like, fn_name)
                if not callable(new_fn):
                    return new_fn
                return new_fn(*args[1:], **kwargs)
            except AttributeError:
>               return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:195: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = KerasConv2D(), args = ('/job:localhost/replica:0/task:0/device:CPU:0',), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fb37cd4af80>
tensorflow_dev = <function tensorflow_dev at 0x7fb3941faf80>, tensorflow_dtype = <function tensorflow_dtype at 0x7fb3941fae60>
tensorflow_check_elem_in_list = <function tensorflow_check_elem_in_list at 0x7fb39418f760>, tensorflow_as_ivy_dev = <function tensorflow_as_ivy_dev at 0x7fb3941fb010>
tensorflow_asarray = <function tensorflow_asarray at 0x7fb3941f92d0>, _all_ivy_dtypes_str = ('int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', ...)
device = '/job:localhost/replica:0/task:0/device:CPU:0', dtype = None, arg = '/job:localhost/replica:0/task:0/device:CPU:0'

    @tensorflow_handle_methods
    def tensorflow_to_frnt_(tensor, *args, **kwargs):
        from ...ivy.general import tensorflow_is_array_bknd
        from ...backends.tensorflow.device import tensorflow_dev
        from ...backends.tensorflow.data_type import tensorflow_dtype
        from ....utils.assertions import tensorflow_check_elem_in_list
        from ...backends.tensorflow.device import tensorflow_as_ivy_dev
        from ...backends.tensorflow.creation import tensorflow_asarray
        from ....__init__ import _all_ivy_dtypes_str
    
        device = None
        dtype = None
        for arg in args:
            if hasattr(arg, "ivy_array") or tensorflow_is_array_bknd(arg):
                device = tensorflow_dev(arg)
                dtype = tensorflow_dtype(arg)
            elif (
                isinstance(arg, (tf.DType,))
                or isinstance(arg, (str,))
                and hasattr(arg, "as_native_dtype")
                or arg in _all_ivy_dtypes_str
            ):
                dtype = arg
            elif isinstance(arg, (str, str, str)):
                if isinstance(arg, (str,)) and not isinstance(arg, (str, str)):
                    tensorflow_check_elem_in_list(
                        arg,
                        [
                            "cpu",
                            "cuda",
                            "mps",
                            "xpu",
                            "mkldnn",
                            "opengl",
                            "opencl",
                            "ideep",
                            "hip",
                            "ve",
                            "ort",
                            "mlc",
                            "xla",
                            "lazy",
                            "vulkan",
                            "meta",
                            "hpu",
                        ],
                    )
                device = arg
        if "device" in kwargs:
            device = kwargs["device"]
        if "dtype" in kwargs:
            dtype = kwargs["dtype"]
        if (dtype is None or tensor.dtype == dtype) and (
>           device is None or tensor.device == tensorflow_as_ivy_dev(device)
        ):

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:233: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D(), name = 'device'

    def __getattribute__(self, name):
        built = object.__getattribute__(self, "__dict__").get("built", False)
        if built:
            attr_map = {"weight": "kernel", "out_channels": "filters"}
        else:
            attr_map = {
                "out_channels": "filters",
            }
    
        new_name = attr_map[name] if name in attr_map else name
>       return super().__getattribute__(new_name)
E       AttributeError: Exception encountered when calling tensorflow_SIFTDescriptor.call().
E       
E       [1m'KerasConv2D' object has no attribute 'device'[0m
E       
E       Arguments received by tensorflow_SIFTDescriptor.call():
E         â€¢ input=tf.Tensor(shape=(23, 1, 41, 41), dtype=float32)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:465: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SIFTDescriptor
_______________________________________________________________________________ test_MKDDescriptor[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_MKDDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.MKDDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledMKDDescriptor = ivy.transpile(kornia.feature.MKDDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(23, 1, 32, 32)
        torch_out = kornia.feature.MKDDescriptor()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledMKDDescriptor()(transpiled_x)

kornia/test_feature2.py:319: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_MKDDescriptor' object has no attribute 'output_dims'") raised in repr()] tensorflow_MKDDescriptor object at 0x7fb37cd82470>, patch_size = 32
kernel_type = 'concat', whitening = 'pcawt', training_set = 'liberty', output_dims = 128

    def __init__(
        self,
        patch_size=32,
        kernel_type="concat",
        whitening="pcawt",
        training_set="liberty",
        output_dims=128,
    ):
        from ..filters.gaussian import tensorflow_GaussianBlur2d
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ..utils.helpers import tensorflow_map_location_to_cpu
    
        self.super___init__(
            patch_size=patch_size,
            kernel_type=kernel_type,
            whitening=whitening,
            training_set=training_set,
            output_dims=output_dims,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size: typing.Any = patch_size
        self.kernel_type: typing.Any = kernel_type
        self.whitening: typing.Any = whitening
        self.training_set: typing.Any = training_set
        self.sigma = 1.4 * (patch_size / 64)
        self.smoothing = tensorflow_GaussianBlur2d(
            (5, 5), (self.sigma, self.sigma), "replicate"
        )
        self.gradients = tensorflow_MKDGradients()
        polar_s: typing.Any = "polar"
        cart_s: typing.Any = "cart"
        self.parametrizations = (
            [polar_s, cart_s] if self.kernel_type == "concat" else [self.kernel_type]
        )
        self.odims: typing.Any = 0
        relative_orientations = {polar_s: True, cart_s: False}
        self.feats = {}
        for parametrization in self.parametrizations:
            gradient_embedding = tensorflow_EmbedGradients(
                patch_size=patch_size,
                relative=tensorflow_get_item(relative_orientations, parametrization),
            )
>           spatial_encoding = tensorflow_ExplicitSpacialEncoding(
                kernel_type=parametrization,
                fmap_size=patch_size,
                in_dims=gradient_embedding.kernel.d,
            )

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:2227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_ExplicitSpacialEncoding' object has no attribute 'out_dims'") raised in repr()] tensorflow_ExplicitSpacialEncoding object at 0x7fb3950cf6d0>, args = ()
kwargs = {'fmap_size': 32, 'in_dims': 7, 'kernel_type': 'polar'}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_ExplicitSpacialEncoding' object has no attribute 'out_dims'") raised in repr()] tensorflow_ExplicitSpacialEncoding object at 0x7fb3950cf6d0>, kernel_type = 'polar'
fmap_size = 32, in_dims = 7, do_gmask = True, do_l2 = True

    @tensorflow_store_config_info
    def __init__(
        self, kernel_type="polar", fmap_size=32, in_dims=7, do_gmask=True, do_l2=True
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        self.super___init__(
            kernel_type=kernel_type,
            fmap_size=fmap_size,
            in_dims=in_dims,
            do_gmask=do_gmask,
            do_l2=do_l2,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if kernel_type not in ["polar", "cart"]:
            raise NotImplementedError(
                f"{kernel_type} is not valid, use polar or cart)."
            )
        self.kernel_type = kernel_type
        self.fmap_size = fmap_size
        self.in_dims = in_dims
        self.do_gmask = do_gmask
        self.do_l2 = do_l2
        self.grid = tensorflow_get_grid_dict(fmap_size)
        self.gmask = None
>       emb = tensorflow_spatial_kernel_embedding(self.kernel_type, self.grid)

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:1322: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kernel_type = 'polar'
grids = {'x': <tf.Tensor: shape=(32, 32), dtype=float32, numpy=
array([[-1.        , -0.9354839 , -0.87096775, ...,  0.8709677...,
       [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
         0.81871915,  0.7853981 ]], dtype=float32)>}

    def tensorflow_spatial_kernel_embedding(kernel_type, grids):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_select_frnt_
        from ..constants import pi
    
        factors = {"phi": 1.0, "rho": pi / sqrt2, "x": pi / 2, "y": pi / 2}
        if kernel_type == "cart":
            coeffs_ = "xy"
            params_ = ["x", "y"]
        elif kernel_type == "polar":
            coeffs_ = "rhophi"
            params_ = ["phi", "rho"]
        keys = list(grids.keys())
        patch_size = tensorflow_shape_frnt_(tensorflow_get_item(grids, keys[0]))[-1]
        grids_normed = {k: (v * tensorflow_get_item(factors, k)) for k, v in grids.items()}
        grids_normed = {
            k: tensorflow_float_frnt_(
                tensorflow_unsqueeze_frnt_(tensorflow_unsqueeze_frnt_(v, 0), 0)
            )
            for k, v in grids_normed.items()
        }
        vm_a = tensorflow_VonMisesKernel(
            patch_size=patch_size, coeffs=tensorflow_get_item(COEFFS, coeffs_)
        )
        vm_b = tensorflow_VonMisesKernel(
            patch_size=patch_size, coeffs=tensorflow_get_item(COEFFS, coeffs_)
        )
        emb_a = tensorflow_squeeze_frnt_(
>           vm_a(tensorflow_get_item(grids_normed, params_[0]))
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:1281: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234])
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.8542...    [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55b29b537b30, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...n='test_MKDDescriptor', code_context=['    transpiled_out = TranspiledMKDDescriptor()(transpiled_x)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), <tf.Tensor: shape=(1, ...     [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.8542...    [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), <tf.Tensor: shape=(1, ...     [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.8542...    [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.85425...      [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]),)
kwargs = {'x': <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0...     [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234])
x = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.85425...      [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>

    def call(self, x):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ..core._backend import cos
        from ..core._backend import sin
    
        if not isinstance(x, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(x)}")
        if not len(tensorflow_shape_frnt_(x)) == 4 or tensorflow_shape_frnt_(x)[1] != 1:
            raise ValueError(
                f"Invalid input shape, we expect Bx1xHxW. Got: {tensorflow_shape_frnt_(x)}"
            )
        if not isinstance(self.emb0, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Emb0 type is not a Tensor. Got {type(x)}")
        emb0 = tensorflow_repeat_frnt_(
            tensorflow_to_frnt_(self.emb0, x), tensorflow_size_frnt_(x, 0), 1, 1, 1
        )
        frange = tensorflow_to_frnt_(self.frange, x) * x
        emb1 = cos(frange)
        emb2 = sin(frange)
        embedding = tensorflow_cat_frnt([emb0, emb1, emb2], dim=1)
>       embedding = self.pt_weights * embedding

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:514: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), name = 'pt_weights'

    @tf.autograph.experimental.do_not_convert
    def __getattr__(self, name):
        if name == "v":
            if not super().__getattribute__("_v") and not getattr(  # noqa: E501
                self, "_built", False
            ):
                return self._build_and_return_v(
                    *self._args, dynamic_backend=self._dynamic_backend, **self._kwargs
                )
    
        _dict = super().__getattribute__("__dict__")
        if name in _dict:
            return _dict[name]
    
        elif "_v" in _dict and name in _dict["_v"]:
            return _dict["_v"][name]
    
>       return super().__getattribute__(name)
E       AttributeError: Exception encountered when calling tensorflow_VonMisesKernel.call().
E       
E       [1m'tensorflow_VonMisesKernel' object has no attribute 'pt_weights'[0m
E       
E       Arguments received by tensorflow_VonMisesKernel.call():
E         â€¢ x=tf.Tensor(shape=(1, 1, 32, 32), dtype=float32)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:998: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.MKDDescriptor
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/manyids2/mkd_pytorch/raw/master/mkd_pytorch/mkd-concat-64.pth" to /root/.cache/torch/hub/checkpoints/mkd-concat-64.pth

  0%|          | 0.00/1.31M [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.31M/1.31M [00:00<00:00, 121MB/s]
_________________________________________________________________________________ test_HardNet8[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HardNet8(target_framework, mode, backend_compile):
        print("kornia.feature.HardNet8")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHardNet8 = ivy.transpile(kornia.feature.HardNet8, source="torch", target=target_framework)
    
        x = torch.rand(16, 1, 32, 32)
        torch_out = kornia.feature.HardNet8()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledHardNet8()(transpiled_x)

kornia/test_feature2.py:353: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.15799665, 0.41153854, 0.7559096 , ..., 0.6417985...
         [0.811816  , 0.51970834, 0.28677595, ..., 0.86063725,
          0.7930168 , 0.22888184]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55b29e8070e0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2):...,
         [0.811816  , 0.51970834, 0.28677595, ..., 0.86063725,
          0.7930168 , 0.22888184]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.15799665, 0.41153854, 0.7559096 , ..., 0.6417985...
         [0.811816  , 0.51970834, 0.28677595, ..., 0.86063725,
          0.7930168 , 0.22888184]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2):...,
         [0.811816  , 0.51970834, 0.28677595, ..., 0.86063725,
          0.7930168 , 0.22888184]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.15799665, 0.41153854, 0.7559096 , ..., 0.6417985...
         [0.811816  , 0.51970834, 0.28677595, ..., 0.86063725,
          0.7930168 , 0.22888184]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2):...    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
),)
kwargs = {'input': <tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.15799665, 0.41153854, 0.7559096 , ..., ...,
         [0.811816  , 0.51970834, 0.28677595, ..., 0.86063725,
          0.7930168 , 0.22888184]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
input = <tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.15799665, 0.41153854, 0.7559096 , ..., 0.64179856...],
         [0.811816  , 0.51970834, 0.28677595, ..., 0.86063725,
          0.7930168 , 0.22888184]]]], dtype=float32)>

    def call(self, input):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.nn.functional.non_linear_activation_functions import (
            tensorflow_normalize_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_mm_frnt,
        )
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "1", "32", "32"])
        x_norm: typing.Any = self._normalize_input(input)
        x_features: typing.Any = self.features(x_norm)
        mean: typing.Any = []
        components: typing.Any = []
        x_prePCA = tensorflow_normalize_frnt(
            tensorflow_view_frnt_(x_features, tensorflow_size_frnt_(x_features, 0), -1)
        )
>       pca = tensorflow_mm_frnt(x_prePCA - mean, components)

Translated_Outputs/tensorflow_outputs/kornia/feature/hardnet.py:868: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(16, 512), dtype=float32, numpy=
array([[-0.01734989, -0.04370397,  0.01622698, ...,  0.01178343,
 ...     [ 0.02009738, -0.01328335,  0.0300489 , ..., -0.01122921,
        -0.01263684,  0.03132686]], dtype=float32)>, [])
kwargs = {}, arg = []

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_HardNet8.call().
E       
E       [1m{{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [16,512] vs. [0] [Op:Sub] name: [0m
E       
E       Arguments received by tensorflow_HardNet8.call():
E         â€¢ input=tf.Tensor(shape=(16, 1, 32, 32), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.HardNet8
___________________________________________________________________________________ test_HyNet[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HyNet(target_framework, mode, backend_compile):
        print("kornia.feature.HyNet")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledHyNet = ivy.transpile(kornia.feature.HyNet, source="torch", target=target_framework)

kornia/test_feature2.py:364: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.feature.hynet.HyNet'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_mixed_handler = local_response_norm.partial_mixed_handler = lambda x, size, **kwargs: size % 2 != 0

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.HyNet
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_feature2.py::test_laf_is_inside_image[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of bool is not in the list of allow...
FAILED kornia/test_feature2.py::test_DenseSIFTDescriptor[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_DenseSIFTDescriptor.call().
FAILED kornia/test_feature2.py::test_SIFTDescriptor[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_SIFTDescriptor.call().
FAILED kornia/test_feature2.py::test_MKDDescriptor[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_VonMisesKernel.call().
FAILED kornia/test_feature2.py::test_HardNet8[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_HardNet8.call().
FAILED kornia/test_feature2.py::test_HyNet[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_mi...
=============================================================================== 6 failed, 11 passed in 538.83s (0:08:58) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 68 items

kornia/augmentation/test_augmentation.py .......F..........F..F.....F.F.FFF.........FF...FFFFF.....F...F..FFF                                                                                    [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________________ test_RandomClahe[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomClahe(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomClahe")
    
        init_args = ()
        init_kwargs = {}
        call_args = (torch.rand(2, 3, 10, 20),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomClahe,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:215: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.clahe.RandomClahe'>, target = 'tensorflow', init_args = (), init_kwargs = {}
call_args = (tensor([[[[0.2776, 0.0710, 0.8514,  ..., 0.0687, 0.8680, 0.0291],
          [0.5277, 0.3040, 0.7391,  ..., 0.6062, 0...., 0.1772, 0.1774,  ..., 0.5140, 0.3960, 0.3782],
          [0.4310, 0.9537, 0.9550,  ..., 0.1602, 0.9312, 0.7623]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
args = (<tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.27759898, 0.0710305 , 0.8513607 , ..., 0.06874269...
         [0.43102598, 0.95366466, 0.9550184 , ..., 0.160236  ,
          0.9311789 , 0.762268  ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f18fc4c40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slo...,
         [0.43102598, 0.95366466, 0.9550184 , ..., 0.160236  ,
          0.9311789 , 0.762268  ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.27759898, 0.0710305 , 0.8513607 , ..., 0.06874269...
         [0.43102598, 0.95366466, 0.9550184 , ..., 0.160236  ,
          0.9311789 , 0.762268  ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slo...,
         [0.43102598, 0.95366466, 0.9550184 , ..., 0.160236  ,
          0.9311789 , 0.762268  ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.27759898, 0.0710305 , 0.8513607 , ..., 0.06874269...
         [0.43102598, 0.95366466, 0.9550184 , ..., 0.160236  ,
          0.9311789 , 0.762268  ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.27759898, 0.0710305 , 0.8513607 , ..., 0.06874269,...],
         [0.43102598, 0.95366466, 0.9550184 , ..., 0.160236  ,
          0.9311789 , 0.762268  ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.27759898, 0.0710305 , 0.8513607 , ..., 0...,
         [0.43102598, 0.95366466, 0.9550184 , ..., 0.160236  ,
          0.9311789 , 0.762268  ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
input = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.27759898, 0.0710305 , 0.8513607 , ..., 0.06874269,...],
         [0.43102598, 0.95366466, 0.9550184 , ..., 0.160236  ,
          0.9311789 , 0.762268  ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f18b4a5f0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f134a4c10>
tensor = <function tensorflow_tensor_frnt at 0x7f3f18c81bd0>
in_tensor = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.27759898, 0.0710305 , 0.8513607 , ..., 0.06874269,...],
         [0.43102598, 0.95366466, 0.9550184 , ..., 0.160236  ,
          0.9311789 , 0.762268  ]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([2, 3, 10, 20]), batch_shape = ivy.frontends.torch.Size([2, 3, 10, 20]), flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
in_tensor = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.27759898, 0.0710305 , 0.8513607 , ..., 0.06874269,...],
         [0.43102598, 0.95366466, 0.9550184 , ..., 0.160236  ,
          0.9311789 , 0.762268  ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
input = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.27759898, 0.0710305 , 0.8513607 , ..., 0.06874269,...],
         [0.43102598, 0.95366466, 0.9550184 , ..., 0.160236  ,
          0.9311789 , 0.762268  ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}
transform = <tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],

       [[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f18b4a5f0>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f3f18b4b1c0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f3f18d9cee0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f3f18800310>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f3f1875a5f0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f3f18d9cd30>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f3f18d9cb80>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
            output = self.apply_transform(in_tensor, params, flags, transform=transform)
        elif not tensorflow_any_frnt_(to_apply):
            output = self.apply_non_transform(
                in_tensor, params, flags, transform=transform
            )
        else:
            output = self.apply_non_transform(
                in_tensor, params, flags, transform=transform
            )
>           applied = self.apply_transform(
                tensorflow_get_item(in_tensor, to_apply),
                params,
                flags,
                transform=transform
                if transform is None
                else tensorflow_get_item(transform, to_apply),
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:616: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
input = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[7.04259276e-02, 7.95610428e-01, 5.39269865e-01,
    ...          3.61104965e-01, 8.98061395e-02, 1.60236001e-01,
          9.31178927e-01, 7.62268007e-01]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....enhance.equalization import tensorflow_equalize_clahe
    
        clip_limit = float(params["clip_limit_factor"][0])
>       return tensorflow_equalize_clahe(
            input, clip_limit, flags["grid_size"], flags["slow_and_differentiable"]
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/clahe.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[7.04259276e-02, 7.95610428e-01, 5.39269865e-01,
    ...          3.61104965e-01, 8.98061395e-02, 1.60236001e-01,
          9.31178927e-01, 7.62268007e-01]]]], dtype=float32)>
args = (40.0, (8, 8), False), kwargs = {}, tensorflow_numel_frnt_ = <function tensorflow_numel_frnt_ at 0x7f3f18b49bd0>, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f18b4a5f0>
tensorflow_view_frnt_ = <function tensorflow_view_frnt_ at 0x7f3f195c7490>, input_shape = ivy.frontends.torch.Size([1, 3, 10, 20])

    @wraps(f)
    def _wrapper(input, *args, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_numel_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
        if tensorflow_numel_frnt_(input) == 0:
            raise ValueError("Invalid input tensor, it is empty.")
        input_shape = tensorflow_shape_frnt_(input)
        input = tensorflow__to_bchw(input)
>       output = f(input, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/utils/image.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[7.04259276e-02, 7.95610428e-01, 5.39269865e-01,
    ...          3.61104965e-01, 8.98061395e-02, 1.60236001e-01,
          9.31178927e-01, 7.62268007e-01]]]], dtype=float32)>
clip_limit = 40.0, grid_size = (8, 8), slow_and_differentiable = False

    @tensorflow_perform_keep_shape_image
    def tensorflow_equalize_clahe(
        input, clip_limit=40.0, grid_size=(8, 8), slow_and_differentiable=False
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_as_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
    
        if not isinstance(clip_limit, (float,)):
            raise TypeError(f"Input clip_limit type is not float. Got {type(clip_limit)}")
        if not isinstance(grid_size, (tuple,)):
            raise TypeError(f"Input grid_size type is not Tuple. Got {type(grid_size)}")
        if len(grid_size) != 2:
            raise TypeError(
                f"Input grid_size is not a Tuple with 2 elements. Got {len(grid_size)}"
            )
        if isinstance(grid_size[0], (float,)) or isinstance(grid_size[1], (float,)):
            raise TypeError("Input grid_size type is not valid, must be a Tuple[int, int].")
        if grid_size[0] <= 0 or grid_size[1] <= 0:
            raise ValueError(f"Input grid_size elements must be positive. Got {grid_size}")
        imgs: typing.Any = input
>       hist_tiles, img_padded = tensorflow__compute_tiles(imgs, grid_size, True)

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:513: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

imgs = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[7.04259276e-02, 7.95610428e-01, 5.39269865e-01,
    ...          3.61104965e-01, 8.98061395e-02, 1.60236001e-01,
          9.31178927e-01, 7.62268007e-01]]]], dtype=float32)>
grid_size = (8, 8), even_tile_size = True

    def tensorflow__compute_tiles(imgs, grid_size, even_tile_size=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_pad_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unfold_frnt_
    
        batch: typing.Any = imgs
        h, w = tensorflow_shape_frnt_(batch)[-2:][0], tensorflow_shape_frnt_(batch)[-2:][1]
        kernel_vert: typing.Any = math.ceil(h / grid_size[0])
        kernel_horz: typing.Any = math.ceil(w / grid_size[1])
        if even_tile_size:
            kernel_vert = kernel_vert + (1 if kernel_vert % 2 else 0)
            kernel_horz = kernel_horz + (1 if kernel_horz % 2 else 0)
        pad_vert = kernel_vert * grid_size[0] - h
        pad_horz = kernel_horz * grid_size[1] - w
        if (
            pad_vert > tensorflow_shape_frnt_(batch)[-2]
            or pad_horz > tensorflow_shape_frnt_(batch)[-1]
        ):
            raise ValueError(
                "Cannot compute tiles on the image according to the given grid size"
            )
        if pad_vert > 0 or pad_horz > 0:
            batch = tensorflow_pad_frnt(batch, [0, pad_horz, 0, pad_vert], mode="reflect")
        c: typing.Any = tensorflow_shape_frnt_(batch)[-3]
        tiles: typing.Any = tensorflow_contiguous_frnt_(
            tensorflow_squeeze_frnt_(
>               tensorflow_unfold_frnt_(
                    tensorflow_unfold_frnt_(
                        tensorflow_unfold_frnt_(batch, 1, c, c), 2, kernel_vert, kernel_vert
                    ),
                    3,
                    kernel_horz,
                    kernel_horz,
                ),
                1,
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 1, 2, 16, 32), dtype=float32, numpy=
array([[[[[[0.07042593, 0.7956104 , 0.53926986, ..., 0.....38193238, 0.9770714 , 0.6079104 , ..., 0.38433886,
            0.12939256, 0.45033675]]]]]], dtype=float32)>, 3, 4, 4)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f3f134a4790>
array_like = <tf.Tensor: shape=(1, 1, 1, 2, 16, 32), dtype=float32, numpy=
array([[[[[[0.07042593, 0.7956104 , 0.53926986, ..., 0.7...        [0.38193238, 0.9770714 , 0.6079104 , ..., 0.38433886,
            0.12939256, 0.45033675]]]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 1, 1, 2, 16, 32), dtype=float32, numpy=
array([[[[[[0.07042593, 0.7956104 , 0.53926986, ..., 0.7...        [0.38193238, 0.9770714 , 0.6079104 , ..., 0.38433886,
            0.12939256, 0.45033675]]]]]], dtype=float32)>
dimension = 3, size = 4, step = 4

    @tensorflow_handle_methods
    def tensorflow_unfold_frnt_(tensor, dimension, size, step):
        from ...backends.tensorflow.general import tensorflow_get_item
        from ...ivy.general import tensorflow_set_item_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_stack_frnt
    
        slices = []
        self_shape = tuple(tensorflow_shape_frnt_(tensor))
        for i in range(0, tensorflow_get_item(self_shape, dimension) - size + 1, step):
            slicing = [slice(None)] * len(tensorflow_shape_frnt_(tensor))
            slicing = tensorflow_set_item_bknd(slicing, dimension, slice(i, i + size))
            slices.append(tensorflow_get_item(tensor, tuple(slicing)))
>       stacked = tensorflow_stack_frnt(slices, dim=dimension)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:679: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = [], dim = 3

    def tensorflow_stack_frnt(tensors, dim=0, *, out=None):
        from ...backends.tensorflow.manipulation import tensorflow_stack
    
>       return tensorflow_stack(tensors, axis=dim, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ([],), kwargs = {'axis': 3, 'out': None}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f3f134a4790>, array_like = []

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
>           array_like = array_like[0]
E           IndexError: Exception encountered when calling tensorflow_RandomClahe.call().
E           
E           [1mlist index out of range[0m
E           
E           Arguments received by tensorflow_RandomClahe.call():
E             â€¢ input=tf.Tensor(shape=(2, 3, 10, 20), dtype=float32)
E             â€¢ params=None
E             â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:183: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomClahe
______________________________________________________________________ test_RandomLinearCornerIllumination[tensorflow-s2s-False] _______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomLinearCornerIllumination(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomLinearCornerIllumination")
    
        init_args = ()
        init_kwargs = {"gain": 0.25, "p": 1.}
        call_args = (torch.ones(1, 3, 3, 3) * 0.5,)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomLinearCornerIllumination,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:435: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.linear_illumination.RandomLinearCornerIllumination'>, target = 'tensorflow', init_args = (), init_kwargs = {'gain': 0.25, 'p': 1.0}
call_args = (tensor([[[[0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000]],

       ...00]],

         [[0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0...  [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x562f354f9d10, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=Fals...   [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0...  [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=Fals...   [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0...  [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0....    [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
  ...   [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0....    [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>
params = None, kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f192105e0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f13bfa320>
tensor = <function tensorflow_tensor_frnt at 0x7f3f13e99e10>
in_tensor = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0....    [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 3, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 3, 3, 3])

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
>           params = self.forward_parameters(batch_shape)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), batch_shape = ivy.frontends.torch.Size([1, 3, 3, 3])

    def forward_parameters(self, batch_shape):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        batch_prob = self.__batch_prob_generator__(
            batch_shape, self.p, self.p_batch, self.same_on_batch
        )
        to_apply = batch_prob > 0.5
>       _params = self.generate_parameters(
            tuple(
                (
                    int(tensorflow_item_frnt_(tensorflow_sum_frnt_(to_apply))),
                    *batch_shape[1:],
                )
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:199: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), batch_shape = (1, 3, 3, 3)

    def generate_parameters(self, batch_shape):
        if self._param_generator is not None:
>           return self._param_generator(batch_shape, self.same_on_batch)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), args = ((1, 3, 3, 3), False), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x562f35e7b2c0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ion.py', lineno=46, function='__call__', code_context=['            return call_fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), v = None, buffers = None, args = ((1, 3, 3, 3), False), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), v = None, buffers = None, args = ((1, 3, 3, 3), False), kwargs = {}, replace_v = False, replace_buffers = False
call_signature = <Signature (batch_shape, same_on_batch=False)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), batch_shape = (1, 3, 3, 3), same_on_batch = False

    def call(self, batch_shape, same_on_batch=False):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...utils.param_validation import tensorflow__common_param_check
        from ....utils.helpers import tensorflow__extract_device_dtype
        from ...utils.helpers import tensorflow__adapted_rsampling
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_where_frnt,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from .....ivy.functional.frontends.torch.tensor import (
            tensorflow_unsqueeze_frnt_,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_linspace_frnt,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_zeros_frnt,
        )
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....enhance.normalize import tensorflow_normalize
    
        batch_size, channels, height, width = (
            batch_shape[0],
            batch_shape[1],
            batch_shape[2],
            batch_shape[3],
        )
        tensorflow__common_param_check(batch_size, same_on_batch)
        _device, _dtype = tensorflow__extract_device_dtype([self.gain, self.sign])
        gain_factor = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 1, 1, 1), self.gain_sampler, same_on_batch
            ),
            device=_device,
            dtype=_dtype,
        )
        sign = tensorflow_to_frnt_(
            tensorflow_where_frnt(
                tensorflow__adapted_rsampling(
                    (batch_size, 1, 1, 1), self.sign_sampler, same_on_batch
                )
                >= 0.0,
                tensorflow_tensor_frnt(1),
                tensorflow_tensor_frnt(-1),
            ),
            device=_device,
            dtype=_dtype,
        )
        directions = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 1, 1, 1), self.directions_sampler, same_on_batch
            ),
            device=_device,
            dtype=tf.int8,
        )
        y_grad = tensorflow_expand_frnt_(
            tensorflow_unsqueeze_frnt_(tensorflow_linspace_frnt(0, 1, height), 1),
            channels,
            height,
            width,
        )
        x_grad = tensorflow_expand_frnt_(
            tensorflow_unsqueeze_frnt_(tensorflow_linspace_frnt(0, 1, width), 0),
            channels,
            height,
            width,
        )
        gradient = tensorflow_zeros_frnt(batch_shape)
        for _b in range(batch_size):
            if tensorflow_get_item(directions, _b) == 0:
                gradient = tensorflow_set_item_bknd(gradient, _b, x_grad + y_grad)
            elif tensorflow_get_item(directions, _b) == 1:
                gradient = tensorflow_set_item_bknd(gradient, _b, -x_grad + y_grad)
            elif tensorflow_get_item(directions, _b) == 2:
                gradient = tensorflow_set_item_bknd(gradient, _b, x_grad - y_grad)
            elif tensorflow_get_item(directions, _b) == 3:
                gradient = tensorflow_set_item_bknd(gradient, _b, 1 - (x_grad + y_grad))
>       gradient = sign * gain_factor * tensorflow_normalize.normalize_min_max(gradient)
E       AttributeError: Exception encountered when calling tensorflow_LinearCornerIlluminationGenerator.call().
E       
E       [1m'function' object has no attribute 'normalize_min_max'[0m
E       
E       Arguments received by tensorflow_LinearCornerIlluminationGenerator.call():
E         â€¢ batch_shape=('1', '3', '3', '3')
E         â€¢ same_on_batch=False

Translated_Outputs/tensorflow_outputs/kornia/augmentation/random_generator/_2d/linear_illumination.py:147: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomLinearCornerIllumination
_____________________________________________________________________________ test_RandomMotionBlur[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomMotionBlur(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomMotionBlur")
    
        init_args = (3, 35., 0.5)
        init_kwargs = {"p": 1.}
        call_args = (torch.ones(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomMotionBlur,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:495: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.motion_blur.RandomMotionBlur'>, target = 'tensorflow', init_args = (3, 35.0, 0.5), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]]]]),), call_kwargs = {}
deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f188b6640, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border..., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border..., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>
params = None, kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f1930bd00>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f13b50b80>
tensor = <function tensorflow_tensor_frnt at 0x7f3f192109d0>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5])

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
>           params = self.forward_parameters(batch_shape)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5])

    def forward_parameters(self, batch_shape):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        batch_prob = self.__batch_prob_generator__(
            batch_shape, self.p, self.p_batch, self.same_on_batch
        )
        to_apply = batch_prob > 0.5
>       _params = self.generate_parameters(
            tuple(
                (
                    int(tensorflow_item_frnt_(tensorflow_sum_frnt_(to_apply))),
                    *batch_shape[1:],
                )
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:199: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest), batch_shape = (1, 1, 5, 5)

    def generate_parameters(self, batch_shape):
        from ....core._backend import tensor
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .....ivy.functional.frontends.torch.random_sampling import (
            tensorflow_randint_frnt,
        )
    
        params = super().generate_parameters(batch_shape)
        params = tensorflow_set_item_bknd(
            params,
            "idx",
            tensor([0])
            if batch_shape[0] == 0
>           else tensorflow_randint_frnt(batch_shape[0], (1,)),
        )
E       TypeError: Exception encountered when calling tensorflow_RandomMotionBlur.call().
E       
E       [1mtensorflow_randint_frnt() missing 1 required positional argument: 'size'[0m
E       
E       Arguments received by tensorflow_RandomMotionBlur.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/motion_blur.py:70: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomMotionBlur
________________________________________________________________________________ test_RandomRain[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRain(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomRain")
    
        init_args = ()
        init_kwargs = {"p": 1, "drop_height": (1,2), "drop_width": (1,2), "number_of_drops": (1,1)}
        call_args = (torch.rand(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomRain,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:615: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.random_rain.RandomRain'>, target = 'tensorflow', init_args = ()
init_kwargs = {'drop_height': (1, 2), 'drop_width': (1, 2), 'number_of_drops': (1, 1), 'p': 1}
call_args = (tensor([[[[0.4874, 0.2311, 0.5399, 0.0532, 0.3107],
          [0.3921, 0.8872, 0.2439, 0.5875, 0.6656],
          [0...., 0.3993],
          [0.8028, 0.9233, 0.0271, 0.0402, 0.0908],
          [0.1593, 0.0736, 0.3790, 0.7871, 0.8629]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.310...4020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x562f3531adc0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=...04020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.310...4020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=...04020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.310...4020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.3107....04020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.053234...04020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.3107....04020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f12ce4160>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f12f18550>
tensor = <function tensorflow_tensor_frnt at 0x7f3f13b4be20>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.3107....04020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.3107....04020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.3107....04020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f12ce4160>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f3f185aadd0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f3f184935b0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f3f12f088b0>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f3f12f3f9a0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f3f18490310>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f3f18492440>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
image = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.3107....04020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, image, params, flags, transform=None):
        from ....core.check import tensorflow_KORNIA_CHECK
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.reduction_ops import (
            tensorflow_all_frnt,
        )
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_abs_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_linspace_frnt,
        )
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        tensorflow_KORNIA_CHECK(
            tensorflow_shape_frnt_(image)[1] in {3, 1},
            "Number of color channels should be 1 or 3.",
        )
        tensorflow_KORNIA_CHECK(
            bool(
                tensorflow_all_frnt(
                    params["drop_height_factor"] <= tensorflow_shape_frnt_(image)[2]
                )
                and tensorflow_all_frnt(params["drop_height_factor"] > 0)
            ),
            "Height of drop should be greater than zero and less than image height.",
        )
        tensorflow_KORNIA_CHECK(
            bool(
                tensorflow_all_frnt(
                    tensorflow_abs_frnt(params["drop_width_factor"])
                    <= tensorflow_shape_frnt_(image)[3]
                )
            ),
            "Width of drop should be less than image width.",
        )
        modeified_img = tensorflow_clone_frnt_(image)
        for i in range(tensorflow_shape_frnt_(image)[0]):
            number_of_drops: typing.Any = int(
                tensorflow_get_item(params["number_of_drops_factor"], i)
            )
            coordinates_of_drops: typing.Any = tensorflow_get_item(
                tensorflow_get_item(params["coordinates_factor"], i),
                slice(None, number_of_drops, None),
            )
            height_of_drop: typing.Any = int(
                tensorflow_get_item(params["drop_height_factor"], i)
            )
            width_of_drop: typing.Any = int(
                tensorflow_get_item(params["drop_width_factor"], i)
            )
            random_y_coords = coordinates_of_drops[:, 0] * (
                tensorflow_shape_frnt_(image)[2] - height_of_drop - 1
            )
            if width_of_drop > 0:
                random_x_coords = coordinates_of_drops[:, 1] * (
                    tensorflow_shape_frnt_(image)[3] - width_of_drop - 1
                )
            else:
                random_x_coords = (
                    coordinates_of_drops[:, 1]
                    * (tensorflow_shape_frnt_(image)[3] + width_of_drop - 1)
                    - width_of_drop
                )
            coords = tensorflow_to_frnt_(
                tensorflow_cat_frnt(
                    [random_y_coords[None], random_x_coords[None]], dim=0
                ),
                image.device,
                dtype=tf.int64,
            )
            size_of_line: typing.Any = max(height_of_drop, abs(width_of_drop))
            x = tensorflow_to_frnt_(
                tensorflow_linspace_frnt(
                    start=0, end=height_of_drop, steps=size_of_line, dtype=tf.int64
                ),
                image.device,
            )
            y = tensorflow_to_frnt_(
                tensorflow_linspace_frnt(
                    start=0, end=width_of_drop, steps=size_of_line, dtype=tf.int64
                ),
                image.device,
            )
            for k in range(tensorflow_shape_frnt_(x)[0]):
>               modeified_img = tensorflow_set_item_bknd(
                    modeified_img,
                    (i, slice(None, None, None), coords[0] + x[k], coords[1] + y[k]),
                    200 / 255,
                )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/random_rain.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inp = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.3107....04020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>
query = (0, slice(None, None, None), <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>), val = 0.7843137254901961, kwargs = {}

    @functools.wraps(fn)
    def wrapper(inp, query, val, **kwargs):
        try:
            inp.__setitem__(query, val)
            res = inp
        except IndexError:
            raise
        except Exception:
>           res = fn(inp, query, val, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.310...hape=(1,), dtype=int64, numpy=array([1])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>), 0.7843137254901961)
kwargs = {}, handle_mixed_in_backend = True

    @functools.wraps(fn)
    def _handle_partial_mixed_function(*args, **kwargs):
        handle_mixed_in_backend = False
        if not hasattr(fn, "partial_mixed_handler"):
            handle_mixed_in_backend = True
        else:
            compos = getattr(fn, "compos")
            condition = getattr(fn, "partial_mixed_handler")
        if handle_mixed_in_backend or condition(*args, **kwargs):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.48741502, 0.23108238, 0.53991014, 0.05323464, 0.3107....04020631, 0.09076649],
         [0.15929377, 0.0735842 , 0.37897706, 0.7870985 , 0.86291754]]]],
      dtype=float32)>
query = (0, slice(None, None, None), <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>)
val = <tf.Tensor: shape=(), dtype=float32, numpy=0.78431374>

    @tensorflow_handle_set_item
    @tensorflow_handle_partial_mixed_function
    def tensorflow_set_item_bknd(
        x: Union[tensorflow.Tensor, tf.Tensor],
        query: Union[tensorflow.Tensor, tf.Tensor, Tuple],
        val: Union[tensorflow.Tensor, tf.Tensor],
        /,
        *,
        copy: Optional[bool] = False,
    ):
        from ..backends.tensorflow.creation import tensorflow_asarray
        from ..backends.tensorflow.creation import tensorflow_copy_array
        from .data_type import tensorflow_is_bool_dtype_bknd
        from ..backends.tensorflow.manipulation import tensorflow_tile
        from ..backends.tensorflow.searching import tensorflow_nonzero
        from ..backends.tensorflow.general import tensorflow_shape
        from ...data_classes.array.data_type import tensorflow_astype_bknd_
        from ..backends.tensorflow.general import tensorflow_scatter_nd
    
        if copy:
            x = tensorflow_copy_array(x)
        if not tensorflow_is_array_bknd(val):
            val = tensorflow_asarray(val)
        if 0 in x.shape or 0 in val.shape:
            return x
        if tensorflow_is_array_bknd(query) and tensorflow_is_bool_dtype_bknd(query):
            if not len(query.shape):
                query = tensorflow_tile(query, (x.shape[0],))
            indices = tensorflow_nonzero(query, as_tuple=False)
        else:
            indices, target_shape, _ = tensorflow__parse_query_bknd(
                query, tensorflow_shape(x, as_array=True), scatter=True
            )
            if indices is None:
                return x
        val = tensorflow_astype_bknd_(val, x.dtype)
>       ret = tensorflow_scatter_nd(indices, val, reduction="replace", out=x)

Translated_Outputs/tensorflow_outputs/ivy/functional/ivy/general.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

indices = <tf.Tensor: shape=(1, 1, 4), dtype=int64, numpy=array([[[0, 1, 2, 0]]])>, updates = <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.78431374]], dtype=float32)>, shape = None

    def tensorflow_scatter_nd(
        indices: Union[tensorflow.Tensor, tensorflow.Variable],
        updates: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Optional[Union[tf.TensorShape, Sequence[int]]] = None,
        *,
        reduction: str = "sum",
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.general import tensorflow_exists_bknd
        from ...ivy.data_type import tensorflow_promote_types_bknd
        from .data_type import tensorflow_as_native_dtype
        from ...ivy.general import tensorflow__broadcast_to_bknd
        from ....utils.assertions import tensorflow_check_equal
        from .elementwise import tensorflow_multiply
    
        updates_dtype = updates.dtype
        if tensorflow_exists_bknd(out):
            dtype = tensorflow_promote_types_bknd(out.dtype, updates_dtype)
        updates = tensorflow.cast(
            updates,
            tensorflow_as_native_dtype(dtype)
            if tensorflow_exists_bknd(out)
            else updates_dtype,
        )
        expected_shape = (
            list(tensorflow.shape(indices)[:-1])
            + list(out.shape[tensorflow.shape(indices)[-1] :])
            if tensorflow_exists_bknd(out)
            else list(tensorflow.shape(indices)[:-1])
            + list(shape[tensorflow.shape(indices)[-1] :])
        )
        updates = tensorflow__broadcast_to_bknd(updates, expected_shape)
        if len(updates.shape) == 0:
            indices = tensorflow.expand_dims(indices, 0)
            updates = tensorflow.expand_dims(updates, 0)
        target = out
        target_given = tensorflow_exists_bknd(target)
        if tensorflow_exists_bknd(shape) and target_given:
            tensorflow_check_equal(tuple(target.shape), tuple(shape), as_array=False)
        if not target_given:
            shape = list(shape) if tensorflow_exists_bknd(shape) else list(out.shape)
            target = tensorflow.zeros(shape, dtype=updates.dtype)
        if reduction == "sum":
            res = tensorflow.tensor_scatter_nd_add(target, indices, updates)
        elif reduction == "min":
            res = tensorflow.tensor_scatter_nd_min(target, indices, updates)
        elif reduction == "max":
            res = tensorflow.tensor_scatter_nd_max(target, indices, updates)
        elif reduction == "mul":
            updates = tensorflow_multiply(tensorflow_gather_nd(target, indices), updates)
            res = tensorflow.tensor_scatter_nd_update(target, indices, updates)
        elif reduction == "replace":
>           res = tensorflow.tensor_scatter_nd_update(target, indices, updates)
E           tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomRain.call().
E           
E           [1m{{function_node __wrapped__TensorScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,0] = [0, 1, 2, 0] does not index into shape [1,1,5,5] [Op:TensorScatterUpdate] name: [0m
E           
E           Arguments received by tensorflow_RandomRain.call():
E             â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E             â€¢ params=None
E             â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/general.py:328: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomRain
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-14 04:31:13.700679: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at scatter_nd_op.cc:218 : INVALID_ARGUMENT: indices[0,0] = [0, 1, 2, 0] does not index into shape [1,1,5,5]
_________________________________________________________________________ test_RandomSaltAndPepperNoise[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSaltAndPepperNoise(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSaltAndPepperNoise")
    
        init_args = ()
        init_kwargs = {"amount": 0.5, "salt_vs_pepper": 0.5, "p": 1.}
        call_args = (torch.rand(1, 3, 3, 3),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSaltAndPepperNoise,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:655: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.salt_pepper_noise.RandomSaltAndPepperNoise'>, target = 'tensorflow', init_args = ()
init_kwargs = {'amount': 0.5, 'p': 1.0, 'salt_vs_pepper': 0.5}
call_args = (tensor([[[[0.7917, 0.4124, 0.7088],
          [0.7115, 0.2447, 0.5950],
          [0.8683, 0.5628, 0.0408]],

       ...89]],

         [[0.6721, 0.8349, 0.9103],
          [0.0467, 0.0972, 0.1259],
          [0.4549, 0.7224, 0.7711]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.79173565, 0.41241735, 0.7088407 ],
         [0.7114...04],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f12ddc440, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=...404],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.79173565, 0.41241735, 0.7088407 ],
         [0.7114...04],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=...404],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.79173565, 0.41241735, 0.7088407 ],
         [0.7114...04],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.79173565, 0.41241735, 0.7088407 ],
         [0.71147...8404],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.79173565, 0.41241735, 0.7088407 ],
       ...404],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.79173565, 0.41241735, 0.7088407 ],
         [0.71147...8404],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...se,  True]],

        [[False,  True, False],
         [False,  True, False],
         [ True, False,  True]]]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f12f8bd00>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f12e430a0>
tensor = <function tensorflow_tensor_frnt at 0x7f3f12fba9e0>
in_tensor = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.79173565, 0.41241735, 0.7088407 ],
         [0.71147...8404],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 3, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 3, 3, 3]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.79173565, 0.41241735, 0.7088407 ],
         [0.71147...8404],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...se,  True]],

        [[False,  True, False],
         [False,  True, False],
         [ True, False,  True]]]])>, ...}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.79173565, 0.41241735, 0.7088407 ],
         [0.71147...8404],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...se,  True]],

        [[False,  True, False],
         [False,  True, False],
         [ True, False,  True]]]])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f12f8bd00>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f3f12f8bb50>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f3f18284a60>, tensorflow_get_item = <function tensorflow_get_item at 0x7f3f1834b5b0>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f3f10bc6710>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f3f18284310>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f3f18286f80>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.79173565, 0.41241735, 0.7088407 ],
         [0.71147...8404],
         [0.04668665, 0.09719312, 0.1259408 ],
         [0.45489419, 0.7223973 , 0.7710538 ]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...se,  True]],

        [[False,  True, False],
         [False,  True, False],
         [ True, False,  True]]]])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....core.check import tensorflow_KORNIA_CHECK
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        tensorflow_KORNIA_CHECK(
            len(tensorflow_shape_frnt_(input)) in (3, 4), "Wrong input dimension."
        )
        if len(tensorflow_shape_frnt_(input)) == 3:
            input = input[None, :, :, :]
        tensorflow_KORNIA_CHECK(
            tensorflow_shape_frnt_(input)[1] in {3, 1},
            "Number of color channels should be 1 or 3.",
        )
        noisy_image = tensorflow_clone_frnt_(input)
        noisy_image = tensorflow_set_item_bknd(
>           noisy_image, params["mask_salt"].to(input.device), 1.0
        )
E       AttributeError: Exception encountered when calling tensorflow_RandomSaltAndPepperNoise.call().
E       
E       [1m'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'to'[0m
E       
E       Arguments received by tensorflow_RandomSaltAndPepperNoise.call():
E         â€¢ input=tf.Tensor(shape=(1, 3, 3, 3), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/salt_pepper_noise.py:96: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSaltAndPepperNoise
______________________________________________________________________________ test_RandomSharpness[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSharpness(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSharpness")
    
        init_args = (1.,)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSharpness,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:695: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.sharpness.RandomSharpness'>, target = 'tensorflow', init_args = (1.0,), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[0.6928, 0.3168, 0.0075, 0.4280, 0.3749],
          [0.3954, 0.3381, 0.8988, 0.3179, 0.8384],
          [0...., 0.8672],
          [0.3464, 0.3330, 0.2560, 0.0916, 0.0807],
          [0.0497, 0.3837, 0.8746, 0.2819, 0.4221]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.6928233 , 0.31678677, 0.00754029, 0.4279713 , 0.374...9161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f184c8840, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(1, 1, 5, 5), d...09161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.6928233 , 0.31678677, 0.00754029, 0.4279713 , 0.374...9161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(1, 1, 5, 5), d...09161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.6928233 , 0.31678677, 0.00754029, 0.4279713 , 0.374...9161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.6928233 , 0.31678677, 0.00754029, 0.4279713 , 0.3749....09161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.6928233 , 0.31678677, 0.00754029, 0.427971...09161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.6928233 , 0.31678677, 0.00754029, 0.4279713 , 0.3749....09161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...mpy=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.8151076], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f1dbc88b0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f1db869e0>
tensor = <function tensorflow_tensor_frnt at 0x7f3f1db777f0>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.6928233 , 0.31678677, 0.00754029, 0.4279713 , 0.3749....09161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.6928233 , 0.31678677, 0.00754029, 0.4279713 , 0.3749....09161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...mpy=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.8151076], dtype=float32)>}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.6928233 , 0.31678677, 0.00754029, 0.4279713 , 0.3749....09161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...mpy=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.8151076], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f1dbc88b0>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f3f1dbc83a0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f3f1db6b250>, tensorflow_get_item = <function tensorflow_get_item at 0x7f3f1d42a9e0>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f3f1d4ff1c0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f3f1db6a200>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f3f1db68b80>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.6928233 , 0.31678677, 0.00754029, 0.4279713 , 0.3749....09161878, 0.08072346],
         [0.04970866, 0.38367158, 0.8746144 , 0.28186744, 0.42212415]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...mpy=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.8151076], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        factor = params["sharpness"]
>       return sharpness(input, factor)
E       NameError: Exception encountered when calling tensorflow_RandomSharpness.call().
E       
E       [1mname 'sharpness' is not defined[0m
E       
E       Arguments received by tensorflow_RandomSharpness.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/sharpness.py:43: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSharpness
________________________________________________________________________________ test_RandomSnow[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSnow(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSnow")
    
        init_args = ()
        init_kwargs = {"p": 1.0, "snow_coefficient": (0.1, 0.6), "brightness": (1.0, 5.0)}
        call_args = (torch.rand(2, 3, 4, 4),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSnow,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:715: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.random_snow.RandomSnow'>, target = 'tensorflow', init_args = ()
init_kwargs = {'brightness': (1.0, 5.0), 'p': 1.0, 'snow_coefficient': (0.1, 0.6)}
call_args = (tensor([[[[9.8606e-01, 2.6436e-01, 5.7956e-01, 5.9473e-01],
          [3.4421e-01, 7.2928e-01, 1.0200e-01, 5.0996e-01...     [9.5468e-01, 7.0280e-02, 3.8288e-01, 7.8919e-01],
          [8.5306e-01, 6.8011e-01, 3.1884e-01, 5.8058e-01]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[9.8605591e-01, 2.6436466e-01, 5.7955664e-01, 5.947318...e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f12ddfc40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), <...2e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[9.8605591e-01, 2.6436466e-01, 5.7955664e-01, 5.947318...e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), <...2e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[9.8605591e-01, 2.6436466e-01, 5.7955664e-01, 5.947318...e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[9.8605591e-01, 2.6436466e-01, 5.7955664e-01, 5.9473181...92e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[9.8605591e-01, 2.6436466e-01, 5.7955664e-01,...2e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[9.8605591e-01, 2.6436466e-01, 5.7955664e-01, 5.9473181...92e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:...4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.21055189, 0.56792384], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f18211480>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f12fa8040>
tensor = <function tensorflow_tensor_frnt at 0x7f3f13019750>
in_tensor = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[9.8605591e-01, 2.6436466e-01, 5.7955664e-01, 5.9473181...92e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([2, 3, 4, 4]), batch_shape = ivy.frontends.torch.Size([2, 3, 4, 4]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[9.8605591e-01, 2.6436466e-01, 5.7955664e-01, 5.9473181...92e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:...4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.21055189, 0.56792384], dtype=float32)>}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[9.8605591e-01, 2.6436466e-01, 5.7955664e-01, 5.9473181...92e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:...4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.21055189, 0.56792384], dtype=float32)>}
flags = {}
transform = <tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],

       [[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f18211480>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f3f18210820>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f3f1d4ae200>, tensorflow_get_item = <function tensorflow_get_item at 0x7f3f12c583a0>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f3f1305a0e0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f3f1d4ad360>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f3f1d4acb80>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[9.8605591e-01, 2.6436466e-01, 5.7955664e-01, 5.9473181...92e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:...4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.21055189, 0.56792384], dtype=float32)>}
flags = {}
transform = <tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],

       [[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....core.check import tensorflow_KORNIA_CHECK
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....color.hls import tensorflow_rgb_to_hls
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_zeros_like_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_where_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clamp_frnt_
        from ....color.hls import tensorflow_hls_to_rgb
    
        tensorflow_KORNIA_CHECK(
            tensorflow_shape_frnt_(input)[1] == 3,
            "Number of color channels should be 3.",
        )
        tensorflow_KORNIA_CHECK(
            len(tensorflow_shape_frnt_(input)) in (3, 4), "Wrong input dimension."
        )
        if len(tensorflow_shape_frnt_(input)) == 3:
            input = input[None, :, :, :]
>       input_HLS = tensorflow_rgb_to_hls(input)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/random_snow.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[9.8605591e-01, 2.6436466e-01, 5.7955664e-01, 5.9473181...92e-01, 7.8918558e-01],
         [8.5306442e-01, 6.8010855e-01, 3.1883687e-01, 5.8057815e-01]]]],
      dtype=float32)>
eps = 1e-08

    def tensorflow_rgb_to_hls(image, eps=1e-08):
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.pointwise_ops import sub
        from ..core._backend import where
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_requires_grad_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_empty_like_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_add_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_mul_frnt
    
        if not isinstance(image, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(image)}")
        if len(tensorflow_shape_frnt_(image)) < 3 or tensorflow_shape_frnt_(image)[-3] != 3:
            raise ValueError(
                f"Input size must have a shape of (*, 3, H, W). Got {tensorflow_shape_frnt_(image)}"
            )
        _RGB2HSL_IDX = tensor(
            [[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype
        )
        _img_max: typing.Any = tensorflow_max_frnt_(image, -3)
        maxc = _img_max[0]
        imax = _img_max[1]
        minc: typing.Any = tensorflow_min_frnt_(image, -3)[0]
        if tensorflow_requires_grad_frnt_(image):
            l_ = maxc + minc
            s = maxc - minc
            h = l_
            image_hls = l_
        else:
            image_hls = tensorflow_empty_like_frnt(image)
            h, l_, s = (
                image_hls[..., 0, :, :],
                image_hls[..., 1, :, :],
                image_hls[..., 2, :, :],
            )
            tensorflow_add_frnt(maxc, minc, out=l_)
            sub(maxc, minc, out=s)
        im = image / tensorflow_unsqueeze_frnt_(s + eps, -3)
        s = s / (where(l_ < 1.0, l_, 2.0 - l_) + eps)
        l_ = l_ / 2
        r, g, b = im[..., 0, :, :], im[..., 1, :, :], im[..., 2, :, :]
        cond = imax[..., None, :, :] == _RGB2HSL_IDX
        if tensorflow_requires_grad_frnt_(image):
            h = (g - b) % 6 * cond[..., 0, :, :]
        else:
>           tensorflow_mul_frnt((g - b) % 6, cond[..., 0, :, :], out=h)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[ 2.,  2.,  4.,  0.],
        [ 2.,  2.,  4.,  2.],
      ...,
        [False, False, False,  True],
        [False, False,  True, False],
        [False, False, False, False]]])>)
kwargs = {'out': <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[0., 0., 0., 0.],
        [0., 0., 0., 0.],
       ...    [[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]]], dtype=float32)>}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f3f12fabeb0>
array_like = <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[ 2.,  2.,  4.,  0.],
        [ 2.,  2.,  4.,  2.],
       ...  4.,  0.],
        [ 2.,  4., -0.,  2.],
        [-0.,  4.,  0.,  2.],
        [ 2.,  4.,  4.,  2.]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[ 2.,  2.,  4.,  0.],
        [ 2.,  2.,  4.,  2.],
       ...  4.,  0.],
        [ 2.,  4., -0.,  2.],
        [-0.,  4.,  0.,  2.],
        [ 2.,  4.,  4.,  2.]]], dtype=float32)>
other = <tf.Tensor: shape=(2, 4, 4), dtype=bool, numpy=
array([[[ True, False, False, False],
        [False, False, False, Fa...],
        [False, False, False,  True],
        [False, False,  True, False],
        [False, False, False, False]]])>

    @tensorflow_handle_methods
    def tensorflow_mul_frnt(input, other, *, out=None):
        from ...backends.tensorflow.elementwise import tensorflow_multiply
    
>       return tensorflow_multiply(input, other, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/pointwise_ops.py:142: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x1 = <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[ 2.,  2.,  4.,  0.],
        [ 2.,  2.,  4.,  2.],
       ...  4.,  0.],
        [ 2.,  4., -0.,  2.],
        [-0.,  4.,  0.,  2.],
        [ 2.,  4.,  4.,  2.]]], dtype=float32)>
x2 = <tf.Tensor: shape=(2, 4, 4), dtype=bool, numpy=
array([[[ True, False, False, False],
        [False, False, False, Fa...],
        [False, False, False,  True],
        [False, False,  True, False],
        [False, False, False, False]]])>

    def tensorflow_multiply(
        x1: Union[float, tensorflow.Tensor, tensorflow.Variable],
        x2: Union[float, tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .creation import tensorflow_asarray
        from ...ivy.data_type import tensorflow_default_dtype_bknd
        from ...ivy.general import tensorflow_is_array_bknd
    
        oirg_x1 = x1
        oirg_x2 = x2
        try:
            dtype = (
                x1.dtype
                if hasattr(x1, "dtype")
                else x2.dtype
                if hasattr(x2, "dtype")
                else tensorflow_default_dtype_bknd()
            )
            if not tensorflow_is_array_bknd(x1):
                x1 = tensorflow_asarray(x1, dtype=dtype)
            if not tensorflow_is_array_bknd(x2):
                x2 = tensorflow_asarray(x2, dtype=dtype)
        except:
            x1 = oirg_x1
            x2 = oirg_x2
>       return tensorflow.math.multiply(x1, x2)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomSnow.call().
E       
E       [1mcannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a bool tensor [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_RandomSnow.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 4, 4), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/elementwise.py:540: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSnow
______________________________________________________________________________ test_RandomSolarize[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSolarize(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSolarize")
    
        init_args = (0.1, 0.1)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSolarize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:735: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.solarize.RandomSolarize'>, target = 'tensorflow', init_args = (0.1, 0.1), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[0.2417, 0.6792, 0.6751, 0.5174, 0.5144],
          [0.8334, 0.9740, 0.3142, 0.5816, 0.8840],
          [0...., 0.3204],
          [0.8044, 0.7700, 0.1292, 0.5133, 0.6968],
          [0.0914, 0.4536, 0.7956, 0.6796, 0.5787]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.24171704, 0.67919743, 0.6751482 , 0.5174259 , 0.514...1330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f12ddd040, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=...51330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.24171704, 0.67919743, 0.6751482 , 0.5174259 , 0.514...1330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=...51330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.24171704, 0.67919743, 0.6751482 , 0.5174259 , 0.514...1330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.24171704, 0.67919743, 0.6751482 , 0.5174259 , 0.5144....51330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.24171704, 0.67919743, 0.6751482 , 0.517425...51330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.24171704, 0.67919743, 0.6751482 , 0.5174259 , 0.5144....51330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02579293], dtype=float32)>, 'batch_prob': <tf.Tens...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.4582803], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f131192d0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f1d494c10>
tensor = <function tensorflow_tensor_frnt at 0x7f3f12f97520>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.24171704, 0.67919743, 0.6751482 , 0.5174259 , 0.5144....51330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.24171704, 0.67919743, 0.6751482 , 0.5174259 , 0.5144....51330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02579293], dtype=float32)>, 'batch_prob': <tf.Tens...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.4582803], dtype=float32)>}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.24171704, 0.67919743, 0.6751482 , 0.5174259 , 0.5144....51330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02579293], dtype=float32)>, 'batch_prob': <tf.Tens...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.4582803], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f131192d0>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f3f13b308b0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f3f12ec2c20>, tensorflow_get_item = <function tensorflow_get_item at 0x7f3f1d392a70>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f3f1d3c9090>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f3f12ec0040>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f3f12ec0820>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.24171704, 0.67919743, 0.6751482 , 0.5174259 , 0.5144....51330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02579293], dtype=float32)>, 'batch_prob': <tf.Tens...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.4582803], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....enhance.adjust import tensorflow_solarize
    
        thresholds = params["thresholds"]
        additions: typing.Any
        if "additions" in params:
            additions = params["additions"]
        else:
            additions = None
>       return tensorflow_solarize(input, thresholds, additions)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/solarize.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.24171704, 0.67919743, 0.6751482 , 0.5174259 , 0.5144....51330394, 0.6968272 ],
         [0.09138232, 0.45359576, 0.7956077 , 0.6796186 , 0.5787073 ]]]],
      dtype=float32)>
thresholds = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.4582803], dtype=float32)>, additions = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02579293], dtype=float32)>

    def tensorflow_solarize(input, thresholds=0.5, additions=None):
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_as_tensor_frnt
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_all_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clamp_frnt_
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
        if not isinstance(thresholds, (float, tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(
                f"The factor should be either a float or Tensor. Got {type(thresholds)}"
            )
        if isinstance(thresholds, (float,)):
            thresholds = tensorflow_as_tensor_frnt(thresholds)
        if additions is not None:
            if not isinstance(additions, (float, tensorflow.Tensor, tensorflow.Variable)):
                raise TypeError(
                    f"The factor should be either a float or Tensor. Got {type(additions)}"
                )
            if isinstance(additions, (float,)):
                additions = tensorflow_as_tensor_frnt(additions)
>           if not tensorflow_all_frnt((additions < 0.5) * (additions > -0.5)):

Translated_Outputs/tensorflow_outputs/kornia/enhance/adjust.py:379: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>, <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>), kwargs = {}
arg = <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomSolarize.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_RandomSolarize.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSolarize
_____________________________________________________________________________ test_RandomResizedCrop[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomResizedCrop(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomResizedCrop")
    
        init_args = ()
        init_kwargs = {"size": (3, 3), "scale": (3., 3.), "ratio": (2., 2.), "p": 1., "cropping_mode": "resample"}
        call_args = (torch.tensor([[[0., 1., 2.],
                                    [3., 4., 5.],
                                    [6., 7., 8.]]]),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomResizedCrop,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:951: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resized_crop.RandomResizedCrop'>, target = 'tensorflow', init_args = ()
init_kwargs = {'cropping_mode': 'resample', 'p': 1.0, 'ratio': (2.0, 2.0), 'scale': (3.0, 3.0), ...}, call_args = (tensor([[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
args = (<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>,), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f10236640, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_...e=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
v = None, buffers = None, args = (<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>,), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_...e=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
v = None, buffers = None, args = (<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>,), kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>, replace_v = False, replace_buffers = False
call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros),)
kwargs = {'input': <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
input = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>, params = None, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f102d8ca0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f103d5fc0>
tensor = <function tensorflow_tensor_frnt at 0x7f3f13d56320>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3])

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
>           params = self.forward_parameters(batch_shape)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3])

    def forward_parameters(self, batch_shape):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        batch_prob = self.__batch_prob_generator__(
            batch_shape, self.p, self.p_batch, self.same_on_batch
        )
        to_apply = batch_prob > 0.5
>       _params = self.generate_parameters(
            tuple(
                (
                    int(tensorflow_item_frnt_(tensorflow_sum_frnt_(to_apply))),
                    *batch_shape[1:],
                )
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:199: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
batch_shape = (1, 1, 3, 3)

    def generate_parameters(self, batch_shape):
        if self._param_generator is not None:
>           return self._param_generator(batch_shape, self.same_on_batch)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), args = ((1, 1, 3, 3), False), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f10236840, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ion.py', lineno=46, function='__call__', code_context=['            return call_fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), v = None, buffers = None, args = ((1, 1, 3, 3), False), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), v = None, buffers = None, args = ((1, 1, 3, 3), False), kwargs = {}, replace_v = False, replace_buffers = False
call_signature = <Signature (batch_shape, same_on_batch=False)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), batch_shape = (1, 1, 3, 3), same_on_batch = False

    def call(self, batch_shape, same_on_batch=False):
        from ....utils.helpers import tensorflow__extract_device_dtype
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...utils.helpers import tensorflow__adapted_rsampling
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_exp_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_floor_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_round_frnt_
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_sqrt_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_int_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_cumsum_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_bool_frnt_
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_arange_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_round_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_where_frnt_
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ....core._backend import tensor
        from ....core._backend import zeros
    
        batch_size = batch_shape[0]
        size = batch_shape[-2], batch_shape[-1]
        _device, _dtype = tensorflow__extract_device_dtype([self.scale, self.ratio])
        if batch_size == 0:
            return {
                "src": zeros([0, 4, 2], device=_device, dtype=_dtype),
                "dst": zeros([0, 4, 2], device=_device, dtype=_dtype),
                "size": zeros([0, 2], device=_device, dtype=_dtype),
            }
        rand = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 10), self.rand_sampler, same_on_batch
            ),
            device=_device,
            dtype=_dtype,
        )
        area = (
            (rand * (self.scale[1] - self.scale[0]) + self.scale[0]) * size[0] * size[1]
        )
        log_ratio = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 10), self.log_ratio_sampler, same_on_batch
            ),
            device=_device,
            dtype=_dtype,
        )
        aspect_ratio = tensorflow_exp_frnt(log_ratio)
        w = tensorflow_floor_frnt_(
            tensorflow_round_frnt_(tensorflow_sqrt_frnt(area * aspect_ratio))
        )
        h = tensorflow_floor_frnt_(
            tensorflow_round_frnt_(tensorflow_sqrt_frnt(area / aspect_ratio))
        )
>       cond = tensorflow_int_frnt_((0 < w) * (w < size[0]) * (0 < h) * (h < size[1]))

Translated_Outputs/tensorflow_outputs/kornia/augmentation/random_generator/_2d/crop.py:328: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 10), dtype=bool, numpy=
array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,
...e=(1, 10), dtype=bool, numpy=
array([[False, False, False, False, False, False, False, False, False,
        False]])>)
kwargs = {}, arg = <tf.Tensor: shape=(1, 10), dtype=bool, numpy=
array([[False, False, False, False, False, False, False, False, False,
        False]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_ResizedCropGenerator.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_ResizedCropGenerator.call():
E         â€¢ batch_shape=('1', '1', '3', '3')
E         â€¢ same_on_batch=False

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomResizedCrop
______________________________________________________________________________ test_RandomRotation[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRotation(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomRotation")
    
        init_args = ()
        init_kwargs = {"degrees": 45.0, "p": 1.}
        call_args = (torch.tensor([[1., 0., 0., 2.],
                                   [0., 0., 0., 0.],
                                   [0., 1., 2., 0.],
                                   [0., 0., 1., 2.]]),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomRotation,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:974: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.rotation.RandomRotation'>, target = 'tensorflow', init_args = (), init_kwargs = {'degrees': 45.0, 'p': 1.0}
call_args = (tensor([[1., 0., 0., 2.],
        [0., 0., 0., 0.],
        [0., 1., 2., 0.],
        [0., 0., 1., 2.]]),), call_kwargs = {}, deterministic_output = False, backend_compile = False
tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
args = (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>,), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f10236c40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=Tru...=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True), v = None, buffers = None
args = (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>,), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=Tru...=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True), v = None, buffers = None
args = (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>,), kwargs = {}
first_arr = <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>, replace_v = False
replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True),)
kwargs = {'input': <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...y([20.172249], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f182a0820>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f102fa710>
tensor = <function tensorflow_tensor_frnt at 0x7f3f10297880>
in_tensor = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([4, 4]), batch_shape = ivy.frontends.torch.Size([1, 1, 4, 4]), flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
in_tensor = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...y([20.172249], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...y([20.172249], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...y([20.172249], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....geometry.transform.affwarp import tensorflow__compute_tensor_center
        from ....geometry.transform.affwarp import tensorflow__compute_rotation_matrix
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        angles: typing.Any = tensorflow_to_frnt_(params["degrees"], input)
        center: typing.Any = tensorflow__compute_tensor_center(input)
        rotation_mat: typing.Any = tensorflow__compute_rotation_matrix(
>           angles, center.expand(tensorflow_shape_frnt_(angles)[0], -1)
        )
E       AttributeError: Exception encountered when calling tensorflow_RandomRotation.call().
E       
E       [1m'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'expand'[0m
E       
E       Arguments received by tensorflow_RandomRotation.call():
E         â€¢ input=tf.Tensor(shape=(4, 4), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/rotation.py:67: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomRotation
______________________________________________________________________________ test_RandomCutMixV2[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomCutMixV2(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomCutMixV2")
    
        input = torch.rand(2, 1, 3, 3)
        input[0] = torch.ones((1, 3, 3))
        label = torch.tensor([0, 1])
    
        init_args = ()
        init_kwargs = {"data_keys": ["input", "class"]}
        call_args = (input, label)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomCutMixV2,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1058: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.cutmix.RandomCutMixV2'>, target = 'tensorflow', init_args = (), init_kwargs = {'data_keys': ['input', 'class']}
call_args = (tensor([[[[1.0000, 1.0000, 1.0000],
          [1.0000, 1.0000, 1.0000],
          [1.0000, 1.0000, 1.0000]]],


        [[[0.4576, 0.6271, 0.9498],
          [0.6292, 0.4671, 0.7641],
          [0.0896, 0.8081, 0.6786]]]]), tensor([0, 1]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
>       transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)

kornia/augmentation/test_augmentation.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation._2d.mix.cutmix.RandomCutMixV2'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: source code not available

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomCutMixV2
_______________________________________________________________________________ test_RandomJigsaw[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomJigsaw(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomJigsaw")
    
        init_args = ((4, 4),)
        init_kwargs = {}
        call_args = (torch.randn(8, 3, 256, 256),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomJigsaw,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1078: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.jigsaw.RandomJigsaw'>, target = 'tensorflow', init_args = ((4, 4),), init_kwargs = {}
call_args = (tensor([[[[ 1.5362e-01, -7.2446e-01,  3.1789e-01,  ..., -2.2119e-01,
            1.8985e-01, -2.0184e+00],
          ...1e-01],
          [-2.2278e-01,  4.1111e-03,  1.3252e-01,  ...,  4.6973e-01,
           -1.2850e+00,  9.2389e-01]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4))
args = (<tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 1.53624788e-01, -7.24461138e-01,  3.17887008e-01...258e-03,  1.32522747e-01, ...,
           4.69731539e-01, -1.28495717e+00,  9.23891068e-01]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f08ac7640, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), <tf.Tensor: shape=(8, 3, ...3258e-03,  1.32522747e-01, ...,
           4.69731539e-01, -1.28495717e+00,  9.23891068e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 1.53624788e-01, -7.24461138e-01,  3.17887008e-01...258e-03,  1.32522747e-01, ...,
           4.69731539e-01, -1.28495717e+00,  9.23891068e-01]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), <tf.Tensor: shape=(8, 3, ...3258e-03,  1.32522747e-01, ...,
           4.69731539e-01, -1.28495717e+00,  9.23891068e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 1.53624788e-01, -7.24461138e-01,  3.17887008e-01...258e-03,  1.32522747e-01, ...,
           4.69731539e-01, -1.28495717e+00,  9.23891068e-01]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 1.53624788e-01, -7.24461138e-01,  3.17887008e-01,...13258e-03,  1.32522747e-01, ...,
           4.69731539e-01, -1.28495717e+00,  9.23891068e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)),)
kwargs = {'input': <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 1.53624788e-01, -7.24461138e-01,  3.178...3258e-03,  1.32522747e-01, ...,
           4.69731539e-01, -1.28495717e+00,  9.23891068e-01]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 1.53624788e-01, -7.24461138e-01,  3.178...3258e-03,  1.32522747e-01, ...,
           4.69731539e-01, -1.28495717e+00,  9.23891068e-01]]]],
      dtype=float32)>}

    def bind(self, /, *args, **kwargs):
        """Get a BoundArguments object, that maps the passed `args`
        and `kwargs` to the function's signature.  Raises `TypeError`
        if the passed arguments can not be bound.
        """
>       return self._bind(args, kwargs)

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 1.53624788e-01, -7.24461138e-01,  3.178...3258e-03,  1.32522747e-01, ...,
           4.69731539e-01, -1.28495717e+00,  9.23891068e-01]]]],
      dtype=float32)>}

    def _bind(self, args, kwargs, *, partial=False):
        """Private method. Don't use directly."""
    
        arguments = {}
    
        parameters = iter(self.parameters.values())
        parameters_ex = ()
        arg_vals = iter(args)
    
        while True:
            # Let's iterate through the positional arguments and corresponding
            # parameters
            try:
                arg_val = next(arg_vals)
            except StopIteration:
                # No more positional arguments
                try:
                    param = next(parameters)
                except StopIteration:
                    # No more parameters. That's it. Just need to check that
                    # we have no `kwargs` after this while loop
                    break
                else:
                    if param.kind == _VAR_POSITIONAL:
                        # That's OK, just empty *args.  Let's start parsing
                        # kwargs
                        break
                    elif param.name in kwargs:
                        if param.kind == _POSITIONAL_ONLY:
                            msg = '{arg!r} parameter is positional only, ' \
                                  'but was passed as a keyword'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
                        parameters_ex = (param,)
                        break
                    elif (param.kind == _VAR_KEYWORD or
                                                param.default is not _empty):
                        # That's fine too - we have a default value for this
                        # parameter.  So, lets start parsing `kwargs`, starting
                        # with the current parameter
                        parameters_ex = (param,)
                        break
                    else:
                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,
                        # not in `kwargs`
                        if partial:
                            parameters_ex = (param,)
                            break
                        else:
                            msg = 'missing a required argument: {arg!r}'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
            else:
                # We have a positional argument to process
                try:
                    param = next(parameters)
                except StopIteration:
                    raise TypeError('too many positional arguments') from None
                else:
                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):
                        # Looks like we have no parameter for this positional
                        # argument
                        raise TypeError(
                            'too many positional arguments') from None
    
                    if param.kind == _VAR_POSITIONAL:
                        # We have an '*args'-like argument, let's fill it with
                        # all positional arguments we have left and move on to
                        # the next phase
                        values = [arg_val]
                        values.extend(arg_vals)
                        arguments[param.name] = tuple(values)
                        break
    
                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:
                        raise TypeError(
                            'multiple values for argument {arg!r}'.format(
                                arg=param.name)) from None
    
                    arguments[param.name] = arg_val
    
        # Now, we iterate through the remaining parameters to process
        # keyword arguments
        kwargs_param = None
        for param in itertools.chain(parameters_ex, parameters):
            if param.kind == _VAR_KEYWORD:
                # Memorize that we have a '**kwargs'-like parameter
                kwargs_param = param
                continue
    
            if param.kind == _VAR_POSITIONAL:
                # Named arguments don't refer to '*args'-like parameters.
                # We only arrive here if the positional arguments ended
                # before reaching the last parameter before *args.
                continue
    
            param_name = param.name
            try:
                arg_val = kwargs.pop(param_name)
            except KeyError:
                # We have no value for this parameter.  It's fine though,
                # if it has a default value, or it is an '*args'-like
                # parameter, left alone by the processing of positional
                # arguments.
                if (not partial and param.kind != _VAR_POSITIONAL and
                                                    param.default is _empty):
                    raise TypeError('missing a required argument: {arg!r}'. \
                                    format(arg=param_name)) from None
    
            else:
                if param.kind == _POSITIONAL_ONLY:
                    # This should never happen in case of a properly built
                    # Signature object (but let's have this check here
                    # to ensure correct behaviour just in case)
                    raise TypeError('{arg!r} parameter is positional only, '
                                    'but was passed as a keyword'. \
                                    format(arg=param.name))
    
                arguments[param_name] = arg_val
    
        if kwargs:
            if kwargs_param is not None:
                # Process our '**kwargs'-like parameter
                arguments[kwargs_param.name] = kwargs
            else:
>               raise TypeError(
                    'got an unexpected keyword argument {arg!r}'.format(
                        arg=next(iter(kwargs))))
E               TypeError: got an unexpected keyword argument 'input'

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3166: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomJigsaw
_______________________________________________________________________________ test_RandomMixUpV2[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomMixUpV2(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomMixUpV2")
    
        init_args = ()
        init_kwargs = {"data_keys": ["input", "class"]}
        call_args = (torch.rand(2, 1, 3, 3), torch.tensor([0, 1]))
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomMixUpV2,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1098: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.mixup.RandomMixUpV2'>, target = 'tensorflow', init_args = (), init_kwargs = {'data_keys': ['input', 'class']}
call_args = (tensor([[[[0.8923, 0.6495, 0.9443],
          [0.3222, 0.4930, 0.6392],
          [0.2411, 0.7261, 0.5024]]],


        [[[0.2018, 0.8998, 0.3351],
          [0.9123, 0.8138, 0.4002],
          [0.7652, 0.9678, 0.5721]]]]), tensor([0, 1]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.89232117, 0.6494924 , 0.94429964],
         [0.3221...   [0.7652212 , 0.9678332 , 0.57209384]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x562f37710740, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 1, 3, 3), d...   [0.7652212 , 0.9678332 , 0.57209384]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.89232117, 0.6494924 , 0.94429964],
         [0.3221...   [0.7652212 , 0.9678332 , 0.57209384]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 1, 3, 3), d...   [0.7652212 , 0.9678332 , 0.57209384]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.89232117, 0.6494924 , 0.94429964],
         [0.3221...   [0.7652212 , 0.9678332 , 0.57209384]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.89232117, 0.6494924 , 0.94429964],
         [0.32217...2193],
         [0.91226786, 0.8138119 , 0.40021408],
         [0.7652212 , 0.9678332 , 0.57209384]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.89232117, 0.6494924 , 0.94429964],
       ...212 , 0.9678332 , 0.57209384]]]], dtype=float32)>, 'params': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.89232117, 0.6494924 , 0.94429964],
       ...193],
         [0.91226786, 0.8138119 , 0.40021408],
         [0.7652212 , 0.9678332 , 0.57209384]]]], dtype=float32)>}

    def bind(self, /, *args, **kwargs):
        """Get a BoundArguments object, that maps the passed `args`
        and `kwargs` to the function's signature.  Raises `TypeError`
        if the passed arguments can not be bound.
        """
>       return self._bind(args, kwargs)

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.89232117, 0.6494924 , 0.94429964],
       ...193],
         [0.91226786, 0.8138119 , 0.40021408],
         [0.7652212 , 0.9678332 , 0.57209384]]]], dtype=float32)>}

    def _bind(self, args, kwargs, *, partial=False):
        """Private method. Don't use directly."""
    
        arguments = {}
    
        parameters = iter(self.parameters.values())
        parameters_ex = ()
        arg_vals = iter(args)
    
        while True:
            # Let's iterate through the positional arguments and corresponding
            # parameters
            try:
                arg_val = next(arg_vals)
            except StopIteration:
                # No more positional arguments
                try:
                    param = next(parameters)
                except StopIteration:
                    # No more parameters. That's it. Just need to check that
                    # we have no `kwargs` after this while loop
                    break
                else:
                    if param.kind == _VAR_POSITIONAL:
                        # That's OK, just empty *args.  Let's start parsing
                        # kwargs
                        break
                    elif param.name in kwargs:
                        if param.kind == _POSITIONAL_ONLY:
                            msg = '{arg!r} parameter is positional only, ' \
                                  'but was passed as a keyword'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
                        parameters_ex = (param,)
                        break
                    elif (param.kind == _VAR_KEYWORD or
                                                param.default is not _empty):
                        # That's fine too - we have a default value for this
                        # parameter.  So, lets start parsing `kwargs`, starting
                        # with the current parameter
                        parameters_ex = (param,)
                        break
                    else:
                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,
                        # not in `kwargs`
                        if partial:
                            parameters_ex = (param,)
                            break
                        else:
                            msg = 'missing a required argument: {arg!r}'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
            else:
                # We have a positional argument to process
                try:
                    param = next(parameters)
                except StopIteration:
                    raise TypeError('too many positional arguments') from None
                else:
                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):
                        # Looks like we have no parameter for this positional
                        # argument
                        raise TypeError(
                            'too many positional arguments') from None
    
                    if param.kind == _VAR_POSITIONAL:
                        # We have an '*args'-like argument, let's fill it with
                        # all positional arguments we have left and move on to
                        # the next phase
                        values = [arg_val]
                        values.extend(arg_vals)
                        arguments[param.name] = tuple(values)
                        break
    
                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:
                        raise TypeError(
                            'multiple values for argument {arg!r}'.format(
                                arg=param.name)) from None
    
                    arguments[param.name] = arg_val
    
        # Now, we iterate through the remaining parameters to process
        # keyword arguments
        kwargs_param = None
        for param in itertools.chain(parameters_ex, parameters):
            if param.kind == _VAR_KEYWORD:
                # Memorize that we have a '**kwargs'-like parameter
                kwargs_param = param
                continue
    
            if param.kind == _VAR_POSITIONAL:
                # Named arguments don't refer to '*args'-like parameters.
                # We only arrive here if the positional arguments ended
                # before reaching the last parameter before *args.
                continue
    
            param_name = param.name
            try:
                arg_val = kwargs.pop(param_name)
            except KeyError:
                # We have no value for this parameter.  It's fine though,
                # if it has a default value, or it is an '*args'-like
                # parameter, left alone by the processing of positional
                # arguments.
                if (not partial and param.kind != _VAR_POSITIONAL and
                                                    param.default is _empty):
                    raise TypeError('missing a required argument: {arg!r}'. \
                                    format(arg=param_name)) from None
    
            else:
                if param.kind == _POSITIONAL_ONLY:
                    # This should never happen in case of a properly built
                    # Signature object (but let's have this check here
                    # to ensure correct behaviour just in case)
                    raise TypeError('{arg!r} parameter is positional only, '
                                    'but was passed as a keyword'. \
                                    format(arg=param.name))
    
                arguments[param_name] = arg_val
    
        if kwargs:
            if kwargs_param is not None:
                # Process our '**kwargs'-like parameter
                arguments[kwargs_param.name] = kwargs
            else:
>               raise TypeError(
                    'got an unexpected keyword argument {arg!r}'.format(
                        arg=next(iter(kwargs))))
E               TypeError: got an unexpected keyword argument 'input'

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3166: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomMixUpV2
_______________________________________________________________________________ test_RandomMosaic[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomMosaic(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomMosaic")
    
        init_args = ((300, 300),)
        init_kwargs = {"data_keys": ["input", "bbox_xyxy"]}
        call_args = (
            torch.randn(8, 3, 224, 224),
            torch.tensor([[
                [70, 5, 150, 100],
                [60, 180, 175, 220],
            ]]).repeat(8, 1, 1),
        )
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomMosaic,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.mosaic.RandomMosaic'>, target = 'tensorflow', init_args = ((300, 300),), init_kwargs = {'data_keys': ['input', 'bbox_xyxy']}
call_args = (tensor([[[[ 9.9098e-01, -1.2901e+00,  2.0427e+00,  ..., -9.4748e-01,
            1.2055e+00,  4.8122e-01],
          ...[ 70,   5, 150, 100],
         [ 60, 180, 175, 220]],

        [[ 70,   5, 150, 100],
         [ 60, 180, 175, 220]]]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0, ..._size=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice)
args = (<tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 9.90980208e-01, -1.29007709e+00,  2.04272223e+00... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f10367040, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0,... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0, ..._size=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice)
v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 9.90980208e-01, -1.29007709e+00,  2.04272223e+00... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0,... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0, ..._size=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice)
v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 9.90980208e-01, -1.29007709e+00,  2.04272223e+00... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 9.90980208e-01, -1.29007709e+00,  2.04272223e+00,...47991e+00, -8.19398999e-01, ...,
          -7.05559671e-01, -1.18344283e+00, -8.22167695e-02]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0,...ize=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice),)
kwargs = {'input': <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 9.90980208e-01, -1.29007709e+00,  2.042... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 9.90980208e-01, -1.29007709e+00,  2.042...7991e+00, -8.19398999e-01, ...,
          -7.05559671e-01, -1.18344283e+00, -8.22167695e-02]]]],
      dtype=float32)>}

    def bind(self, /, *args, **kwargs):
        """Get a BoundArguments object, that maps the passed `args`
        and `kwargs` to the function's signature.  Raises `TypeError`
        if the passed arguments can not be bound.
        """
>       return self._bind(args, kwargs)

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[ 9.90980208e-01, -1.29007709e+00,  2.042...7991e+00, -8.19398999e-01, ...,
          -7.05559671e-01, -1.18344283e+00, -8.22167695e-02]]]],
      dtype=float32)>}

    def _bind(self, args, kwargs, *, partial=False):
        """Private method. Don't use directly."""
    
        arguments = {}
    
        parameters = iter(self.parameters.values())
        parameters_ex = ()
        arg_vals = iter(args)
    
        while True:
            # Let's iterate through the positional arguments and corresponding
            # parameters
            try:
                arg_val = next(arg_vals)
            except StopIteration:
                # No more positional arguments
                try:
                    param = next(parameters)
                except StopIteration:
                    # No more parameters. That's it. Just need to check that
                    # we have no `kwargs` after this while loop
                    break
                else:
                    if param.kind == _VAR_POSITIONAL:
                        # That's OK, just empty *args.  Let's start parsing
                        # kwargs
                        break
                    elif param.name in kwargs:
                        if param.kind == _POSITIONAL_ONLY:
                            msg = '{arg!r} parameter is positional only, ' \
                                  'but was passed as a keyword'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
                        parameters_ex = (param,)
                        break
                    elif (param.kind == _VAR_KEYWORD or
                                                param.default is not _empty):
                        # That's fine too - we have a default value for this
                        # parameter.  So, lets start parsing `kwargs`, starting
                        # with the current parameter
                        parameters_ex = (param,)
                        break
                    else:
                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,
                        # not in `kwargs`
                        if partial:
                            parameters_ex = (param,)
                            break
                        else:
                            msg = 'missing a required argument: {arg!r}'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
            else:
                # We have a positional argument to process
                try:
                    param = next(parameters)
                except StopIteration:
                    raise TypeError('too many positional arguments') from None
                else:
                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):
                        # Looks like we have no parameter for this positional
                        # argument
                        raise TypeError(
                            'too many positional arguments') from None
    
                    if param.kind == _VAR_POSITIONAL:
                        # We have an '*args'-like argument, let's fill it with
                        # all positional arguments we have left and move on to
                        # the next phase
                        values = [arg_val]
                        values.extend(arg_vals)
                        arguments[param.name] = tuple(values)
                        break
    
                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:
                        raise TypeError(
                            'multiple values for argument {arg!r}'.format(
                                arg=param.name)) from None
    
                    arguments[param.name] = arg_val
    
        # Now, we iterate through the remaining parameters to process
        # keyword arguments
        kwargs_param = None
        for param in itertools.chain(parameters_ex, parameters):
            if param.kind == _VAR_KEYWORD:
                # Memorize that we have a '**kwargs'-like parameter
                kwargs_param = param
                continue
    
            if param.kind == _VAR_POSITIONAL:
                # Named arguments don't refer to '*args'-like parameters.
                # We only arrive here if the positional arguments ended
                # before reaching the last parameter before *args.
                continue
    
            param_name = param.name
            try:
                arg_val = kwargs.pop(param_name)
            except KeyError:
                # We have no value for this parameter.  It's fine though,
                # if it has a default value, or it is an '*args'-like
                # parameter, left alone by the processing of positional
                # arguments.
                if (not partial and param.kind != _VAR_POSITIONAL and
                                                    param.default is _empty):
                    raise TypeError('missing a required argument: {arg!r}'. \
                                    format(arg=param_name)) from None
    
            else:
                if param.kind == _POSITIONAL_ONLY:
                    # This should never happen in case of a properly built
                    # Signature object (but let's have this check here
                    # to ensure correct behaviour just in case)
                    raise TypeError('{arg!r} parameter is positional only, '
                                    'but was passed as a keyword'. \
                                    format(arg=param.name))
    
                arguments[param_name] = arg_val
    
        if kwargs:
            if kwargs_param is not None:
                # Process our '**kwargs'-like parameter
                arguments[kwargs_param.name] = kwargs
            else:
>               raise TypeError(
                    'got an unexpected keyword argument {arg!r}'.format(
                        arg=next(iter(kwargs))))
E               TypeError: got an unexpected keyword argument 'input'

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3166: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomMosaic
___________________________________________________________________________ test_RandomTransplantation[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomTransplantation(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomTransplantation")
    
        init_args = ()
        init_kwargs = {"p": 1.}
        call_args = (torch.randn(2, 3, 5, 5), torch.randint(0, 3, (2, 5, 5)))
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomTransplantation,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.transplantation.RandomTransplantation'>, target = 'tensorflow', init_args = (), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[ 9.4381e-01, -1.4776e+00,  1.5148e+00,  2.4399e+00,  5.1348e-01],
          [-1.8730e-01, -5.3163e-02,  1....0, 0, 1, 1],
         [1, 2, 1, 1, 0],
         [0, 0, 0, 2, 1],
         [0, 0, 1, 2, 2],
         [2, 1, 2, 1, 1]]]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 9.4380593e-01, -1.4776206e+00,  1.5147630e+00,
     ...1, 0, 0, 1, 1],
        [1, 2, 1, 1, 0],
        [0, 0, 0, 2, 1],
        [0, 0, 1, 2, 2],
        [2, 1, 2, 1, 1]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f08ac6c40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=floa...1, 0, 0, 1, 1],
        [1, 2, 1, 1, 0],
        [0, 0, 0, 2, 1],
        [0, 0, 1, 2, 2],
        [2, 1, 2, 1, 1]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 9.4380593e-01, -1.4776206e+00,  1.5147630e+00,
     ...1, 0, 0, 1, 1],
        [1, 2, 1, 1, 0],
        [0, 0, 0, 2, 1],
        [0, 0, 1, 2, 2],
        [2, 1, 2, 1, 1]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=floa...1, 0, 0, 1, 1],
        [1, 2, 1, 1, 0],
        [0, 0, 0, 2, 1],
        [0, 0, 1, 2, 2],
        [2, 1, 2, 1, 1]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 9.4380593e-01, -1.4776206e+00,  1.5147630e+00,
     ...1, 0, 0, 1, 1],
        [1, 2, 1, 1, 0],
        [0, 0, 0, 2, 1],
        [0, 0, 1, 2, 2],
        [2, 1, 2, 1, 1]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 9.4380593e-01, -1.4776206e+00,  1.5147630e+00,
      ...         [ 6.8336946e-01, -1.5561553e+00, -4.5914856e-01,
          -6.5943129e-02, -2.4901068e+00]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 9.4380593e-01, -1.4776206e+00,  1.5147630e+...1, 0, 0, 1, 1],
        [1, 2, 1, 1, 0],
        [0, 0, 0, 2, 1],
        [0, 0, 1, 2, 2],
        [2, 1, 2, 1, 1]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False)
params = <tf.Tensor: shape=(2, 5, 5), dtype=int64, numpy=
array([[[0, 0, 2, 1, 2],
        [1, 2, 0, 1, 0],
        [1, 1, 1, 2...[1, 0, 0, 1, 1],
        [1, 2, 1, 1, 0],
        [0, 0, 0, 2, 1],
        [0, 0, 1, 2, 2],
        [2, 1, 2, 1, 1]]])>
data_keys = None, input = ()
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 9.4380593e-01, -1.4776206e+00,  1.5147630e+...        [ 6.8336946e-01, -1.5561553e+00, -4.5914856e-01,
          -6.5943129e-02, -2.4901068e+00]]]], dtype=float32)>}
tensorflow_get_item = <function tensorflow_get_item at 0x7f3f10a2f130>, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f12eefd90>
tensorflow_clone_frnt_ = <function tensorflow_clone_frnt_ at 0x7f3f1d465fc0>, tensorflow__validate_input_dtype = <function tensorflow__validate_input_dtype at 0x7f3f0826aa70>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f3f08bdfbe0>, keys = [<tensorflow_DataKey.IMAGE: 0>, <tensorflow_DataKey.MASK: 1>]

    def call(self, *input, params=None, data_keys=None, **kwargs):
        from ....constants import tensorflow_DataKey
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...utils.helpers import tensorflow__validate_input_dtype
        from .....ivy.functional.frontends.torch.tensor import (
            tensorflow_index_put_frnt_,
        )
    
        keys: typing.Any
        if data_keys is None:
            keys = self.data_keys
        else:
            keys = [tensorflow_DataKey.get(inp) for inp in data_keys]
        if params is None:
            mask: typing.Any = tensorflow_get_item(
                input, keys.index(tensorflow_DataKey.MASK)
            )
            self._params = self.forward_parameters(tensorflow_shape_frnt_(mask))
        else:
            self._params = params
>       if any(
            k not in self._params
            for k in ["acceptor_indices", "donor_indices", "selection"]
        ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:242: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f3f08b3b340>

    if any(
>       k not in self._params
        for k in ["acceptor_indices", "donor_indices", "selection"]
    ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(5, 5), dtype=int64, numpy=
array([[0, 0, 2, 1, 2],
       [1, 2, 0, 1, 0],
       [1, 1, 1, 2, 0],
       [0, 1, 0, 0, 2],
       [0, 0, 2, 0, 1]])>, 'acceptor_indices')
kwargs = {}, arg = 'acceptor_indices'

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       TypeError: Exception encountered when calling tensorflow_RandomTransplantation.call().
E       
E       [1mdata type 'acceptor_indices' not understood[0m
E       
E       Arguments received by tensorflow_RandomTransplantation.call():
E         â€¢ input=<class 'inspect._empty'>
E         â€¢ params=tf.Tensor(shape=(2, 5, 5), dtype=int64)
E         â€¢ data_keys=None
E         â€¢ kwargs={'input': 'tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)'}

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomTransplantation
_____________________________________________________________________________ test_RandomRotation3D[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRotation3D(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomRotation3D")
    
        init_args = ((15., 20., 20.),)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 3, 3, 3),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomRotation3D,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1264: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._3d.geometric.rotation.RandomRotation3D'>, target = 'tensorflow', init_args = ((15.0, 20.0, 20.0),), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[[0.2025, 0.7540, 0.1075],
           [0.6733, 0.8027, 0.9347],
           [0.1755, 0.6811, 0.5090]],

    ...,

          [[0.6368, 0.6164, 0.7475],
           [0.0351, 0.0712, 0.8408],
           [0.7352, 0.0491, 0.8268]]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.20247298, 0.75401247, 0.10750145],
          [0...,
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f081d1040, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, a...],
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.20247298, 0.75401247, 0.10750145],
          [0...,
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, a...],
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.20247298, 0.75401247, 0.10750145],
          [0...,
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.20247298, 0.75401247, 0.10750145],
          [0....5],
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.20247298, 0.75401247, 0.10750145],
   ...],
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.20247298, 0.75401247, 0.10750145],
          [0....5],
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...05183], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.7460327], dtype=float32)>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f08af3010>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f08cdb910>
tensor = <function tensorflow_tensor_frnt at 0x7f3f085bd750>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.20247298, 0.75401247, 0.10750145],
          [0....5],
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 3, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3, 3]), flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.20247298, 0.75401247, 0.10750145],
          [0....5],
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...05183], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.7460327], dtype=float32)>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/base.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.20247298, 0.75401247, 0.10750145],
          [0....5],
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...05183], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.7460327], dtype=float32)>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/base.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.20247298, 0.75401247, 0.10750145],
          [0....5],
          [0.03507662, 0.07122028, 0.8407843 ],
          [0.7351601 , 0.0490815 , 0.82676464]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...05183], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.7460327], dtype=float32)>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....geometry.transform.affwarp import tensorflow__compute_tensor_center3d
        from ....geometry.transform.affwarp import tensorflow__compute_rotation_matrix3d
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        yaw: typing.Any = tensorflow_to_frnt_(params["yaw"], input)
        pitch: typing.Any = tensorflow_to_frnt_(params["pitch"], input)
        roll: typing.Any = tensorflow_to_frnt_(params["roll"], input)
        center: typing.Any = tensorflow__compute_tensor_center3d(input)
        rotation_mat: typing.Any = tensorflow__compute_rotation_matrix3d(
>           yaw, pitch, roll, center.expand(tensorflow_shape_frnt_(yaw)[0], -1)
        )
E       AttributeError: Exception encountered when calling tensorflow_RandomRotation3D.call().
E       
E       [1m'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'expand'[0m
E       
E       Arguments received by tensorflow_RandomRotation3D.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 3, 3, 3), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/geometric/rotation.py:65: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomRotation3D
__________________________________________________________________________ test_RandomTransplantation3D[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomTransplantation3D(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomTransplantation3D")
    
        init_args = ()
        init_kwargs = {"p": 1.}
        call_args = (torch.randn(2, 3, 5, 5), torch.randint(0, 3, (2, 5, 5)))
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomTransplantation3D,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1344: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._3d.mix.transplantation.RandomTransplantation3D'>, target = 'tensorflow', init_args = (), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[ 0.2009, -1.1437,  0.6324, -1.3042, -0.1487],
          [-1.6308,  0.1142, -0.1338,  1.5704,  0.4855],
   ...0, 2, 1, 0],
         [0, 0, 1, 0, 0],
         [1, 1, 1, 2, 2],
         [2, 0, 1, 2, 2],
         [0, 2, 0, 2, 1]]]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.20092617, -1.1436756 ,  0.632363  , -1.3042012 ,
 ...1, 0, 2, 1, 0],
        [0, 0, 1, 0, 0],
        [1, 1, 1, 2, 2],
        [2, 0, 1, 2, 2],
        [0, 2, 0, 2, 1]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3f081d2440, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=fl...1, 0, 2, 1, 0],
        [0, 0, 1, 0, 0],
        [1, 1, 1, 2, 2],
        [2, 0, 1, 2, 2],
        [0, 2, 0, 2, 1]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.20092617, -1.1436756 ,  0.632363  , -1.3042012 ,
 ...1, 0, 2, 1, 0],
        [0, 0, 1, 0, 0],
        [1, 1, 1, 2, 2],
        [2, 0, 1, 2, 2],
        [0, 2, 0, 2, 1]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=fl...1, 0, 2, 1, 0],
        [0, 0, 1, 0, 0],
        [1, 1, 1, 2, 2],
        [2, 0, 1, 2, 2],
        [0, 2, 0, 2, 1]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.20092617, -1.1436756 ,  0.632363  , -1.3042012 ,
 ...1, 0, 2, 1, 0],
        [0, 0, 1, 0, 0],
        [1, 1, 1, 2, 2],
        [2, 0, 1, 2, 2],
        [0, 2, 0, 2, 1]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.20092617, -1.1436756 ,  0.632363  , -1.3042012 ,
  ...  1.3352884 ],
         [ 1.1816189 , -0.5587876 , -1.0794238 ,  0.4407832 ,
           0.31511778]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.20092617, -1.1436756 ,  0.632363  , -1.30...1, 0, 2, 1, 0],
        [0, 0, 1, 0, 0],
        [1, 1, 1, 2, 2],
        [2, 0, 1, 2, 2],
        [0, 2, 0, 2, 1]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False)
params = <tf.Tensor: shape=(2, 5, 5), dtype=int64, numpy=
array([[[1, 2, 1, 2, 0],
        [2, 1, 1, 2, 2],
        [1, 1, 2, 0...[1, 0, 2, 1, 0],
        [0, 0, 1, 0, 0],
        [1, 1, 1, 2, 2],
        [2, 0, 1, 2, 2],
        [0, 2, 0, 2, 1]]])>
data_keys = None, input = ()
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.20092617, -1.1436756 ,  0.632363  , -1.30... 1.3352884 ],
         [ 1.1816189 , -0.5587876 , -1.0794238 ,  0.4407832 ,
           0.31511778]]]], dtype=float32)>}
tensorflow_get_item = <function tensorflow_get_item at 0x7f3f105c7ac0>, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f08889990>
tensorflow_clone_frnt_ = <function tensorflow_clone_frnt_ at 0x7f3f08222d40>, tensorflow__validate_input_dtype = <function tensorflow__validate_input_dtype at 0x7f3ef8cc5d80>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f3f083c37f0>, keys = [<tensorflow_DataKey.IMAGE: 0>, <tensorflow_DataKey.MASK: 1>]

    def call(self, *input, params=None, data_keys=None, **kwargs):
        from ....constants import tensorflow_DataKey
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...utils.helpers import tensorflow__validate_input_dtype
        from .....ivy.functional.frontends.torch.tensor import (
            tensorflow_index_put_frnt_,
        )
    
        keys: typing.Any
        if data_keys is None:
            keys = self.data_keys
        else:
            keys = [tensorflow_DataKey.get(inp) for inp in data_keys]
        if params is None:
            mask: typing.Any = tensorflow_get_item(
                input, keys.index(tensorflow_DataKey.MASK)
            )
            self._params = self.forward_parameters(tensorflow_shape_frnt_(mask))
        else:
            self._params = params
>       if any(
            k not in self._params
            for k in ["acceptor_indices", "donor_indices", "selection"]
        ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:242: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f3ef8bbef20>

    if any(
>       k not in self._params
        for k in ["acceptor_indices", "donor_indices", "selection"]
    ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(5, 5), dtype=int64, numpy=
array([[1, 2, 1, 2, 0],
       [2, 1, 1, 2, 2],
       [1, 1, 2, 0, 0],
       [0, 1, 0, 2, 1],
       [2, 0, 2, 1, 0]])>, 'acceptor_indices')
kwargs = {}, arg = 'acceptor_indices'

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       TypeError: Exception encountered when calling tensorflow_RandomTransplantation3D.call().
E       
E       [1mdata type 'acceptor_indices' not understood[0m
E       
E       Arguments received by tensorflow_RandomTransplantation3D.call():
E         â€¢ input=<class 'inspect._empty'>
E         â€¢ params=tf.Tensor(shape=(2, 5, 5), dtype=int64)
E         â€¢ data_keys=None
E         â€¢ kwargs={'input': 'tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)'}

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomTransplantation3D
______________________________________________________________________________ test_LongestMaxSize[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LongestMaxSize(target_framework, mode, backend_compile):
        print("kornia.augmentation.LongestMaxSize")
    
        init_args = (100,)
        init_kwargs = {}
        call_args = (torch.rand(10, 3, 200, 200),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.LongestMaxSize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=True,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1404: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resize.LongestMaxSize'>, target = 'tensorflow', init_args = (100,), init_kwargs = {}
call_args = (tensor([[[[0.5012, 0.0865, 0.1252,  ..., 0.5303, 0.3583, 0.1842],
          [0.0187, 0.1188, 0.8743,  ..., 0.1506, 0...., 0.3041, 0.8432,  ..., 0.4083, 0.9961, 0.8928],
          [0.7544, 0.9499, 0.7717,  ..., 0.1221, 0.8020, 0.9158]]]]),)
call_kwargs = {}, deterministic_output = True, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
args = (<tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.50117654, 0.08652228, 0.12520695, ..., 0.53030...
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3ef8ea3840, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bili...,
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.50117654, 0.08652228, 0.12520695, ..., 0.53030...
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bili...,
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.50117654, 0.08652228, 0.12520695, ..., 0.53030...
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.50117654, 0.08652228, 0.12520695, ..., 0.530301...],
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False),)
kwargs = {'input': <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.50117654, 0.08652228, 0.12520695, ......,
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.50117654, 0.08652228, 0.12520695, ..., 0.530301...],
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f0823ab00>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f08ef5c60>
tensor = <function tensorflow_tensor_frnt at 0x7f3f081a0e50>
in_tensor = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.50117654, 0.08652228, 0.12520695, ..., 0.530301...],
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([10, 3, 200, 200]), batch_shape = ivy.frontends.torch.Size([10, 3, 200, 200])
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
in_tensor = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.50117654, 0.08652228, 0.12520695, ..., 0.530301...],
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.50117654, 0.08652228, 0.12520695, ..., 0.530301...],
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[0.50117654, 0.08652228, 0.12520695, ..., 0.530301...],
         [0.7544357 , 0.9498883 , 0.77165645, ..., 0.12209606,
          0.8019947 , 0.9158148 ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_as_tensor_frnt,
        )
        from ....geometry.transform.imgwarp import tensorflow_get_perspective_transform
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
    
>       if params["output_size"] == tensorflow_shape_frnt_(input)[-2:]:
E       ValueError: Exception encountered when calling tensorflow_LongestMaxSize.call().
E       
E       [1mThe truth value of an array with more than one element is ambiguous. Use a.any() or a.all()[0m
E       
E       Arguments received by tensorflow_LongestMaxSize.call():
E         â€¢ input=tf.Tensor(shape=(10, 3, 200, 200), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/resize.py:67: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.LongestMaxSize
__________________________________________________________________________________ test_Resize[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Resize(target_framework, mode, backend_compile):
        print("kornia.augmentation.Resize")
    
        init_args = ((100, 100),)
        init_kwargs = {}
        call_args = (torch.rand(10, 3, 50, 50),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.Resize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=True,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1424: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resize.Resize'>, target = 'tensorflow', init_args = ((100, 100),), init_kwargs = {}
call_args = (tensor([[[[9.5779e-01, 1.9005e-01, 5.1775e-01,  ..., 7.8706e-01,
           6.9422e-01, 7.3765e-02],
          [8.081... 8.3633e-01],
          [3.4729e-01, 9.5797e-01, 6.9381e-01,  ..., 3.5383e-01,
           8.8523e-01, 3.0736e-01]]]]),)
call_kwargs = {}, deterministic_output = True, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.57794726e-01, 1.90048575e-01, 5.17746091e-01, .....7967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x562f38eb4580, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resamp...57967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.57794726e-01, 1.90048575e-01, 5.17746091e-01, .....7967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resamp...57967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.57794726e-01, 1.90048575e-01, 5.17746091e-01, .....7967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.57794726e-01, 1.90048575e-01, 5.17746091e-01, .......57967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False),)
kwargs = {'input': <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.57794726e-01, 1.90048575e-01, 5.1774609...57967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.57794726e-01, 1.90048575e-01, 5.17746091e-01, .......57967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3f0823ab00>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3f08ef5c60>
tensor = <function tensorflow_tensor_frnt at 0x7f3f081a0e50>
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.57794726e-01, 1.90048575e-01, 5.17746091e-01, .......57967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([10, 3, 50, 50]), batch_shape = ivy.frontends.torch.Size([10, 3, 50, 50])
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.57794726e-01, 1.90048575e-01, 5.17746091e-01, .......57967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.57794726e-01, 1.90048575e-01, 5.17746091e-01, .......57967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[9.57794726e-01, 1.90048575e-01, 5.17746091e-01, .......57967579e-01, 6.93809986e-01, ...,
          3.53831410e-01, 8.85232270e-01, 3.07356894e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_as_tensor_frnt,
        )
        from ....geometry.transform.imgwarp import tensorflow_get_perspective_transform
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
    
>       if params["output_size"] == tensorflow_shape_frnt_(input)[-2:]:
E       ValueError: Exception encountered when calling tensorflow_Resize.call().
E       
E       [1mThe truth value of an array with more than one element is ambiguous. Use a.any() or a.all()[0m
E       
E       Arguments received by tensorflow_Resize.call():
E         â€¢ input=tf.Tensor(shape=(10, 3, 50, 50), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/resize.py:67: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.Resize
______________________________________________________________________________ test_SmallestMaxSize[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SmallestMaxSize(target_framework, mode, backend_compile):
        print("kornia.augmentation.SmallestMaxSize")
    
        init_args = (100,)
        init_kwargs = {}
        call_args = (torch.rand(10, 3, 50, 50),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.SmallestMaxSize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=True,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resize.SmallestMaxSize'>, target = 'tensorflow', init_args = (100,), init_kwargs = {}
call_args = (tensor([[[[4.1571e-01, 8.8618e-01, 4.1439e-01,  ..., 6.3834e-02,
           3.5210e-01, 3.8786e-01],
          [7.916... 7.6388e-01],
          [3.5390e-01, 2.3069e-01, 9.1740e-01,  ..., 2.1488e-01,
           4.0113e-01, 2.6661e-01]]]]),)
call_kwargs = {}, deterministic_output = True, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.15706217e-01, 8.86175215e-01, 4.14389610e-01, .....0693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3ef8875e40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bi...30693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.15706217e-01, 8.86175215e-01, 4.14389610e-01, .....0693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bi...30693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.15706217e-01, 8.86175215e-01, 4.14389610e-01, .....0693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.15706217e-01, 8.86175215e-01, 4.14389610e-01, .......30693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False),)
kwargs = {'input': <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.15706217e-01, 8.86175215e-01, 4.1438961...30693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.15706217e-01, 8.86175215e-01, 4.14389610e-01, .......30693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3ef8dbaf80>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3ef8ca7d00>
tensor = <function tensorflow_tensor_frnt at 0x7f3f0846a4d0>
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.15706217e-01, 8.86175215e-01, 4.14389610e-01, .......30693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([10, 3, 50, 50]), batch_shape = ivy.frontends.torch.Size([10, 3, 50, 50])
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.15706217e-01, 8.86175215e-01, 4.14389610e-01, .......30693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.15706217e-01, 8.86175215e-01, 4.14389610e-01, .......30693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.15706217e-01, 8.86175215e-01, 4.14389610e-01, .......30693996e-01, 9.17400062e-01, ...,
          2.14884818e-01, 4.01133001e-01, 2.66609967e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_as_tensor_frnt,
        )
        from ....geometry.transform.imgwarp import tensorflow_get_perspective_transform
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
    
>       if params["output_size"] == tensorflow_shape_frnt_(input)[-2:]:
E       ValueError: Exception encountered when calling tensorflow_SmallestMaxSize.call().
E       
E       [1mThe truth value of an array with more than one element is ambiguous. Use a.any() or a.all()[0m
E       
E       Arguments received by tensorflow_SmallestMaxSize.call():
E         â€¢ input=tf.Tensor(shape=(10, 3, 50, 50), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/resize.py:67: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.SmallestMaxSize
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/augmentation/test_augmentation.py::test_RandomClahe[tensorflow-s2s-False] - IndexError: Exception encountered when calling tensorflow_RandomClahe.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomLinearCornerIllumination[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_LinearCornerIlluminationGe...
FAILED kornia/augmentation/test_augmentation.py::test_RandomMotionBlur[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_RandomMotionBlur.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomRain[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_...
FAILED kornia/augmentation/test_augmentation.py::test_RandomSaltAndPepperNoise[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_RandomSaltAndPepperNoise.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomSharpness[tensorflow-s2s-False] - NameError: Exception encountered when calling tensorflow_RandomSharpness.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomSnow[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_...
FAILED kornia/augmentation/test_augmentation.py::test_RandomSolarize[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorf...
FAILED kornia/augmentation/test_augmentation.py::test_RandomResizedCrop[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tens...
FAILED kornia/augmentation/test_augmentation.py::test_RandomRotation[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_RandomRotation.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomCutMixV2[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: source code not available
FAILED kornia/augmentation/test_augmentation.py::test_RandomJigsaw[tensorflow-s2s-False] - TypeError: got an unexpected keyword argument 'input'
FAILED kornia/augmentation/test_augmentation.py::test_RandomMixUpV2[tensorflow-s2s-False] - TypeError: got an unexpected keyword argument 'input'
FAILED kornia/augmentation/test_augmentation.py::test_RandomMosaic[tensorflow-s2s-False] - TypeError: got an unexpected keyword argument 'input'
FAILED kornia/augmentation/test_augmentation.py::test_RandomTransplantation[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_RandomTransplantation.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomRotation3D[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_RandomRotation3D.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomTransplantation3D[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_RandomTransplantation3D.call().
FAILED kornia/augmentation/test_augmentation.py::test_LongestMaxSize[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_LongestMaxSize.call().
FAILED kornia/augmentation/test_augmentation.py::test_Resize[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_Resize.call().
FAILED kornia/augmentation/test_augmentation.py::test_SmallestMaxSize[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_SmallestMaxSize.call().
============================================================================== 20 failed, 48 passed in 1709.92s (0:28:29) ==============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/geometry/test_linalg.py ........                                                                                                                                                          [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 8 passed in 109.85s (0:01:49) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 35 items

kornia/test_losses.py .................FF..F.............                                                                                                                                        [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_HausdorffERLoss[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HausdorffERLoss(target_framework, mode, backend_compile):
        print("kornia.losses.HausdorffERLoss")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHausdorffERLoss = ivy.transpile(kornia.losses.HausdorffERLoss, source="torch", target=target_framework)
    
        torch_loss_fn = kornia.losses.HausdorffERLoss()
        transpiled_loss_fn = TranspiledHausdorffERLoss()
    
        torch_args = (
            torch.randn(5, 3, 20, 20),
            (torch.rand(5, 1, 20, 20) * 2).long(),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_res = torch_loss_fn(*torch_args)
>       transpiled_res = transpiled_loss_fn(*transpiled_args)

kornia/test_losses.py:446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
args = (<tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 4.12215628e-02, -4.85068589e-01, -1.21181473e-01, ...        ...,
         [1, 0, 1, ..., 0, 1, 1],
         [0, 0, 1, ..., 1, 0, 1],
         [0, 0, 0, ..., 1, 1, 0]]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7efd1a73a040, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss(), <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 4.12215628e-02, -4.8...        ...,
         [1, 0, 1, ..., 0, 1, 1],
         [0, 0, 1, ..., 1, 0, 1],
         [0, 0, 0, ..., 1, 1, 0]]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 4.12215628e-02, -4.85068589e-01, -1.21181473e-01, ...        ...,
         [1, 0, 1, ..., 0, 1, 1],
         [0, 0, 1, ..., 1, 0, 1],
         [0, 0, 0, ..., 1, 1, 0]]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss(), <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 4.12215628e-02, -4.8...        ...,
         [1, 0, 1, ..., 0, 1, 1],
         [0, 0, 1, ..., 1, 0, 1],
         [0, 0, 0, ..., 1, 1, 0]]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 4.12215628e-02, -4.85068589e-01, -1.21181473e-01, ...        ...,
         [1, 0, 1, ..., 0, 1, 1],
         [0, 0, 1, ..., 1, 0, 1],
         [0, 0, 0, ..., 1, 1, 0]]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 4.12215628e-02, -4.85068589e-01, -1.21181473e-01, ....34219e-01,  1.74475002e+00, ...,
          -8.23905528e-01, -3.93924147e-01, -4.58561808e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (pred, target)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss(),)
kwargs = {'pred': <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 4.12215628e-02, -4.85068589e-01, -1.211814...        ...,
         [1, 0, 1, ..., 0, 1, 1],
         [0, 0, 1, ..., 1, 0, 1],
         [0, 0, 0, ..., 1, 1, 0]]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
pred = <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 4.12215628e-02, -4.85068589e-01, -1.21181473e-01, ....34219e-01,  1.74475002e+00, ...,
          -8.23905528e-01, -3.93924147e-01, -4.58561808e-01]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20), dtype=int64, numpy=
array([[[[1, 1, 1, ..., 1, 0, 1],
         [0, 1, 1, ..., 0, 0, ...         ...,
         [1, 0, 1, ..., 0, 1, 1],
         [0, 0, 1, ..., 1, 0, 1],
         [0, 0, 0, ..., 1, 1, 0]]]])>

    def call(self, pred, target):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
    
        if tensorflow_dim_frnt_(pred) != 4:
            raise ValueError(
                f"Only 2D images supported. Got {tensorflow_dim_frnt_(pred)}."
            )
        if not (
            tensorflow_max_frnt_(target) < tensorflow_size_frnt_(pred, 1)
            and tensorflow_min_frnt_(target) >= 0
            and target.dtype == tf.int64
        ):
            raise ValueError(
                f"Expect long type target value in range (0, {tensorflow_size_frnt_(pred, 1)}). ({tensorflow_min_frnt_(target)}, {tensorflow_max_frnt_(target)})"
            )
>       return super().call(pred, target)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:537: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
pred = <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 4.12215628e-02, -4.85068589e-01, -1.21181473e-01, ....34219e-01,  1.74475002e+00, ...,
          -8.23905528e-01, -3.93924147e-01, -4.58561808e-01]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20), dtype=int64, numpy=
array([[[[1, 1, 1, ..., 1, 0, 1],
         [0, 1, 1, ..., 0, 0, ...         ...,
         [1, 0, 1, ..., 0, 1, 1],
         [0, 0, 1, ..., 1, 0, 1],
         [0, 0, 0, ..., 1, 1, 0]]]])>

    def call(self, pred, target):
        from ..core._backend import where
        from ..core._backend import stack
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
    
        if not (
            tensorflow_shape_frnt_(pred)[2:] == tensorflow_shape_frnt_(target)[2:]
            and tensorflow_size_frnt_(pred, 0) == tensorflow_size_frnt_(target, 0)
            and tensorflow_size_frnt_(target, 1) == 1
        ):
            raise ValueError(
                f"Prediction and target need to be of same size, and target should not be one-hot.Got {tensorflow_shape_frnt_(pred)} and {tensorflow_shape_frnt_(target)}."
            )
        if tensorflow_size_frnt_(pred, 1) < tensorflow_item_frnt_(
            tensorflow_max_frnt_(target)
        ):
            raise ValueError("Invalid target value.")
        out = stack(
>           [
                self.perform_erosion(
                    tensorflow_get_item(
                        pred, (slice(None, None, None), slice(i, i + 1, None))
                    ),
                    where(
                        target == i,
                        tensor(1, device=target.device, dtype=target.dtype),
                        tensor(0, device=target.device, dtype=target.dtype),
                    ),
                )
                for i in range(tensorflow_size_frnt_(pred, 1))
            ]
        )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <range_iterator object at 0x7efd1b706f10>

        [
>           self.perform_erosion(
                tensorflow_get_item(
                    pred, (slice(None, None, None), slice(i, i + 1, None))
                ),
                where(
                    target == i,
                    tensor(1, device=target.device, dtype=target.dtype),
                    tensor(0, device=target.device, dtype=target.dtype),
                ),
            )
            for i in range(tensorflow_size_frnt_(pred, 1))
        ]
    )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
pred = <tf.Tensor: shape=(5, 1, 20, 20), dtype=float32, numpy=
array([[[[ 0.04122156, -0.4850686 , -0.12118147, ..., -0.86145...      [-0.15695074, -0.65427315,  0.5427711 , ...,  0.24653606,
          -0.305589  ,  0.9025215 ]]]], dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20), dtype=int64, numpy=
array([[[[0, 0, 0, ..., 0, 1, 0],
         [1, 0, 0, ..., 1, 1, ...         ...,
         [0, 1, 0, ..., 1, 0, 0],
         [1, 1, 0, ..., 0, 1, 0],
         [1, 1, 1, ..., 0, 0, 1]]]])>

    def perform_erosion(self, pred, target):
        from ..core._backend import as_tensor
        from ..core._backend import zeros_like
        from ..core._backend import where
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_ones_like_v_0p4p0_and_above_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
    
        bound = (pred - target) ** 2
        kernel = as_tensor(self.kernel, device=pred.device, dtype=pred.dtype)
        eroded = zeros_like(bound, device=pred.device, dtype=pred.dtype)
        mask = tensorflow_ones_like_v_0p4p0_and_above_frnt(
            bound, device=pred.device, dtype=tf.bool
        )
        padding = (tensorflow_size_frnt_(kernel, -1) - 1) // 2
        for k in range(self.k):
>           dilation = self.conv(bound, weight=kernel, padding=padding, groups=1)
E           TypeError: Exception encountered when calling tensorflow_HausdorffERLoss.call().
E           
E           [1mtensorflow_conv2d_frnt() got multiple values for argument 'weight'[0m
E           
E           Arguments received by tensorflow_HausdorffERLoss.call():
E             â€¢ pred=tf.Tensor(shape=(5, 3, 20, 20), dtype=float32)
E             â€¢ target=tf.Tensor(shape=(5, 1, 20, 20), dtype=int64)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:82: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.losses.HausdorffERLoss
_____________________________________________________________________________ test_HausdorffERLoss3D[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HausdorffERLoss3D(target_framework, mode, backend_compile):
        print("kornia.losses.HausdorffERLoss3D")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHausdorffERLoss3D = ivy.transpile(kornia.losses.HausdorffERLoss3D, source="torch", target=target_framework)
    
        torch_loss_fn = kornia.losses.HausdorffERLoss3D()
        transpiled_loss_fn = TranspiledHausdorffERLoss3D()
    
        torch_args = (
            torch.randn(5, 3, 20, 20, 20),
            (torch.rand(5, 1, 20, 20, 20) * 2).long(),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_res = torch_loss_fn(*torch_args)
>       transpiled_res = transpiled_loss_fn(*transpiled_args)

kornia/test_losses.py:471: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
args = (<tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 5.25539756e-01,  1.24999380e+00,  2.92687464e...    ...,
          [0, 1, 1, ..., 1, 0, 0],
          [0, 1, 0, ..., 1, 1, 0],
          [0, 0, 1, ..., 0, 0, 0]]]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7efd19cde2b0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss3D(), <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 5.25539756e-0...    ...,
          [0, 1, 1, ..., 1, 0, 0],
          [0, 1, 0, ..., 1, 1, 0],
          [0, 0, 1, ..., 0, 0, 0]]]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 5.25539756e-01,  1.24999380e+00,  2.92687464e...    ...,
          [0, 1, 1, ..., 1, 0, 0],
          [0, 1, 0, ..., 1, 1, 0],
          [0, 0, 1, ..., 0, 0, 0]]]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss3D(), <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 5.25539756e-0...    ...,
          [0, 1, 1, ..., 1, 0, 0],
          [0, 1, 0, ..., 1, 1, 0],
          [0, 0, 1, ..., 0, 0, 0]]]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 5.25539756e-01,  1.24999380e+00,  2.92687464e...    ...,
          [0, 1, 1, ..., 1, 0, 0],
          [0, 1, 0, ..., 1, 1, 0],
          [0, 0, 1, ..., 0, 0, 0]]]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 5.25539756e-01,  1.24999380e+00,  2.92687464e+...170e+00, -1.23472965e+00, ...,
           -1.29540312e+00,  4.97031331e-01,  8.34235787e-01]]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (pred, target)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss3D(),)
kwargs = {'pred': <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 5.25539756e-01,  1.24999380e+00,  2.9...    ...,
          [0, 1, 1, ..., 1, 0, 0],
          [0, 1, 0, ..., 1, 1, 0],
          [0, 0, 1, ..., 0, 0, 0]]]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
pred = <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 5.25539756e-01,  1.24999380e+00,  2.92687464e+...170e+00, -1.23472965e+00, ...,
           -1.29540312e+00,  4.97031331e-01,  8.34235787e-01]]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=int64, numpy=
array([[[[[0, 0, 0, ..., 0, 1, 0],
          [0, 0, 0, ..., ...     ...,
          [0, 1, 1, ..., 1, 0, 0],
          [0, 1, 0, ..., 1, 1, 0],
          [0, 0, 1, ..., 0, 0, 0]]]]])>

    def call(self, pred, target):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
    
        if tensorflow_dim_frnt_(pred) != 5:
            raise ValueError(
                f"Only 3D images supported. Got {tensorflow_dim_frnt_(pred)}."
            )
>       return super().call(pred, target)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:564: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
pred = <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 5.25539756e-01,  1.24999380e+00,  2.92687464e+...170e+00, -1.23472965e+00, ...,
           -1.29540312e+00,  4.97031331e-01,  8.34235787e-01]]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=int64, numpy=
array([[[[[0, 0, 0, ..., 0, 1, 0],
          [0, 0, 0, ..., ...     ...,
          [0, 1, 1, ..., 1, 0, 0],
          [0, 1, 0, ..., 1, 1, 0],
          [0, 0, 1, ..., 0, 0, 0]]]]])>

    def call(self, pred, target):
        from ..core._backend import where
        from ..core._backend import stack
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
    
        if not (
            tensorflow_shape_frnt_(pred)[2:] == tensorflow_shape_frnt_(target)[2:]
            and tensorflow_size_frnt_(pred, 0) == tensorflow_size_frnt_(target, 0)
            and tensorflow_size_frnt_(target, 1) == 1
        ):
            raise ValueError(
                f"Prediction and target need to be of same size, and target should not be one-hot.Got {tensorflow_shape_frnt_(pred)} and {tensorflow_shape_frnt_(target)}."
            )
        if tensorflow_size_frnt_(pred, 1) < tensorflow_item_frnt_(
            tensorflow_max_frnt_(target)
        ):
            raise ValueError("Invalid target value.")
        out = stack(
>           [
                self.perform_erosion(
                    tensorflow_get_item(
                        pred, (slice(None, None, None), slice(i, i + 1, None))
                    ),
                    where(
                        target == i,
                        tensor(1, device=target.device, dtype=target.dtype),
                        tensor(0, device=target.device, dtype=target.dtype),
                    ),
                )
                for i in range(tensorflow_size_frnt_(pred, 1))
            ]
        )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <range_iterator object at 0x7efd19bf94a0>

        [
>           self.perform_erosion(
                tensorflow_get_item(
                    pred, (slice(None, None, None), slice(i, i + 1, None))
                ),
                where(
                    target == i,
                    tensor(1, device=target.device, dtype=target.dtype),
                    tensor(0, device=target.device, dtype=target.dtype),
                ),
            )
            for i in range(tensorflow_size_frnt_(pred, 1))
        ]
    )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
pred = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=float32, numpy=
array([[[[[ 5.25539756e-01,  1.24999380e+00,  2.92687464e+...007e-01, -2.06564471e-01, ...,
           -1.20149636e+00, -8.08193326e-01, -5.38447797e-01]]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=int64, numpy=
array([[[[[1, 1, 1, ..., 1, 0, 1],
          [1, 1, 1, ..., ...     ...,
          [1, 0, 0, ..., 0, 1, 1],
          [1, 0, 1, ..., 0, 0, 1],
          [1, 1, 0, ..., 1, 1, 1]]]]])>

    def perform_erosion(self, pred, target):
        from ..core._backend import as_tensor
        from ..core._backend import zeros_like
        from ..core._backend import where
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_ones_like_v_0p4p0_and_above_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
    
        bound = (pred - target) ** 2
        kernel = as_tensor(self.kernel, device=pred.device, dtype=pred.dtype)
        eroded = zeros_like(bound, device=pred.device, dtype=pred.dtype)
        mask = tensorflow_ones_like_v_0p4p0_and_above_frnt(
            bound, device=pred.device, dtype=tf.bool
        )
        padding = (tensorflow_size_frnt_(kernel, -1) - 1) // 2
        for k in range(self.k):
>           dilation = self.conv(bound, weight=kernel, padding=padding, groups=1)
E           TypeError: Exception encountered when calling tensorflow_HausdorffERLoss3D.call().
E           
E           [1mtensorflow_conv3d_frnt() got multiple values for argument 'weight'[0m
E           
E           Arguments received by tensorflow_HausdorffERLoss3D.call():
E             â€¢ pred=tf.Tensor(shape=(5, 3, 20, 20, 20), dtype=float32)
E             â€¢ target=tf.Tensor(shape=(5, 1, 20, 20, 20), dtype=int64)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:86: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.losses.HausdorffERLoss3D
________________________________________________________________________________ test_MS_SSIMLoss[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_MS_SSIMLoss(target_framework, mode, backend_compile):
        print("kornia.losses.MS_SSIMLoss")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledMS_SSIMLoss = ivy.transpile(kornia.losses.MS_SSIMLoss, source="torch", target=target_framework)
    
        torch_loss_fn = kornia.losses.MS_SSIMLoss()
        transpiled_loss_fn = TranspiledMS_SSIMLoss()
    
        torch_args = (
            torch.rand(1, 3, 5, 5),
            torch.rand(1, 3, 5, 5),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_res = torch_loss_fn(*torch_args)
>       transpiled_res = transpiled_loss_fn(*transpiled_args)

kornia/test_losses.py:546: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss()
args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.44742608, 0.8455983 , 0.609...0438298 , 0.32980418],
         [0.36074364, 0.74187946, 0.2978813 , 0.8213092 , 0.61560196]]]],
      dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55dc7ece2460, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MS_SSIMLoss(), <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.4...0438298 , 0.32980418],
         [0.36074364, 0.74187946, 0.2978813 , 0.8213092 , 0.61560196]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.44742608, 0.8455983 , 0.609...0438298 , 0.32980418],
         [0.36074364, 0.74187946, 0.2978813 , 0.8213092 , 0.61560196]]]],
      dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MS_SSIMLoss(), <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.4...0438298 , 0.32980418],
         [0.36074364, 0.74187946, 0.2978813 , 0.8213092 , 0.61560196]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.44742608, 0.8455983 , 0.609...0438298 , 0.32980418],
         [0.36074364, 0.74187946, 0.2978813 , 0.8213092 , 0.61560196]]]],
      dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.44742608, 0.8455983 , 0.6093....58203727, 0.3504411 ],
         [0.69495213, 0.5118373 , 0.43328583, 0.14063519, 0.74965525]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (img1, img2)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MS_SSIMLoss(),)
kwargs = {'img1': <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.44742608, 0.8455983...0438298 , 0.32980418],
         [0.36074364, 0.74187946, 0.2978813 , 0.8213092 , 0.61560196]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss()
img1 = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.44742608, 0.8455983 , 0.6093....58203727, 0.3504411 ],
         [0.69495213, 0.5118373 , 0.43328583, 0.14063519, 0.74965525]]]],
      dtype=float32)>
img2 = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.8803826 , 0.49994957, 0.477697  , 0.03566438, 0.7624....0438298 , 0.32980418],
         [0.36074364, 0.74187946, 0.2978813 , 0.8213092 , 0.61560196]]]],
      dtype=float32)>

    def call(self, img1, img2):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_prod_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.loss_functions import (
            tensorflow_l1_loss_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_mean_frnt
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
    
        if not isinstance(img1, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a torch.Tensor. Got {type(img1)}")
        if not isinstance(img2, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Output type is not a torch.Tensor. Got {type(img2)}")
        if not len(tensorflow_shape_frnt_(img1)) == len(tensorflow_shape_frnt_(img2)):
            raise ValueError(
                f"Input shapes should be same. Got {type(img1)} and {type(img2)}."
            )
        g_masks: typing.Any = []
        CH: typing.Any = tensorflow_shape_frnt_(img1)[-3]
>       mux = tensorflow_conv2d_frnt(img1, g_masks, groups=CH, padding=self.pad)

Translated_Outputs/tensorflow_outputs/kornia/losses/ms_ssim.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.44742608, 0.8455983 , 0.6093....58203727, 0.3504411 ],
         [0.69495213, 0.5118373 , 0.43328583, 0.14063519, 0.74965525]]]],
      dtype=float32)>
weight = [], bias = None, stride = 1, padding = 16, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.44742608, 0.8455983 , 0.6093....58203727, 0.3504411 ],
         [0.69495213, 0.5118373 , 0.43328583, 0.14063519, 0.74965525]]]],
      dtype=float32)>
weight = [], bias = None, stride = 1, padding = [(16, 16), (16, 16)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.44742608, 0.8455983 , 0.609...   [0.69495213, 0.5118373 , 0.43328583, 0.14063519, 0.74965525]]]],
      dtype=float32)>, [], 1, [(16, 16), (16, 16)])
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7efd19c67eb0>
tensorflow_get_item = <function tensorflow_get_item at 0x7efd1a2f70a0>, DATA_FORMAT = 'channels_first', value_map = {'NHWC': 'NCHW', 'NSC': 'NCS', 'channel_last': 'channel_first'}

    @functools.wraps(fn)
    def transpose_wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
        from ..functional.backends.tensorflow.general import tensorflow_get_item
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        if DATA_FORMAT == "channels_first":
            value_map = {"channel_last": "channel_first", "NHWC": "NCHW", "NSC": "NCS"}
            if "data_format" in kwargs and kwargs["data_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "data_format",
                    tensorflow_get_item(value_map, kwargs["data_format"]),
                )
            if "filter_format" in kwargs and kwargs["filter_format"] in value_map:
                kwargs = tensorflow_set_item_bknd(
                    kwargs,
                    "filter_format",
                    tensorflow_get_item(value_map, kwargs["filter_format"]),
                )
            os.environ = tensorflow_set_item_bknd(
                os.environ, "DATA_FORMAT", "channels_last"
            )
>       res = fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.44742608, 0.8455983 , 0.609...      dtype=float32)>, <tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)>, 1, [(16, 16), (16, 16)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7efd19c64ca0>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7efd19c67eb0>, tensorflow_asarray = <function tensorflow_asarray at 0x7efd19d6b010>
tensorflow_get_item = <function tensorflow_get_item at 0x7efd1a2f70a0>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.08078367, 0.677403  , 0.44742608, 0.8455983 , 0.6093....58203727, 0.3504411 ],
         [0.69495213, 0.5118373 , 0.43328583, 0.14063519, 0.74965525]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)>, strides = 1, padding = [(16, 16), (16, 16)]

    @tensorflow_handle_transpose_in_input_and_output_for_functions
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
>           filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
E           tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_MS_SSIMLoss.call().
E           
E           [1m{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:CPU:0}} transpose expects a vector of size 1. But input(1) is a vector of size 4 [Op:Transpose][0m
E           
E           Arguments received by tensorflow_MS_SSIMLoss.call():
E             â€¢ img1=tf.Tensor(shape=(1, 3, 5, 5), dtype=float32)
E             â€¢ img2=tf.Tensor(shape=(1, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:164: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.losses.MS_SSIMLoss
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-14 04:23:40.513645: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at transpose_op.cc:142 : INVALID_ARGUMENT: transpose expects a vector of size 1. But input(1) is a vector of size 4
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_losses.py::test_HausdorffERLoss[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_HausdorffERLoss.call().
FAILED kornia/test_losses.py::test_HausdorffERLoss3D[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_HausdorffERLoss3D.call().
FAILED kornia/test_losses.py::test_MS_SSIMLoss[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_MS_SSIMLoss.call().
=============================================================================== 3 failed, 32 passed in 558.45s (0:09:18) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 14 items

kornia/test_feature4.py FFFFF..FFFF.FF                                                                                                                                                           [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________________ test_SIFTFeature[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SIFTFeature(target_framework, mode, backend_compile):
        print("kornia.feature.SIFTFeature")
    
        if backend_compile:
            pytest.skip()
    
        import os
        flag = os.environ.get("APPLY_TRANSPOSE_OPTIMIZATION")
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = "false"
    
        TranspiledSIFTFeature = ivy.transpile(kornia.feature.SIFTFeature, source="torch", target=target_framework)
    
>       os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = flag

kornia/test_feature4.py:29: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = environ({'COLUMNS': '200', 'HOSTNAME': '7fd160925cda', 'IVY_KEY': 'b137n3ADsDjns64kPbdfFUgDWwrWadHYOZjmu7mF-fY=', 'PWD...ivy-integration-tests/.ivy', 'USE_NATIVE_KERAS_LAYERS': 'true', 'PROFILE_S2S': 'false', 'CONV_BLOCK_DETECTED': 'true'})
key = b'APPLY_TRANSPOSE_OPTIMIZATION', value = None

    def __setitem__(self, key, value):
        key = self.encodekey(key)
>       value = self.encodevalue(value)

/opt/miniconda/envs/multienv/lib/python3.10/os.py:684: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

value = None

    def encode(value):
        if not isinstance(value, str):
>           raise TypeError("str expected, not %s" % type(value).__name__)
E           TypeError: str expected, not NoneType

/opt/miniconda/envs/multienv/lib/python3.10/os.py:756: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SIFTFeature
___________________________________________________________________________ test_SIFTFeatureScaleSpace[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SIFTFeatureScaleSpace(target_framework, mode, backend_compile):
        print("kornia.feature.SIFTFeatureScaleSpace")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSIFTFeatureScaleSpace = ivy.transpile(kornia.feature.SIFTFeatureScaleSpace, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.SIFTFeatureScaleSpace(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSIFTFeatureScaleSpace(num_features=5000)(transpiled_x)

kornia/test_feature4.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_py...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.117913...
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7fb117ad22b0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_p...,
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_py...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.117913...
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_p...,
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_py...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.117913...
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.1179134...],
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (img, mask=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_p..._bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0...,
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTFeatureScaleSpace(
  (detector): tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_py...ng_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), patch_size=41, grayscale_descriptor='True)
)
img = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.1179134...],
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>
mask = None

    def call(self, img, mask=None):
        from .laf import tensorflow_scale_laf
    
>       lafs, responses = self.detector(img, mask)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:464: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...ctor=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), aff=tensorflow_PassLAF())
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.117913...     [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>, None)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7fb117794210, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...ctor=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), aff=tensorflow_PassLAF())
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.117913...     [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>, None)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...ctor=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), aff=tensorflow_PassLAF())
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.117913...     [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>, None)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (img, mask=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...ctor=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), aff=tensorflow_PassLAF())
img = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.1179134...],
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>
mask = None

    def call(self, img, mask=None):
>       responses, lafs = self.detect(img, self.num_features, mask)

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:276: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=5000, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...ctor=tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08)), aff=tensorflow_PassLAF())
img = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.1179134...],
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>
num_feats = 5000, mask = None

    def detect(self, img, num_feats, mask=None):
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.comparison_ops import (
            tensorflow_topk_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from .laf import tensorflow_laf_is_inside_image
        from ..core._backend import eye
        from ..core._backend import concatenate
    
        dev: typing.Any = img.device
        dtype: typing.Any = img.dtype
        sigmas: typing.Any
>       sp, sigmas, _ = self.scale_pyr(img)

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=32, extra_levels=3, border=15, sigma_step=1.2599210498948732, double_image=True)
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.117913...
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7fb117794cf0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ls.py', lineno=117, function='error_handler', code_context=['            return fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=32, extra_levels=3, border=15, sigma_step=1.2599210498948732, double_image=True), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.117913...
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=32, extra_levels=3, border=15, sigma_step=1.2599210498948732, double_image=True), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.117913...
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=32, extra_levels=3, border=15, sigma_step=1.2599210498948732, double_image=True)
x = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.1179134...],
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>

    def call(self, x):
        from ...filters.gaussian import tensorflow_gaussian_blur2d
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from ...core._backend import ones
        from ...core._backend import stack
    
        bs, _, _, _ = tensorflow_size_frnt_(x)
>       cur_level, cur_sigma, pixel_distance = self.get_first_level(x)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:115: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScalePyramid(n_levels=3, init_sigma=1.6, min_size=32, extra_levels=3, border=15, sigma_step=1.2599210498948732, double_image=True)
input = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.1179134...],
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>

    def get_first_level(self, input):
        from ...filters.gaussian import tensorflow_gaussian_blur2d
    
        pixel_distance = 1.0
        cur_sigma = 0.5
        if self.double_image:
>           x = tensorflow_upscale_double(input)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 1, 256, 256), dtype=float32, numpy=
array([[[[0.48361278, 0.49746472, 0.5183874 , ..., 0.1179134...],
         [0.23474073, 0.7287229 , 0.36490583, ..., 0.6938143 ,
          0.48113608, 0.5558032 ]]]], dtype=float32)>

    def tensorflow_upscale_double(x):
        from ...core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...core._backend import zeros
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(x)
        tensorflow_KORNIA_CHECK_SHAPE(x, ["*", "H", "W"])
        double_shape = tensorflow_shape_frnt_(x)[:-2] + (
            tensorflow_shape_frnt_(x)[-2] * 2,
            tensorflow_shape_frnt_(x)[-1] * 2,
        )
        upscaled = zeros(double_shape, device=x.device, dtype=x.dtype)
        upscaled = tensorflow_set_item_bknd(
            upscaled, (..., slice(None, None, 2), slice(None, None, 2)), x
        )
>       upscaled[..., ::2, 1::2] = tensorflow_set_item_bknd(
            upscaled[..., ::2, 1::2],
            (..., slice(None, -1, None)),
            (upscaled[..., ::2, ::2][..., :-1] + upscaled[..., ::2, 2::2]) / 2,
        )
E       TypeError: Exception encountered when calling tensorflow_ScalePyramid.call().
E       
E       [1m'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment[0m
E       
E       Arguments received by tensorflow_ScalePyramid.call():
E         â€¢ x=tf.Tensor(shape=(1, 1, 256, 256), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:569: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SIFTFeatureScaleSpace
_____________________________________________________________________________ test_GFTTAffNetHardNet[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_GFTTAffNetHardNet(target_framework, mode, backend_compile):
        print("kornia.feature.GFTTAffNetHardNet")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledGFTTAffNetHardNet = ivy.transpile(kornia.feature.GFTTAffNetHardNet, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.GFTTAffNetHardNet(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledGFTTAffNetHardNet(num_features=5000)(transpiled_x)

kornia/test_feature4.py:69: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_GFTTAffNetHardNet object at 0x7fb121f9cd90>, num_features = 5000, upright = False, device = 'cpu'
config = {'nms_size': 15, 'pyramid_levels': 4, 's_mult': 22.0, 'scale_factor_levels': 1.4142135623730951, ...}

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        config=tensorflow_get_default_detector_config(),
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .scale_space_detector import tensorflow_MultiResolutionDetector
        from .responses import tensorflow_CornerGFTT
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .affine_shape import tensorflow_LAFAffNetShapeEstimator
    
        detector = tensorflow_to_frnt_(
            tensorflow_MultiResolutionDetector(
                tensorflow_CornerGFTT(),
                num_features,
                config,
                ori_module=tensorflow_PassLAF()
                if upright
                else tensorflow_LAFOrienter(19),
>               aff_module=tensorflow_LAFAffNetShapeEstimator(True).eval(),
            ),
            device,
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:937: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
pretrained = True, preserve_orientation = True

    @tensorflow_store_config_info
    def __init__(self, pretrained=False, preserve_orientation=True):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ...torch.nn.modules.activation import tensorflow_Tanh
        from ...torch.nn.modules.pooling import tensorflow_AdaptiveAvgPool2d
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            preserve_orientation=preserve_orientation,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=32,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.25),
            KerasConv2D(
                in_channels=64,
                filters=3,
                kernel_size=8,
                strides=1,
                padding=0,
                use_bias=True,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            tensorflow_Tanh(),
            tensorflow_AdaptiveAvgPool2d(1),
        )
        self.patch_size = 32
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["affnet"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:208: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.GFTTAffNetHardNet
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/ducha-aiki/affnet/raw/master/pretrained/AffNet.pth" to /root/.cache/torch/hub/checkpoints/AffNet.pth

  0%|          | 0.00/332k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332k/332k [00:00<00:00, 19.0MB/s]
Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /root/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth

  0%|          | 0.00/5.10M [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.10M/5.10M [00:00<00:00, 122MB/s]
____________________________________________________________________________ test_KeyNetAffNetHardNet[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KeyNetAffNetHardNet(target_framework, mode, backend_compile):
        print("kornia.feature.KeyNetAffNetHardNet")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetAffNetHardNet = ivy.transpile(kornia.feature.KeyNetAffNetHardNet, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.KeyNetAffNetHardNet(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledKeyNetAffNetHardNet(num_features=5000)(transpiled_x)

kornia/test_feature4.py:86: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_KeyNetAffNetHardNet object at 0x7fb11cf1b850>, num_features = 5000, upright = False, device = 'cpu'
scale_laf = 1.0

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        scale_laf=1.0,
    ):
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .orientation import tensorflow_OriNet
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .keynet import tensorflow_KeyNetDetector
        from .affine_shape import tensorflow_LAFAffNetShapeEstimator
    
        ori_module = (
            tensorflow_PassLAF()
            if upright
>           else tensorflow_LAFOrienter(angle_detector=tensorflow_OriNet(True))
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:966: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
pretrained = True, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, pretrained=False, eps=1e-08):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ...torch.nn.modules.activation import tensorflow_Tanh
        from ...torch.nn.modules.pooling import tensorflow_AdaptiveAvgPool2d
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=32,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.25),
            KerasConv2D(
                in_channels=64,
                filters=2,
                kernel_size=8,
                strides=1,
                padding=1,
                use_bias=True,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            tensorflow_Tanh(),
            tensorflow_AdaptiveAvgPool2d(1),
        )
        self.eps = eps
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["orinet"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:1040: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.KeyNetAffNetHardNet
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/ducha-aiki/affnet/raw/master/pretrained/OriNet.pth" to /root/.cache/torch/hub/checkpoints/OriNet.pth

  0%|          | 0.00/316k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 316k/316k [00:00<00:00, 18.2MB/s]
Downloading: "https://github.com/axelBarroso/Key.Net-Pytorch/raw/main/model/weights/keynet_pytorch.pth" to /root/.cache/torch/hub/checkpoints/keynet_pytorch.pth

  0%|          | 0.00/78.0k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78.0k/78.0k [00:00<00:00, 11.4MB/s]
_______________________________________________________________________________ test_KeyNetHardNet[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KeyNetHardNet(target_framework, mode, backend_compile):
        print("kornia.feature.KeyNetHardNet")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetHardNet = ivy.transpile(kornia.feature.KeyNetHardNet, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.KeyNetHardNet(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledKeyNetHardNet(num_features=5000)(transpiled_x)

kornia/test_feature4.py:103: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_KeyNetHardNet object at 0x7fb1169340a0>, num_features = 5000, upright = False, device = 'cpu'
scale_laf = 1.0

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        scale_laf=1.0,
    ):
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .orientation import tensorflow_OriNet
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .keynet import tensorflow_KeyNetDetector
    
        ori_module = (
            tensorflow_PassLAF()
            if upright
>           else tensorflow_LAFOrienter(angle_detector=tensorflow_OriNet(True))
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:965: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
pretrained = True, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, pretrained=False, eps=1e-08):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ...torch.nn.modules.activation import tensorflow_Tanh
        from ...torch.nn.modules.pooling import tensorflow_AdaptiveAvgPool2d
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=32,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.25),
            KerasConv2D(
                in_channels=64,
                filters=2,
                kernel_size=8,
                strides=1,
                padding=1,
                use_bias=True,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            tensorflow_Tanh(),
            tensorflow_AdaptiveAvgPool2d(1),
        )
        self.eps = eps
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["orinet"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:1040: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.KeyNetHardNet
____________________________________________________________________________ test_LocalFeatureMatcher[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LocalFeatureMatcher(target_framework, mode, backend_compile):
        print("kornia.feature.LocalFeatureMatcher")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledGFTTAffNetHardNet = ivy.transpile(kornia.feature.GFTTAffNetHardNet, source="torch", target=target_framework)
        TranspiledDescriptorMatcher = ivy.transpile(kornia.feature.DescriptorMatcher, source="torch", target=target_framework)
        TranspiledLocalFeatureMatcher = ivy.transpile(kornia.feature.LocalFeatureMatcher, source="torch", target=target_framework)
    
        data = {
            "image0": torch.rand(1, 1, 320, 200),
            "image1": torch.rand(1, 1, 128, 128),
        }
        torch_local_feature = kornia.feature.GFTTAffNetHardNet(10)
        torch_matcher = kornia.feature.DescriptorMatcher('snn', 0.8)
        torch_out = kornia.feature.LocalFeatureMatcher(torch_local_feature, torch_matcher)(data)
    
>       transpiled_local_feature = TranspiledGFTTAffNetHardNet(10)

kornia/test_feature4.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_GFTTAffNetHardNet object at 0x7fb116bb0d90>, num_features = 10, upright = False, device = 'cpu'
config = {'nms_size': 15, 'pyramid_levels': 4, 's_mult': 22.0, 'scale_factor_levels': 1.4142135623730951, ...}

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        config=tensorflow_get_default_detector_config(),
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .scale_space_detector import tensorflow_MultiResolutionDetector
        from .responses import tensorflow_CornerGFTT
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .affine_shape import tensorflow_LAFAffNetShapeEstimator
    
        detector = tensorflow_to_frnt_(
            tensorflow_MultiResolutionDetector(
                tensorflow_CornerGFTT(),
                num_features,
                config,
                ori_module=tensorflow_PassLAF()
                if upright
                else tensorflow_LAFOrienter(19),
>               aff_module=tensorflow_LAFAffNetShapeEstimator(True).eval(),
            ),
            device,
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:939: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LAFAffNetShapeEstimator(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNor...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
pretrained = True, preserve_orientation = True

    @tensorflow_store_config_info
    def __init__(self, pretrained=False, preserve_orientation=True):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ...torch.nn.modules.activation import tensorflow_Tanh
        from ...torch.nn.modules.pooling import tensorflow_AdaptiveAvgPool2d
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            preserve_orientation=preserve_orientation,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=32,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.25),
            KerasConv2D(
                in_channels=64,
                filters=3,
                kernel_size=8,
                strides=1,
                padding=0,
                use_bias=True,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_first",
            ),
            tensorflow_Tanh(),
            tensorflow_AdaptiveAvgPool2d(1),
        )
        self.patch_size = 32
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["affnet"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:208: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LocalFeatureMatcher
_____________________________________________________________________________ test_LightGlueMatcher[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LightGlueMatcher(target_framework, mode, backend_compile):
        print("kornia.feature.LightGlueMatcher")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLightGlueMatcher = ivy.transpile(kornia.feature.LightGlueMatcher, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(2, 128),
            torch.rand(5, 128),
            torch.rand(1, 2, 2, 3),
            torch.rand(1, 5, 2, 3),
        )
        torch_out = kornia.feature.LightGlueMatcher('disk')(*torch_args)
    
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
>       transpiled_out = TranspiledLightGlueMatcher('disk')(*transpiled_args)

kornia/test_feature4.py:192: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlueMatcher(), feature_name = 'disk', params = {}

    def __init__(self, feature_name="disk", params={}):
        from .lightglue import tensorflow_LightGlue
    
        feature_name_: typing.Any = feature_name.lower()
        super().__init__(feature_name_)
        self.feature_name = feature_name_
        self.params = params
>       self.matcher = tensorflow_LightGlue(self.feature_name, **params)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:1497: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): KerasDense()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), args = ('disk',), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): KerasDense()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), features = 'disk', conf_ = {}
tensorflow_KORNIA_CHECK = <function tensorflow_KORNIA_CHECK at 0x7fb115d67760>, tensorflow_get_item = <function tensorflow_get_item at 0x7fb115df49d0>
tensorflow_Identity = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.linear.tensorflow_Identity'>
ModuleList = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.container.tensorflow_ModuleList'>
KerasDense = <class 'Translated_Outputs.tensorflow_outputs.tensorflow__stateful_layers.KerasDense'>
conf = namespace(name='lightglue', input_dim=128, descriptor_dim=256, add_scale_ori=False, add_laf=False, scale_coef=1.0, n_l...=4, flash=True, mp=False, depth_confidence=0.95, width_confidence=0.99, filter_threshold=0.1, weights='disk_lightglue')
k = 'input_dim'

    @tensorflow_store_config_info
    def __init__(self, features="superpoint", **conf_):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...torch.nn.modules.linear import tensorflow_Identity
        from ..core._backend import ModuleList
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            features=features,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.conf = conf = SimpleNamespace(**{**self.default_conf, **conf_})
        if features is not None:
            tensorflow_KORNIA_CHECK(
                features in list(self.features.keys()), "Features keys are wrong"
            )
            for k, v in tensorflow_get_item(self.features, features).items():
                setattr(conf, k, v)
        tensorflow_KORNIA_CHECK(not (self.conf.add_scale_ori and self.conf.add_laf))
        if conf.input_dim != conf.descriptor_dim:
            self.input_proj = KerasDense(
                in_features=conf.input_dim, units=conf.descriptor_dim, use_bias=True
            )
        else:
            self.input_proj = tensorflow_Identity()
        head_dim = conf.descriptor_dim // conf.num_heads
        self.posenc = tensorflow_LearnableFourierPositionalEncoding(
            2 + 2 * conf.add_scale_ori + 4 * conf.add_laf, head_dim, head_dim
        )
        h, n, d = conf.num_heads, conf.n_layers, conf.descriptor_dim
        ag__result_list_0 = []
        for _ in range(n):
>           res = tensorflow_TransformerLayer(d, h, conf.flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:3173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @tensorflow_store_config_info
    def __init__(self, *args, **kwargs):
        self.super___init__(
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.self_attn = tensorflow_SelfBlock(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:2224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), embed_dim = 256, num_heads = 4, flash = True, bias = True

    @tensorflow_store_config_info
    def __init__(self, embed_dim, num_heads, flash=False, bias=True):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.normalization import tensorflow_LayerNorm
        from ...torch.nn.modules.activation import tensorflow_GELU
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            embed_dim,
            num_heads,
            flash=flash,
            bias=bias,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        tensorflow_KORNIA_CHECK(
            self.embed_dim % num_heads == 0,
            "Embed dimension should be dividable by num_heads",
        )
        self.head_dim = self.embed_dim // num_heads
        self.Wqkv = KerasDense(
            in_features=embed_dim, units=3 * embed_dim, use_bias=bias
        )
>       self.inner_attn = tensorflow_Attention(flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:1362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), allow_flash = True

    @tensorflow_store_config_info
    def __init__(self, allow_flash):
        from ...torch.backends.cuda.__init__ import tensorflow_enable_flash_sdp
    
        self.super___init__(
            allow_flash,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if allow_flash and not FLASH_AVAILABLE:
            warnings.warn(
                "FlashAttention is not available. For optimal speed, consider installing torch >= 2.0 or flash-attn.",
                stacklevel=2,
            )
        self.enable_flash = allow_flash and FLASH_AVAILABLE
>       self.has_sdp = hasattr(torch.nn.functional, "scaled_dot_product_attention")
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:911: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LightGlueMatcher
Loaded LightGlue model
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/cvg/LightGlue/releases/download/v0.1_arxiv/disk_lightglue.pth" to /root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth

  0%|          | 0.00/45.4M [00:00<?, ?B/s]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 24.6M/45.4M [00:00<00:00, 258MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45.4M/45.4M [00:00<00:00, 303MB/s]
_________________________________________________________________________________ test_LightGlue[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LightGlue(target_framework, mode, backend_compile):
        print("kornia.feature.LightGlue")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLightGlue = ivy.transpile(kornia.feature.LightGlue, source="torch", target=target_framework)
    
        data = {
            "image0": {
                "keypoints": torch.rand(1, 100, 2),
                "descriptors": torch.rand(1, 100, 256),
                "image_size": torch.tensor([[640, 480]]),
            },
            "image1": {
                "keypoints": torch.rand(1, 120, 2),
                "descriptors": torch.rand(1, 120, 256),
                "image_size": torch.tensor([[640, 480]]),
            }
        }
        torch_out = kornia.feature.LightGlue(features='superpoint')(data)
    
        transpiled_data = _nest_torch_tensor_to_new_framework(data, target_framework)
>       transpiled_out = TranspiledLightGlue(features='superpoint')(transpiled_data)

kornia/test_feature4.py:220: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): tensorflow_Identity()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), args = ()
kwargs = {'features': 'superpoint'}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): tensorflow_Identity()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), features = 'superpoint', conf_ = {}
tensorflow_KORNIA_CHECK = <function tensorflow_KORNIA_CHECK at 0x7fb115d67760>, tensorflow_get_item = <function tensorflow_get_item at 0x7fb115df49d0>
tensorflow_Identity = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.linear.tensorflow_Identity'>
ModuleList = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.container.tensorflow_ModuleList'>
KerasDense = <class 'Translated_Outputs.tensorflow_outputs.tensorflow__stateful_layers.KerasDense'>
conf = namespace(name='lightglue', input_dim=256, descriptor_dim=256, add_scale_ori=False, add_laf=False, scale_coef=1.0, n_l...ash=True, mp=False, depth_confidence=0.95, width_confidence=0.99, filter_threshold=0.1, weights='superpoint_lightglue')
k = 'input_dim'

    @tensorflow_store_config_info
    def __init__(self, features="superpoint", **conf_):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...torch.nn.modules.linear import tensorflow_Identity
        from ..core._backend import ModuleList
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            features=features,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.conf = conf = SimpleNamespace(**{**self.default_conf, **conf_})
        if features is not None:
            tensorflow_KORNIA_CHECK(
                features in list(self.features.keys()), "Features keys are wrong"
            )
            for k, v in tensorflow_get_item(self.features, features).items():
                setattr(conf, k, v)
        tensorflow_KORNIA_CHECK(not (self.conf.add_scale_ori and self.conf.add_laf))
        if conf.input_dim != conf.descriptor_dim:
            self.input_proj = KerasDense(
                in_features=conf.input_dim, units=conf.descriptor_dim, use_bias=True
            )
        else:
            self.input_proj = tensorflow_Identity()
        head_dim = conf.descriptor_dim // conf.num_heads
        self.posenc = tensorflow_LearnableFourierPositionalEncoding(
            2 + 2 * conf.add_scale_ori + 4 * conf.add_laf, head_dim, head_dim
        )
        h, n, d = conf.num_heads, conf.n_layers, conf.descriptor_dim
        ag__result_list_0 = []
        for _ in range(n):
>           res = tensorflow_TransformerLayer(d, h, conf.flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:3173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @tensorflow_store_config_info
    def __init__(self, *args, **kwargs):
        self.super___init__(
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.self_attn = tensorflow_SelfBlock(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:2224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), embed_dim = 256, num_heads = 4, flash = True, bias = True

    @tensorflow_store_config_info
    def __init__(self, embed_dim, num_heads, flash=False, bias=True):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.normalization import tensorflow_LayerNorm
        from ...torch.nn.modules.activation import tensorflow_GELU
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            embed_dim,
            num_heads,
            flash=flash,
            bias=bias,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        tensorflow_KORNIA_CHECK(
            self.embed_dim % num_heads == 0,
            "Embed dimension should be dividable by num_heads",
        )
        self.head_dim = self.embed_dim // num_heads
        self.Wqkv = KerasDense(
            in_features=embed_dim, units=3 * embed_dim, use_bias=bias
        )
>       self.inner_attn = tensorflow_Attention(flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:1362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), allow_flash = True

    @tensorflow_store_config_info
    def __init__(self, allow_flash):
        from ...torch.backends.cuda.__init__ import tensorflow_enable_flash_sdp
    
        self.super___init__(
            allow_flash,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if allow_flash and not FLASH_AVAILABLE:
            warnings.warn(
                "FlashAttention is not available. For optimal speed, consider installing torch >= 2.0 or flash-attn.",
                stacklevel=2,
            )
        self.enable_flash = allow_flash and FLASH_AVAILABLE
>       self.has_sdp = hasattr(torch.nn.functional, "scaled_dot_product_attention")
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:911: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LightGlue
Loaded LightGlue model
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/cvg/LightGlue/releases/download/v0.1_arxiv/superpoint_lightglue.pth" to /root/.cache/torch/hub/checkpoints/superpoint_lightglue_v0-1_arxiv-pth

  0%|          | 0.00/45.3M [00:00<?, ?B/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 24.9M/45.3M [00:00<00:00, 261MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45.3M/45.3M [00:00<00:00, 299MB/s]
___________________________________________________________________________________ test_LoFTR[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LoFTR(target_framework, mode, backend_compile):
        print("kornia.feature.LoFTR")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledLoFTR = ivy.transpile(kornia.feature.LoFTR, source="torch", target=target_framework)

kornia/test_feature4.py:231: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.feature.loftr.loftr.LoFTR'>, source = 'torch', target = 'tensorflow', reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, line 77)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LoFTR
_________________________________________________________________________ test_PatchAffineShapeEstimator[tensorflow-s2s-False] _________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_PatchAffineShapeEstimator(target_framework, mode, backend_compile):
        print("kornia.feature.PatchAffineShapeEstimator")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledPatchAffineShapeEstimator = ivy.transpile(kornia.feature.PatchAffineShapeEstimator, source="torch", target=target_framework)
    
        patch = torch.rand(1, 1, 19, 19)
        torch_out = kornia.feature.PatchAffineShapeEstimator()(patch)
    
        transpiled_patch = _nest_torch_tensor_to_new_framework(patch, target_framework)
>       transpiled_out = TranspiledPatchAffineShapeEstimator()(transpiled_patch)

kornia/test_feature4.py:273: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_PatchAffineShapeEstimator' object has no attribute 'eps'") raised in repr()] tensorflow_PatchAffineShapeEstimator object at 0x7fb11d2a03a0>, patch_size = 19
eps = 1e-10

    def __init__(self, patch_size=19, eps=1e-10):
        from ..filters.sobel import tensorflow_sobel
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size: typing.Any = patch_size
>       self.gradient: typing.Any = tensorflow_sobel.SpatialGradient("sobel", 1)
E       AttributeError: 'function' object has no attribute 'SpatialGradient'

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:53: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.PatchAffineShapeEstimator
__________________________________________________________________________ test_LAFAffineShapeEstimator[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LAFAffineShapeEstimator(target_framework, mode, backend_compile):
        print("kornia.feature.LAFAffineShapeEstimator")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLAFAffineShapeEstimator = ivy.transpile(kornia.feature.LAFAffineShapeEstimator, source="torch", target=target_framework)
    
        laf = torch.rand(1, 2, 2, 3)
        img = torch.rand(1, 1, 32, 32)
        torch_out = kornia.feature.LAFAffineShapeEstimator()(laf, img)
    
        transpiled_laf = _nest_torch_tensor_to_new_framework(laf, target_framework)
        transpiled_img = _nest_torch_tensor_to_new_framework(img, target_framework)
>       transpiled_out = TranspiledLAFAffineShapeEstimator()(transpiled_laf, transpiled_img)

kornia/test_feature4.py:292: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFAffineShapeEstimator' object has no attribute 'affine_shape_detector'") raised in repr()] tensorflow_LAFAffineShapeEstimator object at 0x7fb11d1a93f0>
patch_size = 32, affine_shape_detector = None, preserve_orientation = True

    def __init__(
        self, patch_size=32, affine_shape_detector=None, preserve_orientation=True
    ):
        self.super___init__(
            patch_size=patch_size,
            affine_shape_detector=affine_shape_detector,
            preserve_orientation=preserve_orientation,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.affine_shape_detector = (
            affine_shape_detector
>           or tensorflow_PatchAffineShapeEstimator(self.patch_size)
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:484: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_PatchAffineShapeEstimator' object has no attribute 'eps'") raised in repr()] tensorflow_PatchAffineShapeEstimator object at 0x7fb11d1aa590>, patch_size = 32
eps = 1e-10

    def __init__(self, patch_size=19, eps=1e-10):
        from ..filters.sobel import tensorflow_sobel
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size: typing.Any = patch_size
>       self.gradient: typing.Any = tensorflow_sobel.SpatialGradient("sobel", 1)
E       AttributeError: 'function' object has no attribute 'SpatialGradient'

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:53: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LAFAffineShapeEstimator
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_feature4.py::test_SIFTFeature[tensorflow-s2s-False] - TypeError: str expected, not NoneType
FAILED kornia/test_feature4.py::test_SIFTFeatureScaleSpace[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_ScalePyramid.call().
FAILED kornia/test_feature4.py::test_GFTTAffNetHardNet[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature4.py::test_KeyNetAffNetHardNet[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature4.py::test_KeyNetHardNet[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature4.py::test_LocalFeatureMatcher[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature4.py::test_LightGlueMatcher[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature4.py::test_LightGlue[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature4.py::test_LoFTR[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, line 77)
FAILED kornia/test_feature4.py::test_PatchAffineShapeEstimator[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'SpatialGradient'
FAILED kornia/test_feature4.py::test_LAFAffineShapeEstimator[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'SpatialGradient'
============================================================================== 11 failed, 3 passed in 1230.69s (0:20:30) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 19 items

kornia/geometry/test_camera.py ...................                                                                                                                                               [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 19 passed in 188.05s (0:03:08) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 6 items

kornia/geometry/test_depth.py ...FF.                                                                                                                                                             [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________ test_unproject_meshgrid[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_unproject_meshgrid(target_framework, mode, backend_compile):
        trace_args = (
            4,
            4,
            torch.eye(3),
        )
        trace_kwargs = {'normalize_points': False, 'device': 'cpu', 'dtype': torch.float32}
        test_args = (
            5,
            5,
            torch.eye(3),
        )
        test_kwargs = {'normalize_points': False, 'device': 'cpu', 'dtype': torch.float32}
>       _test_function(
            kornia.geometry.depth.unproject_meshgrid,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_depth.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unproject_meshgrid at 0x7fcb3ee89360>, trace_args = (4, 4, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
trace_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, test_args = (5, 5, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
test_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unproject_meshgrid at 0x7fcb3ee89360>, trace_args = (4, 4, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
trace_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, test_args = (5, 5, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
test_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

height = 4, width = 4, camera_matrix = <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>, normalize_points = False
device = 'cpu', dtype = torch.float32

    def tensorflow_unproject_meshgrid(
        height, width, camera_matrix, normalize_points=False, device=None, dtype=None
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ..utils.grid import tensorflow_create_meshgrid
        from .conversions import tensorflow_normalize_points_with_intrinsics
        from .conversions import tensorflow_convert_points_to_homogeneous
        from ..core._backend import normalize
    
        tensorflow_KORNIA_CHECK_SHAPE(camera_matrix, ["3", "3"])
        points_uv: typing.Any = tensorflow_squeeze_frnt_(
>           tensorflow_create_meshgrid(
                height, width, normalized_coordinates=False, device=device, dtype=dtype
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/geometry/depth.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

height = 4, width = 4, normalized_coordinates = False, device = 'cpu', dtype = torch.float32

    def tensorflow_create_meshgrid(
        height, width, normalized_coordinates=True, device=None, dtype=None
    ):
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_linspace_frnt
        from ._compat import tensorflow_torch_meshgrid
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
    
>       xs: typing.Any = tensorflow_linspace_frnt(
            0, width - 1, width, device=device, dtype=dtype
        )

Translated_Outputs/tensorflow_outputs/kornia/utils/grid.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

start = 0, end = 3, steps = 4

    def tensorflow_linspace_frnt(
        start,
        end,
        steps,
        *,
        out=None,
        dtype=None,
        device=None,
        layout=None,
        requires_grad=False,
    ):
        from .dtype import tensorflow_get_default_dtype_frnt
        from ...backends.tensorflow.creation import tensorflow_linspace
    
        dtype = tensorflow_get_default_dtype_frnt() if dtype is None else dtype
>       return tensorflow_linspace(
            start, end, num=steps, dtype=dtype, device=device, out=out
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/creation_ops.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

dtype = torch.float32, args = (0, 3), kwargs = {'device': 'cpu', 'num': 4, 'out': None}, tensorflow_default_dtype_bknd = <function tensorflow_default_dtype_bknd at 0x7fcb3847f5b0>
tensorflow_exists_bknd = <function tensorflow_exists_bknd at 0x7fcb3847ff40>, arr = None

    @functools.wraps(fn)
    def _infer_dtype(*args, dtype=None, **kwargs):
        from .functional.ivy.data_type import tensorflow_default_dtype_bknd
        from .functional.ivy.general import tensorflow_exists_bknd
    
        arr = (
            None
            if tensorflow_exists_bknd(dtype)
            else tensorflow__get_first_array(*args, **kwargs)
        )
        dtype = tensorflow_default_dtype_bknd(dtype=dtype, item=arr, as_native=True)
>       return fn(*args, dtype=dtype, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [0, 3], kwargs = {'device': 'cpu', 'dtype': torch.float32, 'num': 4, 'out': None}, tensorflow_get_item = <function tensorflow_get_item at 0x7fcb38450790>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fcb3847fe20>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7fcb384401f0>
tensorflow_asarray = <function tensorflow_asarray at 0x7fcb38441c60>, num_args = 2
type_hints = mappingproxy(OrderedDict([('start', <Parameter "start: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.pyt..."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['start', 'stop', 'num', 'axis', 'endpoint', 'dtype', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, float], typing.Unio..., float], <class 'int'>, typing.Optional[int], <class 'bool'>, <class 'tensorflow.python.framework.dtypes.DType'>, ...]
device = None, i = 1

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

start = 0, stop = 3, num = 4

    @tensorflow_infer_dtype
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_linspace(
        start: Union[tensorflow.Tensor, tensorflow.Variable, float],
        stop: Union[tensorflow.Tensor, tensorflow.Variable, float],
        /,
        num: int,
        *,
        axis: Optional[int] = None,
        endpoint: bool = True,
        dtype: tensorflow.DType,
        device: Optional[str] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        if axis is None:
            axis = -1
>       start = tensorflow.cast(tensorflow.constant(start), dtype=dtype)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/creation.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(), dtype=int32, numpy=0>,), kwargs = {'dtype': torch.float32}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

type_value = torch.float32

    @tf_export("dtypes.as_dtype", "as_dtype")
    def as_dtype(type_value):
      """Converts the given `type_value` to a `tf.DType`.
    
      Inputs can be existing `tf.DType` objects, a [`DataType`
      enum](https://www.tensorflow.org/code/tensorflow/core/framework/types.proto),
      a string type name, or a
      [`numpy.dtype`](https://numpy.org/doc/stable/reference/generated/numpy.dtype.html).
    
      Examples:
      >>> tf.as_dtype(2)  # Enum value for float64.
      tf.float64
    
      >>> tf.as_dtype('float')
      tf.float32
    
      >>> tf.as_dtype(np.int32)
      tf.int32
    
      Note: `DType` values are interned (i.e. a single instance of each dtype is
      stored in a map). When passed a new `DType` object, `as_dtype` always returns
      the interned value.
    
      Args:
        type_value: A value that can be converted to a `tf.DType` object.
    
      Returns:
        A `DType` corresponding to `type_value`.
    
      Raises:
        TypeError: If `type_value` cannot be converted to a `DType`.
      """
      if isinstance(type_value, DType):
        if type_value._handle_data is None:  # pylint:disable=protected-access
          return _INTERN_TABLE[type_value.as_datatype_enum]
        else:
          return type_value
    
      if isinstance(type_value, np.dtype):
        try:
          return _NP_TO_TF[type_value.type]
        except KeyError:
          pass
    
      try:
        return _ANY_TO_TF[type_value]
      except (KeyError, TypeError):
        # TypeError indicates that type_value is not hashable.
        pass
    
      if hasattr(type_value, "dtype"):
        try:
          return _NP_TO_TF[np.dtype(type_value.dtype).type]
        except (KeyError, TypeError):
          pass
    
      if isinstance(type_value, _dtypes.DType):
        return _INTERN_TABLE[type_value.as_datatype_enum]
    
>     raise TypeError(f"Cannot convert the argument `type_value`: {type_value!r} "
                      "to a TensorFlow DType.")
E     TypeError: Cannot convert the argument `type_value`: torch.float32 to a TensorFlow DType.

/opt/fw/tensorflow/tensorflow/python/framework/dtypes.py:852: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.depth.unproject_meshgrid
_____________________________________________________________________________ test_depth_to_normals[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_depth_to_normals(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 1, 4, 4),
            torch.eye(3)[None],
        )
        trace_kwargs = {'normalize_points': False}
        test_args = (
            torch.rand(1, 1, 5, 5),
            torch.eye(3)[None],
        )
        test_kwargs = {'normalize_points': False}
>       _test_function(
            kornia.geometry.depth.depth_to_normals,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_depth.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function depth_to_normals at 0x7fcb3ee89480>
trace_args = (tensor([[[[0.7996, 0.7834, 0.7605, 0.6786],
          [0.7045, 0.9941, 0.3070, 0.4648],
          [0.5423, 0.7133, 0....          [0.4080, 0.3001, 0.3878, 0.2592]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
trace_kwargs = {'normalize_points': False}
test_args = (tensor([[[[0.3411, 0.4367, 0.0063, 0.6582, 0.3799],
          [0.5250, 0.3548, 0.7689, 0.1290, 0.6911],
          [0....  [0.6207, 0.1964, 0.5778, 0.3826, 0.2968]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
test_kwargs = {'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function depth_to_normals at 0x7fcb3ee89480>
trace_args = (tensor([[[[0.7996, 0.7834, 0.7605, 0.6786],
          [0.7045, 0.9941, 0.3070, 0.4648],
          [0.5423, 0.7133, 0....          [0.4080, 0.3001, 0.3878, 0.2592]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
trace_kwargs = {'normalize_points': False}
test_args = (tensor([[[[0.3411, 0.4367, 0.0063, 0.6582, 0.3799],
          [0.5250, 0.3548, 0.7689, 0.1290, 0.6911],
          [0....  [0.6207, 0.1964, 0.5778, 0.3826, 0.2968]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
test_kwargs = {'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

depth = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[0.79964226, 0.78335136, 0.76053566, 0.67862254],
     ....71329755, 0.16393381, 0.739833  ],
         [0.40801054, 0.3001184 , 0.3877765 , 0.2592165 ]]]],
      dtype=float32)>
camera_matrix = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, normalize_points = False

    def tensorflow_depth_to_normals(depth, camera_matrix, normalize_points=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..filters.sobel import tensorflow_sobel
        from ...ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_cross_frnt,
        )
        from ..core._backend import normalize
    
        if not isinstance(depth, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input depht type is not a Tensor. Got {type(depth)}.")
        if not (
            len(tensorflow_shape_frnt_(depth)) == 4
            and tensorflow_shape_frnt_(depth)[-3] == 1
        ):
            raise ValueError(
                f"Input depth musth have a shape (B, 1, H, W). Got: {tensorflow_shape_frnt_(depth)}"
            )
        if not isinstance(camera_matrix, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(
                f"Input camera_matrix type is not a Tensor. Got {type(camera_matrix)}."
            )
        if not (
            len(tensorflow_shape_frnt_(camera_matrix)) == 3
            and tensorflow_shape_frnt_(camera_matrix)[-2:] == (3, 3)
        ):
            raise ValueError(
                f"Input camera_matrix must have a shape (B, 3, 3). Got: {tensorflow_shape_frnt_(camera_matrix)}."
            )
        xyz: typing.Any = tensorflow_depth_to_3d(depth, camera_matrix, normalize_points)
>       gradients: typing.Any = tensorflow_sobel.spatial_gradient(xyz)
E       AttributeError: 'function' object has no attribute 'spatial_gradient'

Translated_Outputs/tensorflow_outputs/kornia/geometry/depth.py:153: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.depth.depth_to_normals
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/geometry/depth.py:205: DeprecationWarning: Since kornia 0.7.0 the `depth_to_3d` is deprecated in favor of `depth_to_3d_v2`. This function will be replaced with the `depth_to_3d_v2` behaviour, where the that does not require the creation of a meshgrid. The return shape can be not backward compatible between these implementations.
  xyz: Tensor = depth_to_3d(depth, camera_matrix, normalize_points)  # Bx3xHxW
/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/kornia/geometry/depth.py:152: DeprecationWarning: Since kornia 0.7.0 the `tensorflow_depth_to_3d` is deprecated in favor of `depth_to_3d_v2`. This function will be replaced with the `depth_to_3d_v2` behaviour, where the that does not require the creation of a meshgrid. The return shape can be not backward compatible between these implementations.
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_depth.py::test_unproject_meshgrid[tensorflow-s2s-False] - TypeError: Cannot convert the argument `type_value`: torch.float32 to a TensorFlow DType.
FAILED kornia/geometry/test_depth.py::test_depth_to_normals[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'spatial_gradient'
=============================================================================== 2 failed, 4 passed in 164.00s (0:02:44) ================================================================================

