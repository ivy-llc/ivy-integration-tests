========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 13 items

kornia/test_metrics.py ...F.........                                                                                                                                                             [100%]

=============================================================================================== FAILURES ===============================================================================================
__________________________________________________________________________ test_mean_average_precision[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_mean_average_precision(target_framework, mode, backend_compile):
        trace_args = (
            [torch.tensor([[100, 50, 150, 100.]])],
            [torch.tensor([1.])],
            [torch.tensor([0.7])],
            [torch.tensor([[100, 50, 150, 100.]])],
            [torch.tensor([1.])],
            2,
        )
        trace_kwargs = {}
        test_args = (
            [torch.tensor([[50, 25, 75, 50], [100, 50, 150, 100.]])],
            [torch.tensor([1, 2.])],
            [torch.tensor([0.6, 0.8])],
            [torch.tensor([[50, 25, 75, 50], [100, 50, 150, 100.]])],
            [torch.tensor([1, 2.])],
            3,
        )
        kornia.metrics.mean_average_precision(*trace_args)
        kornia.metrics.mean_average_precision(*test_args)
        test_kwargs = {}
    
        # NOTE: this test fails due to the use of dynamic control flow; skipping
>       _test_function(
            kornia.metrics.mean_average_precision,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
            skip=True,
        )

kornia/test_metrics.py:118: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function mean_average_precision at 0x7f73c4e2a050>
trace_args = ([<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shap....,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>], 2)
trace_kwargs = {}
test_args = ([tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], [tensor([0.6000, 0.8000])], [tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], 3)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = True, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function mean_average_precision at 0x7f73c4e2a050>
trace_args = ([<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shap....,  50., 150., 100.]], dtype=float32)>], [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>], 2)
trace_kwargs = {}
test_args = ([tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], [tensor([0.6000, 0.8000])], [tensor([[ 50.,  25.,  75.,  50.],
        [100.,  50., 150., 100.]])], [tensor([1., 2.])], 3)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pred_boxes = [<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>]
pred_labels = [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>], pred_scores = [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.7], dtype=float32)>]
gt_boxes = [<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[100.,  50., 150., 100.]], dtype=float32)>], gt_labels = [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>]
n_classes = 2, threshold = 0.5

    def tensorflow_mean_average_precision(
        pred_boxes, pred_labels, pred_scores, gt_boxes, gt_labels, n_classes, threshold=0.5
    ):
        from ..core._backend import concatenate
        from ..core._backend import tensor
        from ..core._backend import zeros
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.comparison_ops import tensorflow_sort_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .mean_iou import tensorflow_mean_iou
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_max_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_cumsum_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_tolist_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_arange_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
    
        if (
            not len(pred_boxes)
            == len(pred_labels)
            == len(pred_scores)
            == len(gt_boxes)
            == len(gt_labels)
        ):
            raise AssertionError
        gt_images = []
        for i, labels in enumerate(gt_labels):
            gt_images.extend([i] * tensorflow_size_frnt_(labels, 0))
        _gt_boxes = concatenate(gt_boxes, 0)
        _gt_labels = concatenate(gt_labels, 0)
        _gt_images = tensor(gt_images, device=_gt_boxes.device, dtype=tf.int64)
        if (
            not tensorflow_size_frnt_(_gt_images, 0)
            == tensorflow_size_frnt_(_gt_boxes, 0)
            == tensorflow_size_frnt_(_gt_labels, 0)
        ):
            raise AssertionError
        pred_images = []
        for i, labels in enumerate(pred_labels):
            pred_images.extend([i] * tensorflow_size_frnt_(labels, 0))
        _pred_boxes = concatenate(pred_boxes, 0)
        _pred_labels = concatenate(pred_labels, 0)
        _pred_scores = concatenate(pred_scores, 0)
        _pred_images = tensor(pred_images, device=_pred_boxes.device, dtype=tf.int64)
        if (
            not tensorflow_size_frnt_(_pred_images, 0)
            == tensorflow_size_frnt_(_pred_boxes, 0)
            == tensorflow_size_frnt_(_pred_labels, 0)
            == tensorflow_size_frnt_(_pred_scores, 0)
        ):
            raise AssertionError
        average_precisions = zeros(
            n_classes - 1, device=_pred_boxes.device, dtype=_pred_boxes.dtype
        )
        for c in range(1, n_classes):
            gt_class_images = tensorflow_get_item(_gt_images, _gt_labels == c)
            gt_class_boxes = tensorflow_get_item(_gt_boxes, _gt_labels == c)
            gt_class_boxes_detected = zeros(
                tensorflow_size_frnt_(gt_class_images, 0),
                dtype=tf.uint8,
                device=gt_class_images.device,
            )
            pred_class_images = tensorflow_get_item(_pred_images, _pred_labels == c)
            pred_class_boxes = tensorflow_get_item(_pred_boxes, _pred_labels == c)
            pred_class_scores = tensorflow_get_item(_pred_scores, _pred_labels == c)
            n_class_detections = tensorflow_size_frnt_(pred_class_boxes, 0)
            if n_class_detections == 0:
                continue
            pred_class_scores, sort_ind = tensorflow_sort_frnt(
                pred_class_scores, dim=0, descending=True
            )
            pred_class_images = tensorflow_get_item(pred_class_images, sort_ind)
            pred_class_boxes = tensorflow_get_item(pred_class_boxes, sort_ind)
            gt_positives = zeros(
                (n_class_detections,),
                dtype=pred_class_boxes.dtype,
                device=pred_class_boxes.device,
            )
            false_positives = zeros(
                (n_class_detections,),
                dtype=pred_class_boxes.dtype,
                device=pred_class_boxes.device,
            )
            for d in range(n_class_detections):
                this_detection_box = tensorflow_unsqueeze_frnt_(
                    tensorflow_get_item(pred_class_boxes, d), 0
                )
                this_image = tensorflow_get_item(pred_class_images, d)
                object_boxes = tensorflow_get_item(
                    gt_class_boxes, gt_class_images == this_image
                )
                if tensorflow_size_frnt_(object_boxes, 0) == 0:
                    false_positives = tensorflow_set_item_bknd(false_positives, d, 1)
                    continue
>               overlaps = tensorflow_mean_iou.mean_iou_bbox(
                    this_detection_box, object_boxes
                )
E               AttributeError: 'function' object has no attribute 'mean_iou_bbox'

Translated_Outputs/tensorflow_outputs/kornia/metrics/mean_average_precision.py:131: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.metrics.mean_average_precision.mean_average_precision
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_metrics.py::test_mean_average_precision[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'mean_iou_bbox'
=============================================================================== 1 failed, 12 passed in 211.74s (0:03:31) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 57 items

kornia/geometry/test_transform.py ........................F..................F.......F..FFF                                                                                                      [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_upscale_double[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_upscale_double(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 4, 4),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(5, 3, 8, 8),
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.transform.upscale_double,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_transform.py:612: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function upscale_double at 0x7f183dbc7ac0>
trace_args = (tensor([[[[0.0269, 0.3792, 0.6922, 0.2217],
          [0.0462, 0.7121, 0.4492, 0.2805],
          [0.0389, 0.7943, 0...., 0.9486, 0.8567, 0.1550],
          [0.1512, 0.0130, 0.7448, 0.2458],
          [0.7618, 0.3052, 0.2820, 0.5196]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[1.2834e-02, 4.7981e-01, 1.8986e-01, 1.1909e-01, 2.6425e-01,
           5.4426e-01, 8.1888e-01, 8.2794e-01]...      [3.0787e-01, 9.0324e-01, 9.0727e-01, 2.9373e-01, 4.6800e-01,
           5.0849e-01, 6.2270e-01, 2.0344e-01]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function upscale_double at 0x7f183dbc7ac0>
trace_args = (tensor([[[[0.0269, 0.3792, 0.6922, 0.2217],
          [0.0462, 0.7121, 0.4492, 0.2805],
          [0.0389, 0.7943, 0...., 0.9486, 0.8567, 0.1550],
          [0.1512, 0.0130, 0.7448, 0.2458],
          [0.7618, 0.3052, 0.2820, 0.5196]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[1.2834e-02, 4.7981e-01, 1.8986e-01, 1.1909e-01, 2.6425e-01,
           5.4426e-01, 8.1888e-01, 8.2794e-01]...      [3.0787e-01, 9.0324e-01, 9.0727e-01, 2.9373e-01, 4.6800e-01,
           5.0849e-01, 6.2270e-01, 2.0344e-01]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 3, 4, 4), dtype=float32, numpy=
array([[[[0.02686536, 0.3792168 , 0.6921746 , 0.22166055],
     ....01297486, 0.7448219 , 0.24581903],
         [0.761824  , 0.30520928, 0.2819875 , 0.5196148 ]]]],
      dtype=float32)>

    def tensorflow_upscale_double(x):
        from ...core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...core._backend import zeros
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(x)
        tensorflow_KORNIA_CHECK_SHAPE(x, ["*", "H", "W"])
        double_shape = tensorflow_shape_frnt_(x)[:-2] + (
            tensorflow_shape_frnt_(x)[-2] * 2,
            tensorflow_shape_frnt_(x)[-1] * 2,
        )
        upscaled = zeros(double_shape, device=x.device, dtype=x.dtype)
        upscaled = tensorflow_set_item_bknd(
            upscaled, (..., slice(None, None, 2), slice(None, None, 2)), x
        )
>       upscaled[..., ::2, 1::2] = tensorflow_set_item_bknd(
            upscaled[..., ::2, 1::2],
            (..., slice(None, -1, None)),
            (upscaled[..., ::2, ::2][..., :-1] + upscaled[..., ::2, 2::2]) / 2,
        )
E       TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/pyramid.py:177: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.pyramid.upscale_double
___________________________________________________________________________________ test_Shear[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Shear(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Shear")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledShear = ivy.transpile(kornia.geometry.transform.Shear, source="torch", target=target_framework)
    
        x = torch.rand(2, 3, 4, 4)
        shear = torch.tensor([[0.5, 0.0], [0.0, 0.5]])
        torch_out = kornia.geometry.transform.Shear(shear)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_shear = _nest_torch_tensor_to_new_framework(shear, target_framework)
>       transpiled_out = TranspiledShear(transpiled_shear)(transpiled_x)

kornia/geometry/test_transform.py:1156: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear()
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9903768 , 0.48232627, 0.29086202, 0.42334902],
    ...0393969, 0.93950063, 0.14851683],
         [0.1485641 , 0.7560107 , 0.2113601 , 0.67011946]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55604f0a3840, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Shear(), <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9903768 , 0.48232627, 0.2908620...00393969, 0.93950063, 0.14851683],
         [0.1485641 , 0.7560107 , 0.2113601 , 0.67011946]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9903768 , 0.48232627, 0.29086202, 0.42334902],
    ...0393969, 0.93950063, 0.14851683],
         [0.1485641 , 0.7560107 , 0.2113601 , 0.67011946]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Shear(), <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9903768 , 0.48232627, 0.2908620...00393969, 0.93950063, 0.14851683],
         [0.1485641 , 0.7560107 , 0.2113601 , 0.67011946]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9903768 , 0.48232627, 0.29086202, 0.42334902],
    ...0393969, 0.93950063, 0.14851683],
         [0.1485641 , 0.7560107 , 0.2113601 , 0.67011946]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9903768 , 0.48232627, 0.29086202, 0.42334902],
     ....00393969, 0.93950063, 0.14851683],
         [0.1485641 , 0.7560107 , 0.2113601 , 0.67011946]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Shear(),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9903768 , 0.48232627, 0.29086202, 0.423349...00393969, 0.93950063, 0.14851683],
         [0.1485641 , 0.7560107 , 0.2113601 , 0.67011946]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Shear()
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.9903768 , 0.48232627, 0.29086202, 0.42334902],
     ....00393969, 0.93950063, 0.14851683],
         [0.1485641 , 0.7560107 , 0.2113601 , 0.67011946]]]],
      dtype=float32)>

    def call(self, input):
>       return shear(
            input, self.shear, self.mode, self.padding_mode, self.align_corners
        )
E       NameError: Exception encountered when calling tensorflow_Shear.call().
E       
E       [1mname 'shear' is not defined[0m
E       
E       Arguments received by tensorflow_Shear.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 4, 4), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/affwarp.py:1527: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Shear
__________________________________________________________________________________ test_Rescale[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Rescale(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Rescale")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledRescale = ivy.transpile(kornia.geometry.transform.Rescale, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 4, 4)
        torch_out = kornia.geometry.transform.Rescale((2.0, 3.0))(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_out = TranspiledRescale((2.0, 3.0))(transpiled_x)
    
>       _to_numpy_and_allclose(torch_out, transpiled_out)

kornia/geometry/test_transform.py:1294: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0.9084, 0.8530, 0.7976, 0.7422, 0.7043, 0.7014, 0.6986, 0.6958,
           0.7416, 0.8117, 0.8818, 0.9519],...        [0.7928, 0.8443, 0.8959, 0.9474, 0.9023, 0.6639, 0.4255, 0.1871,
           0.0935, 0.0722, 0.0510, 0.0297]]]])
transpiled_x = <tf.Tensor: shape=(1, 3, 8, 12), dtype=float32, numpy=
array([[[[0.90844333, 0.90844333, 0.840698  , 0.77295274, 0.705...      0.69038004, 0.399015  , 0.1076501 , 0.08166578, 0.05568148,
          0.02969718, 0.02969718]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0.90844333, 0.8530154 , 0.7975874 , 0.7421594 , 0.7042634 ,
          0.7014313 , 0.6985991 , 0.6957669 , 0....       0.6638923 , 0.42550272, 0.18711321, 0.09347684, 0.07221695,
          0.05095707, 0.02969718]]]], dtype=float32)
y = array([[[[0.90844333, 0.90844333, 0.840698  , 0.77295274, 0.70520747,
          0.7017459 , 0.69828445, 0.6948229 , 0....       0.69038004, 0.399015  , 0.1076501 , 0.08166578, 0.05568148,
          0.02969718, 0.02969718]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Rescale
________________________________________________________________________________ test_Homography[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Homography(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Homography")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHomography = ivy.transpile(
            kornia.geometry.transform.image_registrator.Homography, source="torch", target=target_framework
        )
    
        torch_out = kornia.geometry.transform.image_registrator.Homography()()
>       transpiled_out = TranspiledHomography()()

kornia/geometry/test_transform.py:1354: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>), args = (), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55604dab9c80, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>),), kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>), v = None, buffers = None
args = (), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>),)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>), v = None, buffers = None
args = (), kwargs = {}, first_arr = None, replace_v = False, replace_buffers = False, call_signature = <Signature ()>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Homography(<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>),), kwargs = {}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <keras.src.layers.layer.CallSpec object at 0x7f1836351450>, signature = <Signature ()>, args = (), kwargs = {}

    def __init__(self, signature, args, kwargs):
        # `training` and `mask` are special kwargs that are always available in
        # a layer, if user specifies them in their call without adding to spec,
        # we remove them to be able to bind variables. User is not using
        # `training` anyway so we can ignore.
        # TODO: If necessary use workaround for `mask`
        if "training" in kwargs and "training" not in signature.parameters:
            kwargs.pop("training")
            bound_args = signature.bind(*args, **kwargs)
        else:
            bound_args = signature.bind(*args, **kwargs)
        self.user_arguments_dict = {
            k: v for k, v in bound_args.arguments.items()
        }
        bound_args.apply_defaults()
        arg_dict = {}
        arg_names = []
        tensor_arg_dict = {}
        tensor_args = []
        tensor_arg_names = []
        nested_tensor_arg_names = []
        for name, value in bound_args.arguments.items():
            arg_dict[name] = value
            arg_names.append(name)
            if is_backend_tensor_or_symbolic(value):
                tensor_args.append(value)
                tensor_arg_names.append(name)
                tensor_arg_dict[name] = value
            elif tree.is_nested(value) and len(value) > 0:
                flat_values = tree.flatten(value)
                if all(
                    is_backend_tensor_or_symbolic(x, allow_none=True)
                    for x in flat_values
                ):
                    tensor_args.append(value)
                    tensor_arg_names.append(name)
                    tensor_arg_dict[name] = value
                    nested_tensor_arg_names.append(name)
                elif any(is_backend_tensor_or_symbolic(x) for x in flat_values):
                    raise ValueError(
                        "In a nested call() argument, "
                        "you cannot mix tensors and non-tensors. "
                        "Received invalid mixed argument: "
                        f"{name}={value}"
                    )
        self.arguments_dict = arg_dict
        self.argument_names = arg_names
        self.tensor_arguments_dict = tensor_arg_dict
        self.tensor_arguments_names = tensor_arg_names
        self.nested_tensor_argument_names = nested_tensor_arg_names
>       self.first_arg = arg_dict[arg_names[0]]
E       IndexError: list index out of range

/opt/fw/tensorflow/keras/src/layers/layer.py:1614: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Homography
_____________________________________________________________________________ test_ImageRegistrator[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ImageRegistrator(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.ImageRegistrator")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledImageRegistrator = ivy.transpile(
            kornia.geometry.transform.ImageRegistrator, source="torch", target=target_framework
        )

kornia/geometry/test_transform.py:1365: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.transform.image_registrator.ImageRegistrator'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geometry.transform.image_registrator: name 'Args' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.ImageRegistrator
________________________________________________________________________________ test_Similarity[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Similarity(target_framework, mode, backend_compile):
        print("kornia.geometry.transform.Similarity")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledSimilarity = ivy.transpile(
            kornia.geometry.transform.image_registrator.Similarity, source="torch", target=target_framework
        )

kornia/geometry/test_transform.py:1386: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.transform.image_registrator.Similarity'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geometry.transform.image_registrator: name 'Args' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.transform.Similarity
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_transform.py::test_upscale_double[tensorflow-s2s-False] - TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment
FAILED kornia/geometry/test_transform.py::test_Shear[tensorflow-s2s-False] - NameError: Exception encountered when calling tensorflow_Shear.call().
FAILED kornia/geometry/test_transform.py::test_Rescale[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_transform.py::test_Homography[tensorflow-s2s-False] - IndexError: list index out of range
FAILED kornia/geometry/test_transform.py::test_ImageRegistrator[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geom...
FAILED kornia/geometry/test_transform.py::test_Similarity[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.geometry.t...
=============================================================================== 6 failed, 51 passed in 959.99s (0:15:59) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 39 items

kornia/test_enhance.py ..............F........................                                                                                                                                   [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_equalize_clahe[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_equalize_clahe(target_framework, mode, backend_compile):
        trace_args = (torch.rand(1, 10, 20),)
        trace_kwargs = {
            "clip_limit": 40.0,
            "grid_size": (8, 8),
            "slow_and_differentiable": False,
        }
        test_args = (torch.rand(2, 3, 10, 20),)
        test_kwargs = {
            "clip_limit": 20.0,
            "grid_size": (4, 4),
            "slow_and_differentiable": False,
        }
>       _test_function(
            kornia.enhance.equalize_clahe,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
            skip=True,
        )

kornia/test_enhance.py:363: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function equalize_clahe at 0x7fd63c1a6a70>
trace_args = (tensor([[[0.1511, 0.8003, 0.2352, 0.6951, 0.5445, 0.3629, 0.3845, 0.3834,
          0.5887, 0.9510, 0.9610, 0.1570, 0...         0.0911, 0.9915, 0.9529, 0.5364, 0.0650, 0.2909, 0.6440, 0.6127,
          0.9177, 0.2653, 0.2418, 0.1232]]]),)
trace_kwargs = {'clip_limit': 40.0, 'grid_size': (8, 8), 'slow_and_differentiable': False}
test_args = (tensor([[[[4.6826e-01, 5.4436e-01, 3.5068e-01,  ..., 6.8472e-01,
           4.0362e-01, 2.2513e-01],
          [8.769... 3.3201e-01],
          [8.6153e-01, 8.4253e-01, 8.1049e-01,  ..., 9.2727e-02,
           2.6601e-01, 1.7388e-01]]]]),)
test_kwargs = {'clip_limit': 20.0, 'grid_size': (4, 4), 'slow_and_differentiable': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = True
deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function equalize_clahe at 0x7fd63c1a6a70>
trace_args = (tensor([[[0.1511, 0.8003, 0.2352, 0.6951, 0.5445, 0.3629, 0.3845, 0.3834,
          0.5887, 0.9510, 0.9610, 0.1570, 0...         0.0911, 0.9915, 0.9529, 0.5364, 0.0650, 0.2909, 0.6440, 0.6127,
          0.9177, 0.2653, 0.2418, 0.1232]]]),)
trace_kwargs = {'clip_limit': 40.0, 'grid_size': (8, 8), 'slow_and_differentiable': False}
test_args = (tensor([[[[4.6826e-01, 5.4436e-01, 3.5068e-01,  ..., 6.8472e-01,
           4.0362e-01, 2.2513e-01],
          [8.769... 3.3201e-01],
          [8.6153e-01, 8.4253e-01, 8.1049e-01,  ..., 9.2727e-02,
           2.6601e-01, 1.7388e-01]]]]),)
test_kwargs = {'clip_limit': 20.0, 'grid_size': (4, 4), 'slow_and_differentiable': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 10, 20), dtype=float32, numpy=
array([[[[0.1511252 , 0.800265  , 0.23521888, 0.69508237, 0.54...0.29087877, 0.6439838 ,
          0.61267704, 0.91773784, 0.26526695, 0.24179232, 0.12321007]]]],
      dtype=float32)>
args = (), kwargs = {'clip_limit': 40.0, 'grid_size': (8, 8), 'slow_and_differentiable': False}, tensorflow_numel_frnt_ = <function tensorflow_numel_frnt_ at 0x7fd6348ca5f0>
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7fd6348c8940>, tensorflow_view_frnt_ = <function tensorflow_view_frnt_ at 0x7fd6348ca7a0>
input_shape = ivy.frontends.torch.Size([1, 10, 20])

    @wraps(f)
    def _wrapper(input, *args, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_numel_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
        if tensorflow_numel_frnt_(input) == 0:
            raise ValueError("Invalid input tensor, it is empty.")
        input_shape = tensorflow_shape_frnt_(input)
        input = tensorflow__to_bchw(input)
>       output = f(input, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/utils/image.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 10, 20), dtype=float32, numpy=
array([[[[0.1511252 , 0.800265  , 0.23521888, 0.69508237, 0.54...0.29087877, 0.6439838 ,
          0.61267704, 0.91773784, 0.26526695, 0.24179232, 0.12321007]]]],
      dtype=float32)>
clip_limit = 40.0, grid_size = (8, 8), slow_and_differentiable = False

    @tensorflow_perform_keep_shape_image
    def tensorflow_equalize_clahe(
        input, clip_limit=40.0, grid_size=(8, 8), slow_and_differentiable=False
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_as_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
    
        if not isinstance(clip_limit, (float,)):
            raise TypeError(f"Input clip_limit type is not float. Got {type(clip_limit)}")
        if not isinstance(grid_size, (tuple,)):
            raise TypeError(f"Input grid_size type is not Tuple. Got {type(grid_size)}")
        if len(grid_size) != 2:
            raise TypeError(
                f"Input grid_size is not a Tuple with 2 elements. Got {len(grid_size)}"
            )
        if isinstance(grid_size[0], (float,)) or isinstance(grid_size[1], (float,)):
            raise TypeError("Input grid_size type is not valid, must be a Tuple[int, int].")
        if grid_size[0] <= 0 or grid_size[1] <= 0:
            raise ValueError(f"Input grid_size elements must be positive. Got {grid_size}")
        imgs: typing.Any = input
>       hist_tiles, img_padded = tensorflow__compute_tiles(imgs, grid_size, True)

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:513: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

imgs = <tf.Tensor: shape=(1, 1, 10, 20), dtype=float32, numpy=
array([[[[0.1511252 , 0.800265  , 0.23521888, 0.69508237, 0.54...0.29087877, 0.6439838 ,
          0.61267704, 0.91773784, 0.26526695, 0.24179232, 0.12321007]]]],
      dtype=float32)>
grid_size = (8, 8), even_tile_size = True

    def tensorflow__compute_tiles(imgs, grid_size, even_tile_size=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_pad_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unfold_frnt_
    
        batch: typing.Any = imgs
        h, w = tensorflow_shape_frnt_(batch)[-2:][0], tensorflow_shape_frnt_(batch)[-2:][1]
        kernel_vert: typing.Any = math.ceil(h / grid_size[0])
        kernel_horz: typing.Any = math.ceil(w / grid_size[1])
        if even_tile_size:
            kernel_vert = kernel_vert + (1 if kernel_vert % 2 else 0)
            kernel_horz = kernel_horz + (1 if kernel_horz % 2 else 0)
        pad_vert = kernel_vert * grid_size[0] - h
        pad_horz = kernel_horz * grid_size[1] - w
        if (
            pad_vert > tensorflow_shape_frnt_(batch)[-2]
            or pad_horz > tensorflow_shape_frnt_(batch)[-1]
        ):
            raise ValueError(
                "Cannot compute tiles on the image according to the given grid size"
            )
        if pad_vert > 0 or pad_horz > 0:
            batch = tensorflow_pad_frnt(batch, [0, pad_horz, 0, pad_vert], mode="reflect")
        c: typing.Any = tensorflow_shape_frnt_(batch)[-3]
        tiles: typing.Any = tensorflow_contiguous_frnt_(
            tensorflow_squeeze_frnt_(
                tensorflow_unfold_frnt_(
>                   tensorflow_unfold_frnt_(
                        tensorflow_unfold_frnt_(batch, 1, c, c), 2, kernel_vert, kernel_vert
                    ),
                    3,
                    kernel_horz,
                    kernel_horz,
                ),
                1,
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 1, 32, 16), dtype=float32, numpy=
array([[[[[0.1511252 , 0.81706715, 0.9109293 , 0.42849028,...      0.46507484, 0.5348058 , 0.94567096, 0.46309012, 0.6845115 ,
           0.2829553 ]]]]], dtype=float32)>, 2, 2, 2)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fd634889bd0>
array_like = <tf.Tensor: shape=(1, 1, 1, 32, 16), dtype=float32, numpy=
array([[[[[0.1511252 , 0.81706715, 0.9109293 , 0.42849028, ...99 ,
           0.46507484, 0.5348058 , 0.94567096, 0.46309012, 0.6845115 ,
           0.2829553 ]]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 1, 1, 32, 16), dtype=float32, numpy=
array([[[[[0.1511252 , 0.81706715, 0.9109293 , 0.42849028, ...99 ,
           0.46507484, 0.5348058 , 0.94567096, 0.46309012, 0.6845115 ,
           0.2829553 ]]]]], dtype=float32)>
dimension = 2, size = 2, step = 2

    @tensorflow_handle_methods
    def tensorflow_unfold_frnt_(tensor, dimension, size, step):
        from ...backends.tensorflow.general import tensorflow_get_item
        from ...ivy.general import tensorflow_set_item_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_stack_frnt
    
        slices = []
        self_shape = tuple(tensorflow_shape_frnt_(tensor))
        for i in range(0, tensorflow_get_item(self_shape, dimension) - size + 1, step):
            slicing = [slice(None)] * len(tensorflow_shape_frnt_(tensor))
            slicing = tensorflow_set_item_bknd(slicing, dimension, slice(i, i + size))
            slices.append(tensorflow_get_item(tensor, tuple(slicing)))
>       stacked = tensorflow_stack_frnt(slices, dim=dimension)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:423: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = [], dim = 2

    def tensorflow_stack_frnt(tensors, dim=0, *, out=None):
        from ...backends.tensorflow.manipulation import tensorflow_stack
    
>       return tensorflow_stack(tensors, axis=dim, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ([],), kwargs = {'axis': 2, 'out': None}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7fd634889bd0>, array_like = []

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
>           array_like = array_like[0]
E           IndexError: list index out of range

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:44: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.enhance.equalization.equalize_clahe
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_enhance.py::test_equalize_clahe[tensorflow-s2s-False] - IndexError: list index out of range
=============================================================================== 1 failed, 38 passed in 604.62s (0:10:04) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 5 items

kornia/geometry/test_solvers.py .....                                                                                                                                                            [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 5 passed in 108.26s (0:01:48) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 26 items

kornia/geometry/test_epipolar.py F..F.....F..........F..F.F                                                                                                                                      [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_find_essential[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_find_essential(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 8, 2),
            torch.rand(1, 8, 2),
        )
        trace_kwargs = {'weights': torch.rand(1, 8)}
        test_args = (
            torch.rand(5, 8, 2),
            torch.rand(5, 8, 2),
        )
        test_kwargs = {'weights': torch.rand(5, 8)}
>       _test_function(
            kornia.geometry.epipolar.find_essential,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_epipolar.py:20: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function find_essential at 0x7f3b56eb5510>
trace_args = (tensor([[[0.8098, 0.4115],
         [0.5892, 0.6549],
         [0.1953, 0.0218],
         [0.2175, 0.1284],
         ...0.8345],
         [0.8808, 0.3385],
         [0.5140, 0.4807],
         [0.6492, 0.8699],
         [0.5854, 0.1806]]]))
trace_kwargs = {'weights': <tf.Tensor: shape=(1, 8), dtype=float32, numpy=
array([[0.5107001 , 0.3372295 , 0.7664636 , 0.37239337, 0.90924746,
        0.8243018 , 0.35166478, 0.48710537]], dtype=float32)>}
test_args = (tensor([[[0.4370, 0.7467],
         [0.9378, 0.3104],
         [0.9720, 0.2892],
         [0.6780, 0.0855],
         ...0.3853],
         [0.4519, 0.5489],
         [0.7141, 0.2041],
         [0.2840, 0.5312],
         [0.6621, 0.4193]]]))
test_kwargs = {'weights': tensor([[0.8747, 0.8561, 0.7506, 0.4926, 0.2832, 0.0507, 0.1343, 0.2243],
        [0.3149, 0.0702, 0.8234,...4, 0.5183, 0.3779, 0.0253, 0.9402, 0.5124],
        [0.9205, 0.0906, 0.1519, 0.8965, 0.4492, 0.8760, 0.8707, 0.0415]])}
target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function find_essential at 0x7f3b56eb5510>
trace_args = (tensor([[[0.8098, 0.4115],
         [0.5892, 0.6549],
         [0.1953, 0.0218],
         [0.2175, 0.1284],
         ...0.8345],
         [0.8808, 0.3385],
         [0.5140, 0.4807],
         [0.6492, 0.8699],
         [0.5854, 0.1806]]]))
trace_kwargs = {'weights': <tf.Tensor: shape=(1, 8), dtype=float32, numpy=
array([[0.5107001 , 0.3372295 , 0.7664636 , 0.37239337, 0.90924746,
        0.8243018 , 0.35166478, 0.48710537]], dtype=float32)>}
test_args = (tensor([[[0.4370, 0.7467],
         [0.9378, 0.3104],
         [0.9720, 0.2892],
         [0.6780, 0.0855],
         ...0.3853],
         [0.4519, 0.5489],
         [0.7141, 0.2041],
         [0.2840, 0.5312],
         [0.6621, 0.4193]]]))
test_kwargs = {'weights': tensor([[0.8747, 0.8561, 0.7506, 0.4926, 0.2832, 0.0507, 0.1343, 0.2243],
        [0.3149, 0.0702, 0.8234,...4, 0.5183, 0.3779, 0.0253, 0.9402, 0.5124],
        [0.9205, 0.0906, 0.1519, 0.8965, 0.4492, 0.8760, 0.8707, 0.0415]])}
target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[ 0.5523,  0.1375, -0.2322],
          [-0.3067,  0.6022, -0.0933],
          [-0.2321, -0.2730,  0.1681]],
...         [[-0.3133, -0.3988,  0.3407],
          [-0.4733,  0.4806, -0.0785],
          [ 0.3540,  0.0817, -0.1829]]]])
transpiled_x = <tf.Tensor: shape=(1, 10, 3, 3), dtype=float32, numpy=
array([[[[ 0.5523277 ,  0.13740592, -0.23221603],
         [-0....
         [-0.47335544,  0.4805801 , -0.07851709],
         [ 0.35401633,  0.08170772, -0.1829128 ]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[ 0.5523103 ,  0.13745639, -0.23223668],
         [-0.3067454 ,  0.60219157, -0.09333837],
         [-0.23208...,
         [-0.47334418,  0.4805901 , -0.07852568],
         [ 0.354016  ,  0.08169606, -0.18290906]]]], dtype=float32)
y = array([[[[ 0.5523277 ,  0.13740592, -0.23221603],
         [-0.30669898,  0.6022098 , -0.09336835],
         [-0.23211...,
         [-0.47335544,  0.4805801 , -0.07851709],
         [ 0.35401633,  0.08170772, -0.1829128 ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.epipolar.essential.find_essential
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/ivy/ivy/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.
  warnings.warn(
________________________________________________________________________ test_decompose_essential_matrix[tensorflow-s2s-False] _________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_decompose_essential_matrix(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(3, 3),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(3, 3),
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.epipolar.decompose_essential_matrix,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_epipolar.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function decompose_essential_matrix at 0x7f3b56eb51b0>, trace_args = (tensor([[0.3465, 0.9725, 0.3594],
        [0.6033, 0.5763, 0.0016],
        [0.0826, 0.0210, 0.5291]]),), trace_kwargs = {}
test_args = (tensor([[0.8251, 0.3014, 0.3945],
        [0.5958, 0.9542, 0.0655],
        [0.1165, 0.2268, 0.4931]]),), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function decompose_essential_matrix at 0x7f3b56eb51b0>, trace_args = (tensor([[0.3465, 0.9725, 0.3594],
        [0.6033, 0.5763, 0.0016],
        [0.0826, 0.0210, 0.5291]]),), trace_kwargs = {}
test_args = (tensor([[0.8251, 0.3014, 0.3945],
        [0.5958, 0.9542, 0.0655],
        [0.1165, 0.2268, 0.4931]]),), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
            _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)
    
        # test it works with the test_args as input
        orig_out = fn(*test_args, **test_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(test_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(test_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        orig_np = _nest_array_to_numpy(orig_out)
        graph_np = _nest_array_to_numpy(graph_out)
    
>       _check_allclose(orig_np, graph_np, tolerance=tolerance)

helpers.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[ 0.10948417, -0.9936968 , -0.02409318],
        [ 0.56376845,  0.04211596,  0.82485867],
        [-0.8186444...214,  0.4461136 ]]], dtype=float32), array([[-0.51718485],
       [ 0.13003068],
       [ 0.8459387 ]], dtype=float32))
y = (array([[[ 0.5895835 ,  0.5473503 , -0.59396887],
        [-0.73952776,  0.07010422, -0.6694653 ],
        [-0.3247924...186,  0.5648256 ]]], dtype=float32), array([[ 0.51718503],
       [-0.13003065],
       [-0.84593844]], dtype=float32))
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7f3b4be42b00>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[ 0.10948417, -0.9936968 , -0.02409318],
        [ 0.56376845,  0.04211596,  0.82485867],
        [-0.8186444 , -0.10389179,  0.5648259 ]]], dtype=float32)
y = array([[[ 0.5895835 ,  0.5473503 , -0.59396887],
        [-0.73952776,  0.07010422, -0.6694653 ],
        [-0.32479241,  0.8339622 ,  0.44611326]]], dtype=float32), tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.epipolar.essential.decompose_essential_matrix
_______________________________________________________________________ test_fundamental_from_projections[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_fundamental_from_projections(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(3, 4),
            torch.rand(3, 4),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(3, 4),
            torch.rand(3, 4),
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.epipolar.fundamental_from_projections,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_epipolar.py:250: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function fundamental_from_projections at 0x7f3b56eb5d80>
trace_args = (tensor([[0.7001, 0.8296, 0.5152, 0.0766],
        [0.9440, 0.1739, 0.7518, 0.1420],
        [0.3282, 0.8963, 0.4192, ...[0.4842, 0.6258, 0.3303, 0.4332],
        [0.5637, 0.1982, 0.4497, 0.9257],
        [0.6416, 0.4332, 0.3911, 0.5871]]))
trace_kwargs = {}
test_args = (tensor([[0.4926, 0.4354, 0.9038, 0.2881],
        [0.5549, 0.8777, 0.3759, 0.4761],
        [0.1058, 0.4560, 0.8563, ...[0.5973, 0.1379, 0.3014, 0.8898],
        [0.2357, 0.3290, 0.7435, 0.0303],
        [0.3800, 0.5972, 0.5230, 0.4802]]))
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function fundamental_from_projections at 0x7f3b56eb5d80>
trace_args = (tensor([[0.7001, 0.8296, 0.5152, 0.0766],
        [0.9440, 0.1739, 0.7518, 0.1420],
        [0.3282, 0.8963, 0.4192, ...[0.4842, 0.6258, 0.3303, 0.4332],
        [0.5637, 0.1982, 0.4497, 0.9257],
        [0.6416, 0.4332, 0.3911, 0.5871]]))
trace_kwargs = {}
test_args = (tensor([[0.4926, 0.4354, 0.9038, 0.2881],
        [0.5549, 0.8777, 0.3759, 0.4761],
        [0.1058, 0.4560, 0.8563, ...[0.5973, 0.1379, 0.3014, 0.8898],
        [0.2357, 0.3290, 0.7435, 0.0303],
        [0.3800, 0.5972, 0.5230, 0.4802]]))
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

P1 = <tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[0.7000973 , 0.82962537, 0.51522386, 0.07662928],
       [0.9439722 , 0.17394263, 0.75178397, 0.1419878 ],
       [0.3281536 , 0.8963356 , 0.41921258, 0.41830724]], dtype=float32)>
P2 = <tf.Tensor: shape=(3, 4), dtype=float32, numpy=
array([[0.48418158, 0.62582946, 0.33029258, 0.4331624 ],
       [0.56371903, 0.19815105, 0.44968653, 0.925742  ],
       [0.64163154, 0.43323135, 0.3910898 , 0.58706975]], dtype=float32)>

    def tensorflow_fundamental_from_projections(P1, P2):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_det_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
    
        if not (
            len(tensorflow_shape_frnt_(P1)) >= 2
            and tensorflow_shape_frnt_(P1)[-2:] == (3, 4)
        ):
            raise AssertionError(tensorflow_shape_frnt_(P1))
        if not (
            len(tensorflow_shape_frnt_(P2)) >= 2
            and tensorflow_shape_frnt_(P2)[-2:] == (3, 4)
        ):
            raise AssertionError(tensorflow_shape_frnt_(P2))
        if tensorflow_shape_frnt_(P1)[:-2] != tensorflow_shape_frnt_(P2)[:-2]:
            raise AssertionError
    
        def vstack(x, y):
            from ...core._backend import concatenate
    
            return concatenate([x, y], dim=-2)
    
        input_dtype = P1.dtype
        if input_dtype not in (tf.float32, tf.float64):
            P1 = tensorflow_to_frnt_(P1, tf.float32)
            P2 = tensorflow_to_frnt_(P2, tf.float32)
        X1 = P1[..., 1:, :]
        X2 = vstack(P1[..., 2:3, :], P1[..., 0:1, :])
        X3 = P1[..., :2, :]
        Y1 = P2[..., 1:, :]
        Y2 = vstack(P2[..., 2:3, :], P2[..., 0:1, :])
        Y3 = P2[..., :2, :]
        X1Y1, X2Y1, X3Y1 = vstack(X1, Y1), vstack(X2, Y1), vstack(X3, Y1)
        X1Y2, X2Y2, X3Y2 = vstack(X1, Y2), vstack(X2, Y2), vstack(X3, Y2)
        X1Y3, X2Y3, X3Y3 = vstack(X1, Y3), vstack(X2, Y3), vstack(X3, Y3)
        F_vec = tensorflow_cat_frnt(
            [
>               tensorflow_reshape_frnt_(tensorflow_det_frnt_(X1Y1), -1, 1),
                tensorflow_reshape_frnt_(tensorflow_det_frnt_(X2Y1), -1, 1),
                tensorflow_reshape_frnt_(tensorflow_det_frnt_(X3Y1), -1, 1),
                tensorflow_reshape_frnt_(tensorflow_det_frnt_(X1Y2), -1, 1),
                tensorflow_reshape_frnt_(tensorflow_det_frnt_(X2Y2), -1, 1),
                tensorflow_reshape_frnt_(tensorflow_det_frnt_(X3Y2), -1, 1),
                tensorflow_reshape_frnt_(tensorflow_det_frnt_(X1Y3), -1, 1),
                tensorflow_reshape_frnt_(tensorflow_det_frnt_(X2Y3), -1, 1),
                tensorflow_reshape_frnt_(tensorflow_det_frnt_(X3Y3), -1, 1),
            ],
            dim=1,
        )

Translated_Outputs/tensorflow_outputs/kornia/geometry/epipolar/fundamental.py:368: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[0.9439722 , 0.17394263, 0.75178397, 0.1419878 ],
       [0.32...71903, 0.19815105, 0.44968653, 0.925742  ],
       [0.64163154, 0.43323135, 0.3910898 , 0.58706975]], dtype=float32)>,)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f3b4b3e9c60>
array_like = <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[0.9439722 , 0.17394263, 0.75178397, 0.1419878 ],
       [0.328...6371903, 0.19815105, 0.44968653, 0.925742  ],
       [0.64163154, 0.43323135, 0.3910898 , 0.58706975]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[0.9439722 , 0.17394263, 0.75178397, 0.1419878 ],
       [0.328...6371903, 0.19815105, 0.44968653, 0.925742  ],
       [0.64163154, 0.43323135, 0.3910898 , 0.58706975]], dtype=float32)>

    @tensorflow_handle_methods
    def tensorflow_det_frnt_(tensor):
        from .blas_and_lapack_ops import tensorflow_det_frnt
    
>       return tensorflow_det_frnt(tensor)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:398: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[0.9439722 , 0.17394263, 0.75178397, 0.1419878 ],
       [0.328...6371903, 0.19815105, 0.44968653, 0.925742  ],
       [0.64163154, 0.43323135, 0.3910898 , 0.58706975]], dtype=float32)>

    def tensorflow_det_frnt(input):
>       from .linalg import tensorflow_det_frnt_1_frnt
E       ImportError: cannot import name 'tensorflow_det_frnt_1_frnt' from 'Translated_Outputs.tensorflow_outputs.ivy.functional.frontends.torch.linalg' (/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/linalg.py)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/blas_and_lapack_ops.py:38: ImportError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.epipolar.fundamental.fundamental_from_projections
_______________________________________________________________________ test_projections_from_fundamental[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_projections_from_fundamental(target_framework, mode, backend_compile):
        trace_args = (torch.rand(1, 3, 3),)
        trace_kwargs = {}
        test_args = (torch.rand(2, 3, 3),)
        test_kwargs = {}
>       _test_function(
            kornia.geometry.epipolar.projections_from_fundamental,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_epipolar.py:516: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function projections_from_fundamental at 0x7f3b56eb4e50>, trace_args = (tensor([[[0.1741, 0.6136, 0.9484],
         [0.1003, 0.0673, 0.1162],
         [0.1980, 0.8405, 0.8431]]]),)
trace_kwargs = {}
test_args = (tensor([[[0.3212, 0.6869, 0.8666],
         [0.9191, 0.3165, 0.7391],
         [0.7415, 0.6754, 0.4558]],

        [[0.4266, 0.4197, 0.6002],
         [0.1471, 0.3305, 0.6654],
         [0.8564, 0.2216, 0.1323]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function projections_from_fundamental at 0x7f3b56eb4e50>, trace_args = (tensor([[[0.1741, 0.6136, 0.9484],
         [0.1003, 0.0673, 0.1162],
         [0.1980, 0.8405, 0.8431]]]),)
trace_kwargs = {}
test_args = (tensor([[[0.3212, 0.6869, 0.8666],
         [0.9191, 0.3165, 0.7391],
         [0.7415, 0.6754, 0.4558]],

        [[0.4266, 0.4197, 0.6002],
         [0.1471, 0.3305, 0.6654],
         [0.8564, 0.2216, 0.1323]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[ 1.0000, -0.1936],
          [ 0.0000, -0.8294],
          [ 0.0000, -0.8310],
          [ 0.0000,  0.1499]...      [[ 0.0000,  0.1872],
          [ 0.0000,  0.6166],
          [ 1.0000,  0.9549],
          [ 0.0000, -0.0207]]]])
transpiled_x = <tf.Tensor: shape=(1, 3, 4, 2), dtype=float32, numpy=
array([[[[ 1.        ,  0.19360133],
         [ 0.        ,  0.8... 0.        , -0.6165865 ],
         [ 1.        , -0.95492756],
         [ 0.        ,  0.02070733]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[ 1.        , -0.19360137],
         [ 0.        , -0.8294106 ],
         [ 0.        , -0.83102137],
       ...[ 0.        ,  0.6165865 ],
         [ 1.        ,  0.95492756],
         [ 0.        , -0.02070689]]]], dtype=float32)
y = array([[[[ 1.        ,  0.19360133],
         [ 0.        ,  0.82941055],
         [ 0.        ,  0.8310213 ],
       ...[ 0.        , -0.6165865 ],
         [ 1.        , -0.95492756],
         [ 0.        ,  0.02070733]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.epipolar.projection.projections_from_fundamental
_____________________________________________________________________________ test_random_intrinsics[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_random_intrinsics(target_framework, mode, backend_compile):
        trace_args = (0.1, 1.0)
        trace_kwargs = {}
        test_args = (0.2, 2.0)
        test_kwargs = {}
>       _test_function(
            kornia.geometry.epipolar.random_intrinsics,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
            deterministic=False,
        )

kornia/geometry/test_epipolar.py:570: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function random_intrinsics at 0x7f3b56eb4af0>, trace_args = (0.1, 1.0), trace_kwargs = {}, test_args = (0.2, 2.0), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, mode = 's2s', skip = False, deterministic = False

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function random_intrinsics at 0x7f3b56eb4af0>, trace_args = (0.1, 1.0), trace_kwargs = {}, test_args = (0.2, 2.0), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, deterministic = False

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
            _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)
    
        # test it works with the test_args as input
        orig_out = fn(*test_args, **test_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(test_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(test_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        orig_np = _nest_array_to_numpy(orig_out)
        graph_np = _nest_array_to_numpy(graph_out)
    
>       _check_allclose(orig_np, graph_np, tolerance=tolerance)

helpers.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[1.869148 , 0.       , 1.0447098],
        [0.       , 1.0157896, 0.8639322],
        [0.       , 0.       , 1.       ]]], dtype=float32)
y = array([[[1.3020616, 0.       , 1.3831536],
        [0.       , 1.2096109, 1.2290006],
        [0.       , 0.       , 1.       ]]], dtype=float32), tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.epipolar.projection.random_intrinsics
____________________________________________________________________________ test_triangulate_points[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_triangulate_points(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 4),
            torch.rand(1, 3, 4),
            torch.rand(1, 10, 2),
            torch.rand(1, 10, 2),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(2, 3, 4),
            torch.rand(2, 3, 4),
            torch.rand(2, 20, 2),
            torch.rand(2, 20, 2),
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.epipolar.triangulate_points,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_epipolar.py:617: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function triangulate_points at 0x7f3b56eb4f70>
trace_args = (tensor([[[0.1099, 0.4942, 0.1001, 0.4705],
         [0.3375, 0.4640, 0.1197, 0.6868],
         [0.9388, 0.8414, 0.963...0.3688],
         [0.4798, 0.4050],
         [0.3016, 0.1877],
         [0.8873, 0.1864],
         [0.6838, 0.5725]]]))
trace_kwargs = {}
test_args = (tensor([[[0.9809, 0.4692, 0.7220, 0.1794],
         [0.5651, 0.0998, 0.2778, 0.0524],
         [0.9053, 0.5320, 0.818...0.2218],
         [0.8644, 0.8730],
         [0.7540, 0.7526],
         [0.8528, 0.8154],
         [0.4890, 0.7026]]]))
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function triangulate_points at 0x7f3b56eb4f70>
trace_args = (tensor([[[0.1099, 0.4942, 0.1001, 0.4705],
         [0.3375, 0.4640, 0.1197, 0.6868],
         [0.9388, 0.8414, 0.963...0.3688],
         [0.4798, 0.4050],
         [0.3016, 0.1877],
         [0.8873, 0.1864],
         [0.6838, 0.5725]]]))
trace_kwargs = {}
test_args = (tensor([[[0.9809, 0.4692, 0.7220, 0.1794],
         [0.5651, 0.0998, 0.2778, 0.0524],
         [0.9053, 0.5320, 0.818...0.2218],
         [0.8644, 0.8730],
         [0.7540, 0.7526],
         [0.8528, 0.8154],
         [0.4890, 0.7026]]]))
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
            _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)
    
        # test it works with the test_args as input
        orig_out = fn(*test_args, **test_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(test_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(test_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        orig_np = _nest_array_to_numpy(orig_out)
        graph_np = _nest_array_to_numpy(graph_out)
    
>       _check_allclose(orig_np, graph_np, tolerance=tolerance)

helpers.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[-6.6478127e-01, -7.5706921e+00,  7.3288841e+00],
        [ 9.6298227e+00,  6.2228222e+01, -6.0245968e+01],
  ...605936e+00, -6.3814348e-01, -1.0364686e+00],
        [ 1.8056866e+00, -8.3064812e-01, -9.1173404e-01]]], dtype=float32)
y = array([[[-6.6477966e-01, -7.5706744e+00,  7.3288655e+00],
        [ 9.6297903e+00,  6.2228012e+01, -6.0245773e+01],
  ...605933e+00, -6.3814348e-01, -1.0364687e+00],
        [ 1.8056877e+00, -8.3064842e-01, -9.1173309e-01]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.epipolar.triangulation.triangulate_points
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_epipolar.py::test_find_essential[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_epipolar.py::test_decompose_essential_matrix[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_epipolar.py::test_fundamental_from_projections[tensorflow-s2s-False] - ImportError: cannot import name 'tensorflow_det_frnt_1_frnt' from 'Translated_Outputs.tensorflow_o...
FAILED kornia/geometry/test_epipolar.py::test_projections_from_fundamental[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_epipolar.py::test_random_intrinsics[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_epipolar.py::test_triangulate_points[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
=============================================================================== 6 failed, 20 passed in 417.62s (0:06:57) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 16 items

kornia/test_utils.py .F....F...FFFF..                                                                                                                                                            [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_draw_rectangle[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_draw_rectangle(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(2, 3, 10, 12),
            torch.tensor([[[0, 0, 4, 4]], [[4, 4, 10, 10]]]),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(3, 3, 10, 12),
            torch.tensor([[[0, 0, 4, 4]], [[4, 4, 10, 10]], [[2, 2, 6, 6]]]),
        )
        test_kwargs = {}
>       _test_function(
            kornia.utils.draw_rectangle,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_utils.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function draw_rectangle at 0x7f0f50f07130>
trace_args = (tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           8.7083e-01, 5.2750e-01, 6.9377e-01,...000e+00, 0.0000e+00,
           0.0000e+00, 3.5535e-01]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]]]))
trace_kwargs = {}
test_args = (tensor([[[[0.2261, 0.4545, 0.6701,  ..., 0.8900, 0.6657, 0.7553],
          [0.2658, 0.4508, 0.1172,  ..., 0.7408, 0....., 0.9242, 0.6302, 0.7046]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]],

        [[ 2,  2,  6,  6]]]))
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function draw_rectangle at 0x7f0f50f07130>
trace_args = (tensor([[[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
           8.7083e-01, 5.2750e-01, 6.9377e-01,...000e+00, 0.0000e+00,
           0.0000e+00, 3.5535e-01]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]]]))
trace_kwargs = {}
test_args = (tensor([[[[0.2261, 0.4545, 0.6701,  ..., 0.8900, 0.6657, 0.7553],
          [0.2658, 0.4508, 0.1172,  ..., 0.7408, 0....., 0.9242, 0.6302, 0.7046]]]]), tensor([[[ 0,  0,  4,  4]],

        [[ 4,  4, 10, 10]],

        [[ 2,  2,  6,  6]]]))
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
    ...00, 0.00000000e+00, 0.00000000e+00,
          0.00000000e+00, 0.00000000e+00, 3.55352640e-01]]]],
      dtype=float32)>
rectangle = <tf.Tensor: shape=(2, 1, 4), dtype=int64, numpy=
array([[[ 0,  0,  4,  4]],

       [[ 4,  4, 10,  9]]])>
color = <tf.Tensor: shape=(2, 1, 3), dtype=float32, numpy=
array([[[0., 0., 0.]],

       [[0., 0., 0.]]], dtype=float32)>, fill = False

    def tensorflow_draw_rectangle(image, rectangle, color=None, fill=None):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_long_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_clamp_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_tensor_frnt
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
        batch, c, h, w = tensorflow_shape_frnt_(image)
        batch_rect, num_rectangle, num_points = tensorflow_shape_frnt_(rectangle)
        if batch != batch_rect:
            raise AssertionError("Image batch and rectangle batch must be equal")
        if num_points != 4:
            raise AssertionError("Number of points in rectangle must be 4")
        rectangle = tensorflow_clone_frnt_(tensorflow_long_frnt_(rectangle))
        rectangle = tensorflow_set_item_bknd(
            rectangle,
            (slice(None, None, None), slice(None, None, None), slice(1, None, 2)),
            tensorflow_clamp_frnt(rectangle[:, :, 1::2], 0, h - 1),
        )
        rectangle = tensorflow_set_item_bknd(
            rectangle,
            (slice(None, None, None), slice(None, None, None), slice(None, None, 2)),
            tensorflow_clamp_frnt(rectangle[:, :, ::2], 0, w - 1),
        )
        if color is None:
            color = tensorflow_expand_frnt_(
                tensorflow_tensor_frnt([0.0] * c), batch, num_rectangle, c
            )
        if fill is None:
            fill = False
        if len(tensorflow_shape_frnt_(color)) == 1:
            color = tensorflow_expand_frnt_(color, batch, num_rectangle, c)
        b, n, color_channels = tensorflow_shape_frnt_(color)
        if color_channels == 1 and c == 3:
            color = tensorflow_expand_frnt_(color, batch, num_rectangle, c)
        for b in range(batch):
            for n in range(num_rectangle):
                if fill:
                    image = tensorflow_set_item_bknd(
                        image,
                        (
                            b,
                            slice(None, None, None),
                            slice(
                                int(rectangle[b, n, 1]), int(rectangle[b, n, 3] + 1), None
                            ),
                            slice(
                                int(rectangle[b, n, 0]), int(rectangle[b, n, 2] + 1), None
                            ),
                        ),
                        tensorflow_get_item(
                            color, (b, n, slice(None, None, None), None, None)
                        ),
                    )
                else:
                    image = tensorflow_set_item_bknd(
                        image,
                        (
                            b,
                            slice(None, None, None),
                            slice(
                                int(rectangle[b, n, 1]), int(rectangle[b, n, 3] + 1), None
                            ),
                            rectangle[b, n, 0],
                        ),
                        tensorflow_get_item(color, (b, n, slice(None, None, None), None)),
                    )
>                   image = tensorflow_set_item_bknd(
                        image,
                        (
                            b,
                            slice(None, None, None),
                            slice(
                                int(rectangle[b, n, 1]), int(rectangle[b, n, 3] + 1), None
                            ),
                            rectangle[b, n, 2],
                        ),
                        tensorflow_get_item(color, (b, n, slice(None, None, None), None)),
                    )

Translated_Outputs/tensorflow_outputs/kornia/utils/draw.py:275: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inp = <tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
    ...00, 0.00000000e+00, 0.00000000e+00,
          0.00000000e+00, 0.00000000e+00, 3.55352640e-01]]]],
      dtype=float32)>
query = (0, slice(None, None, None), slice(0, 5, None), <tf.Tensor: shape=(), dtype=int64, numpy=4>)
val = <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.],
       [0.],
       [0.]], dtype=float32)>, kwargs = {}

    @functools.wraps(fn)
    def wrapper(inp, query, val, **kwargs):
        try:
            inp.__setitem__(query, val)
            res = inp
        except IndexError:
            raise
        except Exception:
>           res = fn(inp, query, val, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
   ...64, numpy=4>), <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.],
       [0.],
       [0.]], dtype=float32)>)
kwargs = {}, handle_mixed_in_backend = True

    @functools.wraps(fn)
    def _handle_partial_mixed_function(*args, **kwargs):
        handle_mixed_in_backend = False
        if not hasattr(fn, "partial_mixed_handler"):
            handle_mixed_in_backend = True
        else:
            compos = getattr(fn, "compos")
            condition = getattr(fn, "partial_mixed_handler")
        if handle_mixed_in_backend or condition(*args, **kwargs):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
    ...00, 0.00000000e+00, 0.00000000e+00,
          0.00000000e+00, 0.00000000e+00, 3.55352640e-01]]]],
      dtype=float32)>
query = (0, slice(None, None, None), slice(0, 5, None), <tf.Tensor: shape=(), dtype=int64, numpy=4>)
val = <tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.],
       [0.],
       [0.]], dtype=float32)>

    @tensorflow_handle_set_item
    @tensorflow_handle_partial_mixed_function
    def tensorflow_set_item_bknd(
        x: Union[tensorflow.Tensor, tf.Tensor],
        query: Union[tensorflow.Tensor, tf.Tensor, Tuple],
        val: Union[tensorflow.Tensor, tf.Tensor],
        /,
        *,
        copy: Optional[bool] = False,
    ):
        from ..backends.tensorflow.general import tensorflow_shape
        from ..backends.tensorflow.creation import tensorflow_copy_array
        from ..backends.tensorflow.creation import tensorflow_asarray
        from .data_type import tensorflow_is_bool_dtype_bknd
        from ..backends.tensorflow.manipulation import tensorflow_tile
        from ..backends.tensorflow.searching import tensorflow_nonzero
        from ...data_classes.array.data_type import tensorflow_astype_bknd_
        from ..backends.tensorflow.general import tensorflow_scatter_nd
    
        if copy:
            x = tensorflow_copy_array(x)
        if not tensorflow_is_array_bknd(val):
            val = tensorflow_asarray(val)
        if 0 in x.shape or 0 in val.shape:
            return x
        if tensorflow_is_array_bknd(query) and tensorflow_is_bool_dtype_bknd(query):
            if not len(query.shape):
                query = tensorflow_tile(query, (x.shape[0],))
            indices = tensorflow_nonzero(query, as_tuple=False)
        else:
            indices, target_shape, _ = tensorflow__parse_query_bknd(
                query, tensorflow_shape(x, as_array=True), scatter=True
            )
            if indices is None:
                return x
        val = tensorflow_astype_bknd_(val, x.dtype)
>       ret = tensorflow_scatter_nd(indices, val, reduction="replace", out=x)

Translated_Outputs/tensorflow_outputs/ivy/functional/ivy/general.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

indices = <tf.Tensor: shape=(3, 5, 4), dtype=int64, numpy=
array([[[0, 4, 0, 0],
        [0, 4, 0, 1],
        [0, 4, 0, 2],
   ... 4]],

       [[0, 4, 2, 0],
        [0, 4, 2, 1],
        [0, 4, 2, 2],
        [0, 4, 2, 3],
        [0, 4, 2, 4]]])>
updates = <tf.Tensor: shape=(3, 5), dtype=float32, numpy=
array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)>, shape = None

    def tensorflow_scatter_nd(
        indices: Union[tensorflow.Tensor, tensorflow.Variable],
        updates: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Optional[Union[tf.TensorShape, Sequence[int]]] = None,
        *,
        reduction: str = "sum",
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.general import tensorflow_exists_bknd
        from ...ivy.data_type import tensorflow_promote_types_bknd
        from .data_type import tensorflow_as_native_dtype
        from ...ivy.general import tensorflow__broadcast_to_bknd
        from ....utils.assertions import tensorflow_check_equal
        from .elementwise import tensorflow_multiply
    
        updates_dtype = updates.dtype
        if tensorflow_exists_bknd(out):
            dtype = tensorflow_promote_types_bknd(out.dtype, updates_dtype)
        updates = tensorflow.cast(
            updates,
            tensorflow_as_native_dtype(dtype)
            if tensorflow_exists_bknd(out)
            else updates_dtype,
        )
        expected_shape = (
            list(tensorflow.shape(indices)[:-1])
            + list(out.shape[tensorflow.shape(indices)[-1] :])
            if tensorflow_exists_bknd(out)
            else list(tensorflow.shape(indices)[:-1])
            + list(shape[tensorflow.shape(indices)[-1] :])
        )
        updates = tensorflow__broadcast_to_bknd(updates, expected_shape)
        if len(updates.shape) == 0:
            indices = tensorflow.expand_dims(indices, 0)
            updates = tensorflow.expand_dims(updates, 0)
        target = out
        target_given = tensorflow_exists_bknd(target)
        if tensorflow_exists_bknd(shape) and target_given:
            tensorflow_check_equal(tuple(target.shape), tuple(shape), as_array=False)
        if not target_given:
            shape = list(shape) if tensorflow_exists_bknd(shape) else list(out.shape)
            target = tensorflow.zeros(shape, dtype=updates.dtype)
        if reduction == "sum":
            res = tensorflow.tensor_scatter_nd_add(target, indices, updates)
        elif reduction == "min":
            res = tensorflow.tensor_scatter_nd_min(target, indices, updates)
        elif reduction == "max":
            res = tensorflow.tensor_scatter_nd_max(target, indices, updates)
        elif reduction == "mul":
            updates = tensorflow_multiply(tensorflow_gather_nd(target, indices), updates)
            res = tensorflow.tensor_scatter_nd_update(target, indices, updates)
        elif reduction == "replace":
>           res = tensorflow.tensor_scatter_nd_update(target, indices, updates)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/general.py:274: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 3, 10, 12), dtype=float32, numpy=
array([[[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
   ...loat32, numpy=
array([[0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.],
       [0., 0., 0., 0., 0.]], dtype=float32)>)
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__TensorScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,0] = [0, 4, 0, 0] does not index into shape [2,3,10,12] [Op:TensorScatterUpdate] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.draw.draw_rectangle
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-12 00:18:13.538160: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at scatter_nd_op.cc:218 : INVALID_ARGUMENT: indices[0,0] = [0, 4, 0, 0] does not index into shape [2,3,10,12]
______________________________________________________________________________ test_tensor_to_image[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_tensor_to_image(target_framework, mode, backend_compile):
        print("kornia.utils.tensor_to_image")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_func = ivy.transpile(kornia.utils.tensor_to_image, source="torch", target=target_framework)
    
        tensor = torch.ones(1, 3, 3)
        transpiled_tensor = _nest_torch_tensor_to_new_framework(tensor, target_framework)
        torch_image = kornia.utils.tensor_to_image(tensor)
>       transpiled_image = transpiled_func(transpiled_tensor)

kornia/test_utils.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 1., 1.],
        [1., 1., 1.],
        [1., 1., 1.]]], dtype=float32)>, keepdim = False, force_contiguous = False

    def tensorflow_tensor_to_image(tensor, keepdim=False, force_contiguous=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_detach_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_numpy_frnt_
    
        if not isinstance(tensor, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(tensor)}")
        if (
            len(tensorflow_shape_frnt_(tensor)) > 4
            or len(tensorflow_shape_frnt_(tensor)) < 2
        ):
            raise ValueError("Input size must be a two, three or four dimensional tensor")
        input_shape = tensorflow_shape_frnt_(tensor)
        image = tensorflow_detach_frnt_(tensor.cpu())
        if len(input_shape) == 2:
            pass
        elif len(input_shape) == 3:
            if input_shape[0] == 1:
                image = tensorflow_squeeze_frnt_(image)
            else:
                image = tensorflow_permute_frnt_(image, 1, 2, 0)
        elif len(input_shape) == 4:
            image = tensorflow_permute_frnt_(image, 0, 2, 3, 1)
            if input_shape[0] == 1 and not keepdim:
                image = tensorflow_squeeze_frnt_(image, 0)
            if input_shape[1] == 1:
                image = tensorflow_squeeze_frnt_(image, -1)
        else:
            raise ValueError(f"Cannot process tensor with shape {input_shape}")
        if force_contiguous:
            image = tensorflow_contiguous_frnt_(image)
>       return tensorflow_numpy_frnt_(image)

Translated_Outputs/tensorflow_outputs/kornia/utils/image.py:64: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>,), kwargs = {}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f0f493643a0>
array_like = <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>

    @tensorflow_handle_methods
    def tensorflow_numpy_frnt_(tensor):
>       return np_frontend_array(tensor)
E       NameError: name 'np_frontend_array' is not defined

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:381: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.tensor_to_image
____________________________________________________________________________ test_save_pointcloud_ply[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_save_pointcloud_ply(target_framework, mode, backend_compile):
        print("kornia.utils.save_pointcloud_ply")
    
        if backend_compile:
            pytest.skip()
    
        pointcloud = torch.rand(100, 3)
        transpiled_pointcloud = _nest_torch_tensor_to_new_framework(pointcloud, target_framework)
    
        with tempfile.NamedTemporaryFile(suffix=".ply") as temp_file:
            filename = temp_file.name
    
            transpiled_save_pointcloud = ivy.transpile(kornia.utils.save_pointcloud_ply, source="torch", target=target_framework)
            transpiled_load_pointcloud = ivy.transpile(kornia.utils.load_pointcloud_ply, source="torch", target=target_framework)
    
            # Save and load pointcloud to ensure both steps work correctly
            transpiled_save_pointcloud(filename, transpiled_pointcloud)
>           loaded_pointcloud = transpiled_load_pointcloud(filename)

kornia/test_utils.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

filename = '/tmp/tmpo6gd1s0a.ply', header_size = 8

    def tensorflow_load_pointcloud_ply(filename, header_size=8):
        from ...genericpath import tensorflow_isfile
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_split_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_tensor_frnt
    
        if not isinstance(filename, (str,)) and filename[-3:] == ".ply":
            raise TypeError(
                f"Input filename must be a string in with the .ply  extension. Got {filename}"
            )
>       if not tensorflow_isfile(filename):

Translated_Outputs/tensorflow_outputs/kornia/utils/pointcloud_io.py:91: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

path = '/tmp/tmpo6gd1s0a.ply'

    def tensorflow_isfile(path):
        try:
            st = os.stat(path)
        except (OSError, ValueError):
            return False
>       return stat.S_ISREG(st.st_mode)
E       NameError: name 'stat' is not defined

Translated_Outputs/tensorflow_outputs/genericpath.py:35: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.save_pointcloud_ply
_______________________________________________________________________ test_get_cuda_device_if_available[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_cuda_device_if_available(target_framework, mode, backend_compile):
        print("kornia.utils.get_cuda_device_if_available")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_get_cuda_device_if_available = ivy.transpile(kornia.utils.get_cuda_device_if_available, source="torch", target=target_framework)
    
        torch_device = kornia.utils.get_cuda_device_if_available()
>       transpiled_device = transpiled_get_cuda_device_if_available()

kornia/test_utils.py:283: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

index = 0

    def tensorflow_get_cuda_device_if_available(index=0):
        from ...ivy.functional.frontends.torch.__init__ import tensorflow_device_frnt
    
>       if cuda.is_available():
E       NameError: name 'cuda' is not defined

Translated_Outputs/tensorflow_outputs/kornia/utils/helpers.py:31: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.get_cuda_device_if_available
________________________________________________________________________ test_get_mps_device_if_available[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_mps_device_if_available(target_framework, mode, backend_compile):
        print("kornia.utils.get_mps_device_if_available")
    
        if backend_compile:
            pytest.skip()
    
>       transpiled_get_mps_device_if_available = ivy.transpile(kornia.utils.get_mps_device_if_available, source="torch", target=target_framework)

kornia/test_utils.py:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <function get_mps_device_if_available at 0x7f0f50f07b50>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Unsupported type: None

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.get_mps_device_if_available
____________________________________________________________________ test_get_cuda_or_mps_device_if_available[tensorflow-s2s-False] ____________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_cuda_or_mps_device_if_available(target_framework, mode, backend_compile):
        print("kornia.utils.get_cuda_or_mps_device_if_available")
    
        if backend_compile:
            pytest.skip()
    
>       transpiled_get_cuda_or_mps_device_if_available = ivy.transpile(kornia.utils.get_cuda_or_mps_device_if_available, source="torch", target=target_framework)

kornia/test_utils.py:310: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <function get_cuda_or_mps_device_if_available at 0x7f0f50f07be0>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Unsupported type: None

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.utils.get_cuda_or_mps_device_if_available
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_utils.py::test_draw_rectangle[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: {{function_node __wrapped__TensorScatterUpdate_device_/job:loc...
FAILED kornia/test_utils.py::test_tensor_to_image[tensorflow-s2s-False] - NameError: name 'np_frontend_array' is not defined
FAILED kornia/test_utils.py::test_save_pointcloud_ply[tensorflow-s2s-False] - NameError: name 'stat' is not defined
FAILED kornia/test_utils.py::test_get_cuda_device_if_available[tensorflow-s2s-False] - NameError: name 'cuda' is not defined
FAILED kornia/test_utils.py::test_get_mps_device_if_available[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Unsupported type: None
FAILED kornia/test_utils.py::test_get_cuda_or_mps_device_if_available[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Unsupported type: None
=============================================================================== 6 failed, 10 passed in 240.48s (0:04:00) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

kornia/geometry/test_quaternion.py .                                                                                                                                                             [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 1 passed in 114.10s (0:01:54) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 4 items

kornia/test_sensors.py ....                                                                                                                                                                      [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
===================================================================================== 4 passed in 89.15s (0:01:29) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/geometry/test_boxes.py FF                                                                                                                                                                 [100%]

=============================================================================================== FAILURES ===============================================================================================
___________________________________________________________________________________ test_Boxes[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Boxes(target_framework, mode, backend_compile):
        print("kornia.geometry.boxes.Boxes")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledBoxes = ivy.transpile(kornia.geometry.boxes.Boxes, source="torch", target=target_framework)
    
        torch_args = (
            torch.as_tensor([[0, 3, 1, 4], [5, 1, 8, 4]]),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        # test .from_tensor
        torch_boxes = kornia.geometry.boxes.Boxes.from_tensor(*torch_args, mode="xyxy")
        transpiled_boxes = TranspiledBoxes.from_tensor(*transpiled_args, mode="xyxy")
        _check_boxes_same(torch_boxes, transpiled_boxes)
    
        # test .compute_area
        torch_area = torch_boxes.compute_area()
        transpiled_area = transpiled_boxes.compute_area()
        _to_numpy_and_allclose(torch_area, transpiled_area)
    
        # test .get_boxes_shape
        torch_heights, torch_widths = torch_boxes.get_boxes_shape()
        transpiled_heights, transpiled_widths = transpiled_boxes.get_boxes_shape()
        _to_numpy_and_allclose(torch_heights, transpiled_heights)
        _to_numpy_and_allclose(torch_widths, transpiled_widths)
    
        # test .merge
        torch_x = torch.as_tensor([[6, 6, 10, 10], [6, 6, 10, 10]])
        transpiled_x = _nest_torch_tensor_to_new_framework(torch_x, target_framework)
        merge_boxes = kornia.geometry.boxes.Boxes.from_tensor(torch_x, mode="xyxy")
        transpiled_merge_boxes = TranspiledBoxes.from_tensor(transpiled_x, mode="xyxy")
        torch_merged_boxes = torch_boxes.merge(merge_boxes)
        transpiled_merged_boxes = transpiled_boxes.merge(transpiled_merge_boxes)
        _check_boxes_same(torch_merged_boxes, transpiled_merged_boxes)
    
        # test .to_mask
        height, width = 10, 10
        torch_mask = torch_boxes.to_mask(height, width)
        transpiled_mask = transpiled_boxes.to_mask(height, width)
>       _to_numpy_and_allclose(torch_mask, transpiled_mask)

kornia/geometry/test_boxes.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0....., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])
transpiled_x = <tf.Tensor: shape=(2, 10, 10), dtype=float32, numpy=
array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0....,
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0...],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)
y = array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0...],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.boxes.Boxes
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/ivy/ivy/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.
  warnings.warn(
__________________________________________________________________________________ test_Boxes3D[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Boxes3D(target_framework, mode, backend_compile):
        print("kornia.geometry.boxes.Boxes3D")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledBoxes3D = ivy.transpile(kornia.geometry.boxes.Boxes3D, source="torch", target=target_framework)
    
        torch_args = (
            torch.as_tensor([[0, 3, 6, 1, 4, 8], [5, 1, 3, 8, 4, 9]]),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        # test .from_tensor
        torch_boxes3d = kornia.geometry.boxes.Boxes3D.from_tensor(*torch_args, mode="xyzxyz")
        transpiled_boxes3d = TranspiledBoxes3D.from_tensor(*transpiled_args, mode="xyzxyz")
        _check_boxes_same(torch_boxes3d, transpiled_boxes3d)
    
        # test .get_boxes_shape
        torch_depths, torch_heights, torch_widths = torch_boxes3d.get_boxes_shape()
        transpiled_depths, transpiled_heights, transpiled_widths = transpiled_boxes3d.get_boxes_shape()
        _to_numpy_and_allclose(torch_depths, transpiled_depths)
        _to_numpy_and_allclose(torch_heights, transpiled_heights)
        _to_numpy_and_allclose(torch_widths, transpiled_widths)
    
        # test .to_mask
        depth, height, width = 10, 10, 10
        torch_mask = torch_boxes3d.to_mask(depth, height, width)
        transpiled_mask = transpiled_boxes3d.to_mask(depth, height, width)
>       _to_numpy_and_allclose(torch_mask, transpiled_mask)

kornia/geometry/test_boxes.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0... [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.],
          [0., 0., 0.,  ..., 0., 0., 0.]]]])
transpiled_x = <tf.Tensor: shape=(2, 10, 10, 10), dtype=float32, numpy=
array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0.,...., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]...0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)
y = array([[[[0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]...0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.],
         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.boxes.Boxes3D
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_boxes.py::test_Boxes[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_boxes.py::test_Boxes3D[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
==================================================================================== 2 failed in 184.84s (0:03:04) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/test_image.py ....FFFF                                                                                                                                                                    [100%]

=============================================================================================== FAILURES ===============================================================================================
___________________________________________________________________________________ test_Image[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image(target_framework, mode, backend_compile):
        print("kornia.image.Image")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImageSize = ivy.transpile(kornia.image.ImageSize, source="torch", target=target_framework)
        TranspiledPixelFormat = ivy.transpile(kornia.image.PixelFormat, source="torch", target=target_framework)
        TranspiledChannelsOrder = ivy.transpile(kornia.image.ChannelsOrder, source="torch", target=target_framework)
        TranspiledImageLayout = ivy.transpile(kornia.image.ImageLayout, source="torch", target=target_framework)
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        torch_data = torch.randint(0, 255, (3, 4, 5), dtype=torch.uint8)
        transpiled_data = _array_to_new_backend(torch_data, target_framework)
    
        # torch
        pixel_format = kornia.image.PixelFormat(
            color_space=ColorSpace.rgb,
            bit_depth=8,
        )
        layout = kornia.image.ImageLayout(
            image_size=kornia.image.ImageSize(4, 5),
            channels=3,
            channels_order=kornia.image.ChannelsOrder.CHANNELS_FIRST,
        )
        torch_img = kornia.image.Image(torch_data, pixel_format, layout)
    
        # transpiled
        pixel_format = TranspiledPixelFormat(
            color_space=ColorSpace.rgb,
            bit_depth=8,
        )
        layout = TranspiledImageLayout(
            image_size=TranspiledImageSize(4, 5),
            channels=3,
            channels_order=TranspiledChannelsOrder.CHANNELS_FIRST,
        )
>       transpiled_img = TranspiledImage(transpiled_data, pixel_format, layout)

kornia/test_image.py:111: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_Image' object has no attribute '_data'") raised in repr()] tensorflow_Image object at 0x7f0325e25c90>
data = <tf.Tensor: shape=(3, 4, 5), dtype=uint8, numpy=
array([[[ 87, 141, 183, 173, 151],
        [241,  40,  31, 234,  18],...       [141,  52, 137, 167, 134],
        [ 14, 153, 104,  38, 180],
        [236,  67, 158, 220,  17]]], dtype=uint8)>
pixel_format = tensorflow_PixelFormat(color_space=<ColorSpace.rgb: 0>, bit_depth=8)
layout = tensorflow_ImageLayout(image_size=tensorflow_ImageSize(height=4, width=5), channels=3, channels_order=<tensorflow_ChannelsOrder.CHANNELS_FIRST: 0>)

    def __init__(self, data, pixel_format, layout):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
    
        if layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            shape = [
                str(layout.channels),
                str(layout.image_size.height),
                str(layout.image_size.width),
            ]
        elif layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            shape = [
                str(layout.image_size.height),
                str(layout.image_size.width),
                str(layout.channels),
            ]
        else:
            raise NotImplementedError(
                f"Layout {layout.channels_order} not implemented."
            )
        tensorflow_KORNIA_CHECK_SHAPE(data, shape)
        tensorflow_KORNIA_CHECK(
>           data.element_size() == pixel_format.bit_depth // 8, "Invalid bit depth."
        )

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Tensor: shape=(3, 4, 5), dtype=uint8, numpy=
array([[[ 87, 141, 183, 173, 151],
        [241,  40,  31, 234,  18],...       [141,  52, 137, 167, 134],
        [ 14, 153, 104,  38, 180],
        [236,  67, 158, 220,  17]]], dtype=uint8)>
name = 'element_size'

    def __getattr__(self, name):
      if name in {"T", "astype", "ravel", "transpose", "reshape", "clip", "size",
                  "tolist", "data"}:
        # TODO(wangpeng): Export the enable_numpy_behavior knob
        raise AttributeError(
            f"{type(self).__name__} object has no attribute '{name}'. " + """
          If you are looking for numpy-related methods, please run the following:
          tf.experimental.numpy.experimental_enable_numpy_behavior()
        """)
>     self.__getattribute__(name)
E     AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'

/opt/fw/tensorflow/tensorflow/python/framework/tensor.py:260: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image
_____________________________________________________________________________ test_Image_from_numpy[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image_from_numpy(target_framework, mode, backend_compile):
        print("kornia.image.Image.from_numpy")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        data = np.ones((4, 5, 3), dtype=np.uint8)
>       img = TranspiledImage.from_numpy(data, color_space=ColorSpace.rgb)

kornia/test_image.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'Translated_Outputs.tensorflow_outputs.kornia.image.image.tensorflow_Image'>
data = array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]],

       [[1, 1, 1],
  ... 1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)
color_space = <ColorSpace.rgb: 0>, channels_order = <tensorflow_ChannelsOrder.CHANNELS_LAST: 1>

    @classmethod
    def from_numpy(
        cls,
        data,
        color_space=tensorflow_ColorSpace.RGB,
        channels_order=tensorflow_ChannelsOrder.CHANNELS_LAST,
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .base import tensorflow_ImageSize
        from .base import tensorflow_PixelFormat
        from ...ivy.data_classes.array.array import tensorflow_itemsize_bknd_
        from .base import tensorflow_ImageLayout
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_from_numpy_frnt,
        )
    
        if channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[0],
                width=tensorflow_shape_frnt_(data)[1],
            )
            channels = tensorflow_shape_frnt_(data)[2]
        elif channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[1],
                width=tensorflow_shape_frnt_(data)[2],
            )
            channels = tensorflow_shape_frnt_(data)[0]
        else:
            raise ValueError(
                "channels_order must be either `CHANNELS_LAST` or `CHANNELS_FIRST`"
            )
        pixel_format = tensorflow_PixelFormat(
            color_space=color_space, bit_depth=tensorflow_itemsize_bknd_(data) * 8
        )
        layout = tensorflow_ImageLayout(
            image_size=image_size, channels=channels, channels_order=channels_order
        )
>       return cls(tensorflow_from_numpy_frnt(data), pixel_format, layout)

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_Image' object has no attribute '_data'") raised in repr()] tensorflow_Image object at 0x7f0327433100>
data = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
pixel_format = tensorflow_PixelFormat(color_space=<ColorSpace.rgb: 0>, bit_depth=8)
layout = tensorflow_ImageLayout(image_size=tensorflow_ImageSize(height=4, width=5), channels=3, channels_order=<tensorflow_ChannelsOrder.CHANNELS_LAST: 1>)

    def __init__(self, data, pixel_format, layout):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
    
        if layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            shape = [
                str(layout.channels),
                str(layout.image_size.height),
                str(layout.image_size.width),
            ]
        elif layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            shape = [
                str(layout.image_size.height),
                str(layout.image_size.width),
                str(layout.channels),
            ]
        else:
            raise NotImplementedError(
                f"Layout {layout.channels_order} not implemented."
            )
        tensorflow_KORNIA_CHECK_SHAPE(data, shape)
        tensorflow_KORNIA_CHECK(
>           data.element_size() == pixel_format.bit_depth // 8, "Invalid bit depth."
        )

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
name = 'element_size'

    def __getattr__(self, name):
      if name in {"T", "astype", "ravel", "transpose", "reshape", "clip", "size",
                  "tolist", "data"}:
        # TODO(wangpeng): Export the enable_numpy_behavior knob
        raise AttributeError(
            f"{type(self).__name__} object has no attribute '{name}'. " + """
          If you are looking for numpy-related methods, please run the following:
          tf.experimental.numpy.experimental_enable_numpy_behavior()
        """)
>     self.__getattribute__(name)
E     AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'

/opt/fw/tensorflow/tensorflow/python/framework/tensor.py:260: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image.from_numpy
_____________________________________________________________________________ test_Image_from_dlpack[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image_from_dlpack(target_framework, mode, backend_compile):
        print("kornia.image.Image.from_dlpack")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        x = np.ones((4, 5, 3))
>       img = TranspiledImage.from_dlpack(x.__dlpack__())

kornia/test_image.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'Translated_Outputs.tensorflow_outputs.kornia.image.image.tensorflow_Image'>, data = <capsule object "dltensor" at 0x7f032c57bba0>

    @classmethod
    def from_dlpack(cls, data):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .base import tensorflow_ImageSize
        from .base import tensorflow_PixelFormat
        from .base import tensorflow_ImageLayout
        from ...ivy.data_classes.array.creation import tensorflow_from_dlpack_bknd_
    
>       _data: typing.Any = tensorflow_from_dlpack_bknd_(torch.utils.dlpack, data)
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:188: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image.from_dlpack
______________________________________________________________________________ test_Image_to_numpy[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Image_to_numpy(target_framework, mode, backend_compile):
        print("kornia.image.Image.to_numpy")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledImage = ivy.transpile(kornia.image.Image, source="torch", target=target_framework)
    
        data = np.ones((4, 5, 3), dtype=np.uint8)
>       img = TranspiledImage.from_numpy(data, color_space=ColorSpace.rgb)

kornia/test_image.py:160: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'Translated_Outputs.tensorflow_outputs.kornia.image.image.tensorflow_Image'>
data = array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]],

       [[1, 1, 1],
  ... 1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)
color_space = <ColorSpace.rgb: 0>, channels_order = <tensorflow_ChannelsOrder.CHANNELS_LAST: 1>

    @classmethod
    def from_numpy(
        cls,
        data,
        color_space=tensorflow_ColorSpace.RGB,
        channels_order=tensorflow_ChannelsOrder.CHANNELS_LAST,
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .base import tensorflow_ImageSize
        from .base import tensorflow_PixelFormat
        from ...ivy.data_classes.array.array import tensorflow_itemsize_bknd_
        from .base import tensorflow_ImageLayout
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_from_numpy_frnt,
        )
    
        if channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[0],
                width=tensorflow_shape_frnt_(data)[1],
            )
            channels = tensorflow_shape_frnt_(data)[2]
        elif channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            image_size = tensorflow_ImageSize(
                height=tensorflow_shape_frnt_(data)[1],
                width=tensorflow_shape_frnt_(data)[2],
            )
            channels = tensorflow_shape_frnt_(data)[0]
        else:
            raise ValueError(
                "channels_order must be either `CHANNELS_LAST` or `CHANNELS_FIRST`"
            )
        pixel_format = tensorflow_PixelFormat(
            color_space=color_space, bit_depth=tensorflow_itemsize_bknd_(data) * 8
        )
        layout = tensorflow_ImageLayout(
            image_size=image_size, channels=channels, channels_order=channels_order
        )
>       return cls(tensorflow_from_numpy_frnt(data), pixel_format, layout)

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_Image' object has no attribute '_data'") raised in repr()] tensorflow_Image object at 0x7f032739fd30>
data = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
pixel_format = tensorflow_PixelFormat(color_space=<ColorSpace.rgb: 0>, bit_depth=8)
layout = tensorflow_ImageLayout(image_size=tensorflow_ImageSize(height=4, width=5), channels=3, channels_order=<tensorflow_ChannelsOrder.CHANNELS_LAST: 1>)

    def __init__(self, data, pixel_format, layout):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
    
        if layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_FIRST:
            shape = [
                str(layout.channels),
                str(layout.image_size.height),
                str(layout.image_size.width),
            ]
        elif layout.channels_order == tensorflow_ChannelsOrder.CHANNELS_LAST:
            shape = [
                str(layout.image_size.height),
                str(layout.image_size.width),
                str(layout.channels),
            ]
        else:
            raise NotImplementedError(
                f"Layout {layout.channels_order} not implemented."
            )
        tensorflow_KORNIA_CHECK_SHAPE(data, shape)
        tensorflow_KORNIA_CHECK(
>           data.element_size() == pixel_format.bit_depth // 8, "Invalid bit depth."
        )

Translated_Outputs/tensorflow_outputs/kornia/image/image.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Tensor: shape=(4, 5, 3), dtype=uint8, numpy=
array([[[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, ...1, 1]],

       [[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]]], dtype=uint8)>
name = 'element_size'

    def __getattr__(self, name):
      if name in {"T", "astype", "ravel", "transpose", "reshape", "clip", "size",
                  "tolist", "data"}:
        # TODO(wangpeng): Export the enable_numpy_behavior knob
        raise AttributeError(
            f"{type(self).__name__} object has no attribute '{name}'. " + """
          If you are looking for numpy-related methods, please run the following:
          tf.experimental.numpy.experimental_enable_numpy_behavior()
        """)
>     self.__getattribute__(name)
E     AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'

/opt/fw/tensorflow/tensorflow/python/framework/tensor.py:260: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.image.Image.to_numpy
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_image.py::test_Image[tensorflow-s2s-False] - AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'
FAILED kornia/test_image.py::test_Image_from_numpy[tensorflow-s2s-False] - AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'
FAILED kornia/test_image.py::test_Image_from_dlpack[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_image.py::test_Image_to_numpy[tensorflow-s2s-False] - AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'element_size'
=============================================================================== 4 failed, 4 passed in 190.25s (0:03:10) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/geometry/test_vector.py ..                                                                                                                                                                [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
===================================================================================== 2 passed in 91.98s (0:01:31) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 5 items

kornia/test_x.py sssss                                                                                                                                                                           [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 5 skipped in 106.64s (0:01:46) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 3 items

kornia/augmentation/test_auto.py FFF                                                                                                                                                             [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________________ test_AutoAugment[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_AutoAugment(target_framework, mode, backend_compile):
        print("kornia.augmentation.auto.AutoAugment")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledAutoAugment = ivy.transpile(
            kornia.augmentation.auto.AutoAugment,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_auto.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.auto.autoaugment.autoaugment.AutoAugment'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.auto.AutoAugment
________________________________________________________________________________ test_RandAugment[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandAugment(target_framework, mode, backend_compile):
        print("kornia.augmentation.auto.RandAugment")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledRandAugment = ivy.transpile(
            kornia.augmentation.auto.RandAugment,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_auto.py:62: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.auto.rand_augment.rand_augment.RandAugment'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.auto.RandAugment
______________________________________________________________________________ test_TrivialAugment[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_TrivialAugment(target_framework, mode, backend_compile):
        print("kornia.augmentation.auto.TrivialAugment")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledTrivialAugment = ivy.transpile(
            kornia.augmentation.auto.TrivialAugment,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_auto.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.auto.trivial_augment.trivial_augment.TrivialAugment'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.auto.TrivialAugment
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/augmentation/test_auto.py::test_AutoAugment[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix....
FAILED kornia/augmentation/test_auto.py::test_RandAugment[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix....
FAILED kornia/augmentation/test_auto.py::test_TrivialAugment[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.m...
==================================================================================== 3 failed in 299.99s (0:04:59) =====================================================================================


========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

transformers/test_vision.py .                                                                                                                                                                    [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 1 passed in 1056.34s (0:17:36) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

kornia/test_tracking.py F                                                                                                                                                                        [100%]

=============================================================================================== FAILURES ===============================================================================================
_____________________________________________________________________________ test_HomographyTracker[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HomographyTracker(target_framework, mode, backend_compile):
        print("kornia.tracking.HomographyTracker")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledHomographyTracker = ivy.transpile(kornia.tracking.HomographyTracker, source="torch", target=target_framework)

kornia/test_tracking.py:30: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.tracking.planar_tracker.HomographyTracker'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, line 77)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.tracking.HomographyTracker
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/ivy/ivy/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.
  warnings.warn(
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_tracking.py::test_HomographyTracker[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>...
==================================================================================== 1 failed in 705.13s (0:11:45) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 38 items

kornia/geometry/test_conversions.py ......................................                                                                                                                       [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 38 passed in 292.83s (0:04:52) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/test_nerf.py FF.FFFFF                                                                                                                                                                     [100%]

=============================================================================================== FAILURES ===============================================================================================
_________________________________________________________________________________ test_NerfModel[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_NerfModel(target_framework, mode, backend_compile):
        print("kornia.nerf.nerf_model.NerfModel")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledNerfModel = ivy.transpile(nerf_model.NerfModel, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(5, 3),
            torch.rand(5, 3),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_nerf = nerf_model.NerfModel(num_ray_points=32)
>       transpiled_nerf = TranspiledNerfModel(num_ray_points=32)

kornia/test_nerf.py:34: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_NerfModel(
  (_renderer): tensorflow_IrregularRenderer()
  (_pos_encoder): tensorflow_PositionalEncoder()
 ...ense()
  (_fc2): tensorflow_Sequential(
    (0): KerasDense()
    (1): tensorflow_ReLU()
  )
  (_sigma): KerasDense()
)
num_ray_points = 32, irregular_ray_sampling = True, num_pos_freqs = 10, num_dir_freqs = 4, num_units = 2, num_unit_layers = 4, num_hidden = 128, log_space_encoding = True

    def __init__(
        self,
        num_ray_points,
        irregular_ray_sampling=True,
        num_pos_freqs=10,
        num_dir_freqs=4,
        num_units=2,
        num_unit_layers=4,
        num_hidden=128,
        log_space_encoding=True,
    ):
        from .volume_renderer import tensorflow_IrregularRenderer
        from .volume_renderer import tensorflow_RegularRenderer
        from .positional_encoder import tensorflow_PositionalEncoder
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.init import tensorflow_xavier_uniform_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_data_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
        from ...torch.nn.modules.activation import tensorflow_Sigmoid
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            num_ray_points,
            irregular_ray_sampling=irregular_ray_sampling,
            num_pos_freqs=num_pos_freqs,
            num_dir_freqs=num_dir_freqs,
            num_units=num_units,
            num_unit_layers=num_unit_layers,
            num_hidden=num_hidden,
            log_space_encoding=log_space_encoding,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self._num_ray_points = num_ray_points
        self._irregular_ray_sampling = irregular_ray_sampling
        self._renderer = (
            tensorflow_IrregularRenderer()
            if self._irregular_ray_sampling
            else tensorflow_RegularRenderer()
        )
        self._pos_encoder = tensorflow_PositionalEncoder(
            3, num_pos_freqs, log_space=log_space_encoding
        )
        self._dir_encoder = tensorflow_PositionalEncoder(
            3, num_dir_freqs, log_space=log_space_encoding
        )
        self._mlp = tensorflow_MLP(
            self._pos_encoder.num_encoded_dims, num_units, num_unit_layers, num_hidden
        )
        self._fc1 = KerasDense(in_features=num_hidden, units=num_hidden, use_bias=True)
        self._fc2 = tensorflow_Sequential(
            KerasDense(
                in_features=num_hidden + self._dir_encoder.num_encoded_dims,
                units=num_hidden // 2,
                use_bias=True,
            ),
            tensorflow_ReLU(),
        )
        self._sigma = KerasDense(in_features=num_hidden, units=1, use_bias=True)
        self._sigma.weight = tensorflow_xavier_uniform_(
            tensorflow_data_frnt_(self._sigma.weight)
        )
>       self._sigma.bias.data = tensorflow_float_frnt_(tensorflow_tensor_frnt([0.1]))
E       AttributeError: can't set attribute

Translated_Outputs/tensorflow_outputs/kornia/nerf/nerf_model.py:507: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.nerf_model.NerfModel
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/ivy/ivy/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.
  warnings.warn(
_____________________________________________________________________________ test_NerfModelRenderer[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_NerfModelRenderer(target_framework, mode, backend_compile):
        print("kornia.nerf.nerf_model.NerfModelRenderer")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)
        TranspiledNerfModel = ivy.transpile(nerf_model.NerfModel, source="torch", target=target_framework)
        TranspiledNerfModelRenderer = ivy.transpile(nerf_model.NerfModelRenderer, source="torch", target=target_framework)
    
        torch_nerf_model = nerf_model.NerfModel(num_ray_points=32)
>       transpiled_nerf_model = TranspiledNerfModel(num_ray_points=32)

kornia/test_nerf.py:55: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_NerfModel(
  (_renderer): tensorflow_IrregularRenderer()
  (_pos_encoder): tensorflow_PositionalEncoder()
 ...ense()
  (_fc2): tensorflow_Sequential(
    (0): KerasDense()
    (1): tensorflow_ReLU()
  )
  (_sigma): KerasDense()
)
num_ray_points = 32, irregular_ray_sampling = True, num_pos_freqs = 10, num_dir_freqs = 4, num_units = 2, num_unit_layers = 4, num_hidden = 128, log_space_encoding = True

    def __init__(
        self,
        num_ray_points,
        irregular_ray_sampling=True,
        num_pos_freqs=10,
        num_dir_freqs=4,
        num_units=2,
        num_unit_layers=4,
        num_hidden=128,
        log_space_encoding=True,
    ):
        from .volume_renderer import tensorflow_IrregularRenderer
        from .volume_renderer import tensorflow_RegularRenderer
        from .positional_encoder import tensorflow_PositionalEncoder
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.init import tensorflow_xavier_uniform_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_data_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
        from ...torch.nn.modules.activation import tensorflow_Sigmoid
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            num_ray_points,
            irregular_ray_sampling=irregular_ray_sampling,
            num_pos_freqs=num_pos_freqs,
            num_dir_freqs=num_dir_freqs,
            num_units=num_units,
            num_unit_layers=num_unit_layers,
            num_hidden=num_hidden,
            log_space_encoding=log_space_encoding,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self._num_ray_points = num_ray_points
        self._irregular_ray_sampling = irregular_ray_sampling
        self._renderer = (
            tensorflow_IrregularRenderer()
            if self._irregular_ray_sampling
            else tensorflow_RegularRenderer()
        )
        self._pos_encoder = tensorflow_PositionalEncoder(
            3, num_pos_freqs, log_space=log_space_encoding
        )
        self._dir_encoder = tensorflow_PositionalEncoder(
            3, num_dir_freqs, log_space=log_space_encoding
        )
        self._mlp = tensorflow_MLP(
            self._pos_encoder.num_encoded_dims, num_units, num_unit_layers, num_hidden
        )
        self._fc1 = KerasDense(in_features=num_hidden, units=num_hidden, use_bias=True)
        self._fc2 = tensorflow_Sequential(
            KerasDense(
                in_features=num_hidden + self._dir_encoder.num_encoded_dims,
                units=num_hidden // 2,
                use_bias=True,
            ),
            tensorflow_ReLU(),
        )
        self._sigma = KerasDense(in_features=num_hidden, units=1, use_bias=True)
        self._sigma.weight = tensorflow_xavier_uniform_(
            tensorflow_data_frnt_(self._sigma.weight)
        )
>       self._sigma.bias.data = tensorflow_float_frnt_(tensorflow_tensor_frnt([0.1]))
E       AttributeError: can't set attribute

Translated_Outputs/tensorflow_outputs/kornia/nerf/nerf_model.py:507: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.nerf_model.NerfModelRenderer
________________________________________________________________________________ test_NerfSolver[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_NerfSolver(target_framework, mode, backend_compile):
        print("kornia.nerf.nerf_solver.NerfSolver")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)
>       TranspiledNerfSolver = ivy.transpile(nerf_solver.NerfSolver, source="torch", target=target_framework)

kornia/test_nerf.py:110: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.nerf.nerf_solver.NerfSolver'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.nerf_solver.NerfSolver
_____________________________________________________________________________ test_IrregularRenderer[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_IrregularRenderer(target_framework, mode, backend_compile):
        print("kornia.nerf.volume_renderer.IrregularRenderer")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledIrregularRenderer = ivy.transpile(volume_renderer.IrregularRenderer, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(5, 32, 3),
            torch.rand(5, 32, 1),
            torch.rand(5, 32, 3),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_renderer = volume_renderer.IrregularRenderer(shift=1)
        transpiled_renderer = TranspiledIrregularRenderer(shift=1)
    
        torch_out = torch_renderer(*torch_args)
>       transpiled_out = transpiled_renderer(*transpiled_args)

kornia/test_nerf.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer()
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.07628357, 0.97804314, 0.01056957],
        [0.43244362...8821e-01, 4.91443634e-01, 6.91257894e-01],
        [1.78498864e-01, 7.54953921e-01, 1.75491214e-01]]], dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f4696b4a0e0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_IrregularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.07628357, 0.97804314, ...8821e-01, 4.91443634e-01, 6.91257894e-01],
        [1.78498864e-01, 7.54953921e-01, 1.75491214e-01]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.07628357, 0.97804314, 0.01056957],
        [0.43244362...8821e-01, 4.91443634e-01, 6.91257894e-01],
        [1.78498864e-01, 7.54953921e-01, 1.75491214e-01]]], dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_IrregularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.07628357, 0.97804314, ...8821e-01, 4.91443634e-01, 6.91257894e-01],
        [1.78498864e-01, 7.54953921e-01, 1.75491214e-01]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.07628357, 0.97804314, 0.01056957],
        [0.43244362...8821e-01, 4.91443634e-01, 6.91257894e-01],
        [1.78498864e-01, 7.54953921e-01, 1.75491214e-01]]], dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.07628357, 0.97804314, 0.01056957],
        [0.43244362,...9958453],
        [0.8157435 , 0.6345549 , 0.49871063],
        [0.37948716, 0.3766657 , 0.8017824 ]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (rgbs, densities, points_3d)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_IrregularRenderer(),)
kwargs = {'densities': <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[0.63775164],
        [0.6857669 ],
        ...958453],
        [0.8157435 , 0.6345549 , 0.49871063],
        [0.37948716, 0.3766657 , 0.8017824 ]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_IrregularRenderer()
rgbs = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.07628357, 0.97804314, 0.01056957],
        [0.43244362,...9958453],
        [0.8157435 , 0.6345549 , 0.49871063],
        [0.37948716, 0.3766657 , 0.8017824 ]]], dtype=float32)>
densities = <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[0.63775164],
        [0.6857669 ],
        [0.576144  ],
...[0.08863044],
        [0.5077086 ],
        [0.50700843],
        [0.9913041 ],
        [0.01414531]]], dtype=float32)>
points_3d = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[4.44257319e-01, 9.79796588e-01, 6.06035590e-01],
        ...48821e-01, 4.91443634e-01, 6.91257894e-01],
        [1.78498864e-01, 7.54953921e-01, 1.75491214e-01]]], dtype=float32)>

    def call(self, rgbs, densities, points_3d):
        from .samplers import tensorflow_calc_ray_t_vals
        from ...ivy.functional.frontends.torch.tensor import tensorflow_fill__frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_empty_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_exp_frnt
    
>       t_vals = tensorflow_calc_ray_t_vals(points_3d)

Translated_Outputs/tensorflow_outputs/kornia/nerf/volume_renderer.py:443: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

points_3d = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[4.44257319e-01, 9.79796588e-01, 6.06035590e-01],
        ...48821e-01, 4.91443634e-01, 6.91257894e-01],
        [1.78498864e-01, 7.54953921e-01, 1.75491214e-01]]], dtype=float32)>

    def tensorflow_calc_ray_t_vals(points_3d):
        from ...ivy.functional.frontends.torch.linalg import tensorflow_norm_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
    
>       t_vals = tensorflow_norm_frnt(
            points_3d - tensorflow_unsqueeze_frnt_(points_3d[..., 0, :], -2), dim=-1
        )

Translated_Outputs/tensorflow_outputs/kornia/nerf/samplers.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[ 0.        ,  0.        ,  0.        ],
        [ 0.12697...5],
        [-0.05070573, -0.2725498 ,  0.37143213],
        [-0.4121557 , -0.00903952, -0.14433455]]], dtype=float32)>
ord = None, dim = -1, keepdim = False

    def tensorflow_norm_frnt(
        input, ord=None, dim=None, keepdim=False, *, dtype=None, out=None
    ):
        from .tensor import tensorflow_ndim_frnt_
        from ...backends.tensorflow.linear_algebra import tensorflow_vector_norm
        from ...backends.tensorflow.linear_algebra import tensorflow_matrix_norm
        from ...backends.tensorflow.manipulation import tensorflow_flatten
        from ...backends.tensorflow.data_type import tensorflow_astype
    
        if dim is None and ord is not None:
            if tensorflow_ndim_frnt_(input) == 1:
                ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)
            else:
                ret = tensorflow_matrix_norm(input, keepdims=keepdim, ord=ord)
        elif dim is None and ord is None:
            input = tensorflow_flatten(input)
            ret = tensorflow_vector_norm(input, axis=0, keepdims=keepdim, ord=2)
        elif isinstance(dim, (int,)):
>           ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/linalg.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[ 0.        ,  0.        ,  0.        ],
        [ 0.1269...],
        [-0.05070573, -0.2725498 ,  0.37143213],
        [-0.4121557 , -0.00903952, -0.14433455]]], dtype=float32)>]
kwargs = {'axis': -1, 'keepdims': False, 'ord': None}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f4696d57ac0>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f4696d841f0>, tensorflow_asarray = <function tensorflow_asarray at 0x7f4696d871c0>
tensorflow_get_item = <function tensorflow_get_item at 0x7f4696d84af0>, num_args = 1
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'axis', 'keepdims', 'ord', 'dtype', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[int, ...es.DType], typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 0

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[ 0.        ,  0.        ,  0.        ],
        [ 0.12697...5],
        [-0.05070573, -0.2725498 ,  0.37143213],
        [-0.4121557 , -0.00903952, -0.14433455]]], dtype=float32)>

    @tensorflow_handle_array_like_without_promotion
    def tensorflow_vector_norm(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        axis: Optional[Union[int, Sequence[int]]] = None,
        keepdims: bool = False,
        ord: Union[int, float, Literal[inf, -inf]] = 2,
        dtype: Optional[tf.DType] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.constants import inf
    
        if dtype and x.dtype != dtype:
            x = tensorflow.cast(x, dtype)
        abs_x = tensorflow.abs(x)
        if ord == 0:
            return tensorflow.reduce_sum(
                tensorflow.cast(x != 0, abs_x.dtype), axis=axis, keepdims=keepdims
            )
        elif ord == inf:
            return tensorflow.reduce_max(abs_x, axis=axis, keepdims=keepdims)
        elif ord == -inf:
            return tensorflow.reduce_min(abs_x, axis=axis, keepdims=keepdims)
        else:
>           return tensorflow.reduce_sum(abs_x**ord, axis=axis, keepdims=keepdims) ** (
                1.0 / ord
            )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/linear_algebra.py:200: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[0.        , 0.        , 0.        ],
        [0.12697107...],
        [0.05070573, 0.2725498 , 0.37143213],
        [0.4121557 , 0.00903952, 0.14433455]]], dtype=float32)>, None)
kwargs = {}, arg = None

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       ValueError: Exception encountered when calling tensorflow_IrregularRenderer.call().
E       
E       [1mAttempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.[0m
E       
E       Arguments received by tensorflow_IrregularRenderer.call():
E         â€¢ rgbs=tf.Tensor(shape=(5, 32, 3), dtype=float32)
E         â€¢ densities=tf.Tensor(shape=(5, 32, 1), dtype=float32)
E         â€¢ points_3d=tf.Tensor(shape=(5, 32, 3), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.volume_renderer.IrregularRenderer
______________________________________________________________________________ test_RegularRenderer[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RegularRenderer(target_framework, mode, backend_compile):
        print("kornia.nerf.volume_renderer.RegularRenderer")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledRegularRenderer = ivy.transpile(volume_renderer.RegularRenderer, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(5, 32, 3),
            torch.rand(5, 32, 1),
            torch.rand(5, 32, 3),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_renderer = volume_renderer.RegularRenderer(shift=1)
        transpiled_renderer = TranspiledRegularRenderer(shift=1)
    
        torch_out = torch_renderer(*torch_args)
>       transpiled_out = transpiled_renderer(*transpiled_args)

kornia/test_nerf.py:179: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer()
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[4.28159833e-02, 5.27565300e-01, 7.89693594e-02],
       ...3.1362838e-01, 1.7021626e-01, 7.7101970e-01],
        [7.6252645e-01, 9.9533844e-01, 5.0388420e-01]]], dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x56541925e490, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RegularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[4.28159833e-02, 5.27565300...3.1362838e-01, 1.7021626e-01, 7.7101970e-01],
        [7.6252645e-01, 9.9533844e-01, 5.0388420e-01]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[4.28159833e-02, 5.27565300e-01, 7.89693594e-02],
       ...3.1362838e-01, 1.7021626e-01, 7.7101970e-01],
        [7.6252645e-01, 9.9533844e-01, 5.0388420e-01]]], dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RegularRenderer(), <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[4.28159833e-02, 5.27565300...3.1362838e-01, 1.7021626e-01, 7.7101970e-01],
        [7.6252645e-01, 9.9533844e-01, 5.0388420e-01]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[4.28159833e-02, 5.27565300e-01, 7.89693594e-02],
       ...3.1362838e-01, 1.7021626e-01, 7.7101970e-01],
        [7.6252645e-01, 9.9533844e-01, 5.0388420e-01]]], dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[4.28159833e-02, 5.27565300e-01, 7.89693594e-02],
        ...68237e-01, 4.06445563e-01, 6.70707405e-01],
        [2.62266397e-01, 8.87792766e-01, 1.86982393e-01]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (rgbs, densities, points_3d)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RegularRenderer(),)
kwargs = {'densities': <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[0.43244362],
        [0.17666465],
        ...8237e-01, 4.06445563e-01, 6.70707405e-01],
        [2.62266397e-01, 8.87792766e-01, 1.86982393e-01]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RegularRenderer()
rgbs = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[4.28159833e-02, 5.27565300e-01, 7.89693594e-02],
        ...68237e-01, 4.06445563e-01, 6.70707405e-01],
        [2.62266397e-01, 8.87792766e-01, 1.86982393e-01]]], dtype=float32)>
densities = <tf.Tensor: shape=(5, 32, 1), dtype=float32, numpy=
array([[[0.43244362],
        [0.17666465],
        [0.689975  ],
...[0.75704604],
        [0.9490168 ],
        [0.64013976],
        [0.4374581 ],
        [0.91359264]]], dtype=float32)>
points_3d = <tf.Tensor: shape=(5, 32, 3), dtype=float32, numpy=
array([[[4.3435597e-01, 5.7240754e-01, 4.6744061e-01],
        [2....[3.1362838e-01, 1.7021626e-01, 7.7101970e-01],
        [7.6252645e-01, 9.9533844e-01, 5.0388420e-01]]], dtype=float32)>

    def call(self, rgbs, densities, points_3d):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.linalg import tensorflow_norm_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_exp_frnt
    
        tensorflow_KORNIA_CHECK_SHAPE(rgbs, ["*", "N", "3"])
        tensorflow_KORNIA_CHECK_SHAPE(densities, ["*", "N"])
        tensorflow_KORNIA_CHECK_SHAPE(points_3d, ["*", "N", "3"])
        num_ray_points: typing.Any = tensorflow_shape_frnt_(points_3d)[-2]
        points_3d = tensorflow_reshape_frnt_(points_3d, -1, num_ray_points, 3)
        delta_3d = points_3d[0, 1, :] - points_3d[0, 0, :]
>       delta = tensorflow_norm_frnt(delta_3d, dim=-1)

Translated_Outputs/tensorflow_outputs/kornia/nerf/volume_renderer.py:472: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.19235194, -0.08154231,  0.12390488], dtype=float32)>, ord = None, dim = -1, keepdim = False

    def tensorflow_norm_frnt(
        input, ord=None, dim=None, keepdim=False, *, dtype=None, out=None
    ):
        from .tensor import tensorflow_ndim_frnt_
        from ...backends.tensorflow.linear_algebra import tensorflow_vector_norm
        from ...backends.tensorflow.linear_algebra import tensorflow_matrix_norm
        from ...backends.tensorflow.manipulation import tensorflow_flatten
        from ...backends.tensorflow.data_type import tensorflow_astype
    
        if dim is None and ord is not None:
            if tensorflow_ndim_frnt_(input) == 1:
                ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)
            else:
                ret = tensorflow_matrix_norm(input, keepdims=keepdim, ord=ord)
        elif dim is None and ord is None:
            input = tensorflow_flatten(input)
            ret = tensorflow_vector_norm(input, axis=0, keepdims=keepdim, ord=2)
        elif isinstance(dim, (int,)):
>           ret = tensorflow_vector_norm(input, axis=dim, keepdims=keepdim, ord=ord)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/linalg.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.19235194, -0.08154231,  0.12390488], dtype=float32)>], kwargs = {'axis': -1, 'keepdims': False, 'ord': None}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f4696d57ac0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f4696d841f0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f4696d871c0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f4696d84af0>, num_args = 1
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'axis', 'keepdims', 'ord', 'dtype', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[int, ...es.DType], typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 0

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.19235194, -0.08154231,  0.12390488], dtype=float32)>

    @tensorflow_handle_array_like_without_promotion
    def tensorflow_vector_norm(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        axis: Optional[Union[int, Sequence[int]]] = None,
        keepdims: bool = False,
        ord: Union[int, float, Literal[inf, -inf]] = 2,
        dtype: Optional[tf.DType] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.constants import inf
    
        if dtype and x.dtype != dtype:
            x = tensorflow.cast(x, dtype)
        abs_x = tensorflow.abs(x)
        if ord == 0:
            return tensorflow.reduce_sum(
                tensorflow.cast(x != 0, abs_x.dtype), axis=axis, keepdims=keepdims
            )
        elif ord == inf:
            return tensorflow.reduce_max(abs_x, axis=axis, keepdims=keepdims)
        elif ord == -inf:
            return tensorflow.reduce_min(abs_x, axis=axis, keepdims=keepdims)
        else:
>           return tensorflow.reduce_sum(abs_x**ord, axis=axis, keepdims=keepdims) ** (
                1.0 / ord
            )

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/linear_algebra.py:200: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.19235194, 0.08154231, 0.12390488], dtype=float32)>, None), kwargs = {}, arg = None

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       ValueError: Exception encountered when calling tensorflow_RegularRenderer.call().
E       
E       [1mAttempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.[0m
E       
E       Arguments received by tensorflow_RegularRenderer.call():
E         â€¢ rgbs=tf.Tensor(shape=(5, 32, 3), dtype=float32)
E         â€¢ densities=tf.Tensor(shape=(5, 32, 1), dtype=float32)
E         â€¢ points_3d=tf.Tensor(shape=(5, 32, 3), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.volume_renderer.RegularRenderer
________________________________________________________________________________ test_RaySampler[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RaySampler(target_framework, mode, backend_compile):
        print("kornia.nerf.samplers.RaySampler")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)

kornia/test_nerf.py:193: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.camera.pinhole.PinholeCamera'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.samplers.RaySampler
_____________________________________________________________________________ test_RandomRaySampler[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRaySampler(target_framework, mode, backend_compile):
        print("kornia.nerf.samplers.RandomRaySampler")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledPinholeCamera = ivy.transpile(kornia.geometry.camera.pinhole.PinholeCamera, source="torch", target=target_framework)

kornia/test_nerf.py:224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.geometry.camera.pinhole.PinholeCamera'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor' is not defined

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.nerf.samplers.RandomRaySampler
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_nerf.py::test_NerfModel[tensorflow-s2s-False] - AttributeError: can't set attribute
FAILED kornia/test_nerf.py::test_NerfModelRenderer[tensorflow-s2s-False] - AttributeError: can't set attribute
FAILED kornia/test_nerf.py::test_NerfSolver[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor'...
FAILED kornia/test_nerf.py::test_IrregularRenderer[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_IrregularRenderer.call().
FAILED kornia/test_nerf.py::test_RegularRenderer[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_RegularRenderer.call().
FAILED kornia/test_nerf.py::test_RaySampler[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'Tensor'...
FAILED kornia/test_nerf.py::test_RandomRaySampler[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.torch_frontend_outputs.kornia.nerf.core: name 'T...
=============================================================================== 7 failed, 1 passed in 259.57s (0:04:19) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 6 items

kornia/augmentation/test_container.py FFFFFF                                                                                                                                                     [100%]

=============================================================================================== FAILURES ===============================================================================================
__________________________________________________________________________ test_AugmentationSequential[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_AugmentationSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.AugmentationSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledAugmentationSequential = ivy.transpile(
            kornia.augmentation.container.AugmentationSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:23: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.augment.AugmentationSequential'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.AugmentationSequential
______________________________________________________________________ test_ManyToManyAugmentationDispather[tensorflow-s2s-False] ______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ManyToManyAugmentationDispather(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.ManyToManyAugmentationDispather")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledManyToManyAugmentationDispather = ivy.transpile(
            kornia.augmentation.container.ManyToManyAugmentationDispather,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:81: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.dispatcher.ManyToManyAugmentationDispather'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.ManyToManyAugmentationDispather
______________________________________________________________________ test_ManyToOneAugmentationDispather[tensorflow-s2s-False] _______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ManyToOneAugmentationDispather(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.ManyToOneAugmentationDispather")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledManyToOneAugmentationDispather = ivy.transpile(
            kornia.augmentation.container.ManyToOneAugmentationDispather,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:147: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.dispatcher.ManyToOneAugmentationDispather'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.ManyToOneAugmentationDispather
______________________________________________________________________________ test_ImageSequential[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ImageSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.ImageSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledImageSequential = ivy.transpile(
            kornia.augmentation.container.ImageSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:213: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.image.ImageSequential'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.ImageSequential
______________________________________________________________________________ test_PatchSequential[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_PatchSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.PatchSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledPatchSequential = ivy.transpile(
            kornia.augmentation.container.PatchSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:289: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.patch.PatchSequential'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.PatchSequential
______________________________________________________________________________ test_VideoSequential[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_VideoSequential(target_framework, mode, backend_compile):
        print("kornia.augmentation.container.VideoSequential")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledVideoSequential = ivy.transpile(
            kornia.augmentation.container.VideoSequential,
            source="torch",
            target=target_framework,
        )

kornia/augmentation/test_container.py:376: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation.container.video.VideoSequential'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation._2d.mix.base: cannot import name 'ivy__BasicAugmentationBase' from 'Translated_Outputs.ivy_outputs.kornia.augmentation.base' (/ivy/ivy-integration-tests/Translated_Outputs/ivy_outputs/kornia/augmentation/base.py)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.container.VideoSequential
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/augmentation/test_container.py::test_AugmentationSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augme...
FAILED kornia/augmentation/test_container.py::test_ManyToManyAugmentationDispather[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kor...
FAILED kornia/augmentation/test_container.py::test_ManyToOneAugmentationDispather[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.korn...
FAILED kornia/augmentation/test_container.py::test_ImageSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation...
FAILED kornia/augmentation/test_container.py::test_PatchSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation...
FAILED kornia/augmentation/test_container.py::test_VideoSequential[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Error loading module Translated_Outputs.ivy_outputs.kornia.augmentation...
==================================================================================== 6 failed in 317.75s (0:05:17) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/test_io.py ..                                                                                                                                                                             [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
===================================================================================== 2 passed in 84.34s (0:01:24) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 5 items

kornia/geometry/test_calibration.py ....F                                                                                                                                                        [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________________ test_solve_pnp_dlt[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_solve_pnp_dlt(target_framework, mode, backend_compile):
        trace_args = (
            torch.tensor([[
                [5.0, -5.0, 0.0], [0.0, 0.0, 1.5],
                [2.5, 3.0, 6.0], [9.0, -2.0, 3.0],
                [-4.0, 5.0, 2.0], [-5.0, 5.0, 1.0]
            ]], dtype=torch.float64),
            torch.tensor([[
                [1409.1504, -800.936], [407.0207, -182.1229],
                [392.7021, 177.9428], [1016.838, -2.9416],
                [-63.1116, 142.9204], [-219.3874, 99.666]
            ]], dtype=torch.float64),
            torch.tensor([[
                [500.0, 0.0, 250.0],
                [0.0, 500.0, 250.0],
                [0.0, 0.0, 1.0]
            ]], dtype=torch.float64),
        )
        trace_kwargs = {'svd_eps': 1e-3}
        test_args = (
            torch.tensor([[
                [10.0, -10.0, 0.0], [0.0, 0.0, 3.0],
                [5.0, 6.0, 12.0], [18.0, -4.0, 6.0],
                [-8.0, 10.0, 4.0], [-10.0, 10.0, 2.0]
            ]], dtype=torch.float64),
            torch.tensor([[
                [2818.3008, -1601.872], [814.0414, -364.2458],
                [785.4042, 355.8856], [2033.676, -5.8832],
                [-126.2232, 285.8408], [-438.7748, 199.332]
            ]], dtype=torch.float64),
            torch.tensor([[
                [1000.0, 0.0, 500.0],
                [0.0, 1000.0, 500.0],
                [0.0, 0.0, 1.0]
            ]], dtype=torch.float64),
        )
        test_kwargs = {'svd_eps': 1e-3}
>       _test_function(
            kornia.geometry.calibration.solve_pnp_dlt,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_calibration.py:148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function solve_pnp_dlt at 0x7f8b723b4040>
trace_args = (tensor([[[ 5.0000, -5.0000,  0.0000],
         [ 0.0000,  0.0000,  1.5000],
         [ 2.5000,  3.0000,  6.0000],
   ...loat64), tensor([[[500.,   0., 250.],
         [  0., 500., 250.],
         [  0.,   0.,   1.]]], dtype=torch.float64))
trace_kwargs = {'svd_eps': 0.001}
test_args = (tensor([[[ 10., -10.,   0.],
         [  0.,   0.,   3.],
         [  5.,   6.,  12.],
         [ 18.,  -4.,   6.],
 ...tensor([[[1000.,    0.,  500.],
         [   0., 1000.,  500.],
         [   0.,    0.,    1.]]], dtype=torch.float64))
test_kwargs = {'svd_eps': 0.001}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function solve_pnp_dlt at 0x7f8b723b4040>
trace_args = (tensor([[[ 5.0000, -5.0000,  0.0000],
         [ 0.0000,  0.0000,  1.5000],
         [ 2.5000,  3.0000,  6.0000],
   ...loat64), tensor([[[500.,   0., 250.],
         [  0., 500., 250.],
         [  0.,   0.,   1.]]], dtype=torch.float64))
trace_kwargs = {'svd_eps': 0.001}
test_args = (tensor([[[ 10., -10.,   0.],
         [  0.,   0.,   3.],
         [  5.,   6.,  12.],
         [ 18.,  -4.,   6.],
 ...tensor([[[1000.,    0.,  500.],
         [   0., 1000.,  500.],
         [   0.,    0.,    1.]]], dtype=torch.float64))
test_kwargs = {'svd_eps': 0.001}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

world_points = <tf.Tensor: shape=(1, 6, 3), dtype=float64, numpy=
array([[[ 5. , -5. ,  0. ],
        [ 0. ,  0. ,  1.5],
        [ 2.5,  3. ,  6. ],
        [ 9. , -2. ,  3. ],
        [-4. ,  5. ,  2. ],
        [-5. ,  5. ,  1. ]]])>
img_points = <tf.Tensor: shape=(1, 6, 2), dtype=float64, numpy=
array([[[1409.1504, -800.936 ],
        [ 407.0207, -182.1229],
   ...92.7021,  177.9428],
        [1016.838 ,   -2.9416],
        [ -63.1116,  142.9204],
        [-219.3874,   99.666 ]]])>
intrinsics = <tf.Tensor: shape=(1, 3, 3), dtype=float64, numpy=
array([[[500.,   0., 250.],
        [  0., 500., 250.],
        [  0.,   0.,   1.]]])>, weights = None, svd_eps = 0.001

    def tensorflow_solve_pnp_dlt(
        world_points, img_points, intrinsics, weights=None, svd_eps=0.0001
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...utils.helpers import tensorflow__torch_linalg_svdvals
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_any_frnt
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_inverse_frnt,
        )
        from ..conversions import tensorflow_convert_points_to_homogeneous
        from ..linalg import tensorflow_transform_points
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_svd_frnt_1_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...utils.misc import tensorflow_eye_like
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_bmm_frnt,
        )
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_det_frnt,
        )
        from ....ivy.functional.frontends.torch.reduction_ops import tensorflow_norm_frnt
        from ....ivy.functional.frontends.torch.linalg import tensorflow_qr_frnt
        from ....ivy.functional.frontends.torch.pointwise_ops import tensorflow_sign_frnt
        from ....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...core._backend import zeros
        from ...core._backend import ones_like
        from ...core._backend import where
    
        if not isinstance(world_points, (tensorflow.Tensor, tensorflow.Variable)):
            raise AssertionError(
                f"world_points is not an instance of torch.Tensor. Type of world_points is {type(world_points)}"
            )
        if not isinstance(img_points, (tensorflow.Tensor, tensorflow.Variable)):
            raise AssertionError(
                f"img_points is not an instance of torch.Tensor. Type of img_points is {type(img_points)}"
            )
        if not isinstance(intrinsics, (tensorflow.Tensor, tensorflow.Variable)):
            raise AssertionError(
                f"intrinsics is not an instance of torch.Tensor. Type of intrinsics is {type(intrinsics)}"
            )
        if weights is not None and not isinstance(
            weights, (tensorflow.Tensor, tensorflow.Variable)
        ):
            raise AssertionError(
                f"If weights is not None, then weights should be an instance of torch.Tensor. Type of weights is {type(weights)}"
            )
        if not isinstance(svd_eps, (float,)):
            raise AssertionError(f"Type of svd_eps is not float. Got {type(svd_eps)}")
        accepted_dtypes = tf.float32, tf.float64
        if world_points.dtype not in accepted_dtypes:
            raise AssertionError(
                f"world_points must have one of the following dtypes {accepted_dtypes}. Currently it has {world_points.dtype}."
            )
        if img_points.dtype not in accepted_dtypes:
            raise AssertionError(
                f"img_points must have one of the following dtypes {accepted_dtypes}. Currently it has {img_points.dtype}."
            )
        if intrinsics.dtype not in accepted_dtypes:
            raise AssertionError(
                f"intrinsics must have one of the following dtypes {accepted_dtypes}. Currently it has {intrinsics.dtype}."
            )
        if (
            len(tensorflow_shape_frnt_(world_points)) != 3
            or tensorflow_shape_frnt_(world_points)[2] != 3
        ):
            raise AssertionError(
                f"world_points must be of shape (B, N, 3). Got shape {tensorflow_shape_frnt_(world_points)}."
            )
        if (
            len(tensorflow_shape_frnt_(img_points)) != 3
            or tensorflow_shape_frnt_(img_points)[2] != 2
        ):
            raise AssertionError(
                f"img_points must be of shape (B, N, 2). Got shape {tensorflow_shape_frnt_(img_points)}."
            )
        if len(tensorflow_shape_frnt_(intrinsics)) != 3 or tensorflow_shape_frnt_(
            intrinsics
        )[1:] != (3, 3):
            raise AssertionError(
                f"intrinsics must be of shape (B, 3, 3). Got shape {tensorflow_shape_frnt_(intrinsics)}."
            )
        if tensorflow_shape_frnt_(world_points)[1] != tensorflow_shape_frnt_(img_points)[1]:
            raise AssertionError(
                "world_points and img_points must have equal number of points."
            )
        if (
            tensorflow_shape_frnt_(world_points)[0] != tensorflow_shape_frnt_(img_points)[0]
            or tensorflow_shape_frnt_(world_points)[0]
            != tensorflow_shape_frnt_(intrinsics)[0]
        ):
            raise AssertionError(
                "world_points, img_points and intrinsics must have the same batch size."
            )
        if tensorflow_shape_frnt_(world_points)[1] < 6:
            raise AssertionError(
                f"At least 6 points are required to use this function. Got {tensorflow_shape_frnt_(world_points)[1]} points."
            )
        B, N = (
            tensorflow_shape_frnt_(world_points)[:2][0],
            tensorflow_shape_frnt_(world_points)[:2][1],
        )
        world_points_norm, world_transform_norm = (
            tensorflow__mean_isotropic_scale_normalize(world_points)
        )
        s = tensorflow__torch_linalg_svdvals(world_points_norm)
        if tensorflow_any_frnt(s[:, -1] < svd_eps):
            raise AssertionError(
                f"The last singular value of one/more of the elements of the batch is smaller than {svd_eps}. This function cannot be used if all world_points (of any element of the batch) lie on a line or if all world_points (of any element of the batch) lie on a plane."
            )
        intrinsics_inv = tensorflow_inverse_frnt(intrinsics)
        world_points_norm_h = tensorflow_convert_points_to_homogeneous(world_points_norm)
        img_points_inv = tensorflow_transform_points(intrinsics_inv, img_points)
        img_points_norm, img_transform_norm = tensorflow__mean_isotropic_scale_normalize(
            img_points_inv
        )
        inv_img_transform_norm = tensorflow_inverse_frnt(img_transform_norm)
        system = zeros((B, 2 * N, 12), dtype=world_points.dtype, device=world_points.device)
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(0, None, 2), slice(0, 4, None)),
            world_points_norm_h,
        )
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(1, None, 2), slice(4, 8, None)),
            world_points_norm_h,
        )
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(0, None, 2), slice(8, 12, None)),
            world_points_norm_h * -1 * img_points_norm[..., 0:1],
        )
        system = tensorflow_set_item_bknd(
            system,
            (slice(None, None, None), slice(1, None, 2), slice(8, 12, None)),
            world_points_norm_h * -1 * img_points_norm[..., 1:2],
        )
        _, _, v = tensorflow_svd_frnt_1_frnt(system)
        solution = v[..., -1]
        solution = tensorflow_reshape_frnt_(solution, B, 3, 4)
        solution_4x4 = tensorflow_eye_like(4, solution)
        solution_4x4 = tensorflow_set_item_bknd(
            solution_4x4,
            (slice(None, None, None), slice(None, 3, None), slice(None, None, None)),
            solution,
        )
        intermediate = tensorflow_bmm_frnt(solution_4x4, world_transform_norm)
        solution = tensorflow_bmm_frnt(inv_img_transform_norm, intermediate[:, :3, :])
        det = tensorflow_det_frnt(solution[:, :3, :3])
        ones = ones_like(det)
        sign_fix = where(det < 0, ones * -1, ones)
        solution = solution * sign_fix[:, None, None]
>       norm_col = tensorflow_norm_frnt(input=solution[:, :3, 0], p=2, dim=1)

Translated_Outputs/tensorflow_outputs/kornia/geometry/calibration/pnp.py:237: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (), kwargs = {'dim': 1, 'input': <tf.Tensor: shape=(1, 3), dtype=float64, numpy=array([[-0.02017233,  0.02388227,  0.05749283]])>, 'p': 2}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f8b67176320>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
>       array_like = args[0]
E       IndexError: tuple index out of range

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:42: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.calibration.pnp.solve_pnp_dlt
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_calibration.py::test_solve_pnp_dlt[tensorflow-s2s-False] - IndexError: tuple index out of range
=============================================================================== 1 failed, 4 passed in 171.26s (0:02:51) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 15 items

kornia/geometry/test_subpix.py ..F.........F..                                                                                                                                                   [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________ test_conv_quad_interp3d[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_conv_quad_interp3d(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(20, 16, 3, 50, 32),
        )
        trace_kwargs = {
            'strict_maxima_bonus': 10.0,
            'eps': 1e-7,
        }
        test_args = (
            torch.rand(10, 16, 5, 50, 32),
        )
        test_kwargs = {
            'strict_maxima_bonus': 5.0,
            'eps': 1e-7,
        }
>       _test_function(
            kornia.geometry.subpix.conv_quad_interp3d,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_subpix.py:109: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function conv_quad_interp3d at 0x7f890f8b2290>
trace_args = (tensor([[[[[5.2197e-01, 1.7385e-01, 1.9872e-01,  ..., 3.8821e-01,
            4.1638e-01, 8.9494e-01],
           [8....6141e-02],
           [7.3853e-01, 4.7897e-01, 4.0688e-01,  ..., 9.0924e-01,
            2.5875e-01, 7.9722e-01]]]]]),)
trace_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 10.0}
test_args = (tensor([[[[[1.9281e-02, 1.8209e-01, 1.1796e-02,  ..., 5.9216e-01,
            8.9516e-01, 2.7654e-01],
           [1....4834e-01],
           [2.1586e-01, 5.6030e-01, 8.2264e-01,  ..., 3.1150e-01,
            1.3532e-01, 4.2504e-01]]]]]),)
test_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 5.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function conv_quad_interp3d at 0x7f890f8b2290>
trace_args = (tensor([[[[[5.2197e-01, 1.7385e-01, 1.9872e-01,  ..., 3.8821e-01,
            4.1638e-01, 8.9494e-01],
           [8....6141e-02],
           [7.3853e-01, 4.7897e-01, 4.0688e-01,  ..., 9.0924e-01,
            2.5875e-01, 7.9722e-01]]]]]),)
trace_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 10.0}
test_args = (tensor([[[[[1.9281e-02, 1.8209e-01, 1.1796e-02,  ..., 5.9216e-01,
            8.9516e-01, 2.7654e-01],
           [1....4834e-01],
           [2.1586e-01, 5.6030e-01, 8.2264e-01,  ..., 3.1150e-01,
            1.3532e-01, 4.2504e-01]]]]]),)
test_kwargs = {'eps': 1e-07, 'strict_maxima_bonus': 5.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(20, 16, 3, 50, 32), dtype=float32, numpy=
array([[[[[5.21967173e-01, 1.73852324e-01, 1.98715746e-01...8974402e-01, 4.06880438e-01, ...,
           9.09239650e-01, 2.58751512e-01, 7.97217190e-01]]]]],
      dtype=float32)>
strict_maxima_bonus = 10.0, eps = 1e-07

    def tensorflow_conv_quad_interp3d(input, strict_maxima_bonus=10.0, eps=1e-07):
        from ....ivy.functional.frontends.torch.tensor_functions import (
            tensorflow_is_tensor_frnt_,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...utils.grid import tensorflow_create_meshgrid3d
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...filters.sobel import tensorflow_sobel
        from ....ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...utils._compat import tensorflow_torch_version_ge
        from ....ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from .nms import tensorflow_nms3d
        from ...utils.helpers import tensorflow_safe_solve_with_mask
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import (
            tensorflow_masked_scatter_frnt_,
        )
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_masked_fill__frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_expand_as_frnt_
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_bmm_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...core._backend import stack
        from ...core._backend import rand
        from ...core._backend import zeros_like
        from ...core._backend import where
    
        if not tensorflow_is_tensor_frnt_(input):
            raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
        if not len(tensorflow_shape_frnt_(input)) == 5:
            raise ValueError(
                f"Invalid input shape, we expect BxCxDxHxW. Got: {tensorflow_shape_frnt_(input)}"
            )
        B, CH, D, H, W = tensorflow_shape_frnt_(input)
        grid_global: typing.Any = tensorflow_permute_frnt_(
            tensorflow_create_meshgrid3d(D, H, W, False, device=input.device), 0, 4, 1, 2, 3
        )
        grid_global = tensorflow_to_frnt_(grid_global, input.dtype)
>       b: typing.Any = tensorflow_sobel.spatial_gradient3d(input, order=1, mode="diff")
E       AttributeError: 'function' object has no attribute 'spatial_gradient3d'

Translated_Outputs/tensorflow_outputs/kornia/geometry/subpix/spatial_soft_argmax.py:434: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.subpix.spatial_soft_argmax.conv_quad_interp3d
_____________________________________________________________________________ test_ConvQuadInterp3d[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ConvQuadInterp3d(target_framework, mode, backend_compile):
        print("kornia.geometry.subpix.ConvQuadInterp3d")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledConvQuadInterp3d = ivy.transpile(
            kornia.geometry.subpix.ConvQuadInterp3d, source="torch", target=target_framework
        )
    
        conv_quad_interp3d = kornia.geometry.subpix.ConvQuadInterp3d()
        transpiled_conv_quad_interp3d = TranspiledConvQuadInterp3d()
    
        heatmap = torch.randn(1, 1, 3, 5, 5, requires_grad=True)
        transpiled_heatmap = _nest_torch_tensor_to_new_framework(heatmap, target_framework)
    
        torch_output = conv_quad_interp3d(heatmap)
>       transpiled_output = transpiled_conv_quad_interp3d(transpiled_heatmap)

kornia/geometry/test_subpix.py:371: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0)
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.74098074, -0.32565576, -1.4170815 , -0.0276967...0118535],
          [-0.13004579, -0.7812471 ,  1.3093703 ,  0.06331512,
           -2.031264  ]]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f8903acc950, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array...70118535],
          [-0.13004579, -0.7812471 ,  1.3093703 ,  0.06331512,
           -2.031264  ]]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.74098074, -0.32565576, -1.4170815 , -0.0276967...0118535],
          [-0.13004579, -0.7812471 ,  1.3093703 ,  0.06331512,
           -2.031264  ]]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array...70118535],
          [-0.13004579, -0.7812471 ,  1.3093703 ,  0.06331512,
           -2.031264  ]]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.74098074, -0.32565576, -1.4170815 , -0.0276967...0118535],
          [-0.13004579, -0.7812471 ,  1.3093703 ,  0.06331512,
           -2.031264  ]]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.74098074, -0.32565576, -1.4170815 , -0.02769677....70118535],
          [-0.13004579, -0.7812471 ,  1.3093703 ,  0.06331512,
           -2.031264  ]]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0),)
kwargs = {'x': <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.74098074, -0.32565576, -1.4170815 , -0.02...70118535],
          [-0.13004579, -0.7812471 ,  1.3093703 ,  0.06331512,
           -2.031264  ]]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvQuadInterp3d(strict_maxima_bonus=10.0)
x = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.74098074, -0.32565576, -1.4170815 , -0.02769677....70118535],
          [-0.13004579, -0.7812471 ,  1.3093703 ,  0.06331512,
           -2.031264  ]]]]], dtype=float32)>

    def call(self, x):
>       return tensorflow_conv_quad_interp3d(x, self.strict_maxima_bonus, self.eps)

Translated_Outputs/tensorflow_outputs/kornia/geometry/subpix/spatial_soft_argmax.py:1667: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[-0.74098074, -0.32565576, -1.4170815 , -0.02769677....70118535],
          [-0.13004579, -0.7812471 ,  1.3093703 ,  0.06331512,
           -2.031264  ]]]]], dtype=float32)>
strict_maxima_bonus = 10.0, eps = 1e-07

    def tensorflow_conv_quad_interp3d(input, strict_maxima_bonus=10.0, eps=1e-07):
        from ....ivy.functional.frontends.torch.tensor_functions import (
            tensorflow_is_tensor_frnt_,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...utils.grid import tensorflow_create_meshgrid3d
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...filters.sobel import tensorflow_sobel
        from ....ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...utils._compat import tensorflow_torch_version_ge
        from ....ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from .nms import tensorflow_nms3d
        from ...utils.helpers import tensorflow_safe_solve_with_mask
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.tensor import (
            tensorflow_masked_scatter_frnt_,
        )
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_masked_fill__frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_expand_as_frnt_
        from ....ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_bmm_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...core._backend import stack
        from ...core._backend import rand
        from ...core._backend import zeros_like
        from ...core._backend import where
    
        if not tensorflow_is_tensor_frnt_(input):
            raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
        if not len(tensorflow_shape_frnt_(input)) == 5:
            raise ValueError(
                f"Invalid input shape, we expect BxCxDxHxW. Got: {tensorflow_shape_frnt_(input)}"
            )
        B, CH, D, H, W = tensorflow_shape_frnt_(input)
        grid_global: typing.Any = tensorflow_permute_frnt_(
            tensorflow_create_meshgrid3d(D, H, W, False, device=input.device), 0, 4, 1, 2, 3
        )
        grid_global = tensorflow_to_frnt_(grid_global, input.dtype)
>       b: typing.Any = tensorflow_sobel.spatial_gradient3d(input, order=1, mode="diff")
E       AttributeError: Exception encountered when calling tensorflow_ConvQuadInterp3d.call().
E       
E       [1m'function' object has no attribute 'spatial_gradient3d'[0m
E       
E       Arguments received by tensorflow_ConvQuadInterp3d.call():
E         â€¢ x=tf.Tensor(shape=(1, 1, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/geometry/subpix/spatial_soft_argmax.py:1584: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.subpix.ConvQuadInterp3d
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_subpix.py::test_conv_quad_interp3d[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'spatial_gradient3d'
FAILED kornia/geometry/test_subpix.py::test_ConvQuadInterp3d[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_ConvQuadInterp3d.call().
=============================================================================== 2 failed, 13 passed in 294.27s (0:04:54) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/geometry/test_homography.py .F.F...F                                                                                                                                                      [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________ test_find_homography_dlt_iterated[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_find_homography_dlt_iterated(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 4, 2),
            torch.rand(1, 4, 2),
            torch.rand(1, 4),
        )
        trace_kwargs = {'soft_inl_th': 3.0, 'n_iter': 5}
        test_args = (
            torch.rand(5, 4, 2),
            torch.rand(5, 4, 2),
            torch.rand(5, 4),
        )
        test_kwargs = {'soft_inl_th': 4.0, 'n_iter': 5}
>       _test_function(
            kornia.geometry.homography.find_homography_dlt_iterated,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=5e-2,
            mode=mode,
        )

kornia/geometry/test_homography.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function find_homography_dlt_iterated at 0x7fb467632560>
trace_args = (tensor([[[0.1880, 0.6597],
         [0.3003, 0.9330],
         [0.2235, 0.8330],
         [0.8370, 0.5272]]]), tensor... [0.3932, 0.8911],
         [0.4392, 0.3927],
         [0.8427, 0.2013]]]), tensor([[0.8824, 0.6332, 0.6622, 0.7465]]))
trace_kwargs = {'n_iter': 5, 'soft_inl_th': 3.0}
test_args = (tensor([[[0.4138, 0.2346],
         [0.1511, 0.9186],
         [0.4590, 0.6594],
         [0.3143, 0.2885]],

       ...[0.1127, 0.9119, 0.7807, 0.2606],
        [0.6432, 0.8098, 0.2993, 0.3097],
        [0.5546, 0.8572, 0.6047, 0.7322]]))
test_kwargs = {'n_iter': 5, 'soft_inl_th': 4.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.05, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function find_homography_dlt_iterated at 0x7fb467632560>
trace_args = (tensor([[[0.1880, 0.6597],
         [0.3003, 0.9330],
         [0.2235, 0.8330],
         [0.8370, 0.5272]]]), tensor... [0.3932, 0.8911],
         [0.4392, 0.3927],
         [0.8427, 0.2013]]]), tensor([[0.8824, 0.6332, 0.6622, 0.7465]]))
trace_kwargs = {'n_iter': 5, 'soft_inl_th': 3.0}
test_args = (tensor([[[0.4138, 0.2346],
         [0.1511, 0.9186],
         [0.4590, 0.6594],
         [0.3143, 0.2885]],

       ...[0.1127, 0.9119, 0.7807, 0.2606],
        [0.6432, 0.8098, 0.2993, 0.3097],
        [0.5546, 0.8572, 0.6047, 0.7322]]))
test_kwargs = {'n_iter': 5, 'soft_inl_th': 4.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.05, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
            _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)
    
        # test it works with the test_args as input
        orig_out = fn(*test_args, **test_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(test_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(test_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        orig_np = _nest_array_to_numpy(orig_out)
        graph_np = _nest_array_to_numpy(graph_out)
    
>       _check_allclose(orig_np, graph_np, tolerance=tolerance)

helpers.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[-2.1536033e+00,  3.1800362e-01,  7.8784418e-01],
        [-2.9882264e+00,  8.0397135e-01,  8.7437409e-01],
  ...279507e-01, -9.4572061e-01,  4.6359172e-01],
        [-9.0519227e-02, -2.1988521e+00,  1.0000000e+00]]], dtype=float32)
y = array([[[-2.15360069e+00,  3.18002939e-01,  7.87843287e-01],
        [-2.98822308e+00,  8.03970098e-01,  8.74373019e-0... -9.45720553e-01,  4.63591754e-01],
        [-9.05184448e-02, -2.19885206e+00,  1.00000000e+00]]],
      dtype=float32)
tolerance = 0.05

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.homography.find_homography_dlt_iterated
____________________________________________________________________ test_find_homography_lines_dlt_iterated[tensorflow-s2s-False] _____________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_find_homography_lines_dlt_iterated(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 4, 2, 2),
            torch.rand(1, 4, 2, 2),
            torch.rand(1, 4),
        )
        trace_kwargs = {'soft_inl_th': 4.0, 'n_iter': 5}
        test_args = (
            torch.rand(5, 4, 2, 2),
            torch.rand(5, 4, 2, 2),
            torch.rand(5, 4),
        )
        test_kwargs = {'soft_inl_th': 3.0, 'n_iter': 5}
>       _test_function(
            kornia.geometry.homography.find_homography_lines_dlt_iterated,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=5e-2,
            mode=mode,
        )

kornia/geometry/test_homography.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function find_homography_lines_dlt_iterated at 0x7fb4676327a0>
trace_args = (tensor([[[[0.2844, 0.0617],
          [0.1940, 0.2420]],

         [[0.9692, 0.4492],
          [0.9159, 0.5884]],

 ...551, 0.8360]],

         [[0.0835, 0.1903],
          [0.8236, 0.5126]]]]), tensor([[0.8223, 0.7596, 0.0589, 0.6278]]))
trace_kwargs = {'n_iter': 5, 'soft_inl_th': 4.0}
test_args = (tensor([[[[0.0244, 0.4814],
          [0.9941, 0.5415]],

         [[0.1800, 0.0436],
          [0.4442, 0.4852]],

 ...[0.9535, 0.3591, 0.8185, 0.4762],
        [0.0121, 0.2198, 0.9299, 0.3183],
        [0.0956, 0.7162, 0.1629, 0.8837]]))
test_kwargs = {'n_iter': 5, 'soft_inl_th': 3.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.05, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function find_homography_lines_dlt_iterated at 0x7fb4676327a0>
trace_args = (tensor([[[[0.2844, 0.0617],
          [0.1940, 0.2420]],

         [[0.9692, 0.4492],
          [0.9159, 0.5884]],

 ...551, 0.8360]],

         [[0.0835, 0.1903],
          [0.8236, 0.5126]]]]), tensor([[0.8223, 0.7596, 0.0589, 0.6278]]))
trace_kwargs = {'n_iter': 5, 'soft_inl_th': 4.0}
test_args = (tensor([[[[0.0244, 0.4814],
          [0.9941, 0.5415]],

         [[0.1800, 0.0436],
          [0.4442, 0.4852]],

 ...[0.9535, 0.3591, 0.8185, 0.4762],
        [0.0121, 0.2198, 0.9299, 0.3183],
        [0.0956, 0.7162, 0.1629, 0.8837]]))
test_kwargs = {'n_iter': 5, 'soft_inl_th': 3.0}, target = 'tensorflow', backend_compile = False, tolerance = 0.05, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[-2.6451,  3.6169,  0.4325],
         [-4.7849,  7.1059,  0.7357],
         [-6.5034,  9.5047,  1.0000]]])
transpiled_x = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[-2.6586514 ,  3.6400902 ,  0.43277222],
        [-4.809602  ,  7.151501  ,  0.73572016],
        [-6.536942  ,  9.565819  ,  0.99999994]]], dtype=float32)>
tolerance = 0.05

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[-2.6450667 ,  3.6169052 ,  0.43249214],
        [-4.7849197 ,  7.1058745 ,  0.7357495 ],
        [-6.5033655 ,  9.504658  ,  0.99999994]]], dtype=float32)
y = array([[[-2.6586514 ,  3.6400902 ,  0.43277222],
        [-4.809602  ,  7.151501  ,  0.73572016],
        [-6.536942  ,  9.565819  ,  0.99999994]]], dtype=float32), tolerance = 0.05

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.homography.find_homography_lines_dlt_iterated
_________________________________________________________________________ test_symmetric_transfer_error[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_symmetric_transfer_error(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 4, 2),
            torch.rand(1, 4, 2),
            torch.rand(1, 3, 3),
        )
        trace_kwargs = {'squared': True, 'eps': 1e-8}
        test_args = (
            torch.rand(5, 4, 2),
            torch.rand(5, 4, 2),
            torch.rand(5, 3, 3),
        )
        test_kwargs = {'squared': True, 'eps': 1e-7}
>       _test_function(
            kornia.geometry.homography.symmetric_transfer_error,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_homography.py:198: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function symmetric_transfer_error at 0x7fb467632320>
trace_args = (tensor([[[0.3005, 0.0299],
         [0.1790, 0.7467],
         [0.2502, 0.3121],
         [0.2615, 0.3894]]]), tensor...0.5532]]]), tensor([[[0.2412, 0.6112, 0.6173],
         [0.9469, 0.6933, 0.5952],
         [0.0059, 0.2028, 0.6798]]]))
trace_kwargs = {'eps': 1e-08, 'squared': True}
test_args = (tensor([[[0.4687, 0.9025],
         [0.4126, 0.1971],
         [0.1505, 0.5552],
         [0.4695, 0.3536]],

       ... 0.0544]],

        [[0.1116, 0.1043, 0.5985],
         [0.1171, 0.4241, 0.7052],
         [0.5555, 0.4083, 0.8321]]]))
test_kwargs = {'eps': 1e-07, 'squared': True}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function symmetric_transfer_error at 0x7fb467632320>
trace_args = (tensor([[[0.3005, 0.0299],
         [0.1790, 0.7467],
         [0.2502, 0.3121],
         [0.2615, 0.3894]]]), tensor...0.5532]]]), tensor([[[0.2412, 0.6112, 0.6173],
         [0.9469, 0.6933, 0.5952],
         [0.0059, 0.2028, 0.6798]]]))
trace_kwargs = {'eps': 1e-08, 'squared': True}
test_args = (tensor([[[0.4687, 0.9025],
         [0.4126, 0.1971],
         [0.1505, 0.5552],
         [0.4695, 0.3536]],

       ... 0.0544]],

        [[0.1116, 0.1043, 0.5985],
         [0.1171, 0.4241, 0.7052],
         [0.5555, 0.4083, 0.8321]]]))
test_kwargs = {'eps': 1e-07, 'squared': True}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
            _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)
        else:
            _to_numpy_and_shape_allclose(orig_out, graph_out, tolerance=tolerance)
    
        # test it works with the test_args as input
        orig_out = fn(*test_args, **test_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(test_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(test_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        orig_np = _nest_array_to_numpy(orig_out)
        graph_np = _nest_array_to_numpy(graph_out)
    
>       _check_allclose(orig_np, graph_np, tolerance=tolerance)

helpers.py:272: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[2.75864868e+02, 4.59491699e+02, 8.95211060e+02, 6.07310562e+01],
       [4.20940132e+01, 5.98299122e+00, 7.468...59e+00, 4.26671371e+01],
       [2.55485058e+01, 6.20945573e-01, 6.52969434e+03, 1.10715576e+02]],
      dtype=float32)
y = array([[2.75864899e+02, 4.59491760e+02, 8.95220764e+02, 6.07313919e+01],
       [4.20940170e+01, 5.98299170e+00, 7.468...59e+00, 4.26671371e+01],
       [2.55485115e+01, 6.20945811e-01, 6.52968018e+03, 1.10715637e+02]],
      dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.homography.symmetric_transfer_error
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_homography.py::test_find_homography_dlt_iterated[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_homography.py::test_find_homography_lines_dlt_iterated[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/geometry/test_homography.py::test_symmetric_transfer_error[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
=============================================================================== 3 failed, 5 passed in 170.13s (0:02:50) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/geometry/test_bbox.py ..F.....                                                                                                                                                            [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________________ test_bbox_to_mask[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_bbox_to_mask(target_framework, mode, backend_compile):
        trace_args = (
            torch.tensor([[[1., 1.], [3., 1.], [3., 2.], [1., 2.]]]),
            5,
            5,
        )
        trace_kwargs = {}
        test_args = (
            torch.tensor([[[2., 2.], [4., 2.], [4., 3.], [2., 3.]]]),
            6,
            6,
        )
        test_kwargs = {}
>       _test_function(
            kornia.geometry.bbox.bbox_to_mask,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_bbox.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function bbox_to_mask at 0x7fe3825c7880>, trace_args = (tensor([[[1., 1.],
         [3., 1.],
         [3., 2.],
         [1., 2.]]]), 5, 5), trace_kwargs = {}
test_args = (tensor([[[2., 2.],
         [4., 2.],
         [4., 3.],
         [2., 3.]]]), 6, 6), test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s'
skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function bbox_to_mask at 0x7fe3825c7880>, trace_args = (tensor([[[1., 1.],
         [3., 1.],
         [3., 2.],
         [1., 2.]]]), 5, 5), trace_kwargs = {}
test_args = (tensor([[[2., 2.],
         [4., 2.],
         [4., 3.],
         [2., 3.]]]), 6, 6), test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001
deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[0., 0., 0., 0., 0.],
         [0., 1., 1., 1., 0.],
         [0., 1., 1., 1., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.]]])
transpiled_x = <tf.Tensor: shape=(1, 5, 5), dtype=float32, numpy=
array([[[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[0., 0., 0., 0., 0.],
        [0., 1., 1., 1., 0.],
        [0., 1., 1., 1., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]]], dtype=float32)
y = array([[[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]]], dtype=float32), tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.bbox.bbox_to_mask
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_bbox.py::test_bbox_to_mask[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
=============================================================================== 1 failed, 7 passed in 148.96s (0:02:28) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 44 items

kornia/test_filters.py .........FF.........F........F..F.........F.                                                                                                                              [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________________ test_unsharp_mask[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_unsharp_mask(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 5, 5),
            (3, 3),
            (1.5, 1.5),
        )
        trace_kwargs = {'border_type': 'reflect'}
        test_args = (
            torch.rand(5, 3, 5, 5),
            (5, 5),
            (2.0, 2.0),
        )
        test_kwargs = {'border_type': 'reflect'}
>       _test_function(
            kornia.filters.unsharp_mask,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unsharp_mask at 0x7fb163dbf490>
trace_args = (tensor([[[[0.7803, 0.4391, 0.1686, 0.6545, 0.9904],
          [0.8672, 0.0413, 0.2538, 0.4784, 0.9607],
          [0....  [0.2828, 0.7959, 0.9695, 0.8464, 0.1784],
          [0.3419, 0.4539, 0.1555, 0.9020, 0.0838]]]]), (3, 3), (1.5, 1.5))
trace_kwargs = {'border_type': 'reflect'}
test_args = (tensor([[[[0.2627, 0.5446, 0.7337, 0.7870, 0.5507],
          [0.5033, 0.3716, 0.7678, 0.1483, 0.2227],
          [0....  [0.1424, 0.1301, 0.7716, 0.9371, 0.9073],
          [0.2255, 0.6867, 0.5251, 0.7015, 0.1004]]]]), (5, 5), (2.0, 2.0))
test_kwargs = {'border_type': 'reflect'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unsharp_mask at 0x7fb163dbf490>
trace_args = (tensor([[[[0.7803, 0.4391, 0.1686, 0.6545, 0.9904],
          [0.8672, 0.0413, 0.2538, 0.4784, 0.9607],
          [0....  [0.2828, 0.7959, 0.9695, 0.8464, 0.1784],
          [0.3419, 0.4539, 0.1555, 0.9020, 0.0838]]]]), (3, 3), (1.5, 1.5))
trace_kwargs = {'border_type': 'reflect'}
test_args = (tensor([[[[0.2627, 0.5446, 0.7337, 0.7870, 0.5507],
          [0.5033, 0.3716, 0.7678, 0.1483, 0.2227],
          [0....  [0.1424, 0.1301, 0.7716, 0.9371, 0.9073],
          [0.2255, 0.6867, 0.5251, 0.7015, 0.1004]]]]), (5, 5), (2.0, 2.0))
test_kwargs = {'border_type': 'reflect'}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.7802931 , 0.4391355 , 0.16862547, 0.65446484, 0.9903....84641653, 0.17839843],
         [0.34194583, 0.45393264, 0.1554541 , 0.9020205 , 0.08384615]]]],
      dtype=float32)>
kernel_size = (3, 3), sigma = (1.5, 1.5), border_type = 'reflect'

    def tensorflow_unsharp_mask(input, kernel_size, sigma, border_type="reflect"):
        from .kernels import tensorflow_gaussian
    
>       data_blur: typing.Any = tensorflow_gaussian.gaussian_blur2d(
            input, kernel_size, sigma, border_type
        )
E       AttributeError: 'function' object has no attribute 'gaussian_blur2d'

Translated_Outputs/tensorflow_outputs/kornia/filters/unsharp.py:33: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.unsharp.unsharp_mask
___________________________________________________________________________________ test_canny[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_canny(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 4, 4),
        )
        trace_kwargs = {
            'low_threshold': 0.1,
            'high_threshold': 0.2,
            'kernel_size': (5, 5),
            'sigma': (1, 1),
            'hysteresis': True,
            'eps': 1e-6,
        }
        test_args = (
            torch.rand(5, 3, 4, 4),
        )
        test_kwargs = {
            'low_threshold': 0.2,
            'high_threshold': 0.3,
            'kernel_size': (5, 5),
            'sigma': (1, 1),
            'hysteresis': True,
            'eps': 1e-6,
        }
>       _test_function(
            kornia.filters.canny,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:302: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function canny at 0x7fb163ec9fc0>
trace_args = (tensor([[[[0.4387, 0.9098, 0.4228, 0.7684],
          [0.2548, 0.9098, 0.1652, 0.8729],
          [0.9362, 0.5629, 0...., 0.9249, 0.7091, 0.9196],
          [0.0388, 0.9589, 0.3280, 0.9547],
          [0.1097, 0.2802, 0.6593, 0.2255]]]]),)
trace_kwargs = {'eps': 1e-06, 'high_threshold': 0.2, 'hysteresis': True, 'kernel_size': (5, 5), ...}
test_args = (tensor([[[[0.4077, 0.5878, 0.0843, 0.5399],
          [0.0927, 0.5561, 0.9646, 0.9708],
          [0.7815, 0.6802, 0...., 0.6746, 0.6282, 0.9696],
          [0.0554, 0.3966, 0.5980, 0.2503],
          [0.1604, 0.3738, 0.6549, 0.8092]]]]),)
test_kwargs = {'eps': 1e-06, 'high_threshold': 0.3, 'hysteresis': True, 'kernel_size': (5, 5), ...}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False
deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function canny at 0x7fb163ec9fc0>
trace_args = (tensor([[[[0.4387, 0.9098, 0.4228, 0.7684],
          [0.2548, 0.9098, 0.1652, 0.8729],
          [0.9362, 0.5629, 0...., 0.9249, 0.7091, 0.9196],
          [0.0388, 0.9589, 0.3280, 0.9547],
          [0.1097, 0.2802, 0.6593, 0.2255]]]]),)
trace_kwargs = {'eps': 1e-06, 'high_threshold': 0.2, 'hysteresis': True, 'kernel_size': (5, 5), ...}
test_args = (tensor([[[[0.4077, 0.5878, 0.0843, 0.5399],
          [0.0927, 0.5561, 0.9646, 0.9708],
          [0.7815, 0.6802, 0...., 0.6746, 0.6282, 0.9696],
          [0.0554, 0.3966, 0.5980, 0.2503],
          [0.1604, 0.3738, 0.6549, 0.8092]]]]),)
test_kwargs = {'eps': 1e-06, 'high_threshold': 0.3, 'hysteresis': True, 'kernel_size': (5, 5), ...}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[0.60732675, 0.30539644, 0.33172333, 0.6302159 ],
     ....84241396, 0.23227525, 0.4691001 ],
         [0.24751008, 0.79469055, 0.3791987 , 0.2643352 ]]]],
      dtype=float32)>
low_threshold = 0.1, high_threshold = 0.2, kernel_size = (5, 5), sigma = (1, 1), hysteresis = True, eps = 1e-06

    def tensorflow_canny(
        input,
        low_threshold=0.1,
        high_threshold=0.2,
        kernel_size=(5, 5),
        sigma=(1, 1),
        hysteresis=True,
        eps=1e-06,
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..color.gray import tensorflow_rgb_to_grayscale
        from .kernels import tensorflow_gaussian
        from .sobel import tensorflow_sobel
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_atan2_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_round_frnt_
        from .kernels import tensorflow_get_canny_nms_kernel
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_long_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.non_linear_activation_functions import (
            tensorflow_threshold_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from .kernels import tensorflow_get_hysteresis_kernel
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK(
            low_threshold <= high_threshold,
            f"Invalid input thresholds. low_threshold should be smaller than the high_threshold. Got: {low_threshold}>{high_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < low_threshold < 1,
            f"Invalid low threshold. Should be in range (0, 1). Got: {low_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < high_threshold < 1,
            f"Invalid high threshold. Should be in range (0, 1). Got: {high_threshold}",
        )
        device = input.device
        dtype = input.dtype
        if tensorflow_shape_frnt_(input)[1] == 3:
            input = tensorflow_rgb_to_grayscale(input)
>       blurred: typing.Any = tensorflow_gaussian.gaussian_blur2d(input, kernel_size, sigma)
E       AttributeError: 'function' object has no attribute 'gaussian_blur2d'

Translated_Outputs/tensorflow_outputs/kornia/filters/canny.py:91: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.canny.canny
______________________________________________________________________ test_get_gaussian_discrete_kernel1d[tensorflow-s2s-False] _______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_gaussian_discrete_kernel1d(target_framework, mode, backend_compile):
        trace_args = (3, 2.5)
        trace_kwargs = {}
        test_args = (5, 1.5)
        test_kwargs = {}
>       _test_function(
            kornia.filters.get_gaussian_discrete_kernel1d,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_filters.py:546: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function get_gaussian_discrete_kernel1d at 0x7fb163eb3be0>, trace_args = (3, 2.5), trace_kwargs = {}, test_args = (5, 1.5), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function get_gaussian_discrete_kernel1d at 0x7fb163eb3be0>, trace_args = (3, 2.5), trace_kwargs = {}, test_args = (5, 1.5), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kernel_size = 3, sigma = 2.5, force_even = False

    def tensorflow_get_gaussian_discrete_kernel1d(
        kernel_size, sigma, force_even=False, *, device=None, dtype=None
    ):
        tensorflow__check_kernel_size(kernel_size, allow_even=force_even)
>       return tensorflow_gaussian_discrete(kernel_size, sigma, device=device, dtype=dtype)

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:498: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

window_size = 3, sigma = <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[2.5]], dtype=float32)>

    def tensorflow_gaussian_discrete(window_size, sigma, *, device=None, dtype=None):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_exp_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ..core._backend import tensor
        from ..core._backend import concatenate
    
        if isinstance(sigma, (float,)):
            sigma = tensor([[sigma]], device=device, dtype=dtype)
        tensorflow_KORNIA_CHECK_SHAPE(sigma, ["B", "1"])
        sigma2 = sigma * sigma
        tail = int(window_size // 2) + 1
        bessels = [
>           ivy__modified_bessel_0(sigma2),
            tensorflow__modified_bessel_1(sigma2),
            *(tensorflow__modified_bessel_i(k, sigma2) for k in range(2, tail)),
        ]
E       NameError: name 'ivy__modified_bessel_0' is not defined

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:239: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.kernels.get_gaussian_discrete_kernel1d
___________________________________________________________________________________ test_Canny[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Canny(target_framework, mode, backend_compile):
        print("kornia.filters.Canny")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledCanny = ivy.transpile(kornia.filters.Canny, source="torch", target=target_framework)
    
        x = torch.rand(5, 3, 4, 4)
        torch_out_magnitude, torch_out_edges = kornia.filters.Canny()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out_magnitude, transpiled_out_edges = TranspiledCanny()(transpiled_x)

kornia/test_filters.py:713: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7fb15c4430a0>, built=True,...ze at 0x7fb15c443250>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
args = (<tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.42896622, 0.91857034, 0.9260585 , 0.37137878],
    ...0033293, 0.05338699, 0.6404003 ],
         [0.20899874, 0.32086217, 0.9983661 , 0.31471944]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7fb15c6bfc10, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7fb15c4430a0>, built=True...40033293, 0.05338699, 0.6404003 ],
         [0.20899874, 0.32086217, 0.9983661 , 0.31471944]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7fb15c4430a0>, built=True,...ze at 0x7fb15c443250>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.42896622, 0.91857034, 0.9260585 , 0.37137878],
    ...0033293, 0.05338699, 0.6404003 ],
         [0.20899874, 0.32086217, 0.9983661 , 0.31471944]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7fb15c4430a0>, built=True...40033293, 0.05338699, 0.6404003 ],
         [0.20899874, 0.32086217, 0.9983661 , 0.31471944]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7fb15c4430a0>, built=True,...ze at 0x7fb15c443250>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.42896622, 0.91857034, 0.9260585 , 0.37137878],
    ...0033293, 0.05338699, 0.6404003 ],
         [0.20899874, 0.32086217, 0.9983661 , 0.31471944]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.42896622, 0.91857034, 0.9260585 , 0.37137878],
     ....40033293, 0.05338699, 0.6404003 ],
         [0.20899874, 0.32086217, 0.9983661 , 0.31471944]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7fb15c4430a0>, built=True... at 0x7fb15c443250>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None),)
kwargs = {'input': <tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.42896622, 0.91857034, 0.9260585 , 0.371378...40033293, 0.05338699, 0.6404003 ],
         [0.20899874, 0.32086217, 0.9983661 , 0.31471944]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Canny(activity_regularizer=None, autocast=True, build=<function Model.build at 0x7fb15c4430a0>, built=True,...ze at 0x7fb15c443250>, sigma=(1, 1), steps_per_execution=1, supports_jit=True, test_function=None, train_function=None)
input = <tf.Tensor: shape=(5, 3, 4, 4), dtype=float32, numpy=
array([[[[0.42896622, 0.91857034, 0.9260585 , 0.37137878],
     ....40033293, 0.05338699, 0.6404003 ],
         [0.20899874, 0.32086217, 0.9983661 , 0.31471944]]]],
      dtype=float32)>

    def call(self, input):
>       return tensorflow_canny(
            input,
            self.low_threshold,
            self.high_threshold,
            self.kernel_size,
            self.sigma,
            self.hysteresis,
            self.eps,
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/canny.py:205: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(5, 1, 4, 4), dtype=float32, numpy=
array([[[[0.34987646, 0.8705113 , 0.31613716, 0.4805122 ],
     ....2575466 , 0.30102843, 0.7846327 ],
         [0.85836977, 0.36011422, 0.66843426, 0.45445484]]]],
      dtype=float32)>
low_threshold = 0.1, high_threshold = 0.2, kernel_size = (5, 5), sigma = (1, 1), hysteresis = True, eps = 1e-06

    def tensorflow_canny(
        input,
        low_threshold=0.1,
        high_threshold=0.2,
        kernel_size=(5, 5),
        sigma=(1, 1),
        hysteresis=True,
        eps=1e-06,
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..color.gray import tensorflow_rgb_to_grayscale
        from .kernels import tensorflow_gaussian
        from .sobel import tensorflow_sobel
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_atan2_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_round_frnt_
        from .kernels import tensorflow_get_canny_nms_kernel
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_long_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.non_linear_activation_functions import (
            tensorflow_threshold_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_ones_frnt
        from .kernels import tensorflow_get_hysteresis_kernel
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_abs_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK(
            low_threshold <= high_threshold,
            f"Invalid input thresholds. low_threshold should be smaller than the high_threshold. Got: {low_threshold}>{high_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < low_threshold < 1,
            f"Invalid low threshold. Should be in range (0, 1). Got: {low_threshold}",
        )
        tensorflow_KORNIA_CHECK(
            0 < high_threshold < 1,
            f"Invalid high threshold. Should be in range (0, 1). Got: {high_threshold}",
        )
        device = input.device
        dtype = input.dtype
        if tensorflow_shape_frnt_(input)[1] == 3:
            input = tensorflow_rgb_to_grayscale(input)
>       blurred: typing.Any = tensorflow_gaussian.gaussian_blur2d(input, kernel_size, sigma)
E       AttributeError: Exception encountered when calling tensorflow_Canny.call().
E       
E       [1m'function' object has no attribute 'gaussian_blur2d'[0m
E       
E       Arguments received by tensorflow_Canny.call():
E         â€¢ input=tf.Tensor(shape=(5, 3, 4, 4), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/filters/canny.py:95: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.Canny
__________________________________________________________________________________ test_DexiNed[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DexiNed(target_framework, mode, backend_compile):
        print("kornia.filters.DexiNed")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDexiNed = ivy.transpile(kornia.filters.DexiNed, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 320, 320)
        torch_out = kornia.filters.DexiNed(pretrained=False)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDexiNed(pretrained=False)(transpiled_x)

kornia/test_filters.py:765: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
args = (<tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.1754021 , 0.9913188 , 0.49061322, ..., 0.970879...
         [0.2054652 , 0.29656613, 0.0475297 , ..., 0.84171474,
          0.25548542, 0.44177186]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x5623caf16110, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...,
         [0.2054652 , 0.29656613, 0.0475297 , ..., 0.84171474,
          0.25548542, 0.44177186]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.1754021 , 0.9913188 , 0.49061322, ..., 0.970879...
         [0.2054652 , 0.29656613, 0.0475297 , ..., 0.84171474,
          0.25548542, 0.44177186]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...,
         [0.2054652 , 0.29656613, 0.0475297 , ..., 0.84171474,
          0.25548542, 0.44177186]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.1754021 , 0.9913188 , 0.49061322, ..., 0.970879...
         [0.2054652 , 0.29656613, 0.0475297 , ..., 0.84171474,
          0.25548542, 0.44177186]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
),)
kwargs = {'x': <tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.1754021 , 0.9913188 , 0.49061322, ..., 0.9...,
         [0.2054652 , 0.29656613, 0.0475297 , ..., 0.84171474,
          0.25548542, 0.44177186]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
x = <tf.Tensor: shape=(1, 3, 320, 320), dtype=float32, numpy=
array([[[[0.1754021 , 0.9913188 , 0.49061322, ..., 0.9708799...],
         [0.2054652 , 0.29656613, 0.0475297 , ..., 0.84171474,
          0.25548542, 0.44177186]]]], dtype=float32)>

    def call(self, x):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..core._backend import concatenate
    
        block_1 = self.block_1(x)
        block_1_side = self.side_1(block_1)
        block_2 = self.block_2(block_1)
        block_2_down = self.maxpool(block_2)
>       block_2_add = block_2_down + block_1_side

Translated_Outputs/tensorflow_outputs/kornia/filters/dexined.py:1117: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 64, 160, 80), dtype=float32, numpy=
array([[[[ 0.03969129,  0.05074333,  0.06336989, ...,  0.07...     [-0.01114935,  0.02105835,  0.00567079, ...,  0.00753407,
          -0.01228816, -0.0105991 ]]]], dtype=float32)>)
kwargs = {}
arg = <tf.Tensor: shape=(1, 128, 80, 80), dtype=float32, numpy=
array([[[[-0.00341062, -0.0448406 , -0.00736109, ..., -0.022...      [-0.01114935,  0.02105835,  0.00567079, ...,  0.00753407,
          -0.01228816, -0.0105991 ]]]], dtype=float32)>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_DexiNed.call().
E       
E       [1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1,64,160,80] vs. [1,128,80,80] [Op:AddV2] name: [0m
E       
E       Arguments received by tensorflow_DexiNed.call():
E         â€¢ x=tf.Tensor(shape=(1, 3, 320, 320), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.DexiNed
________________________________________________________________________________ test_UnsharpMask[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_UnsharpMask(target_framework, mode, backend_compile):
        print("kornia.filters.UnsharpMask")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledUnsharpMask = ivy.transpile(kornia.filters.UnsharpMask, source="torch", target=target_framework)
    
        x = torch.rand(2, 3, 5, 5)
        torch_out = kornia.filters.UnsharpMask((3, 3), (1.5, 1.5))(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledUnsharpMask((3, 3), (1.5, 1.5))(transpiled_x)

kornia/test_filters.py:939: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask()
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[8.87225807e-01, 1.47237837e-01, 4.95821416e-01,
     ...       [7.91375637e-02, 9.49903488e-01, 6.90292716e-02,
          2.80200183e-01, 1.70497298e-02]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7fb15c6e93a0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_UnsharpMask(), <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[8.87225807e-01, 1.47237837e...        [7.91375637e-02, 9.49903488e-01, 6.90292716e-02,
          2.80200183e-01, 1.70497298e-02]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[8.87225807e-01, 1.47237837e-01, 4.95821416e-01,
     ...       [7.91375637e-02, 9.49903488e-01, 6.90292716e-02,
          2.80200183e-01, 1.70497298e-02]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_UnsharpMask(), <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[8.87225807e-01, 1.47237837e...        [7.91375637e-02, 9.49903488e-01, 6.90292716e-02,
          2.80200183e-01, 1.70497298e-02]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[8.87225807e-01, 1.47237837e-01, 4.95821416e-01,
     ...       [7.91375637e-02, 9.49903488e-01, 6.90292716e-02,
          2.80200183e-01, 1.70497298e-02]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[8.87225807e-01, 1.47237837e-01, 4.95821416e-01,
      ...         [7.91375637e-02, 9.49903488e-01, 6.90292716e-02,
          2.80200183e-01, 1.70497298e-02]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_UnsharpMask(),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[8.87225807e-01, 1.47237837e-01, 4.95821416e-...        [7.91375637e-02, 9.49903488e-01, 6.90292716e-02,
          2.80200183e-01, 1.70497298e-02]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_UnsharpMask()
input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[8.87225807e-01, 1.47237837e-01, 4.95821416e-01,
      ...         [7.91375637e-02, 9.49903488e-01, 6.90292716e-02,
          2.80200183e-01, 1.70497298e-02]]]], dtype=float32)>

    def call(self, input):
>       return tensorflow_unsharp_mask(
            input, self.kernel_size, self.sigma, self.border_type
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/unsharp.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[8.87225807e-01, 1.47237837e-01, 4.95821416e-01,
      ...         [7.91375637e-02, 9.49903488e-01, 6.90292716e-02,
          2.80200183e-01, 1.70497298e-02]]]], dtype=float32)>
kernel_size = (3, 3), sigma = (1.5, 1.5), border_type = 'reflect'

    def tensorflow_unsharp_mask(input, kernel_size, sigma, border_type="reflect"):
        from .kernels import tensorflow_gaussian
    
>       data_blur: typing.Any = tensorflow_gaussian.gaussian_blur2d(
            input, kernel_size, sigma, border_type
        )
E       AttributeError: Exception encountered when calling tensorflow_UnsharpMask.call().
E       
E       [1m'function' object has no attribute 'gaussian_blur2d'[0m
E       
E       Arguments received by tensorflow_UnsharpMask.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/kornia/filters/unsharp.py:37: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.filters.UnsharpMask
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_filters.py::test_unsharp_mask[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'gaussian_blur2d'
FAILED kornia/test_filters.py::test_canny[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'gaussian_blur2d'
FAILED kornia/test_filters.py::test_get_gaussian_discrete_kernel1d[tensorflow-s2s-False] - NameError: name 'ivy__modified_bessel_0' is not defined
FAILED kornia/test_filters.py::test_Canny[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_Canny.call().
FAILED kornia/test_filters.py::test_DexiNed[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_DexiNed.call().
FAILED kornia/test_filters.py::test_UnsharpMask[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_UnsharpMask.call().
=============================================================================== 6 failed, 38 passed in 781.01s (0:13:01) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 1 item

kornia/geometry/test_ransac.py .                                                                                                                                                                 [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 1 passed in 214.13s (0:03:34) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 15 items

kornia/test_contrib.py F...FFF.F...F..                                                                                                                                                           [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_compute_padding[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_compute_padding(target_framework, mode, backend_compile):
        trace_args = (
            (4, 3),
            (3, 3),
        )
        trace_kwargs = {}
        test_args = (
            (8, 5),
            (4, 4),
        )
        test_kwargs = {}
>       _test_function(
            kornia.contrib.compute_padding,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_contrib.py:33: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function compute_padding at 0x7f37c45612d0>, trace_args = ((4, 3), (3, 3)), trace_kwargs = {}, test_args = ((8, 5), (4, 4)), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function compute_padding at 0x7f37c45612d0>, trace_args = ((4, 3), (3, 3)), trace_kwargs = {}, test_args = ((8, 5), (4, 4)), test_kwargs = {}, target = 'tensorflow', backend_compile = False
tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

original_size = (4, 3), window_size = (3, 3), stride = (3, 3)

    def tensorflow_compute_padding(original_size, window_size, stride=None):
        from ...torch.nn.modules.utils import _pair
    
        original_size = cast(Tuple[int, int], _pair(original_size))
        window_size = cast(Tuple[int, int], _pair(window_size))
        if stride is None:
            stride = window_size
        stride = cast(Tuple[int, int], _pair(stride))
        remainder_vertical = (original_size[0] - window_size[0]) % stride[0]
        remainder_horizontal = (original_size[1] - window_size[1]) % stride[1]
        if remainder_vertical != 0:
            vertical_padding = stride[0] - remainder_vertical
        else:
            vertical_padding = 0
        if remainder_horizontal != 0:
            horizontal_padding = stride[1] - remainder_horizontal
        else:
            horizontal_padding = 0
        if vertical_padding % 2 == 0:
            top_padding = bottom_padding = vertical_padding // 2
        else:
            top_padding = vertical_padding // 2
            bottom_padding = ceil(vertical_padding / 2)
        if horizontal_padding % 2 == 0:
            left_padding = right_padding = horizontal_padding // 2
        else:
            left_padding = horizontal_padding // 2
            right_padding = ceil(horizontal_padding / 2)
        padding = (
            int(top_padding),
            int(bottom_padding),
            int(left_padding),
            int(right_padding),
        )
>       return cast(FullPadType, padding)
E       NameError: name 'FullPadType' is not defined

Translated_Outputs/tensorflow_outputs/kornia/contrib/extract_patches.py:66: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.extract_patches.compute_padding
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/ivy/ivy/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.
  warnings.warn(
______________________________________________________________________________ test_diamond_square[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_diamond_square(target_framework, mode, backend_compile):
        trace_args = ((1, 1, 8, 8),)
        trace_kwargs = {
            "roughness": 0.5,
            "random_scale": 1.0,
            "normalize_range": (0.0, 1.0),
            "random_fn": torch.ones,
        }
        test_args = ((5, 1, 8, 8),)
        test_kwargs = {
            "roughness": 0.7,
            "random_scale": 0.9,
            "normalize_range": (-1.0, 1.0),
            "random_fn": torch.ones,
        }
>       _test_function(
            kornia.contrib.diamond_square,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_contrib.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function diamond_square at 0x7f37c4560700>, trace_args = ((1, 1, 8, 8),)
trace_kwargs = {'normalize_range': (0.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f37dfa32900>, 'random_scale': 1.0, 'roughness': 0.5}, test_args = ((5, 1, 8, 8),)
test_kwargs = {'normalize_range': (-1.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f37dfa32900>, 'random_scale': 0.9, 'roughness': 0.7}, target = 'tensorflow'
backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function diamond_square at 0x7f37c4560700>, trace_args = ((1, 1, 8, 8),)
trace_kwargs = {'normalize_range': (0.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f37dfa32900>, 'random_scale': 1.0, 'roughness': 0.5}, test_args = ((5, 1, 8, 8),)
test_kwargs = {'normalize_range': (-1.0, 1.0), 'random_fn': <built-in method ones of type object at 0x7f37dfa32900>, 'random_scale': 0.9, 'roughness': 0.7}, target = 'tensorflow'
backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

output_size = (1, 1, 8, 8), roughness = <tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[0.5]]]], dtype=float32)>
random_scale = <tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[1.]]]], dtype=float32)>, random_fn = <built-in method ones of type object at 0x7f37dfa32900>, normalize_range = (0.0, 1.0)
device = None, dtype = None

    def tensorflow_diamond_square(
        output_size,
        roughness=0.5,
        random_scale=1.0,
        random_fn=tensorflow_rand_frnt,
        normalize_range=None,
        device=None,
        dtype=None,
    ):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..enhance.normalize import tensorflow_normalize_min_max
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
    
        tensorflow_KORNIA_CHECK(len(output_size) == 4, "output_size must be (B,C,H,W)")
        if not isinstance(random_scale, (tensorflow.Tensor, tensorflow.Variable)):
            random_scale = tensorflow_to_frnt_(
                tensorflow.convert_to_tensor([[[[random_scale]]]]), device, dtype
            )
            random_scale = tensorflow_expand_frnt_(
                random_scale, [output_size[0] * output_size[1], 1, 1, 1]
            )
        else:
            tensorflow_KORNIA_CHECK_IS_TENSOR(random_scale)
            random_scale = tensorflow_view_frnt_(random_scale, -1, 1, 1, 1)
            random_scale = tensorflow_expand_frnt_(
                random_scale, [output_size[0], output_size[1], 1, 1]
            )
            random_scale = tensorflow_reshape_frnt_(random_scale, [-1, 1, 1, 1])
        if not isinstance(roughness, (tensorflow.Tensor, tensorflow.Variable)):
            roughness = tensorflow_to_frnt_(
                tensorflow.convert_to_tensor([[[[roughness]]]]), device, dtype
            )
            roughness = tensorflow_expand_frnt_(
                roughness, [output_size[0] * output_size[1], 1, 1, 1]
            )
        else:
            roughness = tensorflow_view_frnt_(roughness, -1, 1, 1, 1)
            roughness = tensorflow_expand_frnt_(
                roughness, [output_size[0], output_size[1], 1, 1]
            )
            roughness = tensorflow_reshape_frnt_(roughness, [-1, 1, 1, 1])
        width, height = output_size[-2:][0], output_size[-2:][1]
        num_samples: typing.Any = 1
        for x in output_size[:-2]:
            num_samples = num_samples * x
        p2_width: typing.Any = 2 ** math.ceil(math.log2(width - 1)) + 1
        p2_height: typing.Any = 2 ** math.ceil(math.log2(height - 1)) + 1
        recursion_depth: typing.Any = int(
            min(math.log2(p2_width - 1) - 1, math.log2(p2_height - 1) - 1)
        )
        seed_width: typing.Any = (p2_width - 1) // 2**recursion_depth + 1
        seed_height: typing.Any = (p2_height - 1) // 2**recursion_depth + 1
>       img: typing.Any = random_scale * tensorflow__diamond_square_seed(
            num_samples, seed_width, seed_height, random_fn, device, dtype
        )

Translated_Outputs/tensorflow_outputs/kornia/contrib/diamond_square.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[1.]]]], dtype=float32)>, tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]]))
kwargs = {}, arg = tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]])

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 1, 1), dtype=float32, numpy=array([[[[1.]]]], dtype=float32)>, tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]]))
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

arrays_and_dtypes = [<class 'numpy.float32'>, tensor([[[[0.3333, 1.0000, 0.3333],
          [1.0000, 0.3333, 1.0000],
          [0.3333, 1.0000, 0.3333]]]])]

    def _result_type(*arrays_and_dtypes):
      """Returns the resulting type given a set of arrays."""
    
      def preprocess_float(x):
        if is_prefer_float32():
          if isinstance(x, float):
            return np.float32(x)
          elif isinstance(x, complex):
            return np.complex64(x)
        return x
    
      arrays_and_dtypes = [preprocess_float(x) for x in arrays_and_dtypes]
>     dtype = np.result_type(*arrays_and_dtypes)
E     TypeError: Cannot interpret 'tensor([[[[0.3333, 1.0000, 0.3333],
E               [1.0000, 0.3333, 1.0000],
E               [0.3333, 1.0000, 0.3333]]]])' as a data type

/opt/fw/tensorflow/tensorflow/python/ops/numpy_ops/np_dtypes.py:190: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.diamond_square.diamond_square
_______________________________________________________________________________ test_EdgeDetector[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_EdgeDetector(target_framework, mode, backend_compile):
        print("kornia.contrib.EdgeDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledEdgeDetector = ivy.transpile(kornia.contrib.EdgeDetector, source="torch", target=target_framework)
    
        torch_detector = kornia.contrib.EdgeDetector()
>       transpiled_detector = TranspiledEdgeDetector()

kornia/test_contrib.py:163: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_EdgeDetector()

    def __init__(self):
        from ..filters.dexined import tensorflow_DexiNed
    
        self.super___init__(
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.model = tensorflow_DexiNed(pretrained=True)

Translated_Outputs/tensorflow_outputs/kornia/contrib/edge_detection.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
args = (), kwargs = {'pretrained': True}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
pretrained = True

    @tensorflow_store_config_info
    def __init__(self, pretrained):
        from ...torch.nn.modules.pooling import tensorflow_MaxPool2d
    
        self.super___init__(
            pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.block_1 = tensorflow_DoubleConvBlock(3, 32, 64, stride=2)
        self.block_2 = tensorflow_DoubleConvBlock(64, 128, use_act=False)
        self.dblock_3 = tensorflow__DenseBlock(2, 128, 256)
        self.dblock_4 = tensorflow__DenseBlock(3, 256, 512)
        self.dblock_5 = tensorflow__DenseBlock(3, 512, 512)
        self.dblock_6 = tensorflow__DenseBlock(3, 512, 256)
        self.maxpool = tensorflow_MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.side_1 = tensorflow_SingleConvBlock(64, 128, 2)
        self.side_2 = tensorflow_SingleConvBlock(128, 256, 2)
        self.side_3 = tensorflow_SingleConvBlock(256, 512, 2)
        self.side_4 = tensorflow_SingleConvBlock(512, 512, 1)
        self.side_5 = tensorflow_SingleConvBlock(512, 256, 1)
        self.pre_dense_2 = tensorflow_SingleConvBlock(128, 256, 2)
        self.pre_dense_3 = tensorflow_SingleConvBlock(128, 256, 1)
        self.pre_dense_4 = tensorflow_SingleConvBlock(256, 512, 1)
        self.pre_dense_5 = tensorflow_SingleConvBlock(512, 512, 1)
        self.pre_dense_6 = tensorflow_SingleConvBlock(512, 256, 1)
        self.up_block_1 = tensorflow_UpConvBlock(64, 1)
        self.up_block_2 = tensorflow_UpConvBlock(128, 1)
        self.up_block_3 = tensorflow_UpConvBlock(256, 2)
        self.up_block_4 = tensorflow_UpConvBlock(512, 3)
        self.up_block_5 = tensorflow_UpConvBlock(512, 4)
        self.up_block_6 = tensorflow_UpConvBlock(256, 4)
        self.block_cat = tensorflow_SingleConvBlock(6, 1, stride=1, use_bs=False)
        if pretrained:
>           self.load_from_file(url)

Translated_Outputs/tensorflow_outputs/kornia/filters/dexined.py:1096: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DexiNed(
  (block_1): tensorflow_DoubleConvBlock(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()
...e2d()
    )
  )
  (block_cat): tensorflow_SingleConvBlock(
    (conv): KerasConv2D()
    (bn): KerasBatchNorm2D()
  )
)
path_file = 'http://cmp.felk.cvut.cz/~mishkdmy/models/DexiNed_BIPED_10.pth'

    def load_from_file(self, path_file):
        from ..utils.helpers import tensorflow_map_location_to_cpu
    
>       pretrained_dict = torch.hub.load_state_dict_from_url(
            path_file, map_location=tensorflow_map_location_to_cpu
        )
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/filters/dexined.py:1103: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.EdgeDetector
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "http://cmp.felk.cvut.cz/~mishkdmy/models/DexiNed_BIPED_10.pth" to /root/.cache/torch/hub/checkpoints/DexiNed_BIPED_10.pth

  0%|          | 0.00/135M [00:00<?, ?B/s]
  0%|          | 128k/135M [00:00<05:11, 453kB/s]
  0%|          | 512k/135M [00:00<01:54, 1.23MB/s]
  1%|â–         | 1.75M/135M [00:00<00:39, 3.51MB/s]
  3%|â–Ž         | 4.25M/135M [00:00<00:15, 8.79MB/s]
  6%|â–Œ         | 7.50M/135M [00:00<00:08, 15.1MB/s]
  8%|â–Š         | 10.4M/135M [00:00<00:06, 19.0MB/s]
 10%|â–‰         | 13.4M/135M [00:01<00:05, 22.1MB/s]
 12%|â–ˆâ–        | 16.5M/135M [00:01<00:04, 25.0MB/s]
 15%|â–ˆâ–        | 19.8M/135M [00:01<00:04, 27.6MB/s]
 17%|â–ˆâ–‹        | 23.1M/135M [00:01<00:03, 29.8MB/s]
 20%|â–ˆâ–‰        | 26.2M/135M [00:01<00:03, 30.3MB/s]
 22%|â–ˆâ–ˆâ–       | 29.4M/135M [00:01<00:04, 26.7MB/s]
 24%|â–ˆâ–ˆâ–       | 32.6M/135M [00:01<00:03, 27.8MB/s]
 26%|â–ˆâ–ˆâ–‹       | 35.6M/135M [00:01<00:03, 28.5MB/s]
 29%|â–ˆâ–ˆâ–‰       | 38.9M/135M [00:01<00:03, 30.0MB/s]
 31%|â–ˆâ–ˆâ–ˆ       | 42.0M/135M [00:02<00:03, 30.5MB/s]
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 45.1M/135M [00:02<00:03, 31.1MB/s]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 48.4M/135M [00:02<00:02, 31.8MB/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 51.8M/135M [00:02<00:02, 32.8MB/s]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 55.0M/135M [00:02<00:02, 33.0MB/s]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 58.2M/135M [00:02<00:02, 33.1MB/s]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 61.5M/135M [00:02<00:02, 29.9MB/s]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 64.6M/135M [00:02<00:02, 29.8MB/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 67.8M/135M [00:02<00:02, 29.4MB/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 71.1M/135M [00:03<00:02, 30.7MB/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 74.2M/135M [00:03<00:02, 31.3MB/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 77.4M/135M [00:03<00:01, 31.7MB/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 80.5M/135M [00:03<00:01, 31.9MB/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 84.0M/135M [00:03<00:01, 33.3MB/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 87.4M/135M [00:03<00:01, 33.9MB/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 90.6M/135M [00:03<00:01, 30.8MB/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 93.9M/135M [00:03<00:01, 30.8MB/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 97.0M/135M [00:03<00:01, 29.9MB/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 100M/135M [00:04<00:01, 30.9MB/s] 
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 104M/135M [00:04<00:01, 31.2MB/s]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 107M/135M [00:04<00:00, 31.5MB/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 110M/135M [00:04<00:00, 31.8MB/s]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 113M/135M [00:04<00:00, 32.4MB/s]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 116M/135M [00:04<00:00, 32.0MB/s]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 119M/135M [00:04<00:00, 30.5MB/s]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 122M/135M [00:04<00:00, 31.1MB/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 126M/135M [00:04<00:00, 31.0MB/s]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 129M/135M [00:04<00:00, 32.2MB/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 132M/135M [00:05<00:00, 31.7MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135M/135M [00:05<00:00, 27.4MB/s]
_______________________________________________________________________________ test_FaceDetector[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_FaceDetector(target_framework, mode, backend_compile):
        print("kornia.contrib.FaceDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledFaceDetector = ivy.transpile(kornia.contrib.FaceDetector, source="torch", target=target_framework)
    
        torch_detector = kornia.contrib.FaceDetector()
>       transpiled_detector = TranspiledFaceDetector()

kornia/test_contrib.py:187: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_FaceDetector(), top_k = 5000, confidence_threshold = 0.3, nms_threshold = 0.3, keep_top_k = 750

    def __init__(
        self, top_k=5000, confidence_threshold=0.3, nms_threshold=0.3, keep_top_k=750
    ):
        from ..geometry.bbox import tensorflow_nms
    
        self.super___init__(
            top_k=top_k,
            confidence_threshold=confidence_threshold,
            nms_threshold=nms_threshold,
            keep_top_k=keep_top_k,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.top_k = top_k
        self.confidence_threshold = confidence_threshold
        self.nms_threshold = nms_threshold
        self.keep_top_k = keep_top_k
        self.config = {
            "name": "YuFaceDetectNet",
            "min_sizes": [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],
            "steps": [8, 16, 32, 64],
            "variance": [0.1, 0.2],
            "clip": False,
        }
        self.min_sizes = [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]]
        self.steps = [8, 16, 32, 64]
        self.variance = [0.1, 0.2]
        self.clip = False
>       self.model = tensorflow_YuFaceDetectNet("test", pretrained=True)

Translated_Outputs/tensorflow_outputs/kornia/contrib/face_detection.py:71: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_YuFaceDetectNet(
  (model0): tensorflow_Conv_head(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...nv2): tensorflow_ConvDPUnit(
        (conv1): KerasConv2D()
        (conv2): KerasDepthWiseConv2D()
      )
    )
  )
)
args = ('test',), kwargs = {'pretrained': True}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_YuFaceDetectNet(
  (model0): tensorflow_Conv_head(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D()...nv2): tensorflow_ConvDPUnit(
        (conv1): KerasConv2D()
        (conv2): KerasDepthWiseConv2D()
      )
    )
  )
)
phase = 'test', pretrained = True

    @tensorflow_store_config_info
    def __init__(self, phase, pretrained):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.init import tensorflow_xavier_normal_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_data_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_fill__frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_normal__frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_zero__frnt_
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            phase,
            pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.phase = phase
        self.num_classes = 2
        self.model0 = tensorflow_Conv_head(3, 16, 16)
        self.model1 = tensorflow_Conv4layerBlock(16, 64)
        self.model2 = tensorflow_Conv4layerBlock(64, 64)
        self.model3 = tensorflow_Conv4layerBlock(64, 64)
        self.model4 = tensorflow_Conv4layerBlock(64, 64)
        self.model5 = tensorflow_Conv4layerBlock(64, 64)
        self.model6 = tensorflow_Conv4layerBlock(64, 64)
        self.head = tensorflow_Sequential(
            tensorflow_Conv4layerBlock(64, 3 * (14 + 2 + 1), False),
            tensorflow_Conv4layerBlock(64, 2 * (14 + 2 + 1), False),
            tensorflow_Conv4layerBlock(64, 2 * (14 + 2 + 1), False),
            tensorflow_Conv4layerBlock(64, 3 * (14 + 2 + 1), False),
        )
        if self.phase == "train":
            for m in self.modules():
                if isinstance(m, (KerasConv2D,)):
                    if m.bias is not None:
                        m.weight = tensorflow_xavier_normal_(
                            tensorflow_data_frnt_(m.weight)
                        )
                        m.bias = tensorflow_fill__frnt_(
                            tensorflow_data_frnt_(m.bias), 0.02
                        )
                    else:
                        m.weight = tensorflow_normal__frnt_(
                            tensorflow_data_frnt_(m.weight), 0, 0.01
                        )
                elif isinstance(m, (KerasBatchNorm2D,)):
                    m.weight = tensorflow_fill__frnt_(
                        tensorflow_data_frnt_(m.weight), 1
                    )
                    m.bias = tensorflow_zero__frnt_(tensorflow_data_frnt_(m.bias))
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                url, map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/contrib/face_detection.py:680: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.FaceDetector
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/kornia/data/raw/main/yunet_final.pth" to /root/.cache/torch/hub/checkpoints/yunet_final.pth

  0%|          | 0.00/396k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 396k/396k [00:00<00:00, 67.1MB/s]
__________________________________________________________________________________ test_KMeans[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KMeans(target_framework, mode, backend_compile):
        print("kornia.contrib.KMeans")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKMeans = ivy.transpile(kornia.contrib.KMeans, source="torch", target=target_framework)
    
        torch_kmeans = kornia.contrib.KMeans(3, None, 10e-4, 100, 0)
        transpiled_kmeans = TranspiledKMeans(3, None, 10e-4, 100, 0)
    
        torch_x1 = torch.rand((1000, 5))
        torch_x2 = torch.rand((10, 5))
        transpiled_x1 = _array_to_new_backend(torch_x1, target_framework)
        transpiled_x2 = _array_to_new_backend(torch_x2, target_framework)
    
        torch_kmeans.fit(torch_x1)
        torch_predictions = torch_kmeans.predict(torch_x2)
    
>       transpiled_kmeans.fit(transpiled_x1)

kornia/test_contrib.py:245: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Translated_Outputs.tensorflow_outputs.kornia.contrib.kmeans.tensorflow_KMeans object at 0x7f37c2af5ff0>
X = <tf.Tensor: shape=(1000, 5), dtype=float32, numpy=
array([[0.4962566 , 0.7682218 , 0.08847743, 0.13203049, 0.30742282]...5, 0.49248123, 0.4287302 ],
       [0.3786084 , 0.04217941, 0.28761953, 0.5166052 , 0.11317676]],
      dtype=float32)>

    def fit(self, X):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_argmin_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_nonzero_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_index_select_frnt,
        )
        from ...ivy.functional.frontends.torch.random_sampling import (
            tensorflow_randint_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
    
        tensorflow_KORNIA_CHECK_SHAPE(X, ["N", "D"])
        if self._cluster_centers is None:
            self._cluster_centers = self._initialise_cluster_centers(
                X, self.num_clusters
            )
        else:
            tensorflow_KORNIA_CHECK(
                tensorflow_shape_frnt_(X)[1]
                == tensorflow_shape_frnt_(self._cluster_centers)[1],
                f"Dimensions at position 1 of X and cluster_centers do not match.                 {tensorflow_shape_frnt_(X)[1]} != {tensorflow_shape_frnt_(self._cluster_centers)[1]}",
            )
        current_centers = self._cluster_centers
        previous_centers: typing.Any = None
        iteration: typing.Any = 0
        while True:
            distance: typing.Any = self._pairwise_euclidean_distance(X, current_centers)
            cluster_assignment = tensorflow_argmin_frnt_(distance, -1)
            previous_centers = tensorflow_clone_frnt_(current_centers)
            for index in range(self.num_clusters):
                selected = tensorflow_squeeze_frnt_(
                    tensorflow_nonzero_frnt(cluster_assignment == index)
                )
                selected = tensorflow_index_select_frnt(X, 0, selected)
                if tensorflow_shape_frnt_(selected)[0] == 0:
                    selected = X[tensorflow_randint_frnt(len(X), (1,), device=X.device)]
>               current_centers[index] = tensorflow_mean_frnt_(selected, dim=0)
E               TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment

Translated_Outputs/tensorflow_outputs/kornia/contrib/kmeans.py:143: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.KMeans
_______________________________________________________________________________ test_ImageStitcher[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ImageStitcher(target_framework, mode, backend_compile):
        print("kornia.contrib.ImageStitcher")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledLoFTR = ivy.transpile(kornia.contrib.LoFTR, source="torch", target=target_framework)
E       AttributeError: module 'kornia.contrib' has no attribute 'LoFTR'

kornia/test_contrib.py:310: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.contrib.ImageStitcher
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_contrib.py::test_compute_padding[tensorflow-s2s-False] - NameError: name 'FullPadType' is not defined
FAILED kornia/test_contrib.py::test_diamond_square[tensorflow-s2s-False] - TypeError: Cannot interpret 'tensor([[[[0.3333, 1.0000, 0.3333],
FAILED kornia/test_contrib.py::test_EdgeDetector[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_contrib.py::test_FaceDetector[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_contrib.py::test_KMeans[tensorflow-s2s-False] - TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment
FAILED kornia/test_contrib.py::test_ImageStitcher[tensorflow-s2s-False] - AttributeError: module 'kornia.contrib' has no attribute 'LoFTR'
=============================================================================== 6 failed, 9 passed in 619.98s (0:10:19) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 7 items

kornia/test_morphology.py .......                                                                                                                                                                [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 7 passed in 118.14s (0:01:58) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 2 items

kornia/geometry/test_line.py F.                                                                                                                                                                  [100%]

=============================================================================================== FAILURES ===============================================================================================
_________________________________________________________________________________ test_fit_line[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_fit_line(target_framework, mode, backend_compile):
        print("kornia.geometry.line.fit_line")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_fit_line = ivy.transpile(kornia.geometry.line.fit_line, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(2, 10, 3),
            torch.ones(2, 10),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_line = kornia.geometry.line.fit_line(*torch_args)
        transpiled_line = transpiled_fit_line(*transpiled_args)
        _to_numpy_and_allclose(torch_line.origin, transpiled_line.origin)
>       _to_numpy_and_allclose(torch_line.direction, transpiled_line.direction)

kornia/geometry/test_line.py:34: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = Parameter containing:
tensor([[-0.6183, -0.6581, -0.4297],
        [-0.6317, -0.7188, -0.2903]], requires_grad=True)
transpiled_x = <tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=
array([[0.6182712 , 0.6580769 , 0.42973915],
       [0.63170683, 0.7187848 , 0.2903361 ]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[-0.618271  , -0.65807676, -0.4297391 ],
       [-0.63170713, -0.71878433, -0.29033637]], dtype=float32)
y = <tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=
array([[0.6182712 , 0.6580769 , 0.42973915],
       [0.63170683, 0.7187848 , 0.2903361 ]], dtype=float32)>, tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.line.fit_line
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/ivy/ivy/ivy/utils/exceptions.py:383: UserWarning: The current backend: 'tensorflow' does not support inplace updates natively. Ivy would quietly create new arrays when using inplace updates with this backend, leading to memory overhead (same applies for views). If you want to control your memory management, consider doing ivy.set_inplace_mode('strict') which should raise an error whenever an inplace update is attempted with this backend.
  warnings.warn(
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_line.py::test_fit_line[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
================================================================================ 1 failed, 1 passed in 91.45s (0:01:31) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 69 items

kornia/test_color.py ...........F...........F......F.............F...........F.....F.....F                                                                                                       [100%]

=============================================================================================== FAILURES ===============================================================================================
________________________________________________________________________________ test_rgb_to_hls[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_rgb_to_hls(target_framework, mode, backend_compile):
        # Note: We test this function with requires_grad=True,
        # because otherwise we simply get an empty_like tensor
        # with garbage values on each run leading to test failures
        trace_args = (
            torch.rand(1, 3, 4, 5).requires_grad_(True),
        )
        trace_kwargs = {'eps': 1e-8}
        test_args = (
            torch.rand(5, 3, 4, 5).requires_grad_(True),
        )
        test_kwargs = {'eps': 1e-8}
>       _test_function(
            kornia.color.rgb_to_hls,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_color.py:291: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_hls at 0x7f13f69df5b0>
trace_args = (tensor([[[[0.3047, 0.2018, 0.4640, 0.6690, 0.4091],
          [0.0621, 0.1005, 0.9873, 0.2321, 0.9495],
          [0.... [0.8788, 0.3734, 0.3948, 0.5735, 0.6918],
          [0.9518, 0.8803, 0.9342, 0.0133, 0.1865]]]], requires_grad=True),)
trace_kwargs = {'eps': 1e-08}
test_args = (tensor([[[[9.4169e-01, 2.1014e-01, 6.7080e-02, 1.8043e-01, 2.5704e-01],
          [1.3115e-01, 7.9979e-01, 7.8511e-01...1, 1.3428e-01],
          [9.6766e-01, 6.7473e-01, 2.7604e-01, 8.4535e-01, 1.0084e-01]]]],
       requires_grad=True),)
test_kwargs = {'eps': 1e-08}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_hls at 0x7f13f69df5b0>
trace_args = (tensor([[[[0.3047, 0.2018, 0.4640, 0.6690, 0.4091],
          [0.0621, 0.1005, 0.9873, 0.2321, 0.9495],
          [0.... [0.8788, 0.3734, 0.3948, 0.5735, 0.6918],
          [0.9518, 0.8803, 0.9342, 0.0133, 0.1865]]]], requires_grad=True),)
trace_kwargs = {'eps': 1e-08}
test_args = (tensor([[[[9.4169e-01, 2.1014e-01, 6.7080e-02, 1.8043e-01, 2.5704e-01],
          [1.3115e-01, 7.9979e-01, 7.8511e-01...1, 1.3428e-01],
          [9.6766e-01, 6.7473e-01, 2.7604e-01, 8.4535e-01, 1.0084e-01]]]],
       requires_grad=True),)
test_kwargs = {'eps': 1e-08}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(1, 3, 4, 5), dtype=float32, numpy=
array([[[[0.30469918, 0.20179445, 0.463966  , 0.66898775, 0.4091....5735009 , 0.6917642 ],
         [0.9517779 , 0.88027227, 0.9341958 , 0.01325911, 0.18649644]]]],
      dtype=float32)>
eps = 1e-08

    def tensorflow_rgb_to_hls(image, eps=1e-08):
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.pointwise_ops import sub
        from ..core._backend import where
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_requires_grad_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_empty_like_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_add_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_mul_frnt
    
        if not isinstance(image, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(image)}")
        if len(tensorflow_shape_frnt_(image)) < 3 or tensorflow_shape_frnt_(image)[-3] != 3:
            raise ValueError(
                f"Input size must have a shape of (*, 3, H, W). Got {tensorflow_shape_frnt_(image)}"
            )
        _RGB2HSL_IDX = tensor(
            [[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype
        )
        _img_max: typing.Any = tensorflow_max_frnt_(image, -3)
        maxc = _img_max[0]
        imax = _img_max[1]
        minc: typing.Any = tensorflow_min_frnt_(image, -3)[0]
        if tensorflow_requires_grad_frnt_(image):
            l_ = maxc + minc
            s = maxc - minc
            h = l_
            image_hls = l_
        else:
            image_hls = tensorflow_empty_like_frnt(image)
            h, l_, s = (
                image_hls[..., 0, :, :],
                image_hls[..., 1, :, :],
                image_hls[..., 2, :, :],
            )
            tensorflow_add_frnt(maxc, minc, out=l_)
            sub(maxc, minc, out=s)
        im = image / tensorflow_unsqueeze_frnt_(s + eps, -3)
        s = s / (where(l_ < 1.0, l_, 2.0 - l_) + eps)
        l_ = l_ / 2
        r, g, b = im[..., 0, :, :], im[..., 1, :, :], im[..., 2, :, :]
        cond = imax[..., None, :, :] == _RGB2HSL_IDX
        if tensorflow_requires_grad_frnt_(image):
            h = (g - b) % 6 * cond[..., 0, :, :]
        else:
>           tensorflow_mul_frnt((g - b) % 6, cond[..., 0, :, :], out=h)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  4.,  4.],
        [ 2.,  2.,  2.,  4.,  4.],
        [-0., -0.,  2., -0.,  4.],
        [ 4., -0., -0.,  0.,  0.]]], dtype=float32)>
other = <tf.Tensor: shape=(1, 4, 5), dtype=bool, numpy=
array([[[False, False, False,  True, False],
        [False, False,  True, False,  True],
        [False,  True, False,  True,  True],
        [False, False, False, False, False]]])>

    def tensorflow_mul_frnt(input, other, *, out=None):
        from ...backends.tensorflow.elementwise import tensorflow_multiply
    
>       return tensorflow_multiply(input, other, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/pointwise_ops.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x1 = <tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  4.,  4.],
        [ 2.,  2.,  2.,  4.,  4.],
        [-0., -0.,  2., -0.,  4.],
        [ 4., -0., -0.,  0.,  0.]]], dtype=float32)>
x2 = <tf.Tensor: shape=(1, 4, 5), dtype=bool, numpy=
array([[[False, False, False,  True, False],
        [False, False,  True, False,  True],
        [False,  True, False,  True,  True],
        [False, False, False, False, False]]])>

    def tensorflow_multiply(
        x1: Union[float, tensorflow.Tensor, tensorflow.Variable],
        x2: Union[float, tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.data_type import tensorflow_default_dtype_bknd
        from ...ivy.general import tensorflow_is_array_bknd
        from .creation import tensorflow_asarray
    
        oirg_x1 = x1
        oirg_x2 = x2
        try:
            dtype = (
                x1.dtype
                if hasattr(x1, "dtype")
                else x2.dtype
                if hasattr(x2, "dtype")
                else tensorflow_default_dtype_bknd()
            )
            if not tensorflow_is_array_bknd(x1):
                x1 = tensorflow_asarray(x1, dtype=dtype)
            if not tensorflow_is_array_bknd(x2):
                x2 = tensorflow_asarray(x2, dtype=dtype)
        except:
            x1 = oirg_x1
            x2 = oirg_x2
>       return tensorflow.math.multiply(x1, x2)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/elementwise.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  4.,  4.],
        [ 2.,  2.,  2.,  4.,  4...se,  True, False,  True],
        [False,  True, False,  True,  True],
        [False, False, False, False, False]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      if not ops.is_auto_dtype_conversion_enabled():
>       return op(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/ops/weak_tensor_ops.py:142: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 4, 5), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  4.,  4.],
        [ 2.,  2.,  2.,  4.,  4...se,  True, False,  True],
        [False,  True, False,  True,  True],
        [False, False, False, False, False]]])>)
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a bool tensor [Op:Mul] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.hls.rgb_to_hls
_______________________________________________________________________________ test_rgb_to_yuv420[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_rgb_to_yuv420(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 4, 6),
        )
        trace_kwargs = {}
        test_args = (
            torch.rand(5, 3, 4, 6),
        )
        test_kwargs = {}
>       _test_function(
            kornia.color.rgb_to_yuv420,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_color.py:559: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_yuv420 at 0x7f13f63ed360>
trace_args = (tensor([[[[0.3503, 0.7172, 0.3413, 0.8822, 0.2022, 0.3696],
          [0.7859, 0.6430, 0.6994, 0.4113, 0.3873, 0.4550...     [0.8434, 0.9534, 0.8418, 0.2267, 0.0101, 0.3604],
          [0.4167, 0.4015, 0.8846, 0.0348, 0.3208, 0.0085]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[0.3016, 0.1237, 0.1354, 0.9417, 0.2161, 0.6885],
          [0.4289, 0.0392, 0.3660, 0.8453, 0.0439, 0.2866...     [0.3329, 0.8865, 0.5171, 0.1464, 0.3494, 0.0400],
          [0.5721, 0.9258, 0.2378, 0.4986, 0.3028, 0.7536]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function rgb_to_yuv420 at 0x7f13f63ed360>
trace_args = (tensor([[[[0.3503, 0.7172, 0.3413, 0.8822, 0.2022, 0.3696],
          [0.7859, 0.6430, 0.6994, 0.4113, 0.3873, 0.4550...     [0.8434, 0.9534, 0.8418, 0.2267, 0.0101, 0.3604],
          [0.4167, 0.4015, 0.8846, 0.0348, 0.3208, 0.0085]]]]),)
trace_kwargs = {}
test_args = (tensor([[[[0.3016, 0.1237, 0.1354, 0.9417, 0.2161, 0.6885],
          [0.4289, 0.0392, 0.3660, 0.8453, 0.0439, 0.2866...     [0.3329, 0.8865, 0.5171, 0.1464, 0.3494, 0.0400],
          [0.5721, 0.9258, 0.2378, 0.4986, 0.3028, 0.7536]]]]),)
test_kwargs = {}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[[0.2579, 0.4379, 0.2953, 0.5588, 0.2240, 0.3387],
          [0.3724, 0.5623, 0.8197, 0.4255, 0.7110, 0.3026...       [ 0.0706, -0.0614, -0.1666]],

         [[ 0.1899,  0.0515, -0.0356],
          [ 0.0266, -0.1288, -0.1064]]]]))
transpiled_x = (<tf.Tensor: shape=(1, 1, 4, 6), dtype=float32, numpy=
array([[[[0.25793096, 0.43788943, 0.29526517, 0.5588254 , 0.224...        [[ 0.00590866, -0.02092496, -0.05638797],
         [-0.01388825,  0.10749644, -0.02491982]]]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[0.25793096, 0.43788943, 0.29526517, 0.5588254 , 0.22404622,
          0.3387357 ],
         [0.3724044 , 0....
        [[ 0.18992424,  0.05150574, -0.03557685],
         [ 0.0266153 , -0.12879533, -0.10638902]]]], dtype=float32))
y = (array([[[[0.25793096, 0.43788943, 0.29526517, 0.5588254 , 0.22404622,
          0.3387357 ],
         [0.3724044 , 0....
        [[ 0.00590866, -0.02092496, -0.05638797],
         [-0.01388825,  0.10749644, -0.02491982]]]], dtype=float32))
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7f13eba32840>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[ 0.06959186,  0.06269553,  0.10193218],
         [ 0.07059513, -0.06135118, -0.16664559]],

        [[ 0.18992424,  0.05150574, -0.03557685],
         [ 0.0266153 , -0.12879533, -0.10638902]]]], dtype=float32)
y = array([[[[ 0.1709146 ,  0.06608981,  0.03662327],
         [ 0.02564668, -0.07577834, -0.14667809]],

        [[ 0.00590866, -0.02092496, -0.05638797],
         [-0.01388825,  0.10749644, -0.02491982]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.yuv.rgb_to_yuv420
________________________________________________________________________________ test_raw_to_rgb[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_raw_to_rgb(target_framework, mode, backend_compile):
        print("kornia.color.raw_to_rgb")
    
        transpiled_raw_to_rgb = ivy.transpile(kornia.color.raw_to_rgb, source="torch", target=target_framework)
        TranspiledCFA = ivy.transpile(kornia.color.CFA, source="torch", target=target_framework)
    
        torch_x = torch.rand(5, 1, 4, 6)
        transpiled_x = _array_to_new_backend(torch_x, target_framework)
    
        torch_out = kornia.color.raw_to_rgb(torch_x, kornia.color.CFA.RG)
        transpiled_out = transpiled_raw_to_rgb(transpiled_x, TranspiledCFA.RG)
    
>       _to_numpy_and_allclose(torch_out, transpiled_out)

kornia/test_color.py:713: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0.7351, 0.7351, 0.4184, 0.1018, 0.1317, 0.1616],
          [0.7351, 0.7351, 0.4184, 0.1018, 0.1317, 0.1616]...       [0.4565, 0.6576, 0.8586, 0.7235, 0.5884, 0.5884],
          [0.4565, 0.6576, 0.8586, 0.7235, 0.5884, 0.5884]]]])
transpiled_x = <tf.Tensor: shape=(5, 3, 4, 6), dtype=float32, numpy=
array([[[[0.73505116, 0.73505116, 0.4184479 , 0.10611542, 0.1402...37163],
         [0.4564966 , 0.6001076 , 0.8298851 , 0.7234895 , 0.58837163,
          0.58837163]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0.73505116, 0.73505116, 0.4184479 , 0.10184467, 0.13173988,
          0.1616351 ],
         [0.73505116, 0.7...837163],
         [0.4564966 , 0.657552  , 0.85860735, 0.7234895 , 0.58837163,
          0.58837163]]]], dtype=float32)
y = array([[[[0.73505116, 0.73505116, 0.4184479 , 0.10611542, 0.14028138,
          0.1616351 ],
         [0.73505116, 0.7...837163],
         [0.4564966 , 0.6001076 , 0.8298851 , 0.7234895 , 0.58837163,
          0.58837163]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.raw_to_rgb
_________________________________________________________________________________ test_RgbToHls[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RgbToHls(target_framework, mode, backend_compile):
        args = (
            torch.rand(2, 3, 4, 5),
        )
>       _test_color_class(
            kornia.color.RgbToHls,
            args,
            target_framework,
            backend_compile,
            tolerance=1e-3,
        )

kornia/test_color.py:908: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'kornia.color.hls.RgbToHls'>
args = (tensor([[[[0.5003, 0.5906, 0.2685, 0.0668, 0.1243],
          [0.3231, 0.6572, 0.0402, 0.3560, 0.4149],
          [0...., 0.4137],
          [0.7949, 0.4235, 0.7728, 0.8181, 0.2768],
          [0.7889, 0.5118, 0.6876, 0.9533, 0.1585]]]]),)
target = 'tensorflow', backend_compile = False, tolerance = 0.001, init_args = ()

    def _test_color_class(
        cls,
        args,
        target,
        backend_compile=False,
        tolerance=1e-3,
        init_args=(),
    ):
        print(f"{cls.__module__}.{cls.__name__}")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(cls, source="torch", target=target)
    
        torch_obj = cls(*init_args)
        transpiled_obj = transpiled_cls(*init_args)
    
        torch_out = torch_obj(*args)
        transpile_args = _nest_torch_tensor_to_new_framework(args, target)
>       transpiled_out = transpiled_obj(*transpile_args)

kornia/test_color.py:38: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls()
args = (<tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.50025177, 0.5906133 , 0.26846498, 0.06677103, 0.124...181444 , 0.27681214],
         [0.78886276, 0.51175123, 0.6876267 , 0.95327795, 0.15846592]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f13eb599e40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RgbToHls(), <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.50025177, 0.5906133 , 0.2684...8181444 , 0.27681214],
         [0.78886276, 0.51175123, 0.6876267 , 0.95327795, 0.15846592]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.50025177, 0.5906133 , 0.26846498, 0.06677103, 0.124...181444 , 0.27681214],
         [0.78886276, 0.51175123, 0.6876267 , 0.95327795, 0.15846592]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RgbToHls(), <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.50025177, 0.5906133 , 0.2684...8181444 , 0.27681214],
         [0.78886276, 0.51175123, 0.6876267 , 0.95327795, 0.15846592]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls(), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.50025177, 0.5906133 , 0.26846498, 0.06677103, 0.124...181444 , 0.27681214],
         [0.78886276, 0.51175123, 0.6876267 , 0.95327795, 0.15846592]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.50025177, 0.5906133 , 0.26846498, 0.06677103, 0.1243....8181444 , 0.27681214],
         [0.78886276, 0.51175123, 0.6876267 , 0.95327795, 0.15846592]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (image)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RgbToHls(),)
kwargs = {'image': <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.50025177, 0.5906133 , 0.26846498, 0.066771...8181444 , 0.27681214],
         [0.78886276, 0.51175123, 0.6876267 , 0.95327795, 0.15846592]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RgbToHls()
image = <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.50025177, 0.5906133 , 0.26846498, 0.06677103, 0.1243....8181444 , 0.27681214],
         [0.78886276, 0.51175123, 0.6876267 , 0.95327795, 0.15846592]]]],
      dtype=float32)>

    def call(self, image):
>       return tensorflow_rgb_to_hls(image)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy=
array([[[[0.50025177, 0.5906133 , 0.26846498, 0.06677103, 0.1243....8181444 , 0.27681214],
         [0.78886276, 0.51175123, 0.6876267 , 0.95327795, 0.15846592]]]],
      dtype=float32)>
eps = 1e-08

    def tensorflow_rgb_to_hls(image, eps=1e-08):
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.pointwise_ops import sub
        from ..core._backend import where
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_requires_grad_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_empty_like_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_add_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_mul_frnt
    
        if not isinstance(image, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(image)}")
        if len(tensorflow_shape_frnt_(image)) < 3 or tensorflow_shape_frnt_(image)[-3] != 3:
            raise ValueError(
                f"Input size must have a shape of (*, 3, H, W). Got {tensorflow_shape_frnt_(image)}"
            )
        _RGB2HSL_IDX = tensor(
            [[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype
        )
        _img_max: typing.Any = tensorflow_max_frnt_(image, -3)
        maxc = _img_max[0]
        imax = _img_max[1]
        minc: typing.Any = tensorflow_min_frnt_(image, -3)[0]
        if tensorflow_requires_grad_frnt_(image):
            l_ = maxc + minc
            s = maxc - minc
            h = l_
            image_hls = l_
        else:
            image_hls = tensorflow_empty_like_frnt(image)
            h, l_, s = (
                image_hls[..., 0, :, :],
                image_hls[..., 1, :, :],
                image_hls[..., 2, :, :],
            )
            tensorflow_add_frnt(maxc, minc, out=l_)
            sub(maxc, minc, out=s)
        im = image / tensorflow_unsqueeze_frnt_(s + eps, -3)
        s = s / (where(l_ < 1.0, l_, 2.0 - l_) + eps)
        l_ = l_ / 2
        r, g, b = im[..., 0, :, :], im[..., 1, :, :], im[..., 2, :, :]
        cond = imax[..., None, :, :] == _RGB2HSL_IDX
        if tensorflow_requires_grad_frnt_(image):
            h = (g - b) % 6 * cond[..., 0, :, :]
        else:
>           tensorflow_mul_frnt((g - b) % 6, cond[..., 0, :, :], out=h)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:86: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 5), dtype=float32, numpy=
array([[[ 4., -0.,  4.,  2.,  4.],
        [ 3.,  4., -0.,  4.,  2....     [ 2.,  2.,  2.,  2.,  0.],
        [ 0.,  2.,  2.,  2.,  4.],
        [ 2.,  0., -0.,  4.,  3.]]], dtype=float32)>
other = <tf.Tensor: shape=(2, 4, 5), dtype=bool, numpy=
array([[[False, False, False, False, False],
        [ True,  True, Fa...lse, False,  True,  True],
        [False,  True, False, False, False],
        [False, False,  True, False,  True]]])>

    def tensorflow_mul_frnt(input, other, *, out=None):
        from ...backends.tensorflow.elementwise import tensorflow_multiply
    
>       return tensorflow_multiply(input, other, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/pointwise_ops.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x1 = <tf.Tensor: shape=(2, 4, 5), dtype=float32, numpy=
array([[[ 4., -0.,  4.,  2.,  4.],
        [ 3.,  4., -0.,  4.,  2....     [ 2.,  2.,  2.,  2.,  0.],
        [ 0.,  2.,  2.,  2.,  4.],
        [ 2.,  0., -0.,  4.,  3.]]], dtype=float32)>
x2 = <tf.Tensor: shape=(2, 4, 5), dtype=bool, numpy=
array([[[False, False, False, False, False],
        [ True,  True, Fa...lse, False,  True,  True],
        [False,  True, False, False, False],
        [False, False,  True, False,  True]]])>

    def tensorflow_multiply(
        x1: Union[float, tensorflow.Tensor, tensorflow.Variable],
        x2: Union[float, tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.data_type import tensorflow_default_dtype_bknd
        from ...ivy.general import tensorflow_is_array_bknd
        from .creation import tensorflow_asarray
    
        oirg_x1 = x1
        oirg_x2 = x2
        try:
            dtype = (
                x1.dtype
                if hasattr(x1, "dtype")
                else x2.dtype
                if hasattr(x2, "dtype")
                else tensorflow_default_dtype_bknd()
            )
            if not tensorflow_is_array_bknd(x1):
                x1 = tensorflow_asarray(x1, dtype=dtype)
            if not tensorflow_is_array_bknd(x2):
                x2 = tensorflow_asarray(x2, dtype=dtype)
        except:
            x1 = oirg_x1
            x2 = oirg_x2
>       return tensorflow.math.multiply(x1, x2)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RgbToHls.call().
E       
E       [1mcannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a bool tensor [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_RgbToHls.call():
E         â€¢ image=tf.Tensor(shape=(2, 3, 4, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/elementwise.py:290: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.hls.RgbToHls
________________________________________________________________________________ test_RgbToYuv420[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RgbToYuv420(target_framework, mode, backend_compile):
        args = (
            torch.rand(2, 3, 4, 6),
        )
>       _test_color_class(
            kornia.color.RgbToYuv420,
            args,
            target_framework,
            backend_compile,
            tolerance=1e-3,
        )

kornia/test_color.py:1064: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'kornia.color.yuv.RgbToYuv420'>
args = (tensor([[[[0.6535, 0.4397, 0.9718, 0.7490, 0.8482, 0.3089],
          [0.0170, 0.2550, 0.2565, 0.0761, 0.4125, 0.7440...     [0.1725, 0.3954, 0.4132, 0.0795, 0.5320, 0.4981],
          [0.9905, 0.5937, 0.8714, 0.2487, 0.7332, 0.5851]]]]),)
target = 'tensorflow', backend_compile = False, tolerance = 0.001, init_args = ()

    def _test_color_class(
        cls,
        args,
        target,
        backend_compile=False,
        tolerance=1e-3,
        init_args=(),
    ):
        print(f"{cls.__module__}.{cls.__name__}")
    
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(cls, source="torch", target=target)
    
        torch_obj = cls(*init_args)
        transpiled_obj = transpiled_cls(*init_args)
    
        torch_out = torch_obj(*args)
        transpile_args = _nest_torch_tensor_to_new_framework(args, target)
        transpiled_out = transpiled_obj(*transpile_args)
    
        orig_np = _nest_array_to_numpy(torch_out)
        graph_np = _nest_array_to_numpy(transpiled_out)
    
>       _check_allclose(orig_np, graph_np, tolerance=tolerance)

kornia/test_color.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[0.73812854, 0.20979002, 0.8783535 , 0.62220263, 0.6943686 ,
          0.32758906],
         [0.62029994, 0....
        [[ 0.01392659,  0.08091704, -0.03326259],
         [ 0.02991616,  0.18117505, -0.04124139]]]], dtype=float32))
y = (array([[[[0.73812854, 0.20979002, 0.8783535 , 0.62220263, 0.6943686 ,
          0.32758906],
         [0.62029994, 0....
        [[ 0.12795377, -0.05130059,  0.01149907],
         [ 0.13195515,  0.09050632, -0.07918285]]]], dtype=float32))
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:26: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7f13eba164c0>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[ 0.02671976,  0.02711527, -0.1563156 ],
         [-0.0010161 , -0.07312466, -0.04373142]],

        [[-0.068...

        [[ 0.01392659,  0.08091704, -0.03326259],
         [ 0.02991616,  0.18117505, -0.04124139]]]], dtype=float32)
y = array([[[[-0.03691588, -0.0994225 , -0.12958515],
         [ 0.0234255 ,  0.02573187, -0.0035866 ]],

        [[-0.004...

        [[ 0.12795377, -0.05130059,  0.01149907],
         [ 0.13195515,  0.09050632, -0.07918285]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.yuv.RgbToYuv420
_________________________________________________________________________________ test_RawToRgb[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RawToRgb(target_framework, mode, backend_compile):
        print("kornia.color.RawToRgb")
    
        transpiled_RawToRgb = ivy.transpile(kornia.color.RawToRgb, source="torch", target=target_framework)
        TranspiledCFA = ivy.transpile(kornia.color.CFA, source="torch", target=target_framework)
    
        torch_x = torch.rand(2, 1, 4, 6)
        transpiled_x = _array_to_new_backend(torch_x, target_framework)
    
        torch_out = kornia.color.RawToRgb(kornia.color.CFA.RG)(torch_x)
        transpiled_out = transpiled_RawToRgb(TranspiledCFA.RG)(transpiled_x)
    
>       _to_numpy_and_allclose(torch_out, transpiled_out)

kornia/test_color.py:1152: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[0.6699, 0.6699, 0.5265, 0.3832, 0.6335, 0.8839],
          [0.6699, 0.6699, 0.5265, 0.3832, 0.6335, 0.8839]...       [0.4546, 0.2576, 0.0605, 0.3039, 0.5473, 0.5473],
          [0.4546, 0.2576, 0.0605, 0.3039, 0.5473, 0.5473]]]])
transpiled_x = <tf.Tensor: shape=(2, 3, 4, 6), dtype=float32, numpy=
array([[[[0.66993773, 0.66993773, 0.526549  , 0.41892955, 0.7050...3134 ],
         [0.45463097, 0.31388688, 0.08869639, 0.3039305 , 0.5473134 ,
          0.5473134 ]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0.66993773, 0.66993773, 0.526549  , 0.38316023, 0.6335447 ,
          0.8839292 ],
         [0.66993773, 0.6...73134 ],
         [0.45463097, 0.25758928, 0.06054759, 0.3039305 , 0.5473134 ,
          0.5473134 ]]]], dtype=float32)
y = array([[[[0.66993773, 0.66993773, 0.526549  , 0.41892955, 0.70508325,
          0.8839292 ],
         [0.66993773, 0.6...73134 ],
         [0.45463097, 0.31388688, 0.08869639, 0.3039305 , 0.5473134 ,
          0.5473134 ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.RawToRgb
______________________________________________________________________________ test_apply_colormap[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_apply_colormap(target_framework, mode, backend_compile):
        print("kornia.color.ColorMap")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledColorMapType = ivy.transpile(kornia.color.ColorMapType, source="torch", target=target_framework)
        TranspiledColorMap = ivy.transpile(kornia.color.ColorMap, source="torch", target=target_framework)
        transpiled_apply_colormap = ivy.transpile(kornia.color.apply_colormap, source="torch", target=target_framework)
    
        torch_x = torch.tensor([[[0, 1, 2], [15, 25, 33], [128, 158, 188]]])
        transpiled_x = _array_to_new_backend(torch_x, target_framework)
    
        colormap = kornia.color.ColorMap(base=kornia.color.ColorMapType.autumn)
        torch_out = kornia.color.apply_colormap(torch_x, colormap)
    
        colormap = TranspiledColorMap(base=TranspiledColorMapType.autumn)
>       transpiled_out = transpiled_apply_colormap(transpiled_x, colormap)

kornia/test_color.py:1250: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input_tensor = <tf.Tensor: shape=(1, 1, 9), dtype=float32, numpy=
array([[[0.        , 0.00392157, 0.00784314, 0.05882353, 0.09803922,
         0.12941177, 0.5019608 , 0.61960787, 0.7372549 ]]], dtype=float32)>
colormap = <Translated_Outputs.tensorflow_outputs.kornia.color.colormap.tensorflow_ColorMap object at 0x7f13eaced5d0>

    def tensorflow_apply_colormap(input_tensor, colormap):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze__frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_div__frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_linspace_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
    
        tensorflow_KORNIA_CHECK(
            isinstance(input_tensor, (tensorflow.Tensor, tensorflow.Variable)),
            f"`input_tensor` must be a Tensor. Got: {type(input_tensor)}",
        )
        valid_types = [
            tf.float16,
            tf.float32,
            tf.float64,
            tf.uint8,
            tf.int32,
            tf.int64,
            tf.int16,
        ]
        tensorflow_KORNIA_CHECK(
            input_tensor.dtype in valid_types,
            f"`input_tensor` must be a {valid_types}. Got: {input_tensor.dtype}",
        )
        tensorflow_KORNIA_CHECK(
            len(tensorflow_shape_frnt_(input_tensor)) in (3, 4),
            "Wrong input tensor dimension.",
        )
        if len(tensorflow_shape_frnt_(input_tensor)) == 3:
            input_tensor = tensorflow_unsqueeze__frnt_(input_tensor, 0)
        B, C, H, W = tensorflow_shape_frnt_(input_tensor)
        input_tensor = tensorflow_reshape_frnt_(input_tensor, B, C, -1)
        max_value = 1.0 if tensorflow_max_frnt_(input_tensor) <= 1.0 else 255.0
        input_tensor = tensorflow_div__frnt_(
            tensorflow_float_frnt_(input_tensor), max_value
        )
        colors = tensorflow_permute_frnt_(colormap.colors, 1, 0)
        num_colors, channels_cmap = tensorflow_shape_frnt_(colors)
        keys = tensorflow_linspace_frnt(
            0.0, 1.0, num_colors - 1, device=input_tensor.device, dtype=input_tensor.dtype
        )
        indices = tensorflow_expand_frnt_(
>           tensorflow_unsqueeze_frnt_(torch.bucketize(input_tensor, keys), -1),
            -1,
            -1,
            -1,
            3,
        )
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/color/colormap.py:205: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.color.ColorMap
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_color.py::test_rgb_to_hls[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a fl...
FAILED kornia/test_color.py::test_rgb_to_yuv420[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_color.py::test_raw_to_rgb[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_color.py::test_RgbToHls[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RgbToHls.call().
FAILED kornia/test_color.py::test_RgbToYuv420[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_color.py::test_RawToRgb[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_color.py::test_apply_colormap[tensorflow-s2s-False] - NameError: name 'torch' is not defined
=============================================================================== 7 failed, 62 passed in 807.30s (0:13:27) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 4 items

kornia/geometry/test_liegroup.py ..FF                                                                                                                                                            [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________________ test_So2[tensorflow-s2s-False] ____________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_So2(target_framework, mode, backend_compile):
        print("kornia.geometry.liegroup.So2")
    
        if backend_compile:
            pytest.skip()
    
        real_part = torch.tensor([1.0], requires_grad=True)
        imaginary_part = torch.tensor([2.0], requires_grad=True)
        complex_number = torch.complex(real_part, imaginary_part)
        torch_so2 = kornia.geometry.liegroup.So2(complex_number)
    
        TranspiledSo2 = ivy.transpile(kornia.geometry.liegroup.So2, source="torch", target=target_framework)
        transpiled_complex_number = _nest_torch_tensor_to_new_framework(complex_number, target_framework)
        transpiled_so2 = TranspiledSo2(transpiled_complex_number)
    
        # Test .matrix()
        torch_matrix = torch_so2.matrix()
        transpiled_matrix = transpiled_so2.matrix()
        _to_numpy_and_allclose(torch_matrix, transpiled_matrix)
    
        # Test .inverse()
        torch_inverse = torch_so2.inverse()
        transpiled_inverse = transpiled_so2.inverse()
        _to_numpy_and_allclose(torch_inverse.z, transpiled_inverse.z)
    
        # Test .log()
        torch_log = torch_so2.log()
        transpiled_log = transpiled_so2.log()
        _to_numpy_and_allclose(torch_log, transpiled_log)
    
        # Test .__mul__()
        other_real_part = torch.tensor([0.5], requires_grad=True)
        other_imaginary_part = torch.tensor([0.5], requires_grad=True)
        other_complex_number = torch.complex(other_real_part, other_imaginary_part)
        other_torch_so2 = kornia.geometry.liegroup.So2(other_complex_number)
    
        transpiled_other_complex_number = _nest_torch_tensor_to_new_framework(other_complex_number, target_framework)
        transpiled_other_so2 = TranspiledSo2(transpiled_other_complex_number)
    
        torch_composed_so2 = torch_so2 * other_torch_so2
        transpiled_composed_so2 = transpiled_so2 * transpiled_other_so2
        _to_numpy_and_allclose(torch_composed_so2.z, transpiled_composed_so2.z)
    
        # Test .adjoint()
        torch_adjoint = torch_so2.adjoint()
>       transpiled_adjoint = transpiled_so2.adjoint()

kornia/geometry/test_liegroup.py:202: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <tf.Variable 'Variable:0' shape=(1,) dtype=complex64, numpy=array([1.+2.j], dtype=complex64)>

    def adjoint(self):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_real_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
>       batch_size = len(self.z) if len(tensorflow_shape_frnt_(self.z)) > 0 else None
E       TypeError: object of type 'ResourceVariable' has no len()

Translated_Outputs/tensorflow_outputs/kornia/geometry/liegroup/so2.py:195: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.liegroup.So2
____________________________________________________________________________________ test_Se2[tensorflow-s2s-False] ____________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Se2(target_framework, mode, backend_compile):
        print("kornia.geometry.liegroup.Se2")
    
        if backend_compile:
            pytest.skip()
    
        so2_rotation = kornia.geometry.liegroup.So2.identity(1)
        translation_vector = torch.ones((1, 2), requires_grad=True)
        torch_se2 = kornia.geometry.liegroup.Se2(so2_rotation, translation_vector)
    
        TranspiledSe2 = ivy.transpile(kornia.geometry.liegroup.Se2, source="torch", target=target_framework)
        TranspiledSo2 = ivy.transpile(kornia.geometry.liegroup.So2, source="torch", target=target_framework)
    
        transpiled_so2_rotation = TranspiledSo2.identity(1)
        transpiled_translation_vector = _nest_torch_tensor_to_new_framework(translation_vector, target_framework)
        transpiled_se2 = TranspiledSe2(transpiled_so2_rotation, transpiled_translation_vector)
    
        # Test .matrix()
        torch_matrix = torch_se2.matrix()
        transpiled_matrix = transpiled_se2.matrix()
        _to_numpy_and_allclose(torch_matrix, transpiled_matrix)
    
        # Test .inverse()
        torch_inverse = torch_se2.inverse()
        transpiled_inverse = transpiled_se2.inverse()
        _to_numpy_and_allclose(torch_inverse.rotation.z, transpiled_inverse.rotation.z)
        _to_numpy_and_allclose(torch_inverse.translation, transpiled_inverse.translation)
    
        # Test .log()
        torch_log = torch_se2.log()
        transpiled_log = transpiled_se2.log()
        _to_numpy_and_allclose(torch_log, transpiled_log)
    
        # Test .__mul__()
        other_so2_rotation = kornia.geometry.liegroup.So2.identity(1)
        other_translation_vector = torch.tensor([[0.5, 0.5]], requires_grad=True)
        other_torch_se2 = kornia.geometry.liegroup.Se2(other_so2_rotation, other_translation_vector)
    
        transpiled_other_so2_rotation = TranspiledSo2.identity(1)
        transpiled_other_translation_vector = _nest_torch_tensor_to_new_framework(other_translation_vector, target_framework)
        transpiled_other_se2 = TranspiledSe2(transpiled_other_so2_rotation, transpiled_other_translation_vector)
    
        torch_composed_se2 = torch_se2 * other_torch_se2
        transpiled_composed_se2 = transpiled_se2 * transpiled_other_se2
        _to_numpy_and_allclose(torch_composed_se2.rotation.z, transpiled_composed_se2.rotation.z)
        _to_numpy_and_allclose(torch_composed_se2.translation, transpiled_composed_se2.translation)
    
        # Test .adjoint()
        torch_adjoint = torch_se2.adjoint()
        transpiled_adjoint = transpiled_se2.adjoint()
        _to_numpy_and_allclose(torch_adjoint, transpiled_adjoint)
    
        # Test .from_matrix()
        rotation_matrix = torch.eye(3).repeat(2, 1, 1)
        transpiled_rotation_matrix = _nest_torch_tensor_to_new_framework(rotation_matrix, target_framework)
    
        torch_from_matrix = kornia.geometry.liegroup.Se2.from_matrix(rotation_matrix)
        transpiled_from_matrix = TranspiledSe2.from_matrix(transpiled_rotation_matrix)
        _to_numpy_and_allclose(torch_from_matrix.rotation.z, transpiled_from_matrix.rotation.z)
        _to_numpy_and_allclose(torch_from_matrix.translation, transpiled_from_matrix.translation)
    
        # Test .exp()
        v = torch.ones((1, 3))
        transpiled_v = _nest_torch_tensor_to_new_framework(v, target_framework)
    
        torch_exp = kornia.geometry.liegroup.Se2.exp(v)
        transpiled_exp = TranspiledSe2.exp(transpiled_v)
        _to_numpy_and_allclose(torch_exp.rotation.z, transpiled_exp.rotation.z)
        _to_numpy_and_allclose(torch_exp.translation, transpiled_exp.translation)
    
        # Test .identity()
        torch_identity = kornia.geometry.liegroup.Se2.identity(1)
        transpiled_identity = TranspiledSe2.identity(1)
        _to_numpy_and_allclose(torch_identity.rotation.z, transpiled_identity.rotation.z)
>       _to_numpy_and_allclose(torch_identity.translation, transpiled_identity.translation)

kornia/geometry/test_liegroup.py:313: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = x: tensor([0.])
y: tensor([0.]), transpiled_x = x: [0.]
y: [0.], tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = x: tensor([0.])
y: tensor([0.]), y = x: [0.]
y: [0.], tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
            all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])
            return
    
        if isinstance(x, dict):
            all([key_x == key_y for key_x, key_y in zip(x.keys(), y.keys())])
            all([
                _check_allclose(element_x, element_y, tolerance=tolerance)
                for element_x, element_y in zip(x.values(), y.values())
            ])
            return
    
        if isinstance(x, float):
            assert x - y < tolerance, f"float values differ: {x} != {y}"
            return
    
>       assert x == y, f"values differ: {x} != {y}"

helpers.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = x: tensor([0.])
y: tensor([0.]), other = x: [0.]
y: [0.]

    def __eq__(self, other):
>       return self.__unary_op__(torch.eq, other)

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/core/tensor_wrapper.py:139: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = x: tensor([0.])
y: tensor([0.]), func = <built-in method eq of type object at 0x7f8fbce76900>, other = x: [0.]
y: [0.]

    def __unary_op__(self, func, other=None):
        args = (self, other) if other is not None else (self,)
>       return self.__torch_function__(func, (type(self),), args)

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/core/tensor_wrapper.py:155: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'kornia.geometry.vector.Vector2'>, func = <built-in method eq of type object at 0x7f8fbce76900>, types = (<class 'kornia.geometry.vector.Vector2'>,)
args = (tensor([[0., 0.]]), x: [0.]
y: [0.]), kwargs = {}

    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        if kwargs is None:
            kwargs = {}
        # Find an instance of this class in the arguments
        args_of_this_cls = []
        for a in args:
            if isinstance(a, cls):
                args_of_this_cls.append(a)
            elif isinstance(a, collections.abc.Sequence):
                args_of_this_cls.extend(el for el in a if isinstance(el, cls))
        # assert len(args_of_this_cls) > 0
        for a in args_of_this_cls:
            a.used_calls.add(func)
        args = unwrap(tuple(args))
        kwargs = {k: unwrap(v) for k, v in kwargs.items()}
    
>       return wrap(func(*args, **kwargs), cls)

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/core/tensor_wrapper.py:99: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'Translated_Outputs.tensorflow_outputs.kornia.geometry.vector.tensorflow_Vector2'>, func = <built-in method eq of type object at 0x7f8fbce76900>
types = (<class 'Translated_Outputs.tensorflow_outputs.kornia.geometry.vector.tensorflow_Vector2'>,)
args = (tensor([[0., 0.]]), <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0., 0.]], dtype=float32)>), kwargs = {}

    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_add_frnt_
    
        if kwargs is None:
            kwargs = {}
        args_of_this_cls = []
        for a in args:
            if isinstance(a, (cls,)):
                args_of_this_cls.append(a)
            elif isinstance(a, (collections.abc.Sequence,)):
                args_of_this_cls.extend(el for el in a if isinstance(el, (cls,)))
        for a in args_of_this_cls:
            tensorflow_add_frnt_(a.used_calls, func)
        args = tensorflow_unwrap(tuple(args))
        kwargs = {k: tensorflow_unwrap(v) for k, v in kwargs.items()}
>       return tensorflow_wrap(func(*args, **kwargs), cls)
E       TypeError: eq() received an invalid combination of arguments - got (Tensor, tensorflow.python.framework.ops.EagerTensor), but expected one of:
E        * (Tensor input, Tensor other, *, Tensor out = None)
E        * (Tensor input, Number other, *, Tensor out = None)

Translated_Outputs/tensorflow_outputs/kornia/core/tensor_wrapper.py:109: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.liegroup.Se2
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_liegroup.py::test_So2[tensorflow-s2s-False] - TypeError: object of type 'ResourceVariable' has no len()
FAILED kornia/geometry/test_liegroup.py::test_Se2[tensorflow-s2s-False] - TypeError: eq() received an invalid combination of arguments - got (Tensor, tensorflow.python.framework.ops.EagerTensor), b...
=============================================================================== 2 failed, 2 passed in 231.83s (0:03:51) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 68 items

kornia/augmentation/test_augmentation.py ...F...F...F.....FF..F.....F.F.FFFFFFF....FFFF..FFFFFFFF..F..FF..FFF                                                                                    [100%]

=============================================================================================== FAILURES ===============================================================================================
_______________________________________________________________________________ test_RandomBoxBlur[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomBoxBlur(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomBoxBlur")
    
        init_args = ((7, 7),)
        init_kwargs = {}
        call_args = (torch.ones(1, 1, 24, 24),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomBoxBlur,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:135: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.box_blur.RandomBoxBlur'>, target = 'tensorflow', init_args = ((7, 7),), init_kwargs = {}
call_args = (tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1.]...      [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
           1., 1., 1., 1., 1., 1., 1.]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomBoxBlur(p=0.5, p_batch=1.0, same_on_batch=False, kernel_size=(7, 7), border_type=reflect, normalized=True)
args = (<tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,...1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f36a62b6440, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomBoxBlur(p=0.5, p_batch=1.0, same_on_batch=False, kernel_size=(7, 7), border_type=reflect, normalized... 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomBoxBlur(p=0.5, p_batch=1.0, same_on_batch=False, kernel_size=(7, 7), border_type=reflect, normalized=True), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,...1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomBoxBlur(p=0.5, p_batch=1.0, same_on_batch=False, kernel_size=(7, 7), border_type=reflect, normalized... 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomBoxBlur(p=0.5, p_batch=1.0, same_on_batch=False, kernel_size=(7, 7), border_type=reflect, normalized=True), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,...1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ..., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomBoxBlur(p=0.5, p_batch=1.0, same_on_batch=False, kernel_size=(7, 7), border_type=reflect, normalized=True),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.... 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomBoxBlur(p=0.5, p_batch=1.0, same_on_batch=False, kernel_size=(7, 7), border_type=reflect, normalized=True)
input = <tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ..., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 1,  1, 24, 24])>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a427fc70>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36a58e85e0>
tensor = <function tensorflow_tensor_frnt at 0x7f36a4df6290>
in_tensor = <tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ..., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 24, 24]), batch_shape = ivy.frontends.torch.Size([1, 1, 24, 24]), flags = {'border_type': 'reflect', 'kernel_size': (7, 7), 'normalized': True}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomBoxBlur(p=0.5, p_batch=1.0, same_on_batch=False, kernel_size=(7, 7), border_type=reflect, normalized=True)
in_tensor = <tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ..., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 1,  1, 24, 24])>}
flags = {'border_type': 'reflect', 'kernel_size': (7, 7), 'normalized': True}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomBoxBlur(p=0.5, p_batch=1.0, same_on_batch=False, kernel_size=(7, 7), border_type=reflect, normalized=True)
input = <tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ..., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 1,  1, 24, 24])>}
flags = {'border_type': 'reflect', 'kernel_size': (7, 7), 'normalized': True}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a427fc70>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f36a427fac0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f36a4dd8700>, tensorflow_get_item = <function tensorflow_get_item at 0x7f36a58becb0>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f36a5f74310>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f36a4ddb640>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f36a4dd9a20>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomBoxBlur(p=0.5, p_batch=1.0, same_on_batch=False, kernel_size=(7, 7), border_type=reflect, normalized=True)
input = <tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ..., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 1,  1, 24, 24])>}
flags = {'border_type': 'reflect', 'kernel_size': (7, 7), 'normalized': True}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....filters.blur import tensorflow_box_blur
    
>       return tensorflow_box_blur(
            input, flags["kernel_size"], flags["border_type"], flags["normalized"]
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/box_blur.py:50: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ..., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>
kernel_size = (7, 7), border_type = 'reflect', separable = True

    def tensorflow_box_blur(input, kernel_size, border_type="reflect", separable=False):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from .kernels import tensorflow__unpack_2d_ks
        from .kernels import tensorflow_get_box_kernel1d
        from .filter import tensorflow_filter2d_separable
        from .kernels import tensorflow_get_box_kernel2d
        from .filter import tensorflow_filter2d
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        if separable:
            ky, kx = tensorflow__unpack_2d_ks(kernel_size)
            kernel_y = tensorflow_get_box_kernel1d(
                ky, device=input.device, dtype=input.dtype
            )
            kernel_x = tensorflow_get_box_kernel1d(
                kx, device=input.device, dtype=input.dtype
            )
>           out = tensorflow_filter2d_separable(input, kernel_x, kernel_y, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/blur.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 24, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ..., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>
kernel_x = <tf.Tensor: shape=(1, 7), dtype=float32, numpy=
array([[0.14285715, 0.14285715, 0.14285715, 0.14285715, 0.14285715,
        0.14285715, 0.14285715]], dtype=float32)>
kernel_y = <tf.Tensor: shape=(1, 7), dtype=float32, numpy=
array([[0.14285715, 0.14285715, 0.14285715, 0.14285715, 0.14285715,
        0.14285715, 0.14285715]], dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same'

    def tensorflow_filter2d_separable(
        input, kernel_x, kernel_y, border_type="reflect", normalized=False, padding="same"
    ):
>       out_x = tensorflow_filter2d(
            input, kernel_x[..., None, :], border_type, normalized, padding
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 30, 24), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., ..., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
          1., 1., 1., 1., 1., 1., 1., 1., 1.]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 1, 7), dtype=float32, numpy=
array([[[0.14285715, 0.14285715, 0.14285715, 0.14285715, 0.14285715,
         0.14285715, 0.14285715]]], dtype=float32)>
border_type = 'reflect', normalized = False, padding = 'same', behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
        output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )
        if padding == "same":
>           out = tensorflow_view_frnt_(output, b, c, h, w)

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:132: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 30, 18), dtype=float32, numpy=
array([[[[1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.00000...0000001, 1.0000001, 1.0000001, 1.0000001,
          1.0000001, 1.0000001, 1.0000001]]]], dtype=float32)>, 1, 1, 24, 24)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a58e81f0>
array_like = <tf.Tensor: shape=(1, 1, 30, 18), dtype=float32, numpy=
array([[[[1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.000000...  1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001,
          1.0000001, 1.0000001, 1.0000001]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 1, 30, 18), dtype=float32, numpy=
array([[[[1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.000000...  1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001,
          1.0000001, 1.0000001, 1.0000001]]]], dtype=float32)>
size = None, args = (1, 1, 24, 24), tensorflow_exists_bknd = <function tensorflow_exists_bknd at 0x7f36a58e83a0>, tensorflow_reshape_frnt = <function tensorflow_reshape_frnt at 0x7f36a4df70a0>

    @tensorflow_handle_methods
    def tensorflow_view_frnt_(tensor, *args, size=None):
        from ...ivy.general import tensorflow_exists_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_reshape_frnt
    
        """Reshape Tensor.
    
        possible arguments are either:
            - size
            - tuple of ints
            - list of ints
            - torch.Size object
            - ints
    
        Parameters
        ----------
        args:int arguments
        size: optional shape
    
        Returns reshaped tensor
        -------
        """
        if tensorflow_exists_bknd(size) and not args:
            shape_tup = size
        elif args and not tensorflow_exists_bknd(size):
            if (
                isinstance(args[0], (tuple, list, tuple, tf.TensorShape))
                or type(args[0]).__name__ == "Size"
            ) and len(args) == 1:
                shape_tup = args[0]
            else:
                shape_tup = args
        else:
            raise ValueError(
                "View only accepts as argument ints, tuple or list of ints or the keyword argument size."
            )
>       return tensorflow_reshape_frnt(tensor, shape_tup)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:318: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 30, 18), dtype=float32, numpy=
array([[[[1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.00000...00001, 1.0000001, 1.0000001, 1.0000001,
          1.0000001, 1.0000001, 1.0000001]]]], dtype=float32)>, (1, 1, 24, 24))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a58e81f0>
array_like = <tf.Tensor: shape=(1, 1, 30, 18), dtype=float32, numpy=
array([[[[1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.000000...  1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001,
          1.0000001, 1.0000001, 1.0000001]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 30, 18), dtype=float32, numpy=
array([[[[1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.000000...  1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001,
          1.0000001, 1.0000001, 1.0000001]]]], dtype=float32)>
shape = (1, 1, 24, 24)

    @tensorflow_handle_methods
    def tensorflow_reshape_frnt(input, shape):
        from ...backends.tensorflow.manipulation import tensorflow_reshape
    
>       return tensorflow_reshape(input, shape)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 30, 18), dtype=float32, numpy=
array([[[[1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.00000...00001, 1.0000001, 1.0000001, 1.0000001,
          1.0000001, 1.0000001, 1.0000001]]]], dtype=float32)>, (1, 1, 24, 24))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a58e81f0>
array_like = <tf.Tensor: shape=(1, 1, 30, 18), dtype=float32, numpy=
array([[[[1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.000000...  1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001,
          1.0000001, 1.0000001, 1.0000001]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 1, 30, 18), dtype=float32, numpy=
array([[[[1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.00000...00001, 1.0000001, 1.0000001, 1.0000001,
          1.0000001, 1.0000001, 1.0000001]]]], dtype=float32)>, (1, 1, 24, 24)]
kwargs = {}, tensorflow_asarray = <function tensorflow_asarray at 0x7f36a58eacb0>, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a58e81f0>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36a58e85e0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f36a58becb0>, num_args = 2
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'shape', 'copy', 'order', 'allowzero', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...s 'bool'>, typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 1

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 1, 30, 18), dtype=float32, numpy=
array([[[[1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.000000...  1.0000001, 1.0000001, 1.0000001, 1.0000001, 1.0000001,
          1.0000001, 1.0000001, 1.0000001]]]], dtype=float32)>
shape = (1, 1, 24, 24)

    @tensorflow_handle_methods
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_reshape(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Union[tf.TensorShape, Sequence[int]],
        *,
        copy: Optional[bool] = None,
        order: str = "C",
        allowzero: bool = True,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ....utils.assertions import tensorflow_check_elem_in_list
    
        tensorflow_check_elem_in_list(order, ["C", "F"])
        if not allowzero:
            shape = [
                (new_s if con else old_s)
                for new_s, con, old_s in zip(
                    shape, tensorflow.constant(shape) != 0, x.shape
                )
            ]
        if order == "F":
            return tensorflow__reshape_fortran_tf(x, shape)
>       return tensorflow.reshape(x, shape)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomBoxBlur.call().
E       
E       [1m{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 540 values, but the requested shape has 576 [Op:Reshape][0m
E       
E       Arguments received by tensorflow_RandomBoxBlur.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 24, 24), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/manipulation.py:157: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomBoxBlur
________________________________________________________________________________ test_RandomClahe[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomClahe(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomClahe")
    
        init_args = ()
        init_kwargs = {}
        call_args = (torch.rand(2, 3, 10, 20),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomClahe,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:215: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.clahe.RandomClahe'>, target = 'tensorflow', init_args = (), init_kwargs = {}
call_args = (tensor([[[[0.1984, 0.4428, 0.7323,  ..., 0.7605, 0.3204, 0.1689],
          [0.6056, 0.7998, 0.5551,  ..., 0.1427, 0...., 0.0053, 0.8697,  ..., 0.1743, 0.0143, 0.1981],
          [0.3429, 0.6849, 0.6620,  ..., 0.2249, 0.2473, 0.9910]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
args = (<tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.19837314, 0.44282305, 0.73227084, ..., 0.76054   ...
         [0.34285307, 0.68489426, 0.66199434, ..., 0.22491032,
          0.2473082 , 0.9910176 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55b3bfb11440, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slo...,
         [0.34285307, 0.68489426, 0.66199434, ..., 0.22491032,
          0.2473082 , 0.9910176 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.19837314, 0.44282305, 0.73227084, ..., 0.76054   ...
         [0.34285307, 0.68489426, 0.66199434, ..., 0.22491032,
          0.2473082 , 0.9910176 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slo...,
         [0.34285307, 0.68489426, 0.66199434, ..., 0.22491032,
          0.2473082 , 0.9910176 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.19837314, 0.44282305, 0.73227084, ..., 0.76054   ...
         [0.34285307, 0.68489426, 0.66199434, ..., 0.22491032,
          0.2473082 , 0.9910176 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.19837314, 0.44282305, 0.73227084, ..., 0.76054   ,...],
         [0.34285307, 0.68489426, 0.66199434, ..., 0.22491032,
          0.2473082 , 0.9910176 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.19837314, 0.44282305, 0.73227084, ..., 0...,
         [0.34285307, 0.68489426, 0.66199434, ..., 0.22491032,
          0.2473082 , 0.9910176 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
input = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.19837314, 0.44282305, 0.73227084, ..., 0.76054   ,...],
         [0.34285307, 0.68489426, 0.66199434, ..., 0.22491032,
          0.2473082 , 0.9910176 ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a4be1b40>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36a4c429e0>
tensor = <function tensorflow_tensor_frnt at 0x7f36a41f1990>
in_tensor = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.19837314, 0.44282305, 0.73227084, ..., 0.76054   ,...],
         [0.34285307, 0.68489426, 0.66199434, ..., 0.22491032,
          0.2473082 , 0.9910176 ]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([2, 3, 10, 20]), batch_shape = ivy.frontends.torch.Size([2, 3, 10, 20]), flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
in_tensor = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.19837314, 0.44282305, 0.73227084, ..., 0.76054   ,...],
         [0.34285307, 0.68489426, 0.66199434, ..., 0.22491032,
          0.2473082 , 0.9910176 ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
input = <tf.Tensor: shape=(2, 3, 10, 20), dtype=float32, numpy=
array([[[[0.19837314, 0.44282305, 0.73227084, ..., 0.76054   ,...],
         [0.34285307, 0.68489426, 0.66199434, ..., 0.22491032,
          0.2473082 , 0.9910176 ]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}
transform = <tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],

       [[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a4be1b40>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f36a4be1630>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f36a4ba05e0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f36a46a56c0>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f36a46f75b0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f36a4ba0790>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f36a4ba0940>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
            output = self.apply_transform(in_tensor, params, flags, transform=transform)
        elif not tensorflow_any_frnt_(to_apply):
            output = self.apply_non_transform(
                in_tensor, params, flags, transform=transform
            )
        else:
            output = self.apply_non_transform(
                in_tensor, params, flags, transform=transform
            )
>           applied = self.apply_transform(
                tensorflow_get_item(in_tensor, to_apply),
                params,
                flags,
                transform=transform
                if transform is None
                else tensorflow_get_item(transform, to_apply),
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:616: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomClahe(clip_limit_factor=(40.0, 40.0), p=0.5, p_batch=1.0, same_on_batch=False, grid_size=(8, 8), slow_and_differentiable=False)
input = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[0.68130285, 0.12800032, 0.15445751, 0.441351  , 0.64...0.737263  , 0.9881273 ,
          0.33308643, 0.26824492, 0.22491032, 0.2473082 , 0.9910176 ]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 1.], dtype=float32)>, 'clip_limit_factor': <tf....ray([40.], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 2,  3, 10, 20])>}
flags = {'grid_size': (8, 8), 'slow_and_differentiable': False}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....enhance.equalization import tensorflow_equalize_clahe
    
        clip_limit = float(params["clip_limit_factor"][0])
>       return tensorflow_equalize_clahe(
            input, clip_limit, flags["grid_size"], flags["slow_and_differentiable"]
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/clahe.py:58: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[0.68130285, 0.12800032, 0.15445751, 0.441351  , 0.64...0.737263  , 0.9881273 ,
          0.33308643, 0.26824492, 0.22491032, 0.2473082 , 0.9910176 ]]]],
      dtype=float32)>
args = (40.0, (8, 8), False), kwargs = {}, tensorflow_numel_frnt_ = <function tensorflow_numel_frnt_ at 0x7f36a4be2050>, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a4be1b40>
tensorflow_view_frnt_ = <function tensorflow_view_frnt_ at 0x7f36a4be32e0>, input_shape = ivy.frontends.torch.Size([1, 3, 10, 20])

    @wraps(f)
    def _wrapper(input, *args, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_numel_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
        if tensorflow_numel_frnt_(input) == 0:
            raise ValueError("Invalid input tensor, it is empty.")
        input_shape = tensorflow_shape_frnt_(input)
        input = tensorflow__to_bchw(input)
>       output = f(input, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/utils/image.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[0.68130285, 0.12800032, 0.15445751, 0.441351  , 0.64...0.737263  , 0.9881273 ,
          0.33308643, 0.26824492, 0.22491032, 0.2473082 , 0.9910176 ]]]],
      dtype=float32)>
clip_limit = 40.0, grid_size = (8, 8), slow_and_differentiable = False

    @tensorflow_perform_keep_shape_image
    def tensorflow_equalize_clahe(
        input, clip_limit=40.0, grid_size=(8, 8), slow_and_differentiable=False
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_as_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
    
        if not isinstance(clip_limit, (float,)):
            raise TypeError(f"Input clip_limit type is not float. Got {type(clip_limit)}")
        if not isinstance(grid_size, (tuple,)):
            raise TypeError(f"Input grid_size type is not Tuple. Got {type(grid_size)}")
        if len(grid_size) != 2:
            raise TypeError(
                f"Input grid_size is not a Tuple with 2 elements. Got {len(grid_size)}"
            )
        if isinstance(grid_size[0], (float,)) or isinstance(grid_size[1], (float,)):
            raise TypeError("Input grid_size type is not valid, must be a Tuple[int, int].")
        if grid_size[0] <= 0 or grid_size[1] <= 0:
            raise ValueError(f"Input grid_size elements must be positive. Got {grid_size}")
        imgs: typing.Any = input
>       hist_tiles, img_padded = tensorflow__compute_tiles(imgs, grid_size, True)

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:513: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

imgs = <tf.Tensor: shape=(1, 3, 10, 20), dtype=float32, numpy=
array([[[[0.68130285, 0.12800032, 0.15445751, 0.441351  , 0.64...0.737263  , 0.9881273 ,
          0.33308643, 0.26824492, 0.22491032, 0.2473082 , 0.9910176 ]]]],
      dtype=float32)>
grid_size = (8, 8), even_tile_size = True

    def tensorflow__compute_tiles(imgs, grid_size, even_tile_size=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_pad_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unfold_frnt_
    
        batch: typing.Any = imgs
        h, w = tensorflow_shape_frnt_(batch)[-2:][0], tensorflow_shape_frnt_(batch)[-2:][1]
        kernel_vert: typing.Any = math.ceil(h / grid_size[0])
        kernel_horz: typing.Any = math.ceil(w / grid_size[1])
        if even_tile_size:
            kernel_vert = kernel_vert + (1 if kernel_vert % 2 else 0)
            kernel_horz = kernel_horz + (1 if kernel_horz % 2 else 0)
        pad_vert = kernel_vert * grid_size[0] - h
        pad_horz = kernel_horz * grid_size[1] - w
        if (
            pad_vert > tensorflow_shape_frnt_(batch)[-2]
            or pad_horz > tensorflow_shape_frnt_(batch)[-1]
        ):
            raise ValueError(
                "Cannot compute tiles on the image according to the given grid size"
            )
        if pad_vert > 0 or pad_horz > 0:
            batch = tensorflow_pad_frnt(batch, [0, pad_horz, 0, pad_vert], mode="reflect")
        c: typing.Any = tensorflow_shape_frnt_(batch)[-3]
        tiles: typing.Any = tensorflow_contiguous_frnt_(
            tensorflow_squeeze_frnt_(
>               tensorflow_unfold_frnt_(
                    tensorflow_unfold_frnt_(
                        tensorflow_unfold_frnt_(batch, 1, c, c), 2, kernel_vert, kernel_vert
                    ),
                    3,
                    kernel_horz,
                    kernel_horz,
                ),
                1,
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/enhance/equalization.py:63: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 4, 2, 22, 20), dtype=float32, numpy=
array([[[[[[0.68130285, 0.12800032, 0.15445751, ..., 0.....63352084, 0.17262983, 0.64792067, ..., 0.988712  ,
            0.04489982, 0.60261685]]]]]], dtype=float32)>, 3, 4, 4)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a4c425f0>
array_like = <tf.Tensor: shape=(1, 1, 4, 2, 22, 20), dtype=float32, numpy=
array([[[[[[0.68130285, 0.12800032, 0.15445751, ..., 0.4...        [0.63352084, 0.17262983, 0.64792067, ..., 0.988712  ,
            0.04489982, 0.60261685]]]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 1, 4, 2, 22, 20), dtype=float32, numpy=
array([[[[[[0.68130285, 0.12800032, 0.15445751, ..., 0.4...        [0.63352084, 0.17262983, 0.64792067, ..., 0.988712  ,
            0.04489982, 0.60261685]]]]]], dtype=float32)>
dimension = 3, size = 4, step = 4

    @tensorflow_handle_methods
    def tensorflow_unfold_frnt_(tensor, dimension, size, step):
        from ...backends.tensorflow.general import tensorflow_get_item
        from ...ivy.general import tensorflow_set_item_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_stack_frnt
    
        slices = []
        self_shape = tuple(tensorflow_shape_frnt_(tensor))
        for i in range(0, tensorflow_get_item(self_shape, dimension) - size + 1, step):
            slicing = [slice(None)] * len(tensorflow_shape_frnt_(tensor))
            slicing = tensorflow_set_item_bknd(slicing, dimension, slice(i, i + size))
            slices.append(tensorflow_get_item(tensor, tuple(slicing)))
>       stacked = tensorflow_stack_frnt(slices, dim=dimension)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:679: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = [], dim = 3

    def tensorflow_stack_frnt(tensors, dim=0, *, out=None):
        from ...backends.tensorflow.manipulation import tensorflow_stack
    
>       return tensorflow_stack(tensors, axis=dim, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:171: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ([],), kwargs = {'axis': 3, 'out': None}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a4c425f0>, array_like = []

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
>           array_like = array_like[0]
E           IndexError: Exception encountered when calling tensorflow_RandomClahe.call().
E           
E           [1mlist index out of range[0m
E           
E           Arguments received by tensorflow_RandomClahe.call():
E             â€¢ input=tf.Tensor(shape=(2, 3, 10, 20), dtype=float32)
E             â€¢ params=None
E             â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:183: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomClahe
____________________________________________________________________________ test_RandomGaussianBlur[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomGaussianBlur(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomGaussianBlur")
    
        init_args = ((3, 3), (0.1, 2.0))
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomGaussianBlur,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.gaussian_blur.RandomGaussianBlur'>, target = 'tensorflow', init_args = ((3, 3), (0.1, 2.0)), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[4.7708e-01, 2.6515e-01, 7.0853e-01, 6.1903e-01, 9.9713e-01],
          [2.9862e-02, 5.3777e-02, 8.6965e-01...e-01, 5.8762e-01, 4.6549e-01, 6.0426e-01],
          [3.3896e-01, 2.3612e-01, 7.9159e-01, 8.2021e-01, 1.7630e-01]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomGaussianBlur(sigma=(0.1, 2.0), p=1.0, p_batch=1.0, same_on_batch=False, kernel_size=(3, 3), separable=True, border_type=reflect)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01, 6.190334...],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f36a602d240, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomGaussianBlur(sigma=(0.1, 2.0), p=1.0, p_batch=1.0, same_on_batch=False, kernel_size=(3, 3), separabl...1],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomGaussianBlur(sigma=(0.1, 2.0), p=1.0, p_batch=1.0, same_on_batch=False, kernel_size=(3, 3), separable=True, border_type=reflect), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01, 6.190334...],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomGaussianBlur(sigma=(0.1, 2.0), p=1.0, p_batch=1.0, same_on_batch=False, kernel_size=(3, 3), separabl...1],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomGaussianBlur(sigma=(0.1, 2.0), p=1.0, p_batch=1.0, same_on_batch=False, kernel_size=(3, 3), separable=True, border_type=reflect), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01, 6.190334...],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01, 6.1903340...01],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomGaussianBlur(sigma=(0.1, 2.0), p=1.0, p_batch=1.0, same_on_batch=False, kernel_size=(3, 3), separable=True, border_type=reflect),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01,...1],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomGaussianBlur(sigma=(0.1, 2.0), p=1.0, p_batch=1.0, same_on_batch=False, kernel_size=(3, 3), separable=True, border_type=reflect)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01, 6.1903340...01],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te..., numpy=array([1, 1, 5, 5])>, 'sigma': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.0793759], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a63fb0a0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36a56aaa70>
tensor = <function tensorflow_tensor_frnt at 0x7f36a5e78040>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01, 6.1903340...01],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5])
flags = {'border_type': <tensorflow_BorderType.REFLECT: 1>, 'kernel_size': (3, 3), 'separable': True}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomGaussianBlur(sigma=(0.1, 2.0), p=1.0, p_batch=1.0, same_on_batch=False, kernel_size=(3, 3), separable=True, border_type=reflect)
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01, 6.1903340...01],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te..., numpy=array([1, 1, 5, 5])>, 'sigma': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.0793759], dtype=float32)>}
flags = {'border_type': <tensorflow_BorderType.REFLECT: 1>, 'kernel_size': (3, 3), 'separable': True}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomGaussianBlur(sigma=(0.1, 2.0), p=1.0, p_batch=1.0, same_on_batch=False, kernel_size=(3, 3), separable=True, border_type=reflect)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01, 6.1903340...01],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te..., numpy=array([1, 1, 5, 5])>, 'sigma': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.0793759], dtype=float32)>}
flags = {'border_type': <tensorflow_BorderType.REFLECT: 1>, 'kernel_size': (3, 3), 'separable': True}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a63fb0a0>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f36a637eb00>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f36a5630670>, tensorflow_get_item = <function tensorflow_get_item at 0x7f36a5841750>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f36a57779a0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f36a5630820>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f36a56309d0>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomGaussianBlur(sigma=(0.1, 2.0), p=1.0, p_batch=1.0, same_on_batch=False, kernel_size=(3, 3), separable=True, border_type=reflect)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01, 6.1903340...01],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te..., numpy=array([1, 1, 5, 5])>, 'sigma': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.0793759], dtype=float32)>}
flags = {'border_type': <tensorflow_BorderType.REFLECT: 1>, 'kernel_size': (3, 3), 'separable': True}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from .....ivy.functional.frontends.torch.tensor import (
            tensorflow_unsqueeze_frnt_,
        )
    
        sigma = tensorflow_expand_frnt_(
            tensorflow_unsqueeze_frnt_(params["sigma"], -1), -1, 2
        )
>       return self._gaussian_blur2d_fn(
            input,
            kernel_size=self.flags["kernel_size"],
            sigma=sigma,
            border_type=self.flags["border_type"].name.lower(),
            separable=self.flags["separable"],
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/gaussian_blur.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01, 6.1903340...01],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>
kernel_size = (3, 3), sigma = <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1.0793759, 1.0793759]], dtype=float32)>, border_type = 'reflect', separable = True

    def tensorflow_gaussian_blur2d(
        input, kernel_size, sigma, border_type="reflect", separable=True
    ):
        from ..core._backend import tensor
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .kernels import tensorflow__unpack_2d_ks
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .kernels import tensorflow_get_gaussian_kernel1d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from .filter import tensorflow_filter2d_separable
        from .kernels import tensorflow_get_gaussian_kernel2d
        from .filter import tensorflow_filter2d
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        if isinstance(sigma, (tuple,)):
            sigma = tensor([sigma], device=input.device, dtype=input.dtype)
        else:
            tensorflow_KORNIA_CHECK_IS_TENSOR(sigma)
            sigma = tensorflow_to_frnt_(sigma, device=input.device, dtype=input.dtype)
        if separable:
            ky, kx = tensorflow__unpack_2d_ks(kernel_size)
            bs = tensorflow_shape_frnt_(sigma)[0]
            kernel_x = tensorflow_get_gaussian_kernel1d(
                kx, tensorflow_view_frnt_(sigma[:, 1], bs, 1)
            )
            kernel_y = tensorflow_get_gaussian_kernel1d(
                ky, tensorflow_view_frnt_(sigma[:, 0], bs, 1)
            )
>           out = tensorflow_filter2d_separable(input, kernel_x, kernel_y, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/gaussian.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[4.7708029e-01, 2.6515210e-01, 7.0852959e-01, 6.1903340...01],
         [3.3895677e-01, 2.3612374e-01, 7.9158717e-01, 8.2020748e-01,
          1.7629999e-01]]]], dtype=float32)>
kernel_x = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.28280744, 0.43438515, 0.28280744]], dtype=float32)>
kernel_y = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.28280744, 0.43438515, 0.28280744]], dtype=float32)>, border_type = 'reflect', normalized = False, padding = 'same'

    def tensorflow_filter2d_separable(
        input, kernel_x, kernel_y, border_type="reflect", normalized=False, padding="same"
    ):
>       out_x = tensorflow_filter2d(
            input, kernel_x[..., None, :], border_type, normalized, padding
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 7, 5), dtype=float32, numpy=
array([[[[2.9861569e-02, 5.3777099e-02, 8.6964786e-01, 7.9332733...01],
         [8.5683161e-01, 1.0275835e-01, 5.8761531e-01, 4.6548611e-01,
          6.0425675e-01]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 1, 3), dtype=float32, numpy=array([[[0.28280744, 0.43438515, 0.28280744]]], dtype=float32)>, border_type = 'reflect', normalized = False, padding = 'same'
behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
        output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )
        if padding == "same":
>           out = tensorflow_view_frnt_(output, b, c, h, w)

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:132: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 7, 3), dtype=float32, numpy=
array([[[[0.27774793, 0.61732954, 0.62629986],
         [0.4504...   [0.4222949 , 0.6425921 , 0.6300116 ],
         [0.45313704, 0.41595513, 0.5392705 ]]]], dtype=float32)>, 1, 1, 5, 5)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a56aa680>
array_like = <tf.Tensor: shape=(1, 1, 7, 3), dtype=float32, numpy=
array([[[[0.27774793, 0.61732954, 0.62629986],
         [0.45047...705 ],
         [0.4222949 , 0.6425921 , 0.6300116 ],
         [0.45313704, 0.41595513, 0.5392705 ]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 1, 7, 3), dtype=float32, numpy=
array([[[[0.27774793, 0.61732954, 0.62629986],
         [0.45047...705 ],
         [0.4222949 , 0.6425921 , 0.6300116 ],
         [0.45313704, 0.41595513, 0.5392705 ]]]], dtype=float32)>
size = None, args = (1, 1, 5, 5), tensorflow_exists_bknd = <function tensorflow_exists_bknd at 0x7f36a56aa830>, tensorflow_reshape_frnt = <function tensorflow_reshape_frnt at 0x7f36a5a70550>

    @tensorflow_handle_methods
    def tensorflow_view_frnt_(tensor, *args, size=None):
        from ...ivy.general import tensorflow_exists_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_reshape_frnt
    
        """Reshape Tensor.
    
        possible arguments are either:
            - size
            - tuple of ints
            - list of ints
            - torch.Size object
            - ints
    
        Parameters
        ----------
        args:int arguments
        size: optional shape
    
        Returns reshaped tensor
        -------
        """
        if tensorflow_exists_bknd(size) and not args:
            shape_tup = size
        elif args and not tensorflow_exists_bknd(size):
            if (
                isinstance(args[0], (tuple, list, tuple, tf.TensorShape))
                or type(args[0]).__name__ == "Size"
            ) and len(args) == 1:
                shape_tup = args[0]
            else:
                shape_tup = args
        else:
            raise ValueError(
                "View only accepts as argument ints, tuple or list of ints or the keyword argument size."
            )
>       return tensorflow_reshape_frnt(tensor, shape_tup)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:318: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 7, 3), dtype=float32, numpy=
array([[[[0.27774793, 0.61732954, 0.62629986],
         [0.4504... [0.4222949 , 0.6425921 , 0.6300116 ],
         [0.45313704, 0.41595513, 0.5392705 ]]]], dtype=float32)>, (1, 1, 5, 5))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a56aa680>
array_like = <tf.Tensor: shape=(1, 1, 7, 3), dtype=float32, numpy=
array([[[[0.27774793, 0.61732954, 0.62629986],
         [0.45047...705 ],
         [0.4222949 , 0.6425921 , 0.6300116 ],
         [0.45313704, 0.41595513, 0.5392705 ]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 7, 3), dtype=float32, numpy=
array([[[[0.27774793, 0.61732954, 0.62629986],
         [0.45047...705 ],
         [0.4222949 , 0.6425921 , 0.6300116 ],
         [0.45313704, 0.41595513, 0.5392705 ]]]], dtype=float32)>
shape = (1, 1, 5, 5)

    @tensorflow_handle_methods
    def tensorflow_reshape_frnt(input, shape):
        from ...backends.tensorflow.manipulation import tensorflow_reshape
    
>       return tensorflow_reshape(input, shape)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 7, 3), dtype=float32, numpy=
array([[[[0.27774793, 0.61732954, 0.62629986],
         [0.4504... [0.4222949 , 0.6425921 , 0.6300116 ],
         [0.45313704, 0.41595513, 0.5392705 ]]]], dtype=float32)>, (1, 1, 5, 5))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a56aa680>
array_like = <tf.Tensor: shape=(1, 1, 7, 3), dtype=float32, numpy=
array([[[[0.27774793, 0.61732954, 0.62629986],
         [0.45047...705 ],
         [0.4222949 , 0.6425921 , 0.6300116 ],
         [0.45313704, 0.41595513, 0.5392705 ]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 1, 7, 3), dtype=float32, numpy=
array([[[[0.27774793, 0.61732954, 0.62629986],
         [0.4504... [0.4222949 , 0.6425921 , 0.6300116 ],
         [0.45313704, 0.41595513, 0.5392705 ]]]], dtype=float32)>, (1, 1, 5, 5)]
kwargs = {}, tensorflow_asarray = <function tensorflow_asarray at 0x7f36a56e91b0>, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a56aa680>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36a56aaa70>, tensorflow_get_item = <function tensorflow_get_item at 0x7f36a5841750>, num_args = 2
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'shape', 'copy', 'order', 'allowzero', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...s 'bool'>, typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 1

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 1, 7, 3), dtype=float32, numpy=
array([[[[0.27774793, 0.61732954, 0.62629986],
         [0.45047...705 ],
         [0.4222949 , 0.6425921 , 0.6300116 ],
         [0.45313704, 0.41595513, 0.5392705 ]]]], dtype=float32)>
shape = (1, 1, 5, 5)

    @tensorflow_handle_methods
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_reshape(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Union[tf.TensorShape, Sequence[int]],
        *,
        copy: Optional[bool] = None,
        order: str = "C",
        allowzero: bool = True,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ....utils.assertions import tensorflow_check_elem_in_list
    
        tensorflow_check_elem_in_list(order, ["C", "F"])
        if not allowzero:
            shape = [
                (new_s if con else old_s)
                for new_s, con, old_s in zip(
                    shape, tensorflow.constant(shape) != 0, x.shape
                )
            ]
        if order == "F":
            return tensorflow__reshape_fortran_tf(x, shape)
>       return tensorflow.reshape(x, shape)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomGaussianBlur.call().
E       
E       [1m{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 21 values, but the requested shape has 25 [Op:Reshape][0m
E       
E       Arguments received by tensorflow_RandomGaussianBlur.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/manipulation.py:201: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomGaussianBlur
________________________________________________________________________________ test_RandomJPEG[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomJPEG(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomJPEG")
    
        init_args = ()
        init_kwargs = {"jpeg_quality": (1.0, 50.0), "p": 1.}
        call_args = (0.1904 * torch.ones(2, 3, 32, 32), )
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomJPEG,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:415: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.jpeg.RandomJPEG'>, target = 'tensorflow', init_args = (), init_kwargs = {'jpeg_quality': (1.0, 50.0), 'p': 1.0}
call_args = (tensor([[[[0.1904, 0.1904, 0.1904,  ..., 0.1904, 0.1904, 0.1904],
          [0.1904, 0.1904, 0.1904,  ..., 0.1904, 0...., 0.1904, 0.1904,  ..., 0.1904, 0.1904, 0.1904],
          [0.1904, 0.1904, 0.1904,  ..., 0.1904, 0.1904, 0.1904]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJPEG(RandomJPEG quality=(1.0, 50.0), p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904....., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f36a4247040, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJPEG(RandomJPEG quality=(1.0, 50.0), p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2,......, 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJPEG(RandomJPEG quality=(1.0, 50.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904....., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJPEG(RandomJPEG quality=(1.0, 50.0), p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2,......, 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJPEG(RandomJPEG quality=(1.0, 50.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904....., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]... ..., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJPEG(RandomJPEG quality=(1.0, 50.0), p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.190......, 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJPEG(RandomJPEG quality=(1.0, 50.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]... ..., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'forward_input_shape': <t...2, 32])>, 'jpeg_quality': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([30.939697 ,  4.4468603], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a5111240>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36a503d900>
tensor = <function tensorflow_tensor_frnt at 0x7f36a57749d0>
in_tensor = <tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]... ..., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([2, 3, 32, 32]), batch_shape = ivy.frontends.torch.Size([2, 3, 32, 32]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJPEG(RandomJPEG quality=(1.0, 50.0), p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]... ..., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'forward_input_shape': <t...2, 32])>, 'jpeg_quality': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([30.939697 ,  4.4468603], dtype=float32)>}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJPEG(RandomJPEG quality=(1.0, 50.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]... ..., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'forward_input_shape': <t...2, 32])>, 'jpeg_quality': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([30.939697 ,  4.4468603], dtype=float32)>}
flags = {}
transform = <tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],

       [[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a5111240>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f36a51135b0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f36a4bfa200>, tensorflow_get_item = <function tensorflow_get_item at 0x7f36a5a97a30>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f36a5332290>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f36a4bf9e10>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f36a4bf9c60>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJPEG(RandomJPEG quality=(1.0, 50.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]... ..., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'forward_input_shape': <t...2, 32])>, 'jpeg_quality': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([30.939697 ,  4.4468603], dtype=float32)>}
flags = {}
transform = <tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],

       [[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....enhance.jpeg import tensorflow_jpeg_codec_differentiable
    
>       jpeg_output: typing.Any = tensorflow_jpeg_codec_differentiable(
            input, params["jpeg_quality"]
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/jpeg.py:42: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]... ..., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>
args = (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([30.939697 ,  4.4468603], dtype=float32)>,), kwargs = {}, tensorflow_numel_frnt_ = <function tensorflow_numel_frnt_ at 0x7f36a4bfb250>
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a5111240>, tensorflow_view_frnt_ = <function tensorflow_view_frnt_ at 0x7f36a4bfadd0>
input_shape = ivy.frontends.torch.Size([2, 3, 32, 32])

    @wraps(f)
    def _wrapper(input, *args, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_numel_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
        if tensorflow_numel_frnt_(input) == 0:
            raise ValueError("Invalid input tensor, it is empty.")
        input_shape = tensorflow_shape_frnt_(input)
        input = tensorflow__to_bchw(input)
>       output = f(input, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/utils/image.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image_rgb = <tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]... ..., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>
jpeg_quality = <tf.Tensor: shape=(2,), dtype=float32, numpy=array([30.939697 ,  4.4468603], dtype=float32)>
quantization_table_y = <tf.Tensor: shape=(1, 8, 8), dtype=float32, numpy=
array([[[ 16.,  11.,  10.,  16.,  24.,  40.,  51.,  61.],
        [...  64.,  78.,  87., 103., 121., 120., 101.],
        [ 72.,  92.,  95.,  98., 112., 100., 103.,  99.]]], dtype=float32)>
quantization_table_c = <tf.Tensor: shape=(1, 8, 8), dtype=float32, numpy=
array([[[17., 18., 24., 47., 99., 99., 99., 99.],
        [18., 21....,
        [99., 99., 99., 99., 99., 99., 99., 99.],
        [99., 99., 99., 99., 99., 99., 99., 99.]]], dtype=float32)>

    @tensorflow_perform_keep_shape_image
    def tensorflow_jpeg_codec_differentiable(
        image_rgb, jpeg_quality, quantization_table_y=None, quantization_table_c=None
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_ndim_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_amin_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_amax_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ..utils.misc import tensorflow_differentiable_clipping
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(image_rgb)
        tensorflow_KORNIA_CHECK_IS_TENSOR(jpeg_quality)
        dtype: typing.Any = image_rgb.dtype
        device: typing.Any = image_rgb.device
        quantization_table_y = (
            tensorflow__get_default_qt_y(device, dtype)
            if quantization_table_y is None
            else quantization_table_y
        )
        quantization_table_c = (
            tensorflow__get_default_qt_c(device, dtype)
            if quantization_table_c is None
            else quantization_table_c
        )
        tensorflow_KORNIA_CHECK_IS_TENSOR(quantization_table_y)
        tensorflow_KORNIA_CHECK_IS_TENSOR(quantization_table_c)
        tensorflow_KORNIA_CHECK_SHAPE(image_rgb, ["*", "3", "H", "W"])
        tensorflow_KORNIA_CHECK_SHAPE(jpeg_quality, ["B"])
        if tensorflow_ndim_frnt_(quantization_table_y) == 2:
            quantization_table_y = tensorflow_unsqueeze_frnt_(quantization_table_y, dim=0)
        if tensorflow_ndim_frnt_(quantization_table_c) == 2:
            quantization_table_c = tensorflow_unsqueeze_frnt_(quantization_table_c, dim=0)
        tensorflow_KORNIA_CHECK_SHAPE(quantization_table_y, ["B", "8", "8"])
        tensorflow_KORNIA_CHECK_SHAPE(quantization_table_c, ["B", "8", "8"])
        tensorflow_KORNIA_CHECK(
            tensorflow_item_frnt_(tensorflow_amin_frnt_(jpeg_quality)) >= 0.0
            and tensorflow_item_frnt_(tensorflow_amax_frnt_(jpeg_quality)) <= 100.0,
            f"JPEG quality is out of range. Expected range is [0, 100], got [{tensorflow_item_frnt_(tensorflow_amin_frnt_(jpeg_quality))}, {tensorflow_item_frnt_(tensorflow_amax_frnt_(jpeg_quality))}]. Consider clipping jpeg_quality.",
        )
        image_rgb, h_pad, w_pad = tensorflow__perform_padding(image_rgb)
        H, W = (
            tensorflow_shape_frnt_(image_rgb)[-2:][0],
            tensorflow_shape_frnt_(image_rgb)[-2:][1],
        )
        if tensorflow_shape_frnt_(quantization_table_y)[0] != 1:
            tensorflow_KORNIA_CHECK(
                tensorflow_shape_frnt_(quantization_table_y)[0]
                == tensorflow_shape_frnt_(image_rgb)[0],
                f"Batch dimensions do not match. Got {tensorflow_shape_frnt_(image_rgb)[0]} images and {tensorflow_shape_frnt_(quantization_table_y)[0]} quantization tables (Y).",
            )
        if tensorflow_shape_frnt_(quantization_table_c)[0] != 1:
            tensorflow_KORNIA_CHECK(
                tensorflow_shape_frnt_(quantization_table_c)[0]
                == tensorflow_shape_frnt_(image_rgb)[0],
                f"Batch dimensions do not match. Got {tensorflow_shape_frnt_(image_rgb)[0]} images and {tensorflow_shape_frnt_(quantization_table_c)[0]} quantization tables (C).",
            )
        if tensorflow_shape_frnt_(jpeg_quality)[0] != 1:
            tensorflow_KORNIA_CHECK(
                tensorflow_shape_frnt_(jpeg_quality)[0]
                == tensorflow_shape_frnt_(image_rgb)[0],
                f"Batch dimensions do not match. Got {tensorflow_shape_frnt_(image_rgb)[0]} images and {tensorflow_shape_frnt_(jpeg_quality)[0]} JPEG qualities.",
            )
        quantization_table_y = tensorflow_to_frnt_(quantization_table_y, device, dtype)
        quantization_table_c = tensorflow_to_frnt_(quantization_table_c, device, dtype)
>       y_encoded, cb_encoded, cr_encoded = tensorflow__jpeg_encode(
            image_rgb=image_rgb,
            jpeg_quality=jpeg_quality,
            quantization_table_c=quantization_table_c,
            quantization_table_y=quantization_table_y,
        )

Translated_Outputs/tensorflow_outputs/kornia/enhance/jpeg.py:414: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image_rgb = <tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]... ..., 0.1904, 0.1904, 0.1904],
         [0.1904, 0.1904, 0.1904, ..., 0.1904, 0.1904, 0.1904]]]],
      dtype=float32)>
jpeg_quality = <tf.Tensor: shape=(2,), dtype=float32, numpy=array([30.939697 ,  4.4468603], dtype=float32)>
quantization_table_y = <tf.Tensor: shape=(1, 8, 8), dtype=float32, numpy=
array([[[ 16.,  11.,  10.,  16.,  24.,  40.,  51.,  61.],
        [...  64.,  78.,  87., 103., 121., 120., 101.],
        [ 72.,  92.,  95.,  98., 112., 100., 103.,  99.]]], dtype=float32)>
quantization_table_c = <tf.Tensor: shape=(1, 8, 8), dtype=float32, numpy=
array([[[17., 18., 24., 47., 99., 99., 99., 99.],
        [18., 21....,
        [99., 99., 99., 99., 99., 99., 99., 99.],
        [99., 99., 99., 99., 99., 99., 99., 99.]]], dtype=float32)>

    def tensorflow__jpeg_encode(
        image_rgb, jpeg_quality, quantization_table_y, quantization_table_c
    ):
        from ..color.ycbcr import tensorflow_rgb_to_ycbcr
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_chunk_frnt_
    
        image_ycbcr: typing.Any = tensorflow_rgb_to_ycbcr(image_rgb)
        image_ycbcr = 255.0 * image_ycbcr
>       input_y, input_cb, input_cr = tensorflow__chroma_subsampling(image_ycbcr)

Translated_Outputs/tensorflow_outputs/kornia/enhance/jpeg.py:270: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input_ycbcr = <tf.Tensor: shape=(2, 3, 32, 32), dtype=float32, numpy=
array([[[[ 48.552002,  48.552002,  48.552002, ...,  48.552002,...],
         [127.5     , 127.5     , 127.5     , ..., 127.5     ,
          127.5     , 127.5     ]]]], dtype=float32)>

    def tensorflow__chroma_subsampling(input_ycbcr):
        from ..geometry.transform.affwarp import tensorflow_rescale
    
        output_y: typing.Any = input_ycbcr[:, 0]
        output_cb: typing.Any = input_ycbcr[:, 1]
        output_cr: typing.Any = input_ycbcr[:, 2]
>       output_cb = tensorflow_rescale(
            output_cb[:, None],
            factor=0.5,
            interpolation="bilinear",
            align_corners=False,
            antialias=True,
        )

Translated_Outputs/tensorflow_outputs/kornia/enhance/jpeg.py:229: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 1, 32, 32), dtype=float32, numpy=
array([[[[127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5],
    ..., 127.5, 127.5, ..., 127.5, 127.5, 127.5],
         [127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5]]]], dtype=float32)>
factor = 0.5, interpolation = 'bilinear', align_corners = False, antialias = True

    def tensorflow_rescale(
        input, factor, interpolation="bilinear", align_corners=None, antialias=False
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
    
        if isinstance(factor, (float,)):
            factor_vert = factor_horz = factor
        else:
            factor_vert, factor_horz = factor[0], factor[1]
        height, width = (
            tensorflow_size_frnt_(input)[-2:][0],
            tensorflow_size_frnt_(input)[-2:][1],
        )
        size = int(height * factor_vert), int(width * factor_horz)
>       return tensorflow_resize(
            input,
            size,
            interpolation=interpolation,
            align_corners=align_corners,
            antialias=antialias,
        )

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/affwarp.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 1, 32, 32), dtype=float32, numpy=
array([[[[127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5],
    ..., 127.5, 127.5, ..., 127.5, 127.5, 127.5],
         [127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5]]]], dtype=float32)>
args = ((16, 16),), kwargs = {'align_corners': False, 'antialias': True, 'interpolation': 'bilinear'}, tensorflow_numel_frnt_ = <function tensorflow_numel_frnt_ at 0x7f36a4bfb250>
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a5111240>, tensorflow_view_frnt_ = <function tensorflow_view_frnt_ at 0x7f36a4bfadd0>
input_shape = ivy.frontends.torch.Size([2, 1, 32, 32])

    @wraps(f)
    def _wrapper(input, *args, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_numel_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input input type is not a Tensor. Got {type(input)}")
        if tensorflow_numel_frnt_(input) == 0:
            raise ValueError("Invalid input tensor, it is empty.")
        input_shape = tensorflow_shape_frnt_(input)
        input = tensorflow__to_bchw(input)
>       output = f(input, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/utils/image.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 1, 32, 32), dtype=float32, numpy=
array([[[[127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5],
    ..., 127.5, 127.5, ..., 127.5, 127.5, 127.5],
         [127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5]]]], dtype=float32)>
size = (16, 16), interpolation = 'bilinear', align_corners = False, side = 'short', antialias = True

    @tensorflow_perform_keep_shape_image
    def tensorflow_resize(
        input,
        size,
        interpolation="bilinear",
        align_corners=None,
        side="short",
        antialias=False,
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...filters.gaussian import tensorflow_gaussian_blur2d
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_interpolate_frnt,
        )
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input tensor type is not a torch.Tensor. Got {type(input)}")
        if len(tensorflow_shape_frnt_(input)) < 2:
            raise ValueError(
                f"Input tensor must have at least two dimensions. Got {len(tensorflow_shape_frnt_(input))}"
            )
        input_size = h, w = tensorflow_shape_frnt_(input)[-2:]
        if isinstance(size, (int,)):
            aspect_ratio = w / h
            size = tensorflow__side_to_image_size(size, aspect_ratio, side)
        if size == input_size:
            return input
        factors = h / size[0], w / size[1]
        antialias = antialias and max(factors) > 1
        if antialias:
            sigmas = (
                max((factors[0] - 1.0) / 2.0, 0.001),
                max((factors[1] - 1.0) / 2.0, 0.001),
            )
            ks = int(max(2.0 * 2 * sigmas[0], 3)), int(max(2.0 * 2 * sigmas[1], 3))
            if ks[0] % 2 == 0:
                ks = ks[0] + 1, ks[1]
            if ks[1] % 2 == 0:
                ks = ks[0], ks[1] + 1
>           input = tensorflow_gaussian_blur2d(input, ks, sigmas)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/affwarp.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 1, 32, 32), dtype=float32, numpy=
array([[[[127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5],
    ..., 127.5, 127.5, ..., 127.5, 127.5, 127.5],
         [127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5]]]], dtype=float32)>
kernel_size = (3, 3), sigma = <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.5, 0.5]], dtype=float32)>, border_type = 'reflect', separable = True

    def tensorflow_gaussian_blur2d(
        input, kernel_size, sigma, border_type="reflect", separable=True
    ):
        from ..core._backend import tensor
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .kernels import tensorflow__unpack_2d_ks
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .kernels import tensorflow_get_gaussian_kernel1d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from .filter import tensorflow_filter2d_separable
        from .kernels import tensorflow_get_gaussian_kernel2d
        from .filter import tensorflow_filter2d
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        if isinstance(sigma, (tuple,)):
            sigma = tensor([sigma], device=input.device, dtype=input.dtype)
        else:
            tensorflow_KORNIA_CHECK_IS_TENSOR(sigma)
            sigma = tensorflow_to_frnt_(sigma, device=input.device, dtype=input.dtype)
        if separable:
            ky, kx = tensorflow__unpack_2d_ks(kernel_size)
            bs = tensorflow_shape_frnt_(sigma)[0]
            kernel_x = tensorflow_get_gaussian_kernel1d(
                kx, tensorflow_view_frnt_(sigma[:, 1], bs, 1)
            )
            kernel_y = tensorflow_get_gaussian_kernel1d(
                ky, tensorflow_view_frnt_(sigma[:, 0], bs, 1)
            )
>           out = tensorflow_filter2d_separable(input, kernel_x, kernel_y, border_type)

Translated_Outputs/tensorflow_outputs/kornia/filters/gaussian.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 1, 32, 32), dtype=float32, numpy=
array([[[[127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5],
    ..., 127.5, 127.5, ..., 127.5, 127.5, 127.5],
         [127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5]]]], dtype=float32)>
kernel_x = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.10650697, 0.786986  , 0.10650697]], dtype=float32)>
kernel_y = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.10650697, 0.786986  , 0.10650697]], dtype=float32)>, border_type = 'reflect', normalized = False, padding = 'same'

    def tensorflow_filter2d_separable(
        input, kernel_x, kernel_y, border_type="reflect", normalized=False, padding="same"
    ):
>       out_x = tensorflow_filter2d(
            input, kernel_x[..., None, :], border_type, normalized, padding
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:141: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 1, 34, 32), dtype=float32, numpy=
array([[[[127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5],
    ..., 127.5, 127.5, ..., 127.5, 127.5, 127.5],
         [127.5, 127.5, 127.5, ..., 127.5, 127.5, 127.5]]]], dtype=float32)>
kernel = <tf.Tensor: shape=(1, 1, 3), dtype=float32, numpy=array([[[0.10650697, 0.786986  , 0.10650697]]], dtype=float32)>, border_type = 'reflect', normalized = False, padding = 'same'
behaviour = 'corr'

    def tensorflow_filter2d(
        input,
        kernel,
        border_type="reflect",
        normalized=False,
        padding="same",
        behaviour="corr",
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_flip_frnt_
        from .kernels import tensorflow_normalize_kernel2d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ..core._backend import pad
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(input)
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "C", "H", "W"])
        tensorflow_KORNIA_CHECK_IS_TENSOR(kernel)
        tensorflow_KORNIA_CHECK_SHAPE(kernel, ["B", "H", "W"])
        tensorflow_KORNIA_CHECK(
            str(border_type).lower() in _VALID_BORDERS,
            f"Invalid border, gotcha {border_type}. Expected one of {_VALID_BORDERS}",
        )
        tensorflow_KORNIA_CHECK(
            str(padding).lower() in _VALID_PADDING,
            f"Invalid padding mode, gotcha {padding}. Expected one of {_VALID_PADDING}",
        )
        tensorflow_KORNIA_CHECK(
            str(behaviour).lower() in _VALID_BEHAVIOUR,
            f"Invalid padding mode, gotcha {behaviour}. Expected one of {_VALID_BEHAVIOUR}",
        )
        b, c, h, w = tensorflow_shape_frnt_(input)
        if str(behaviour).lower() == "conv":
            tmp_kernel = tensorflow_to_frnt_(
                tensorflow_flip_frnt_(kernel, (-2, -1))[:, None, ...],
                device=input.device,
                dtype=input.dtype,
            )
        else:
            tmp_kernel = tensorflow_to_frnt_(
                kernel[:, None, ...], device=input.device, dtype=input.dtype
            )
        if normalized:
            tmp_kernel = tensorflow_normalize_kernel2d(tmp_kernel)
        tmp_kernel = tensorflow_expand_frnt_(tmp_kernel, -1, c, -1, -1)
        height, width = (
            tensorflow_shape_frnt_(tmp_kernel)[-2:][0],
            tensorflow_shape_frnt_(tmp_kernel)[-2:][1],
        )
        if padding == "same":
            padding_shape: typing.Any = tensorflow__compute_padding([height, width])
            input = pad(input, padding_shape, mode=border_type)
        tmp_kernel = tensorflow_reshape_frnt_(tmp_kernel, -1, 1, height, width)
        input = tensorflow_view_frnt_(
            input,
            -1,
            tensorflow_size_frnt_(tmp_kernel, 0),
            tensorflow_size_frnt_(input, -2),
            tensorflow_size_frnt_(input, -1),
        )
        output = tensorflow_conv2d_frnt(
            input,
            tmp_kernel,
            groups=tensorflow_size_frnt_(tmp_kernel, 0),
            padding=0,
            stride=1,
        )
        if padding == "same":
>           out = tensorflow_view_frnt_(output, b, c, h, w)

Translated_Outputs/tensorflow_outputs/kornia/filters/filter.py:132: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 1, 34, 30), dtype=float32, numpy=
array([[[[127.499985, 127.499985, 127.499985, ..., 127.499985...7.499985, 127.499985, 127.499985, ..., 127.499985,
          127.499985, 127.499985]]]], dtype=float32)>, 2, 1, 32, 32)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a4768ca0>
array_like = <tf.Tensor: shape=(2, 1, 34, 30), dtype=float32, numpy=
array([[[[127.499985, 127.499985, 127.499985, ..., 127.499985,...],
         [127.499985, 127.499985, 127.499985, ..., 127.499985,
          127.499985, 127.499985]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(2, 1, 34, 30), dtype=float32, numpy=
array([[[[127.499985, 127.499985, 127.499985, ..., 127.499985,...],
         [127.499985, 127.499985, 127.499985, ..., 127.499985,
          127.499985, 127.499985]]]], dtype=float32)>
size = None, args = (2, 1, 32, 32), tensorflow_exists_bknd = <function tensorflow_exists_bknd at 0x7f36a4768dc0>, tensorflow_reshape_frnt = <function tensorflow_reshape_frnt at 0x7f36a5474a60>

    @tensorflow_handle_methods
    def tensorflow_view_frnt_(tensor, *args, size=None):
        from ...ivy.general import tensorflow_exists_bknd
        from .indexing_slicing_joining_mutating_ops import tensorflow_reshape_frnt
    
        """Reshape Tensor.
    
        possible arguments are either:
            - size
            - tuple of ints
            - list of ints
            - torch.Size object
            - ints
    
        Parameters
        ----------
        args:int arguments
        size: optional shape
    
        Returns reshaped tensor
        -------
        """
        if tensorflow_exists_bknd(size) and not args:
            shape_tup = size
        elif args and not tensorflow_exists_bknd(size):
            if (
                isinstance(args[0], (tuple, list, tuple, tf.TensorShape))
                or type(args[0]).__name__ == "Size"
            ) and len(args) == 1:
                shape_tup = args[0]
            else:
                shape_tup = args
        else:
            raise ValueError(
                "View only accepts as argument ints, tuple or list of ints or the keyword argument size."
            )
>       return tensorflow_reshape_frnt(tensor, shape_tup)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:318: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 1, 34, 30), dtype=float32, numpy=
array([[[[127.499985, 127.499985, 127.499985, ..., 127.499985...499985, 127.499985, 127.499985, ..., 127.499985,
          127.499985, 127.499985]]]], dtype=float32)>, (2, 1, 32, 32))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a4768ca0>
array_like = <tf.Tensor: shape=(2, 1, 34, 30), dtype=float32, numpy=
array([[[[127.499985, 127.499985, 127.499985, ..., 127.499985,...],
         [127.499985, 127.499985, 127.499985, ..., 127.499985,
          127.499985, 127.499985]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 1, 34, 30), dtype=float32, numpy=
array([[[[127.499985, 127.499985, 127.499985, ..., 127.499985,...],
         [127.499985, 127.499985, 127.499985, ..., 127.499985,
          127.499985, 127.499985]]]], dtype=float32)>
shape = (2, 1, 32, 32)

    @tensorflow_handle_methods
    def tensorflow_reshape_frnt(input, shape):
        from ...backends.tensorflow.manipulation import tensorflow_reshape
    
>       return tensorflow_reshape(input, shape)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/indexing_slicing_joining_mutating_ops.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 1, 34, 30), dtype=float32, numpy=
array([[[[127.499985, 127.499985, 127.499985, ..., 127.499985...499985, 127.499985, 127.499985, ..., 127.499985,
          127.499985, 127.499985]]]], dtype=float32)>, (2, 1, 32, 32))
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a4768ca0>
array_like = <tf.Tensor: shape=(2, 1, 34, 30), dtype=float32, numpy=
array([[[[127.499985, 127.499985, 127.499985, ..., 127.499985,...],
         [127.499985, 127.499985, 127.499985, ..., 127.499985,
          127.499985, 127.499985]]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(2, 1, 34, 30), dtype=float32, numpy=
array([[[[127.499985, 127.499985, 127.499985, ..., 127.499985...499985, 127.499985, 127.499985, ..., 127.499985,
          127.499985, 127.499985]]]], dtype=float32)>, (2, 1, 32, 32)]
kwargs = {}, tensorflow_asarray = <function tensorflow_asarray at 0x7f36a503fe20>, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36a4768ca0>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36a503d900>, tensorflow_get_item = <function tensorflow_get_item at 0x7f36a5a97a30>, num_args = 2
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'shape', 'copy', 'order', 'allowzero', 'out']
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...s 'bool'>, typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType]]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 1

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(2, 1, 34, 30), dtype=float32, numpy=
array([[[[127.499985, 127.499985, 127.499985, ..., 127.499985,...],
         [127.499985, 127.499985, 127.499985, ..., 127.499985,
          127.499985, 127.499985]]]], dtype=float32)>
shape = (2, 1, 32, 32)

    @tensorflow_handle_methods
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_reshape(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Union[tf.TensorShape, Sequence[int]],
        *,
        copy: Optional[bool] = None,
        order: str = "C",
        allowzero: bool = True,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ....utils.assertions import tensorflow_check_elem_in_list
    
        tensorflow_check_elem_in_list(order, ["C", "F"])
        if not allowzero:
            shape = [
                (new_s if con else old_s)
                for new_s, con, old_s in zip(
                    shape, tensorflow.constant(shape) != 0, x.shape
                )
            ]
        if order == "F":
            return tensorflow__reshape_fortran_tf(x, shape)
>       return tensorflow.reshape(x, shape)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomJPEG.call().
E       
E       [1m{{function_node __wrapped__Reshape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Input to reshape is a tensor with 2040 values, but the requested shape has 2048 [Op:Reshape][0m
E       
E       Arguments received by tensorflow_RandomJPEG.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 32, 32), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/manipulation.py:201: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomJPEG
______________________________________________________________________ test_RandomLinearCornerIllumination[tensorflow-s2s-False] _______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomLinearCornerIllumination(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomLinearCornerIllumination")
    
        init_args = ()
        init_kwargs = {"gain": 0.25, "p": 1.}
        call_args = (torch.ones(1, 3, 3, 3) * 0.5,)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomLinearCornerIllumination,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:435: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.linear_illumination.RandomLinearCornerIllumination'>, target = 'tensorflow', init_args = (), init_kwargs = {'gain': 0.25, 'p': 1.0}
call_args = (tensor([[[[0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000]],

       ...00]],

         [[0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000],
          [0.5000, 0.5000, 0.5000]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0...  [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55b3c01c2050, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=Fals...   [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0...  [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=Fals...   [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0...  [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0....    [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
  ...   [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0....    [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>
params = None, kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a5494af0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36a4575ea0>
tensor = <function tensorflow_tensor_frnt at 0x7f36a5157880>
in_tensor = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0....    [0.5, 0.5, 0.5]],

        [[0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5],
         [0.5, 0.5, 0.5]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 3, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 3, 3, 3])

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
>           params = self.forward_parameters(batch_shape)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), batch_shape = ivy.frontends.torch.Size([1, 3, 3, 3])

    def forward_parameters(self, batch_shape):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        batch_prob = self.__batch_prob_generator__(
            batch_shape, self.p, self.p_batch, self.same_on_batch
        )
        to_apply = batch_prob > 0.5
>       _params = self.generate_parameters(
            tuple(
                (
                    int(tensorflow_item_frnt_(tensorflow_sum_frnt_(to_apply))),
                    *batch_shape[1:],
                )
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:199: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomLinearCornerIllumination(gain=(0.25, 0.25), sign=(-1.0, 1.0), p=1.0, p_batch=1.0, same_on_batch=False), batch_shape = (1, 3, 3, 3)

    def generate_parameters(self, batch_shape):
        if self._param_generator is not None:
>           return self._param_generator(batch_shape, self.same_on_batch)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), args = ((1, 3, 3, 3), False), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55b3bfc83b90, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ion.py', lineno=46, function='__call__', code_context=['            return call_fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), v = None, buffers = None, args = ((1, 3, 3, 3), False), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), v = None, buffers = None, args = ((1, 3, 3, 3), False), kwargs = {}, replace_v = False, replace_buffers = False
call_signature = <Signature (batch_shape, same_on_batch=False)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = gain=(0.25, 0.25), sign=(-1.0, 1.0), batch_shape = (1, 3, 3, 3), same_on_batch = False

    def call(self, batch_shape, same_on_batch=False):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...utils.param_validation import tensorflow__common_param_check
        from ....utils.helpers import tensorflow__extract_device_dtype
        from ...utils.helpers import tensorflow__adapted_rsampling
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_where_frnt,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from .....ivy.functional.frontends.torch.tensor import (
            tensorflow_unsqueeze_frnt_,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_linspace_frnt,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_zeros_frnt,
        )
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....enhance.normalize import tensorflow_normalize
    
        batch_size, channels, height, width = (
            batch_shape[0],
            batch_shape[1],
            batch_shape[2],
            batch_shape[3],
        )
        tensorflow__common_param_check(batch_size, same_on_batch)
        _device, _dtype = tensorflow__extract_device_dtype([self.gain, self.sign])
        gain_factor = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 1, 1, 1), self.gain_sampler, same_on_batch
            ),
            device=_device,
            dtype=_dtype,
        )
        sign = tensorflow_to_frnt_(
            tensorflow_where_frnt(
                tensorflow__adapted_rsampling(
                    (batch_size, 1, 1, 1), self.sign_sampler, same_on_batch
                )
                >= 0.0,
                tensorflow_tensor_frnt(1),
                tensorflow_tensor_frnt(-1),
            ),
            device=_device,
            dtype=_dtype,
        )
        directions = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 1, 1, 1), self.directions_sampler, same_on_batch
            ),
            device=_device,
            dtype=tf.int8,
        )
        y_grad = tensorflow_expand_frnt_(
            tensorflow_unsqueeze_frnt_(tensorflow_linspace_frnt(0, 1, height), 1),
            channels,
            height,
            width,
        )
        x_grad = tensorflow_expand_frnt_(
            tensorflow_unsqueeze_frnt_(tensorflow_linspace_frnt(0, 1, width), 0),
            channels,
            height,
            width,
        )
        gradient = tensorflow_zeros_frnt(batch_shape)
        for _b in range(batch_size):
            if tensorflow_get_item(directions, _b) == 0:
                gradient = tensorflow_set_item_bknd(gradient, _b, x_grad + y_grad)
            elif tensorflow_get_item(directions, _b) == 1:
                gradient = tensorflow_set_item_bknd(gradient, _b, -x_grad + y_grad)
            elif tensorflow_get_item(directions, _b) == 2:
                gradient = tensorflow_set_item_bknd(gradient, _b, x_grad - y_grad)
            elif tensorflow_get_item(directions, _b) == 3:
                gradient = tensorflow_set_item_bknd(gradient, _b, 1 - (x_grad + y_grad))
>       gradient = sign * gain_factor * tensorflow_normalize.normalize_min_max(gradient)
E       AttributeError: Exception encountered when calling tensorflow_LinearCornerIlluminationGenerator.call().
E       
E       [1m'function' object has no attribute 'normalize_min_max'[0m
E       
E       Arguments received by tensorflow_LinearCornerIlluminationGenerator.call():
E         â€¢ batch_shape=('1', '3', '3', '3')
E         â€¢ same_on_batch=False

Translated_Outputs/tensorflow_outputs/kornia/augmentation/random_generator/_2d/linear_illumination.py:147: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomLinearCornerIllumination
_____________________________________________________________________________ test_RandomMotionBlur[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomMotionBlur(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomMotionBlur")
    
        init_args = (3, 35., 0.5)
        init_kwargs = {"p": 1.}
        call_args = (torch.ones(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomMotionBlur,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:495: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.motion_blur.RandomMotionBlur'>, target = 'tensorflow', init_args = (3, 35.0, 0.5), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.],
          [1., 1., 1., 1., 1.]]]]),), call_kwargs = {}
deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f36a4054040, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border..., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border..., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>
params = None, kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a4fb0ca0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36a5331360>
tensor = <function tensorflow_tensor_frnt at 0x7f36a4563eb0>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5])

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
>           params = self.forward_parameters(batch_shape)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5])

    def forward_parameters(self, batch_shape):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        batch_prob = self.__batch_prob_generator__(
            batch_shape, self.p, self.p_batch, self.same_on_batch
        )
        to_apply = batch_prob > 0.5
>       _params = self.generate_parameters(
            tuple(
                (
                    int(tensorflow_item_frnt_(tensorflow_sum_frnt_(to_apply))),
                    *batch_shape[1:],
                )
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:199: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest), batch_shape = (1, 1, 5, 5)

    def generate_parameters(self, batch_shape):
        from ....core._backend import tensor
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .....ivy.functional.frontends.torch.random_sampling import (
            tensorflow_randint_frnt,
        )
    
        params = super().generate_parameters(batch_shape)
        params = tensorflow_set_item_bknd(
            params,
            "idx",
            tensor([0])
            if batch_shape[0] == 0
>           else tensorflow_randint_frnt(batch_shape[0], (1,)),
        )
E       TypeError: Exception encountered when calling tensorflow_RandomMotionBlur.call().
E       
E       [1mtensorflow_randint_frnt() missing 1 required positional argument: 'size'[0m
E       
E       Arguments received by tensorflow_RandomMotionBlur.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/motion_blur.py:70: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomMotionBlur
________________________________________________________________________________ test_RandomRain[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRain(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomRain")
    
        init_args = ()
        init_kwargs = {"p": 1, "drop_height": (1,2), "drop_width": (1,2), "number_of_drops": (1,1)}
        call_args = (torch.rand(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomRain,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:615: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.random_rain.RandomRain'>, target = 'tensorflow', init_args = ()
init_kwargs = {'drop_height': (1, 2), 'drop_width': (1, 2), 'number_of_drops': (1, 1), 'p': 1}
call_args = (tensor([[[[9.8213e-01, 9.6659e-01, 8.1722e-02, 7.2681e-01, 5.8429e-01],
          [6.3636e-01, 8.0222e-01, 3.1118e-01...e-02, 4.2987e-02, 2.8242e-01, 5.3733e-01],
          [2.1972e-01, 7.0125e-04, 5.4072e-02, 3.4951e-01, 7.1179e-01]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.268119...],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55b3c0a9c050, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=...1],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.268119...],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=...1],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.268119...],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.2681195...01],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02,...1],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.2681195...01],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a524fc70>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f369fc41750>
tensor = <function tensorflow_tensor_frnt at 0x7f36a45a0d30>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.2681195...01],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.2681195...01],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.2681195...01],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a524fc70>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f36a524e7a0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f369ff612d0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f36a4a35990>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f36aa2d8700>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f369ff61630>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f369fd26710>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRain(number_of_drops=(1, 1), drop_height=(1, 2), drop_width=(1, 2), p=1, p_batch=1.0, same_on_batch=False)
image = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.2681195...01],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'coordinates_factor': <tf.Ten...(1,), dtype=int64, numpy=array([1])>, 'drop_width_factor': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, image, params, flags, transform=None):
        from ....core.check import tensorflow_KORNIA_CHECK
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.reduction_ops import (
            tensorflow_all_frnt,
        )
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_abs_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_linspace_frnt,
        )
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        tensorflow_KORNIA_CHECK(
            tensorflow_shape_frnt_(image)[1] in {3, 1},
            "Number of color channels should be 1 or 3.",
        )
        tensorflow_KORNIA_CHECK(
            bool(
                tensorflow_all_frnt(
                    params["drop_height_factor"] <= tensorflow_shape_frnt_(image)[2]
                )
                and tensorflow_all_frnt(params["drop_height_factor"] > 0)
            ),
            "Height of drop should be greater than zero and less than image height.",
        )
        tensorflow_KORNIA_CHECK(
            bool(
                tensorflow_all_frnt(
                    tensorflow_abs_frnt(params["drop_width_factor"])
                    <= tensorflow_shape_frnt_(image)[3]
                )
            ),
            "Width of drop should be less than image width.",
        )
        modeified_img = tensorflow_clone_frnt_(image)
        for i in range(tensorflow_shape_frnt_(image)[0]):
            number_of_drops: typing.Any = int(
                tensorflow_get_item(params["number_of_drops_factor"], i)
            )
            coordinates_of_drops: typing.Any = tensorflow_get_item(
                tensorflow_get_item(params["coordinates_factor"], i),
                slice(None, number_of_drops, None),
            )
            height_of_drop: typing.Any = int(
                tensorflow_get_item(params["drop_height_factor"], i)
            )
            width_of_drop: typing.Any = int(
                tensorflow_get_item(params["drop_width_factor"], i)
            )
            random_y_coords = coordinates_of_drops[:, 0] * (
                tensorflow_shape_frnt_(image)[2] - height_of_drop - 1
            )
            if width_of_drop > 0:
                random_x_coords = coordinates_of_drops[:, 1] * (
                    tensorflow_shape_frnt_(image)[3] - width_of_drop - 1
                )
            else:
                random_x_coords = (
                    coordinates_of_drops[:, 1]
                    * (tensorflow_shape_frnt_(image)[3] + width_of_drop - 1)
                    - width_of_drop
                )
            coords = tensorflow_to_frnt_(
                tensorflow_cat_frnt(
                    [random_y_coords[None], random_x_coords[None]], dim=0
                ),
                image.device,
                dtype=tf.int64,
            )
            size_of_line: typing.Any = max(height_of_drop, abs(width_of_drop))
            x = tensorflow_to_frnt_(
                tensorflow_linspace_frnt(
                    start=0, end=height_of_drop, steps=size_of_line, dtype=tf.int64
                ),
                image.device,
            )
            y = tensorflow_to_frnt_(
                tensorflow_linspace_frnt(
                    start=0, end=width_of_drop, steps=size_of_line, dtype=tf.int64
                ),
                image.device,
            )
            for k in range(tensorflow_shape_frnt_(x)[0]):
>               modeified_img = tensorflow_set_item_bknd(
                    modeified_img,
                    (i, slice(None, None, None), coords[0] + x[k], coords[1] + y[k]),
                    200 / 255,
                )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/random_rain.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inp = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.2681195...01],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>
query = (0, slice(None, None, None), <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>), val = 0.7843137254901961, kwargs = {}

    @functools.wraps(fn)
    def wrapper(inp, query, val, **kwargs):
        try:
            inp.__setitem__(query, val)
            res = inp
        except IndexError:
            raise
        except Exception:
>           res = fn(inp, query, val, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:223: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.268119...hape=(1,), dtype=int64, numpy=array([1])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>), 0.7843137254901961)
kwargs = {}, handle_mixed_in_backend = True

    @functools.wraps(fn)
    def _handle_partial_mixed_function(*args, **kwargs):
        handle_mixed_in_backend = False
        if not hasattr(fn, "partial_mixed_handler"):
            handle_mixed_in_backend = True
        else:
            compos = getattr(fn, "compos")
            condition = getattr(fn, "partial_mixed_handler")
        if handle_mixed_in_backend or condition(*args, **kwargs):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:170: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[9.8213166e-01, 9.6658850e-01, 8.1721723e-02, 7.2681195...01],
         [2.1971768e-01, 7.0124865e-04, 5.4071844e-02, 3.4951216e-01,
          7.1178770e-01]]]], dtype=float32)>
query = (0, slice(None, None, None), <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>)
val = <tf.Tensor: shape=(), dtype=float32, numpy=0.78431374>

    @tensorflow_handle_set_item
    @tensorflow_handle_partial_mixed_function
    def tensorflow_set_item_bknd(
        x: Union[tensorflow.Tensor, tf.Tensor],
        query: Union[tensorflow.Tensor, tf.Tensor, Tuple],
        val: Union[tensorflow.Tensor, tf.Tensor],
        /,
        *,
        copy: Optional[bool] = False,
    ):
        from ..backends.tensorflow.creation import tensorflow_asarray
        from ..backends.tensorflow.creation import tensorflow_copy_array
        from .data_type import tensorflow_is_bool_dtype_bknd
        from ..backends.tensorflow.manipulation import tensorflow_tile
        from ..backends.tensorflow.searching import tensorflow_nonzero
        from ..backends.tensorflow.general import tensorflow_shape
        from ...data_classes.array.data_type import tensorflow_astype_bknd_
        from ..backends.tensorflow.general import tensorflow_scatter_nd
    
        if copy:
            x = tensorflow_copy_array(x)
        if not tensorflow_is_array_bknd(val):
            val = tensorflow_asarray(val)
        if 0 in x.shape or 0 in val.shape:
            return x
        if tensorflow_is_array_bknd(query) and tensorflow_is_bool_dtype_bknd(query):
            if not len(query.shape):
                query = tensorflow_tile(query, (x.shape[0],))
            indices = tensorflow_nonzero(query, as_tuple=False)
        else:
            indices, target_shape, _ = tensorflow__parse_query_bknd(
                query, tensorflow_shape(x, as_array=True), scatter=True
            )
            if indices is None:
                return x
        val = tensorflow_astype_bknd_(val, x.dtype)
>       ret = tensorflow_scatter_nd(indices, val, reduction="replace", out=x)

Translated_Outputs/tensorflow_outputs/ivy/functional/ivy/general.py:167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

indices = <tf.Tensor: shape=(1, 1, 4), dtype=int64, numpy=array([[[0, 1, 0, 0]]])>, updates = <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.78431374]], dtype=float32)>, shape = None

    def tensorflow_scatter_nd(
        indices: Union[tensorflow.Tensor, tensorflow.Variable],
        updates: Union[tensorflow.Tensor, tensorflow.Variable],
        /,
        shape: Optional[Union[tf.TensorShape, Sequence[int]]] = None,
        *,
        reduction: str = "sum",
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.general import tensorflow_exists_bknd
        from ...ivy.data_type import tensorflow_promote_types_bknd
        from .data_type import tensorflow_as_native_dtype
        from ...ivy.general import tensorflow__broadcast_to_bknd
        from ....utils.assertions import tensorflow_check_equal
        from .elementwise import tensorflow_multiply
    
        updates_dtype = updates.dtype
        if tensorflow_exists_bknd(out):
            dtype = tensorflow_promote_types_bknd(out.dtype, updates_dtype)
        updates = tensorflow.cast(
            updates,
            tensorflow_as_native_dtype(dtype)
            if tensorflow_exists_bknd(out)
            else updates_dtype,
        )
        expected_shape = (
            list(tensorflow.shape(indices)[:-1])
            + list(out.shape[tensorflow.shape(indices)[-1] :])
            if tensorflow_exists_bknd(out)
            else list(tensorflow.shape(indices)[:-1])
            + list(shape[tensorflow.shape(indices)[-1] :])
        )
        updates = tensorflow__broadcast_to_bknd(updates, expected_shape)
        if len(updates.shape) == 0:
            indices = tensorflow.expand_dims(indices, 0)
            updates = tensorflow.expand_dims(updates, 0)
        target = out
        target_given = tensorflow_exists_bknd(target)
        if tensorflow_exists_bknd(shape) and target_given:
            tensorflow_check_equal(tuple(target.shape), tuple(shape), as_array=False)
        if not target_given:
            shape = list(shape) if tensorflow_exists_bknd(shape) else list(out.shape)
            target = tensorflow.zeros(shape, dtype=updates.dtype)
        if reduction == "sum":
            res = tensorflow.tensor_scatter_nd_add(target, indices, updates)
        elif reduction == "min":
            res = tensorflow.tensor_scatter_nd_min(target, indices, updates)
        elif reduction == "max":
            res = tensorflow.tensor_scatter_nd_max(target, indices, updates)
        elif reduction == "mul":
            updates = tensorflow_multiply(tensorflow_gather_nd(target, indices), updates)
            res = tensorflow.tensor_scatter_nd_update(target, indices, updates)
        elif reduction == "replace":
>           res = tensorflow.tensor_scatter_nd_update(target, indices, updates)
E           tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomRain.call().
E           
E           [1m{{function_node __wrapped__TensorScatterUpdate_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0,0] = [0, 1, 0, 0] does not index into shape [1,1,5,5] [Op:TensorScatterUpdate] name: [0m
E           
E           Arguments received by tensorflow_RandomRain.call():
E             â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E             â€¢ params=None
E             â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/general.py:328: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomRain
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-12 00:31:18.223121: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at scatter_nd_op.cc:218 : INVALID_ARGUMENT: indices[0,0] = [0, 1, 0, 0] does not index into shape [1,1,5,5]
_________________________________________________________________________ test_RandomSaltAndPepperNoise[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSaltAndPepperNoise(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSaltAndPepperNoise")
    
        init_args = ()
        init_kwargs = {"amount": 0.5, "salt_vs_pepper": 0.5, "p": 1.}
        call_args = (torch.rand(1, 3, 3, 3),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSaltAndPepperNoise,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:655: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.salt_pepper_noise.RandomSaltAndPepperNoise'>, target = 'tensorflow', init_args = ()
init_kwargs = {'amount': 0.5, 'p': 1.0, 'salt_vs_pepper': 0.5}
call_args = (tensor([[[[0.1537, 0.3767, 0.6860],
          [0.0047, 0.0438, 0.5942],
          [0.9775, 0.8317, 0.0787]],

       ...20]],

         [[0.8431, 0.1600, 0.7165],
          [0.3953, 0.2195, 0.9532],
          [0.5800, 0.5017, 0.9492]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.1537041 , 0.3766805 , 0.68600285],
         [0.0046...  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f369fa1ae40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=...3  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.1537041 , 0.3766805 , 0.68600285],
         [0.0046...  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=...3  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.1537041 , 0.3766805 , 0.68600285],
         [0.0046...  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.1537041 , 0.3766805 , 0.68600285],
         [0.00466...83  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.1537041 , 0.3766805 , 0.68600285],
       ...3  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.1537041 , 0.3766805 , 0.68600285],
         [0.00466...83  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...se, False]],

        [[False, False,  True],
         [ True,  True,  True],
         [False, False, False]]]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369fbfa3b0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f369fabf910>
tensor = <function tensorflow_tensor_frnt at 0x7f369fbc4040>
in_tensor = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.1537041 , 0.3766805 , 0.68600285],
         [0.00466...83  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 3, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 3, 3, 3]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.1537041 , 0.3766805 , 0.68600285],
         [0.00466...83  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...se, False]],

        [[False, False,  True],
         [ True,  True,  True],
         [False, False, False]]]])>, ...}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.1537041 , 0.3766805 , 0.68600285],
         [0.00466...83  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...se, False]],

        [[False, False,  True],
         [ True,  True,  True],
         [False, False, False]]]])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369fbfa3b0>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f369fbf9480>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f36a4a42c20>, tensorflow_get_item = <function tensorflow_get_item at 0x7f36a495b400>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f369c836170>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f36a4a41630>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f36a4a40d30>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSaltAndPepperNoise(amount=(0.5, 0.5), salt_and_pepper=(0.5, 0.5), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[0.1537041 , 0.3766805 , 0.68600285],
         [0.00466...83  ],
         [0.3953197 , 0.2195074 , 0.9531867 ],
         [0.5799851 , 0.5017364 , 0.94917583]]]], dtype=float32)>
params = {'amount_factor': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5], dtype=float32)>, 'batch_prob': <tf.Tensor:...se, False]],

        [[False, False,  True],
         [ True,  True,  True],
         [False, False, False]]]])>, ...}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....core.check import tensorflow_KORNIA_CHECK
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        tensorflow_KORNIA_CHECK(
            len(tensorflow_shape_frnt_(input)) in (3, 4), "Wrong input dimension."
        )
        if len(tensorflow_shape_frnt_(input)) == 3:
            input = input[None, :, :, :]
        tensorflow_KORNIA_CHECK(
            tensorflow_shape_frnt_(input)[1] in {3, 1},
            "Number of color channels should be 1 or 3.",
        )
        noisy_image = tensorflow_clone_frnt_(input)
        noisy_image = tensorflow_set_item_bknd(
>           noisy_image, params["mask_salt"].to(input.device), 1.0
        )
E       AttributeError: Exception encountered when calling tensorflow_RandomSaltAndPepperNoise.call().
E       
E       [1m'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'to'[0m
E       
E       Arguments received by tensorflow_RandomSaltAndPepperNoise.call():
E         â€¢ input=tf.Tensor(shape=(1, 3, 3, 3), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/salt_pepper_noise.py:96: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSaltAndPepperNoise
______________________________________________________________________________ test_RandomSharpness[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSharpness(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSharpness")
    
        init_args = (1.,)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSharpness,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:695: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.sharpness.RandomSharpness'>, target = 'tensorflow', init_args = (1.0,), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[0.8740, 0.5085, 0.6690, 0.4805, 0.9138],
          [0.1408, 0.9543, 0.5702, 0.1438, 0.0085],
          [0...., 0.6262],
          [0.2334, 0.7713, 0.3444, 0.6318, 0.0173],
          [0.3566, 0.5218, 0.4817, 0.7546, 0.2942]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.8740196 , 0.5085074 , 0.6689739 , 0.48045462, 0.913...317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f36a4c9ec40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(1, 1, 5, 5), d...6317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.8740196 , 0.5085074 , 0.6689739 , 0.48045462, 0.913...317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(1, 1, 5, 5), d...6317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.8740196 , 0.5085074 , 0.6689739 , 0.48045462, 0.913...317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.8740196 , 0.5085074 , 0.6689739 , 0.48045462, 0.9138....6317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.8740196 , 0.5085074 , 0.6689739 , 0.480454...6317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.8740196 , 0.5085074 , 0.6689739 , 0.48045462, 0.9138....6317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...mpy=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.8037267], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369c8e3880>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36aa2a6a70>
tensor = <function tensorflow_tensor_frnt at 0x7f369c845ab0>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.8740196 , 0.5085074 , 0.6689739 , 0.48045462, 0.9138....6317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.8740196 , 0.5085074 , 0.6689739 , 0.48045462, 0.9138....6317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...mpy=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.8037267], dtype=float32)>}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.8740196 , 0.5085074 , 0.6689739 , 0.48045462, 0.9138....6317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...mpy=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.8037267], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369c8e3880>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f369c8e30a0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f369c838310>, tensorflow_get_item = <function tensorflow_get_item at 0x7f36aa097f40>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f36aa0769e0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f369c83b490>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f369c83b130>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSharpness(sharpness=1.0, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.8740196 , 0.5085074 , 0.6689739 , 0.48045462, 0.9138....6317554 , 0.01729649],
         [0.35664082, 0.5217598 , 0.4816985 , 0.7546374 , 0.29420978]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...mpy=array([1, 1, 5, 5])>, 'sharpness': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.8037267], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        factor = params["sharpness"]
>       return sharpness(input, factor)
E       NameError: Exception encountered when calling tensorflow_RandomSharpness.call().
E       
E       [1mname 'sharpness' is not defined[0m
E       
E       Arguments received by tensorflow_RandomSharpness.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/sharpness.py:43: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSharpness
________________________________________________________________________________ test_RandomSnow[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSnow(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSnow")
    
        init_args = ()
        init_kwargs = {"p": 1.0, "snow_coefficient": (0.1, 0.6), "brightness": (1.0, 5.0)}
        call_args = (torch.rand(2, 3, 4, 4),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSnow,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:715: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.random_snow.RandomSnow'>, target = 'tensorflow', init_args = ()
init_kwargs = {'brightness': (1.0, 5.0), 'p': 1.0, 'snow_coefficient': (0.1, 0.6)}
call_args = (tensor([[[[0.1862, 0.6981, 0.6858, 0.6862],
          [0.3267, 0.0663, 0.7517, 0.6894],
          [0.2646, 0.7974, 0...., 0.4329, 0.3886, 0.2879],
          [0.5002, 0.3275, 0.1364, 0.6119],
          [0.6829, 0.2091, 0.6903, 0.0876]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.18618059, 0.69807386, 0.6857564 , 0.686231  ],
    ...2745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f36aa257640, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), <...32745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.18618059, 0.69807386, 0.6857564 , 0.686231  ],
    ...2745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), <...32745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.18618059, 0.69807386, 0.6857564 , 0.686231  ],
    ...2745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.18618059, 0.69807386, 0.6857564 , 0.686231  ],
     ....32745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.18618059, 0.69807386, 0.6857564 , 0.686231...32745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.18618059, 0.69807386, 0.6857564 , 0.686231  ],
     ....32745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:..., 4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.3274761, 0.3067226], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a4a37ac0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36aa119630>
tensor = <function tensorflow_tensor_frnt at 0x7f36aa079990>
in_tensor = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.18618059, 0.69807386, 0.6857564 , 0.686231  ],
     ....32745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([2, 3, 4, 4]), batch_shape = ivy.frontends.torch.Size([2, 3, 4, 4]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.18618059, 0.69807386, 0.6857564 , 0.686231  ],
     ....32745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:..., 4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.3274761, 0.3067226], dtype=float32)>}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.18618059, 0.69807386, 0.6857564 , 0.686231  ],
     ....32745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:..., 4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.3274761, 0.3067226], dtype=float32)>}
flags = {}
transform = <tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],

       [[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36a4a37ac0>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f36a4a370a0>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f369c8e08b0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f36aa075c60>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f36aa2d8310>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f369c8e3490>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f369c8e3400>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSnow(snow_coefficient=(0.1, 0.6), brightness=(1.0, 5.0), p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.18618059, 0.69807386, 0.6857564 , 0.686231  ],
     ....32745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>, 'brightness': <tf.Tensor:..., 4])>, 'snow_coefficient': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.3274761, 0.3067226], dtype=float32)>}
flags = {}
transform = <tf.Tensor: shape=(2, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]],

       [[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....core.check import tensorflow_KORNIA_CHECK
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....color.hls import tensorflow_rgb_to_hls
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_zeros_like_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_where_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clamp_frnt_
        from ....color.hls import tensorflow_hls_to_rgb
    
        tensorflow_KORNIA_CHECK(
            tensorflow_shape_frnt_(input)[1] == 3,
            "Number of color channels should be 3.",
        )
        tensorflow_KORNIA_CHECK(
            len(tensorflow_shape_frnt_(input)) in (3, 4), "Wrong input dimension."
        )
        if len(tensorflow_shape_frnt_(input)) == 3:
            input = input[None, :, :, :]
>       input_HLS = tensorflow_rgb_to_hls(input)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/random_snow.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

image = <tf.Tensor: shape=(2, 3, 4, 4), dtype=float32, numpy=
array([[[[0.18618059, 0.69807386, 0.6857564 , 0.686231  ],
     ....32745212, 0.13644147, 0.6119479 ],
         [0.6828771 , 0.20908761, 0.6902576 , 0.08762246]]]],
      dtype=float32)>
eps = 1e-08

    def tensorflow_rgb_to_hls(image, eps=1e-08):
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.pointwise_ops import sub
        from ..core._backend import where
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_requires_grad_frnt_
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_empty_like_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_add_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_mul_frnt
    
        if not isinstance(image, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(image)}")
        if len(tensorflow_shape_frnt_(image)) < 3 or tensorflow_shape_frnt_(image)[-3] != 3:
            raise ValueError(
                f"Input size must have a shape of (*, 3, H, W). Got {tensorflow_shape_frnt_(image)}"
            )
        _RGB2HSL_IDX = tensor(
            [[[0.0]], [[1.0]], [[2.0]]], device=image.device, dtype=image.dtype
        )
        _img_max: typing.Any = tensorflow_max_frnt_(image, -3)
        maxc = _img_max[0]
        imax = _img_max[1]
        minc: typing.Any = tensorflow_min_frnt_(image, -3)[0]
        if tensorflow_requires_grad_frnt_(image):
            l_ = maxc + minc
            s = maxc - minc
            h = l_
            image_hls = l_
        else:
            image_hls = tensorflow_empty_like_frnt(image)
            h, l_, s = (
                image_hls[..., 0, :, :],
                image_hls[..., 1, :, :],
                image_hls[..., 2, :, :],
            )
            tensorflow_add_frnt(maxc, minc, out=l_)
            sub(maxc, minc, out=s)
        im = image / tensorflow_unsqueeze_frnt_(s + eps, -3)
        s = s / (where(l_ < 1.0, l_, 2.0 - l_) + eps)
        l_ = l_ / 2
        r, g, b = im[..., 0, :, :], im[..., 1, :, :], im[..., 2, :, :]
        cond = imax[..., None, :, :] == _RGB2HSL_IDX
        if tensorflow_requires_grad_frnt_(image):
            h = (g - b) % 6 * cond[..., 0, :, :]
        else:
>           tensorflow_mul_frnt((g - b) % 6, cond[..., 0, :, :], out=h)

Translated_Outputs/tensorflow_outputs/kornia/color/hls.py:83: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  0.],
        [ 2., -0.,  2.,  4.],
      ...,
        [False, False, False,  True],
        [ True, False,  True, False],
        [ True, False, False,  True]]])>)
kwargs = {'out': <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[0., 0., 0., 0.],
        [0., 0., 0., 0.],
       ...    [[0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.],
        [0., 0., 0., 0.]]], dtype=float32)>}
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f36aa11a320>
array_like = <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  0.],
        [ 2., -0.,  2.,  4.],
       ...  5.,  4.],
        [ 4.,  2.,  0.,  0.],
        [-0.,  2.,  2.,  0.],
        [ 4.,  4.,  4.,  2.]]], dtype=float32)>

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
>           return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  0.],
        [ 2., -0.,  2.,  4.],
       ...  5.,  4.],
        [ 4.,  2.,  0.,  0.],
        [-0.,  2.,  2.,  0.],
        [ 4.,  4.,  4.,  2.]]], dtype=float32)>
other = <tf.Tensor: shape=(2, 4, 4), dtype=bool, numpy=
array([[[False,  True,  True, False],
        [False, False, False, Fa...],
        [False, False, False,  True],
        [ True, False,  True, False],
        [ True, False, False,  True]]])>

    @tensorflow_handle_methods
    def tensorflow_mul_frnt(input, other, *, out=None):
        from ...backends.tensorflow.elementwise import tensorflow_multiply
    
>       return tensorflow_multiply(input, other, out=out)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/pointwise_ops.py:142: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x1 = <tf.Tensor: shape=(2, 4, 4), dtype=float32, numpy=
array([[[-0.,  0.,  2.,  0.],
        [ 2., -0.,  2.,  4.],
       ...  5.,  4.],
        [ 4.,  2.,  0.,  0.],
        [-0.,  2.,  2.,  0.],
        [ 4.,  4.,  4.,  2.]]], dtype=float32)>
x2 = <tf.Tensor: shape=(2, 4, 4), dtype=bool, numpy=
array([[[False,  True,  True, False],
        [False, False, False, Fa...],
        [False, False, False,  True],
        [ True, False,  True, False],
        [ True, False, False,  True]]])>

    def tensorflow_multiply(
        x1: Union[float, tensorflow.Tensor, tensorflow.Variable],
        x2: Union[float, tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .creation import tensorflow_asarray
        from ...ivy.data_type import tensorflow_default_dtype_bknd
        from ...ivy.general import tensorflow_is_array_bknd
    
        oirg_x1 = x1
        oirg_x2 = x2
        try:
            dtype = (
                x1.dtype
                if hasattr(x1, "dtype")
                else x2.dtype
                if hasattr(x2, "dtype")
                else tensorflow_default_dtype_bknd()
            )
            if not tensorflow_is_array_bknd(x1):
                x1 = tensorflow_asarray(x1, dtype=dtype)
            if not tensorflow_is_array_bknd(x2):
                x2 = tensorflow_asarray(x2, dtype=dtype)
        except:
            x1 = oirg_x1
            x2 = oirg_x2
>       return tensorflow.math.multiply(x1, x2)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomSnow.call().
E       
E       [1mcannot compute Mul as input #1(zero-based) was expected to be a float tensor but is a bool tensor [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_RandomSnow.call():
E         â€¢ input=tf.Tensor(shape=(2, 3, 4, 4), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/elementwise.py:540: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSnow
______________________________________________________________________________ test_RandomSolarize[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomSolarize(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomSolarize")
    
        init_args = (0.1, 0.1)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomSolarize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:735: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.intensity.solarize.RandomSolarize'>, target = 'tensorflow', init_args = (0.1, 0.1), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[0.0938, 0.1747, 0.6100, 0.4569, 0.6845],
          [0.1542, 0.8700, 0.5988, 0.3418, 0.0692],
          [0...., 0.3665],
          [0.1348, 0.8476, 0.9687, 0.7129, 0.5390],
          [0.6339, 0.6333, 0.8830, 0.0887, 0.6061]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.09381306, 0.17467183, 0.61000544, 0.45688856, 0.684...1293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f36aa0f3c40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=...71293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.09381306, 0.17467183, 0.61000544, 0.45688856, 0.684...1293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=...71293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.09381306, 0.17467183, 0.61000544, 0.45688856, 0.684...1293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.09381306, 0.17467183, 0.61000544, 0.45688856, 0.6844....71293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.09381306, 0.17467183, 0.61000544, 0.456888...71293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.09381306, 0.17467183, 0.61000544, 0.45688856, 0.6844....71293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.08256949], dtype=float32)>, 'batch_prob': <tf.Tens...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5963477], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369fbc6f80>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f369c8db910>
tensor = <function tensorflow_tensor_frnt at 0x7f36a426b6d0>
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.09381306, 0.17467183, 0.61000544, 0.45688856, 0.6844....71293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 5, 5]), flags = {}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
in_tensor = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.09381306, 0.17467183, 0.61000544, 0.45688856, 0.6844....71293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.08256949], dtype=float32)>, 'batch_prob': <tf.Tens...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5963477], dtype=float32)>}
flags = {}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.09381306, 0.17467183, 0.61000544, 0.45688856, 0.6844....71293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.08256949], dtype=float32)>, 'batch_prob': <tf.Tens...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5963477], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369fbc6f80>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f369fa91f30>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f369fd33c70>, tensorflow_get_item = <function tensorflow_get_item at 0x7f369c963910>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f369c920700>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f369fd335b0>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f369fd33640>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomSolarize(thresholds=0.1, additions=0.1, p=1.0, p_batch=1.0, same_on_batch=False)
input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.09381306, 0.17467183, 0.61000544, 0.45688856, 0.6844....71293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>
params = {'additions': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.08256949], dtype=float32)>, 'batch_prob': <tf.Tens...py=array([1, 1, 5, 5])>, 'thresholds': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5963477], dtype=float32)>}
flags = {}, transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....enhance.adjust import tensorflow_solarize
    
        thresholds = params["thresholds"]
        additions: typing.Any
        if "additions" in params:
            additions = params["additions"]
        else:
            additions = None
>       return tensorflow_solarize(input, thresholds, additions)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/intensity/solarize.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0.09381306, 0.17467183, 0.61000544, 0.45688856, 0.6844....71293473, 0.5390428 ],
         [0.63394284, 0.6332798 , 0.8830063 , 0.08874154, 0.60614175]]]],
      dtype=float32)>
thresholds = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5963477], dtype=float32)>, additions = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.08256949], dtype=float32)>

    def tensorflow_solarize(input, thresholds=0.5, additions=None):
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_as_tensor_frnt
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_all_frnt
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clamp_frnt_
    
        if not isinstance(input, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(input)}")
        if not isinstance(thresholds, (float, tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(
                f"The factor should be either a float or Tensor. Got {type(thresholds)}"
            )
        if isinstance(thresholds, (float,)):
            thresholds = tensorflow_as_tensor_frnt(thresholds)
        if additions is not None:
            if not isinstance(additions, (float, tensorflow.Tensor, tensorflow.Variable)):
                raise TypeError(
                    f"The factor should be either a float or Tensor. Got {type(additions)}"
                )
            if isinstance(additions, (float,)):
                additions = tensorflow_as_tensor_frnt(additions)
>           if not tensorflow_all_frnt((additions < 0.5) * (additions > -0.5)):

Translated_Outputs/tensorflow_outputs/kornia/enhance/adjust.py:379: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>, <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>), kwargs = {}
arg = <tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_RandomSolarize.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_RandomSolarize.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 5, 5), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomSolarize
________________________________________________________________________________ test_CenterCrop[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_CenterCrop(target_framework, mode, backend_compile):
        print("kornia.augmentation.CenterCrop")
    
        init_args = (2,)
        init_kwargs = {"p": 1., "cropping_mode": "resample"}
        call_args = (torch.randn(1, 1, 4, 4),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.CenterCrop,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:755: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.center_crop.CenterCrop'>, target = 'tensorflow', init_args = (2,), init_kwargs = {'cropping_mode': 'resample', 'p': 1.0}
call_args = (tensor([[[[-1.5788, -0.4976,  0.9497,  0.5228],
          [-0.3259, -1.8520,  0.3637, -0.1619],
          [-0.5207,  1.5354, -1.3318,  0.5852],
          [-0.8286,  0.2931,  1.2018, -0.1354]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop(p=1.0, p_batch=1.0, same_on_batch=True, resample=bilinear, cropping_mode=resample, align_corners=True, size=(2, 2), padding_mode=zeros)
args = (<tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.5228454 ],
... , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f369c924e40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_CenterCrop(p=1.0, p_batch=1.0, same_on_batch=True, resample=bilinear, cropping_mode=resample, align_corner...2 , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop(p=1.0, p_batch=1.0, same_on_batch=True, resample=bilinear, cropping_mode=resample, align_corners=True, size=(2, 2), padding_mode=zeros), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.5228454 ],
... , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_CenterCrop(p=1.0, p_batch=1.0, same_on_batch=True, resample=bilinear, cropping_mode=resample, align_corner...2 , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop(p=1.0, p_batch=1.0, same_on_batch=True, resample=bilinear, cropping_mode=resample, align_corners=True, size=(2, 2), padding_mode=zeros), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.5228454 ],
... , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.5228454 ],
 ...02 , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_CenterCrop(p=1.0, p_batch=1.0, same_on_batch=True, resample=bilinear, cropping_mode=resample, align_corners=True, size=(2, 2), padding_mode=zeros),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.52...2 , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop(p=1.0, p_batch=1.0, same_on_batch=True, resample=bilinear, cropping_mode=resample, align_corners=True, size=(2, 2), padding_mode=zeros)
input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.5228454 ],
 ...02 , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...pe=int64, numpy=array([1, 1, 4, 4])>, 'input_size': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[4, 4]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369ffd8040>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36aa053520>
tensor = <function tensorflow_tensor_frnt at 0x7f369c8f7b50>
in_tensor = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.5228454 ],
 ...02 , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 4, 4]), batch_shape = ivy.frontends.torch.Size([1, 1, 4, 4])
flags = {'align_corners': True, 'cropping_mode': 'resample', 'padding_mode': 'zeros', 'resample': <tensorflow_Resample.BILINEAR: 1>, ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop(p=1.0, p_batch=1.0, same_on_batch=True, resample=bilinear, cropping_mode=resample, align_corners=True, size=(2, 2), padding_mode=zeros)
in_tensor = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.5228454 ],
 ...02 , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...pe=int64, numpy=array([1, 1, 4, 4])>, 'input_size': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[4, 4]])>, ...}
flags = {'align_corners': True, 'cropping_mode': 'resample', 'padding_mode': 'zeros', 'resample': <tensorflow_Resample.BILINEAR: 1>, ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop(p=1.0, p_batch=1.0, same_on_batch=True, resample=bilinear, cropping_mode=resample, align_corners=True, size=(2, 2), padding_mode=zeros)
input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.5228454 ],
 ...02 , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...pe=int64, numpy=array([1, 1, 4, 4])>, 'input_size': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[4, 4]])>, ...}
flags = {'align_corners': True, 'cropping_mode': 'resample', 'padding_mode': 'zeros', 'resample': <tensorflow_Resample.BILINEAR: 1>, ...}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [...11151e-17,  1.0000000e+00, -1.0000000e+00],
        [-1.1102230e-16, -0.0000000e+00,  1.0000000e+00]]], dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369ffd8040>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f369ffdb400>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f36a49d2710>, tensorflow_get_item = <function tensorflow_get_item at 0x7f369c794670>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f369c7176d0>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f36a49d3370>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f36a49d2c20>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop(p=1.0, p_batch=1.0, same_on_batch=True, resample=bilinear, cropping_mode=resample, align_corners=True, size=(2, 2), padding_mode=zeros)
input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.5228454 ],
 ...02 , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...pe=int64, numpy=array([1, 1, 4, 4])>, 'input_size': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[4, 4]])>, ...}
flags = {'align_corners': True, 'cropping_mode': 'resample', 'padding_mode': 'zeros', 'resample': <tensorflow_Resample.BILINEAR: 1>, ...}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [...11151e-17,  1.0000000e+00, -1.0000000e+00],
        [-1.1102230e-16, -0.0000000e+00,  1.0000000e+00]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....geometry.transform.crop2d import tensorflow_crop_by_transform_mat
        from ....geometry.transform.crop2d import tensorflow_crop_by_indices
    
        if flags["cropping_mode"] == "resample":
            if not isinstance(transform, (tensorflow.Tensor, tensorflow.Variable)):
                raise TypeError(
                    f"Expected the `transform` be a Tensor. Got {type(transform)}."
                )
>           return tensorflow_crop_by_transform_mat(
                input,
                transform[:, :2, :],
                self.size,
                flags["resample"].name.lower(),
                "zeros",
                flags["align_corners"],
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/center_crop.py:93: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input_tensor = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.5228454 ],
 ...02 , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>
transform = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [-5.5511151e-17,  1.0000000e+00, -1.0000000e+00]]], dtype=float32)>
out_size = (2, 2), mode = 'bilinear', padding_mode = 'zeros', align_corners = True

    def tensorflow_crop_by_transform_mat(
        input_tensor,
        transform,
        out_size,
        mode="bilinear",
        padding_mode="zeros",
        align_corners=True,
    ):
        from ...core._backend import as_tensor
        from ....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .imgwarp import tensorflow_warp_affine
    
        dst_trans_src = as_tensor(
            tensorflow_expand_frnt_(
                transform, tensorflow_shape_frnt_(input_tensor)[0], -1, -1
            ),
            device=input_tensor.device,
            dtype=input_tensor.dtype,
        )
>       patches: typing.Any = tensorflow_warp_affine(
            input_tensor,
            dst_trans_src[:, :2, :],
            out_size,
            mode=mode,
            padding_mode=padding_mode,
            align_corners=align_corners,
        )

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/crop2d.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

src = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[-1.5788356 , -0.49763334,  0.94965005,  0.5228454 ],
 ...02 , -1.3318255 ,  0.58521444],
         [-0.8286493 ,  0.2930782 ,  1.2017977 , -0.13539675]]]],
      dtype=float32)>
M = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [-5.5511151e-17,  1.0000000e+00, -1.0000000e+00]]], dtype=float32)>
dsize = (2, 2), mode = 'bilinear', padding_mode = 'zeros', align_corners = True, fill_value = <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>

    def tensorflow_warp_affine(
        src,
        M,
        dsize,
        mode="bilinear",
        padding_mode="zeros",
        align_corners=True,
        fill_value=zeros(3),
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ..conversions import tensorflow_convert_affinematrix_to_homography
        from ..conversions import tensorflow_normalize_homography
        from ...utils.helpers import tensorflow__torch_inverse_cast
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_affine_grid_frnt,
        )
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_grid_sample_frnt,
        )
    
        if not isinstance(src, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input src type is not a Tensor. Got {type(src)}")
        if not isinstance(M, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input M type is not a Tensor. Got {type(M)}")
        if not len(tensorflow_shape_frnt_(src)) == 4:
            raise ValueError(
                f"Input src must be a BxCxHxW tensor. Got {tensorflow_shape_frnt_(src)}"
            )
        if not (
            len(tensorflow_shape_frnt_(M)) == 3 or tensorflow_shape_frnt_(M)[-2:] == (2, 3)
        ):
            raise ValueError(
                f"Input M must be a Bx2x3 tensor. Got {tensorflow_shape_frnt_(M)}"
            )
        B, C, H, W = tensorflow_size_frnt_(src)
>       M_3x3: typing.Any = tensorflow_convert_affinematrix_to_homography(M)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/imgwarp.py:68: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [-5.5511151e-17,  1.0000000e+00, -1.0000000e+00]]], dtype=float32)>

    def tensorflow_convert_affinematrix_to_homography(A):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        if not isinstance(A, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(A)}")
        if not (
            len(tensorflow_shape_frnt_(A)) == 3 and tensorflow_shape_frnt_(A)[-2:] == (2, 3)
        ):
            raise ValueError(
                f"Input matrix must be a Bx2x3 tensor. Got {tensorflow_shape_frnt_(A)}"
            )
>       return tensorflow__convert_affinematrix_to_homography_impl(A)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:86: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [-5.5511151e-17,  1.0000000e+00, -1.0000000e+00]]], dtype=float32)>

    def tensorflow__convert_affinematrix_to_homography_impl(A):
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import pad
    
>       H: typing.Any = pad(A, [0, 0, 0, 1], "constant", value=0.0)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:70: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [-5.5511151e-17,  1.0000000e+00, -1.0000000e+00]]], dtype=float32)>
pad = ((0, 0), (0, 1), (0, 0)), mode = 'constant', value = 0.0

    def tensorflow_pad_frnt(input, pad, mode="constant", value=0):
        from .....backends.tensorflow.general import tensorflow_get_item
        from .....ivy.general import tensorflow_set_item_bknd
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.experimental.manipulation import tensorflow_pad
    
        if any([(pad_value < 0) for pad_value in pad]):
            pad = list(pad)
            slices = []
            for n in reversed(range(len(pad) // 2)):
                i = n * 2
                j = i + 1
                start = None
                stop = None
                if tensorflow_get_item(pad, i) < 0:
                    start = -tensorflow_get_item(pad, i)
                    pad = tensorflow_set_item_bknd(pad, i, 0)
                if tensorflow_get_item(pad, j) < 0:
                    stop = tensorflow_get_item(pad, j)
                    pad = tensorflow_set_item_bknd(pad, j, 0)
                slices.append(slice(start, stop))
            ndim = len(tensorflow_shape_frnt_(input))
            while len(slices) < ndim:
                slices.insert(0, slice(None))
            input = tensorflow_get_item(input, tuple(slices))
        value = 0 if value is None else value
        mode_dict = {
            "constant": "constant",
            "reflect": "reflect",
            "replicate": "edge",
            "circular": "wrap",
        }
        if mode not in mode_dict:
            raise ValueError(f"Unsupported padding mode: {mode}")
        pad = tensorflow__handle_padding_shape_frnt(
            pad, len(tensorflow_shape_frnt_(input)), mode
        )
        order = 0, 2, 3, 1
>       pad = tuple(pad[i] for i in order)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f36a43c60b0>

>   pad = tuple(pad[i] for i in order)
E   IndexError: Exception encountered when calling tensorflow_CenterCrop.call().
E   
E   [1mtuple index out of range[0m
E   
E   Arguments received by tensorflow_CenterCrop.call():
E     â€¢ input=tf.Tensor(shape=(1, 1, 4, 4), dtype=float32)
E     â€¢ params=None
E     â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.CenterCrop
___________________________________________________________________________________ test_PadTo[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_PadTo(target_framework, mode, backend_compile):
        print("kornia.augmentation.PadTo")
    
        init_args = ((4, 5),)
        init_kwargs = {"pad_value": 1.}
        call_args = (torch.tensor([[[[0., 0., 0.],
                                     [0., 0., 0.],
                                     [0., 0., 0.]]]]),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.PadTo,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:779: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.pad.PadTo'>, target = 'tensorflow', init_args = ((4, 5),), init_kwargs = {'pad_value': 1.0}
call_args = (tensor([[[[0., 0., 0.],
          [0., 0., 0.],
          [0., 0., 0.]]]]),), call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
        transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)
    
        orig_np = _nest_array_to_numpy(torch_out)
        transpiled_np = _nest_array_to_numpy(transpiled_out)
    
>       _check_shape_allclose(orig_np, transpiled_np)

kornia/augmentation/test_augmentation.py:48: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[0., 0., 0., 1., 1.],
         [0., 0., 0., 1., 1.],
         [0., 0., 0., 1., 1.],
         [1., 1., 1., 1., 1.]]]], dtype=float32)
y = array([[[[0., 0., 0.],
         [0., 0., 0.],
         [0., 0., 0.],
         [1., 1., 1.],
         [1., 1., 1.]],

 ... 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.],
         [1., 1., 1.]]]], dtype=float32)
tolerance = 0.001

    def _check_shape_allclose(x, y, tolerance=1e-3):
        """
        Checks that all array shapes are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x.shape, y.shape, atol=tolerance), "numpy array shapes are not all close"
E           AssertionError: numpy array shapes are not all close

helpers.py:52: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.PadTo
_______________________________________________________________________________ test_RandomAffine[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomAffine(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomAffine")
    
        init_args = ((-15., 20.),)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 3, 3),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomAffine,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:799: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.affine.RandomAffine'>, target = 'tensorflow', init_args = ((-15.0, 20.0),), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[0.1838, 0.4345, 0.9484],
          [0.8292, 0.2569, 0.1728],
          [0.9010, 0.1254, 0.1395]]]]),), call_kwargs = {}, deterministic_output = False, backend_compile = False
tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine(degrees=(-15.0, 20.0), translate=None, scale=None, shear=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.18379694, 0.4344933 , 0.94841594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f369c822c40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomAffine(degrees=(-15.0, 20.0), translate=None, scale=None, shear=None, p=1.0, p_batch=1.0, same_on_ba...594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine(degrees=(-15.0, 20.0), translate=None, scale=None, shear=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.18379694, 0.4344933 , 0.94841594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomAffine(degrees=(-15.0, 20.0), translate=None, scale=None, shear=None, p=1.0, p_batch=1.0, same_on_ba...594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine(degrees=(-15.0, 20.0), translate=None, scale=None, shear=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.18379694, 0.4344933 , 0.94841594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.18379694, 0.4344933 , 0.94841594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomAffine(degrees=(-15.0, 20.0), translate=None, scale=None, shear=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.18379694, 0.4344933 , 0.94841594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine(degrees=(-15.0, 20.0), translate=None, scale=None, shear=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.18379694, 0.4344933 , 0.94841594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>
params = {'angle': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([8.600502], dtype=float32)>, 'batch_prob': <tf.Tensor: sh...1., 1.]], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 3, 3])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369c92c430>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f369fad9990>
tensor = <function tensorflow_tensor_frnt at 0x7f369c447a30>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.18379694, 0.4344933 , 0.94841594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3])
flags = {'align_corners': False, 'padding_mode': <tensorflow_SamplePadding.ZEROS: 0>, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine(degrees=(-15.0, 20.0), translate=None, scale=None, shear=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.18379694, 0.4344933 , 0.94841594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>
params = {'angle': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([8.600502], dtype=float32)>, 'batch_prob': <tf.Tensor: sh...1., 1.]], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 3, 3])>, ...}
flags = {'align_corners': False, 'padding_mode': <tensorflow_SamplePadding.ZEROS: 0>, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine(degrees=(-15.0, 20.0), translate=None, scale=None, shear=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.18379694, 0.4344933 , 0.94841594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>
params = {'angle': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([8.600502], dtype=float32)>, 'batch_prob': <tf.Tensor: sh...1., 1.]], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 3, 3])>, ...}
flags = {'align_corners': False, 'padding_mode': <tensorflow_SamplePadding.ZEROS: 0>, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine(degrees=(-15.0, 20.0), translate=None, scale=None, shear=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.18379694, 0.4344933 , 0.94841594],
         [0.8292114 , 0.2569164 , 0.1728118 ],
         [0.9010483 , 0.12538487, 0.13949871]]]], dtype=float32)>
params = {'angle': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([8.600502], dtype=float32)>, 'batch_prob': <tf.Tensor: sh...1., 1.]], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 3, 3])>, ...}
flags = {'align_corners': False, 'padding_mode': <tensorflow_SamplePadding.ZEROS: 0>, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def compute_transformation(self, input, params, flags):
        from ....core._backend import as_tensor
        from ....geometry.transform.imgwarp import tensorflow_get_affine_matrix2d
        from ....geometry.conversions import tensorflow_deg2rad
    
>       return tensorflow_get_affine_matrix2d(
            as_tensor(params["translations"], device=input.device, dtype=input.dtype),
            as_tensor(params["center"], device=input.device, dtype=input.dtype),
            as_tensor(params["scale"], device=input.device, dtype=input.dtype),
            as_tensor(params["angle"], device=input.device, dtype=input.dtype),
            tensorflow_deg2rad(
                as_tensor(params["shear_x"], device=input.device, dtype=input.dtype)
            ),
            tensorflow_deg2rad(
                as_tensor(params["shear_y"], device=input.device, dtype=input.dtype)
            ),
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/affine.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

translations = <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0., 0.]], dtype=float32)>, center = <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 1.]], dtype=float32)>
scale = <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 1.]], dtype=float32)>, angle = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([8.600502], dtype=float32)>
sx = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, sy = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>

    def tensorflow_get_affine_matrix2d(
        translations, center, scale, angle, sx=None, sy=None
    ):
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..conversions import tensorflow_convert_affinematrix_to_homography
    
        transform: typing.Any = tensorflow_get_rotation_matrix2d(center, -angle, scale)
        transform = tensorflow_set_item_bknd(
            transform, (..., 2), transform[..., 2] + translations
        )
>       transform_h = tensorflow_convert_affinematrix_to_homography(transform)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/imgwarp.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 0.98875505, -0.149544  ,  0.16078895],
        [ 0.149544  ,  0.98875505, -0.13829899]]], dtype=float32)>

    def tensorflow_convert_affinematrix_to_homography(A):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        if not isinstance(A, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(A)}")
        if not (
            len(tensorflow_shape_frnt_(A)) == 3 and tensorflow_shape_frnt_(A)[-2:] == (2, 3)
        ):
            raise ValueError(
                f"Input matrix must be a Bx2x3 tensor. Got {tensorflow_shape_frnt_(A)}"
            )
>       return tensorflow__convert_affinematrix_to_homography_impl(A)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 0.98875505, -0.149544  ,  0.16078895],
        [ 0.149544  ,  0.98875505, -0.13829899]]], dtype=float32)>

    def tensorflow__convert_affinematrix_to_homography_impl(A):
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import pad
    
>       H: typing.Any = pad(A, [0, 0, 0, 1], "constant", value=0.0)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 0.98875505, -0.149544  ,  0.16078895],
        [ 0.149544  ,  0.98875505, -0.13829899]]], dtype=float32)>
pad = ((0, 0), (0, 1), (0, 0)), mode = 'constant', value = 0.0

    def tensorflow_pad_frnt(input, pad, mode="constant", value=0):
        from .....backends.tensorflow.general import tensorflow_get_item
        from .....ivy.general import tensorflow_set_item_bknd
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.experimental.manipulation import tensorflow_pad
    
        if any([(pad_value < 0) for pad_value in pad]):
            pad = list(pad)
            slices = []
            for n in reversed(range(len(pad) // 2)):
                i = n * 2
                j = i + 1
                start = None
                stop = None
                if tensorflow_get_item(pad, i) < 0:
                    start = -tensorflow_get_item(pad, i)
                    pad = tensorflow_set_item_bknd(pad, i, 0)
                if tensorflow_get_item(pad, j) < 0:
                    stop = tensorflow_get_item(pad, j)
                    pad = tensorflow_set_item_bknd(pad, j, 0)
                slices.append(slice(start, stop))
            ndim = len(tensorflow_shape_frnt_(input))
            while len(slices) < ndim:
                slices.insert(0, slice(None))
            input = tensorflow_get_item(input, tuple(slices))
        value = 0 if value is None else value
        mode_dict = {
            "constant": "constant",
            "reflect": "reflect",
            "replicate": "edge",
            "circular": "wrap",
        }
        if mode not in mode_dict:
            raise ValueError(f"Unsupported padding mode: {mode}")
        pad = tensorflow__handle_padding_shape_frnt(
            pad, len(tensorflow_shape_frnt_(input)), mode
        )
        order = 0, 2, 3, 1
>       pad = tuple(pad[i] for i in order)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f369fdec340>

>   pad = tuple(pad[i] for i in order)
E   IndexError: Exception encountered when calling tensorflow_RandomAffine.call().
E   
E   [1mtuple index out of range[0m
E   
E   Arguments received by tensorflow_RandomAffine.call():
E     â€¢ input=tf.Tensor(shape=(1, 1, 3, 3), dtype=float32)
E     â€¢ params=None
E     â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomAffine
________________________________________________________________________________ test_RandomCrop[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomCrop(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomCrop")
    
        init_args = ((2, 2),)
        init_kwargs = {"p": 1., "cropping_mode": "resample"}
        call_args = (torch.arange(1*1*3*3.).view(1, 1, 3, 3),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomCrop,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:821: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.crop.RandomCrop'>, target = 'tensorflow', init_args = ((2, 2),), init_kwargs = {'cropping_mode': 'resample', 'p': 1.0}
call_args = (tensor([[[[0., 1., 2.],
          [3., 4., 5.],
          [6., 7., 8.]]]]),), call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop(crop_size=(2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2), padding=None, pad_if_needed=False, fill=0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=resample)
args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>,), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f36aa0e4440, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomCrop(crop_size=(2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2), padding=None, pad_if_ne..., 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop(crop_size=(2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2), padding=None, pad_if_needed=False, fill=0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=resample)
v = None, buffers = None, args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>,), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomCrop(crop_size=(2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2), padding=None, pad_if_ne..., 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop(crop_size=(2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2), padding=None, pad_if_needed=False, fill=0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=resample)
v = None, buffers = None, args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>,), kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>, replace_v = False, replace_buffers = False
call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomCrop(crop_size=(2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2), padding=None, pad_if_needed=False, fill=0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=resample),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop(crop_size=(2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2), padding=None, pad_if_needed=False, fill=0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=resample)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...pe=int64, numpy=array([1, 1, 3, 3])>, 'input_size': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[3, 3]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369c3f23b0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f369c4401f0>
tensor = <function tensorflow_tensor_frnt at 0x7f369c5a64d0>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3])
flags = {'align_corners': True, 'cropping_mode': 'resample', 'fill': 0, 'pad_if_needed': False, ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop(crop_size=(2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2), padding=None, pad_if_needed=False, fill=0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=resample)
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...pe=int64, numpy=array([1, 1, 3, 3])>, 'input_size': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[3, 3]])>, ...}
flags = {'align_corners': True, 'cropping_mode': 'resample', 'fill': 0, 'pad_if_needed': False, ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop(crop_size=(2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2), padding=None, pad_if_needed=False, fill=0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=resample)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...pe=int64, numpy=array([1, 1, 3, 3])>, 'input_size': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[3, 3]])>, ...}
flags = {'align_corners': True, 'cropping_mode': 'resample', 'fill': 0, 'pad_if_needed': False, ...}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [...11151e-17,  1.0000000e+00, -1.0000000e+00],
        [-1.1102230e-16, -0.0000000e+00,  1.0000000e+00]]], dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369c3f23b0>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f369c3f2b00>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f369c3e4a60>, tensorflow_get_item = <function tensorflow_get_item at 0x7f369c3ac790>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f369c327880>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f369c3e4c10>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f369c3e4dc0>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop(crop_size=(2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2), padding=None, pad_if_needed=False, fill=0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=resample)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...pe=int64, numpy=array([1, 1, 3, 3])>, 'input_size': <tf.Tensor: shape=(1, 2), dtype=int64, numpy=array([[3, 3]])>, ...}
flags = {'align_corners': True, 'cropping_mode': 'resample', 'fill': 0, 'pad_if_needed': False, ...}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [...11151e-17,  1.0000000e+00, -1.0000000e+00],
        [-1.1102230e-16, -0.0000000e+00,  1.0000000e+00]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_tolist_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_unique_frnt_
        from ....geometry.transform.crop2d import tensorflow_crop_by_transform_mat
        from ....geometry.transform.crop2d import tensorflow_crop_by_indices
    
        padding_size: typing.Any = None
        if "padding_size" in params and isinstance(
            params["padding_size"], (tensorflow.Tensor, tensorflow.Variable)
        ):
            padding_size = tensorflow_tolist_frnt_(
                tensorflow_squeeze_frnt_(
                    tensorflow_unique_frnt_(params["padding_size"], dim=0).cpu()
                )
            )
        input = self.precrop_padding(input, padding_size, flags)
        flags = self.flags if flags is None else flags
        if flags["cropping_mode"] == "resample":
            if not isinstance(transform, (tensorflow.Tensor, tensorflow.Variable)):
                raise TypeError(
                    f"Expected the `transform` be a Tensor. Got {type(transform)}."
                )
            if flags["padding_mode"] == "constant":
                padding_mode = "zeros"
            elif flags["padding_mode"] == "replicate":
                padding_mode = "border"
            elif flags["padding_mode"] == "reflect":
                padding_mode = "reflection"
            else:
                padding_mode = flags["padding_mode"]
>           return tensorflow_crop_by_transform_mat(
                input,
                transform,
                flags["size"],
                mode=flags["resample"].name.lower(),
                padding_mode=padding_mode,
                align_corners=flags["align_corners"],
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/crop.py:186: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [...11151e-17,  1.0000000e+00, -1.0000000e+00],
        [-1.1102230e-16, -0.0000000e+00,  1.0000000e+00]]], dtype=float32)>
out_size = (2, 2), mode = 'bilinear', padding_mode = 'zeros', align_corners = True

    def tensorflow_crop_by_transform_mat(
        input_tensor,
        transform,
        out_size,
        mode="bilinear",
        padding_mode="zeros",
        align_corners=True,
    ):
        from ...core._backend import as_tensor
        from ....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .imgwarp import tensorflow_warp_affine
    
        dst_trans_src = as_tensor(
            tensorflow_expand_frnt_(
                transform, tensorflow_shape_frnt_(input_tensor)[0], -1, -1
            ),
            device=input_tensor.device,
            dtype=input_tensor.dtype,
        )
>       patches: typing.Any = tensorflow_warp_affine(
            input_tensor,
            dst_trans_src[:, :2, :],
            out_size,
            mode=mode,
            padding_mode=padding_mode,
            align_corners=align_corners,
        )

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/crop2d.py:51: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

src = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>
M = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [-5.5511151e-17,  1.0000000e+00, -1.0000000e+00]]], dtype=float32)>
dsize = (2, 2), mode = 'bilinear', padding_mode = 'zeros', align_corners = True, fill_value = <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>

    def tensorflow_warp_affine(
        src,
        M,
        dsize,
        mode="bilinear",
        padding_mode="zeros",
        align_corners=True,
        fill_value=zeros(3),
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ..conversions import tensorflow_convert_affinematrix_to_homography
        from ..conversions import tensorflow_normalize_homography
        from ...utils.helpers import tensorflow__torch_inverse_cast
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_affine_grid_frnt,
        )
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_grid_sample_frnt,
        )
    
        if not isinstance(src, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input src type is not a Tensor. Got {type(src)}")
        if not isinstance(M, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input M type is not a Tensor. Got {type(M)}")
        if not len(tensorflow_shape_frnt_(src)) == 4:
            raise ValueError(
                f"Input src must be a BxCxHxW tensor. Got {tensorflow_shape_frnt_(src)}"
            )
        if not (
            len(tensorflow_shape_frnt_(M)) == 3 or tensorflow_shape_frnt_(M)[-2:] == (2, 3)
        ):
            raise ValueError(
                f"Input M must be a Bx2x3 tensor. Got {tensorflow_shape_frnt_(M)}"
            )
        B, C, H, W = tensorflow_size_frnt_(src)
>       M_3x3: typing.Any = tensorflow_convert_affinematrix_to_homography(M)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/imgwarp.py:68: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [-5.5511151e-17,  1.0000000e+00, -1.0000000e+00]]], dtype=float32)>

    def tensorflow_convert_affinematrix_to_homography(A):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        if not isinstance(A, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(A)}")
        if not (
            len(tensorflow_shape_frnt_(A)) == 3 and tensorflow_shape_frnt_(A)[-2:] == (2, 3)
        ):
            raise ValueError(
                f"Input matrix must be a Bx2x3 tensor. Got {tensorflow_shape_frnt_(A)}"
            )
>       return tensorflow__convert_affinematrix_to_homography_impl(A)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [-5.5511151e-17,  1.0000000e+00, -1.0000000e+00]]], dtype=float32)>

    def tensorflow__convert_affinematrix_to_homography_impl(A):
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import pad
    
>       H: typing.Any = pad(A, [0, 0, 0, 1], "constant", value=0.0)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.0000000e+00, -1.6434603e-32, -1.0000000e+00],
        [-5.5511151e-17,  1.0000000e+00, -1.0000000e+00]]], dtype=float32)>
pad = ((0, 0), (0, 1), (0, 0)), mode = 'constant', value = 0.0

    def tensorflow_pad_frnt(input, pad, mode="constant", value=0):
        from .....backends.tensorflow.general import tensorflow_get_item
        from .....ivy.general import tensorflow_set_item_bknd
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.experimental.manipulation import tensorflow_pad
    
        if any([(pad_value < 0) for pad_value in pad]):
            pad = list(pad)
            slices = []
            for n in reversed(range(len(pad) // 2)):
                i = n * 2
                j = i + 1
                start = None
                stop = None
                if tensorflow_get_item(pad, i) < 0:
                    start = -tensorflow_get_item(pad, i)
                    pad = tensorflow_set_item_bknd(pad, i, 0)
                if tensorflow_get_item(pad, j) < 0:
                    stop = tensorflow_get_item(pad, j)
                    pad = tensorflow_set_item_bknd(pad, j, 0)
                slices.append(slice(start, stop))
            ndim = len(tensorflow_shape_frnt_(input))
            while len(slices) < ndim:
                slices.insert(0, slice(None))
            input = tensorflow_get_item(input, tuple(slices))
        value = 0 if value is None else value
        mode_dict = {
            "constant": "constant",
            "reflect": "reflect",
            "replicate": "edge",
            "circular": "wrap",
        }
        if mode not in mode_dict:
            raise ValueError(f"Unsupported padding mode: {mode}")
        pad = tensorflow__handle_padding_shape_frnt(
            pad, len(tensorflow_shape_frnt_(input)), mode
        )
        order = 0, 2, 3, 1
>       pad = tuple(pad[i] for i in order)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f369fb5a440>

>   pad = tuple(pad[i] for i in order)
E   IndexError: Exception encountered when calling tensorflow_RandomCrop.call().
E   
E   [1mtuple index out of range[0m
E   
E   Arguments received by tensorflow_RandomCrop.call():
E     â€¢ input=tf.Tensor(shape=(1, 1, 3, 3), dtype=float32)
E     â€¢ params=None
E     â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomCrop
_____________________________________________________________________________ test_RandomPerspective[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomPerspective(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomPerspective")
    
        init_args = (0.5,)
        init_kwargs = {"p": 0.5}
        call_args = (torch.tensor([[[[1., 0., 0.],
                                     [0., 1., 0.],
                                     [0., 0., 1.]]]]),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomPerspective,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:929: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.perspective.RandomPerspective'>, target = 'tensorflow', init_args = (0.5,), init_kwargs = {'p': 0.5}
call_args = (tensor([[[[1., 0., 0.],
          [0., 1., 0.],
          [0., 0., 1.]]]]),), call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomPerspective(distortion_scale=0.5, p=0.5, p_batch=1.0, same_on_batch=False, align_corners=False, resample=bilinear)
args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>,), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f369c1a9a40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomPerspective(distortion_scale=0.5, p=0.5, p_batch=1.0, same_on_batch=False, align_corners=False, resa..., 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomPerspective(distortion_scale=0.5, p=0.5, p_batch=1.0, same_on_batch=False, align_corners=False, resample=bilinear), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>,), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomPerspective(distortion_scale=0.5, p=0.5, p_batch=1.0, same_on_batch=False, align_corners=False, resa..., 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomPerspective(distortion_scale=0.5, p=0.5, p_batch=1.0, same_on_batch=False, align_corners=False, resample=bilinear), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>,), kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>, replace_v = False, replace_buffers = False
call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomPerspective(distortion_scale=0.5, p=0.5, p_batch=1.0, same_on_batch=False, align_corners=False, resample=bilinear),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomPerspective(distortion_scale=0.5, p=0.5, p_batch=1.0, same_on_batch=False, align_corners=False, resample=bilinear)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'end_points': <tf.Tensor: sha...4, 2), dtype=float32, numpy=
array([[[0., 0.],
        [2., 0.],
        [2., 2.],
        [0., 2.]]], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369c26b520>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36a4958670>
tensor = <function tensorflow_tensor_frnt at 0x7f36a4ee11b0>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3]), flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomPerspective(distortion_scale=0.5, p=0.5, p_batch=1.0, same_on_batch=False, align_corners=False, resample=bilinear)
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'end_points': <tf.Tensor: sha...4, 2), dtype=float32, numpy=
array([[[0., 0.],
        [2., 0.],
        [2., 2.],
        [0., 2.]]], dtype=float32)>}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomPerspective(distortion_scale=0.5, p=0.5, p_batch=1.0, same_on_batch=False, align_corners=False, resample=bilinear)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'end_points': <tf.Tensor: sha...4, 2), dtype=float32, numpy=
array([[[0., 0.],
        [2., 0.],
        [2., 2.],
        [0., 2.]]], dtype=float32)>}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[ 0.6665877 , -0.20789641,  0.7277527 ],
        [ 0.17826489,  0.18665618,  0.47367132],
        [ 0.11639891, -0.17914331,  1.        ]]], dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f369c26b520>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f369c26bc70>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f36a5389cf0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f369c2ca950>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f368cfa9e10>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f36a538a290>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f36a538bd00>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomPerspective(distortion_scale=0.5, p=0.5, p_batch=1.0, same_on_batch=False, align_corners=False, resample=bilinear)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'end_points': <tf.Tensor: sha...4, 2), dtype=float32, numpy=
array([[[0., 0.],
        [2., 0.],
        [2., 2.],
        [0., 2.]]], dtype=float32)>}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}
transform = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[ 0.6665877 , -0.20789641,  0.7277527 ],
        [ 0.17826489,  0.18665618,  0.47367132],
        [ 0.11639891, -0.17914331,  1.        ]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....geometry.transform.imgwarp import tensorflow_warp_perspective
    
        _, _, height, width = tensorflow_shape_frnt_(input)
        if not isinstance(transform, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(
                f"Expected the `transform` be a Tensor. Got {type(transform)}."
            )
>       return tensorflow_warp_perspective(
            input,
            transform,
            (height, width),
            mode=flags["resample"].name.lower(),
            align_corners=flags["align_corners"],
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/perspective.py:73: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

src = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]], dtype=float32)>
M = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[ 0.6665877 , -0.20789641,  0.7277527 ],
        [ 0.17826489,  0.18665618,  0.47367132],
        [ 0.11639891, -0.17914331,  1.        ]]], dtype=float32)>
dsize = (3, 3), mode = 'bilinear', padding_mode = 'zeros', align_corners = False, fill_value = <tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>

    def tensorflow_warp_perspective(
        src,
        M,
        dsize,
        mode="bilinear",
        padding_mode="zeros",
        align_corners=True,
        fill_value=zeros(3),
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ..conversions import tensorflow_normalize_homography
        from ...utils.helpers import tensorflow__torch_inverse_cast
        from ....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...utils.grid import tensorflow_create_meshgrid
        from ..linalg import tensorflow_transform_points
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_grid_sample_frnt,
        )
    
        if not isinstance(src, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input src type is not a Tensor. Got {type(src)}")
        if not isinstance(M, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input M type is not a Tensor. Got {type(M)}")
        if not len(tensorflow_shape_frnt_(src)) == 4:
            raise ValueError(
                f"Input src must be a BxCxHxW tensor. Got {tensorflow_shape_frnt_(src)}"
            )
        if not (
            len(tensorflow_shape_frnt_(M)) == 3 and tensorflow_shape_frnt_(M)[-2:] == (3, 3)
        ):
            raise ValueError(
                f"Input M must be a Bx3x3 tensor. Got {tensorflow_shape_frnt_(M)}"
            )
        if padding_mode == "fill" and tensorflow_shape_frnt_(fill_value) != tuple([3]):
            raise ValueError(
                f"Padding_tensor only supported for 3 channels. Got {tensorflow_shape_frnt_(fill_value)}"
            )
        B, _, H, W = tensorflow_size_frnt_(src)
        h_out, w_out = dsize[0], dsize[1]
        dst_norm_trans_src_norm: typing.Any = tensorflow_normalize_homography(
            M, (H, W), (h_out, w_out)
        )
        src_norm_trans_dst_norm = tensorflow__torch_inverse_cast(dst_norm_trans_src_norm)
        grid = tensorflow_expand_frnt_(
            tensorflow_to_frnt_(
                tensorflow_create_meshgrid(
                    h_out, w_out, normalized_coordinates=True, device=src.device
                ),
                src.dtype,
            ),
            B,
            h_out,
            w_out,
            2,
        )
>       grid = tensorflow_transform_points(src_norm_trans_dst_norm[:, None, None], grid)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/imgwarp.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

trans_01 = <tf.Tensor: shape=(3, 3, 3), dtype=float32, numpy=
array([[[ 1.940959  , -0.1056003 , -0.5271597 ],
        [-0.414658... ],
        [-0.41465828,  2.9048915 ,  0.416038  ],
        [-0.32030618,  0.568344  ,  1.2119333 ]]], dtype=float32)>
points_1 = <tf.Tensor: shape=(3, 3, 2), dtype=float32, numpy=
array([[[-1., -1.],
        [ 0., -1.],
        [ 1., -1.]],

     ...        [ 0.,  0.],
        [ 1.,  0.]],

       [[-1.,  1.],
        [ 0.,  1.],
        [ 1.,  1.]]], dtype=float32)>

    def tensorflow_transform_points(trans_01, points_1):
        from ..core.check import tensorflow_KORNIA_CHECK_IS_TENSOR
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_repeat_interleave_frnt,
        )
        from .conversions import tensorflow_convert_points_to_homogeneous
        from ...ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_bmm_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_squeeze_frnt,
        )
        from .conversions import tensorflow_convert_points_from_homogeneous
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        tensorflow_KORNIA_CHECK_IS_TENSOR(trans_01)
        tensorflow_KORNIA_CHECK_IS_TENSOR(points_1)
        if (
            not tensorflow_shape_frnt_(trans_01)[0] == tensorflow_shape_frnt_(points_1)[0]
            and tensorflow_shape_frnt_(trans_01)[0] != 1
        ):
            raise ValueError(
                f"Input batch size must be the same for both tensors or 1. Got {tensorflow_shape_frnt_(trans_01)} and {tensorflow_shape_frnt_(points_1)}"
            )
        if (
            not tensorflow_shape_frnt_(trans_01)[-1]
            == tensorflow_shape_frnt_(points_1)[-1] + 1
        ):
            raise ValueError(
                f"Last input dimensions must differ by one unit Got{trans_01} and {points_1}"
            )
        shape_inp = list(tensorflow_shape_frnt_(points_1))
        points_1 = tensorflow_reshape_frnt_(
            points_1,
            -1,
            tensorflow_shape_frnt_(points_1)[-2],
            tensorflow_shape_frnt_(points_1)[-1],
        )
        trans_01 = tensorflow_reshape_frnt_(
            trans_01,
            -1,
            tensorflow_shape_frnt_(trans_01)[-2],
            tensorflow_shape_frnt_(trans_01)[-1],
        )
        trans_01 = tensorflow_repeat_interleave_frnt(
            trans_01,
            repeats=tensorflow_shape_frnt_(points_1)[0]
            // tensorflow_shape_frnt_(trans_01)[0],
            dim=0,
        )
>       points_1_h = tensorflow_convert_points_to_homogeneous(points_1)

Translated_Outputs/tensorflow_outputs/kornia/geometry/linalg.py:81: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

points = <tf.Tensor: shape=(3, 3, 2), dtype=float32, numpy=
array([[[-1., -1.],
        [ 0., -1.],
        [ 1., -1.]],

     ...        [ 0.,  0.],
        [ 1.,  0.]],

       [[-1.,  1.],
        [ 0.,  1.],
        [ 1.,  1.]]], dtype=float32)>

    def tensorflow_convert_points_to_homogeneous(points):
        from ..core._backend import pad
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        if not isinstance(points, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(points)}")
        if len(tensorflow_shape_frnt_(points)) < 2:
            raise ValueError(
                f"Input must be at least a 2D tensor. Got {tensorflow_shape_frnt_(points)}"
            )
>       return pad(points, [0, 1], "constant", 1.0)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:77: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(3, 3, 2), dtype=float32, numpy=
array([[[-1., -1.],
        [ 0., -1.],
        [ 1., -1.]],

     ...        [ 0.,  0.],
        [ 1.,  0.]],

       [[-1.,  1.],
        [ 0.,  1.],
        [ 1.,  1.]]], dtype=float32)>
pad = ((0, 0), (0, 0), (0, 1)), mode = 'constant', value = 1.0

    def tensorflow_pad_frnt(input, pad, mode="constant", value=0):
        from .....backends.tensorflow.general import tensorflow_get_item
        from .....ivy.general import tensorflow_set_item_bknd
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.experimental.manipulation import tensorflow_pad
    
        if any([(pad_value < 0) for pad_value in pad]):
            pad = list(pad)
            slices = []
            for n in reversed(range(len(pad) // 2)):
                i = n * 2
                j = i + 1
                start = None
                stop = None
                if tensorflow_get_item(pad, i) < 0:
                    start = -tensorflow_get_item(pad, i)
                    pad = tensorflow_set_item_bknd(pad, i, 0)
                if tensorflow_get_item(pad, j) < 0:
                    stop = tensorflow_get_item(pad, j)
                    pad = tensorflow_set_item_bknd(pad, j, 0)
                slices.append(slice(start, stop))
            ndim = len(tensorflow_shape_frnt_(input))
            while len(slices) < ndim:
                slices.insert(0, slice(None))
            input = tensorflow_get_item(input, tuple(slices))
        value = 0 if value is None else value
        mode_dict = {
            "constant": "constant",
            "reflect": "reflect",
            "replicate": "edge",
            "circular": "wrap",
        }
        if mode not in mode_dict:
            raise ValueError(f"Unsupported padding mode: {mode}")
        pad = tensorflow__handle_padding_shape_frnt(
            pad, len(tensorflow_shape_frnt_(input)), mode
        )
        order = 0, 2, 3, 1
>       pad = tuple(pad[i] for i in order)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f36a4b496f0>

>   pad = tuple(pad[i] for i in order)
E   IndexError: Exception encountered when calling tensorflow_RandomPerspective.call().
E   
E   [1mtuple index out of range[0m
E   
E   Arguments received by tensorflow_RandomPerspective.call():
E     â€¢ input=tf.Tensor(shape=(1, 1, 3, 3), dtype=float32)
E     â€¢ params=None
E     â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomPerspective
_____________________________________________________________________________ test_RandomResizedCrop[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomResizedCrop(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomResizedCrop")
    
        init_args = ()
        init_kwargs = {"size": (3, 3), "scale": (3., 3.), "ratio": (2., 2.), "p": 1., "cropping_mode": "resample"}
        call_args = (torch.tensor([[[0., 1., 2.],
                                    [3., 4., 5.],
                                    [6., 7., 8.]]]),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomResizedCrop,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:951: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resized_crop.RandomResizedCrop'>, target = 'tensorflow', init_args = ()
init_kwargs = {'cropping_mode': 'resample', 'p': 1.0, 'ratio': (2.0, 2.0), 'scale': (3.0, 3.0), ...}, call_args = (tensor([[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
args = (<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>,), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f369c29a440, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_...e=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
v = None, buffers = None, args = (<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>,), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_...e=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
v = None, buffers = None, args = (<tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>,), kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>, replace_v = False, replace_buffers = False
call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros),)
kwargs = {'input': <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
input = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[0., 1., 2.],
        [3., 4., 5.],
        [6., 7., 8.]]], dtype=float32)>, params = None, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f368cfbed40>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f369c0ed750>
tensor = <function tensorflow_tensor_frnt at 0x7f36a5330c10>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0., 1., 2.],
         [3., 4., 5.],
         [6., 7., 8.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3])

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
>           params = self.forward_parameters(batch_shape)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3])

    def forward_parameters(self, batch_shape):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        batch_prob = self.__batch_prob_generator__(
            batch_shape, self.p, self.p_batch, self.same_on_batch
        )
        to_apply = batch_prob > 0.5
>       _params = self.generate_parameters(
            tuple(
                (
                    int(tensorflow_item_frnt_(tensorflow_sum_frnt_(to_apply))),
                    *batch_shape[1:],
                )
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:199: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomResizedCrop(scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), p=1.0, p_batch=1.0, same_on_batch=False, size=(3, 3), resample=bilinear, align_corners=True, cropping_mode=resample, padding_mode=zeros)
batch_shape = (1, 1, 3, 3)

    def generate_parameters(self, batch_shape):
        if self._param_generator is not None:
>           return self._param_generator(batch_shape, self.same_on_batch)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), args = ((1, 1, 3, 3), False), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f369c29a840, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ion.py', lineno=46, function='__call__', code_context=['            return call_fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), v = None, buffers = None, args = ((1, 1, 3, 3), False), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), v = None, buffers = None, args = ((1, 1, 3, 3), False), kwargs = {}, replace_v = False, replace_buffers = False
call_signature = <Signature (batch_shape, same_on_batch=False)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = scale=(3.0, 3.0), resize_to=(2.0, 2.0), output_size=(3, 3), batch_shape = (1, 1, 3, 3), same_on_batch = False

    def call(self, batch_shape, same_on_batch=False):
        from ....utils.helpers import tensorflow__extract_device_dtype
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...utils.helpers import tensorflow__adapted_rsampling
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_exp_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_floor_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_round_frnt_
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_sqrt_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_int_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_cumsum_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_bool_frnt_
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_arange_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
        from .....ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_round_frnt,
        )
        from .....ivy.functional.frontends.torch.tensor import tensorflow_where_frnt_
        from .....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_stack_frnt,
        )
        from ....core._backend import tensor
        from ....core._backend import zeros
    
        batch_size = batch_shape[0]
        size = batch_shape[-2], batch_shape[-1]
        _device, _dtype = tensorflow__extract_device_dtype([self.scale, self.ratio])
        if batch_size == 0:
            return {
                "src": zeros([0, 4, 2], device=_device, dtype=_dtype),
                "dst": zeros([0, 4, 2], device=_device, dtype=_dtype),
                "size": zeros([0, 2], device=_device, dtype=_dtype),
            }
        rand = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 10), self.rand_sampler, same_on_batch
            ),
            device=_device,
            dtype=_dtype,
        )
        area = (
            (rand * (self.scale[1] - self.scale[0]) + self.scale[0]) * size[0] * size[1]
        )
        log_ratio = tensorflow_to_frnt_(
            tensorflow__adapted_rsampling(
                (batch_size, 10), self.log_ratio_sampler, same_on_batch
            ),
            device=_device,
            dtype=_dtype,
        )
        aspect_ratio = tensorflow_exp_frnt(log_ratio)
        w = tensorflow_floor_frnt_(
            tensorflow_round_frnt_(tensorflow_sqrt_frnt(area * aspect_ratio))
        )
        h = tensorflow_floor_frnt_(
            tensorflow_round_frnt_(tensorflow_sqrt_frnt(area / aspect_ratio))
        )
>       cond = tensorflow_int_frnt_((0 < w) * (w < size[0]) * (0 < h) * (h < size[1]))

Translated_Outputs/tensorflow_outputs/kornia/augmentation/random_generator/_2d/crop.py:328: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 10), dtype=bool, numpy=
array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,
...e=(1, 10), dtype=bool, numpy=
array([[False, False, False, False, False, False, False, False, False,
        False]])>)
kwargs = {}, arg = <tf.Tensor: shape=(1, 10), dtype=bool, numpy=
array([[False, False, False, False, False, False, False, False, False,
        False]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_ResizedCropGenerator.call().
E       
E       [1mValue for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E       	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_ResizedCropGenerator.call():
E         â€¢ batch_shape=('1', '1', '3', '3')
E         â€¢ same_on_batch=False

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomResizedCrop
______________________________________________________________________________ test_RandomRotation[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRotation(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomRotation")
    
        init_args = ()
        init_kwargs = {"degrees": 45.0, "p": 1.}
        call_args = (torch.tensor([[1., 0., 0., 2.],
                                   [0., 0., 0., 0.],
                                   [0., 1., 2., 0.],
                                   [0., 0., 1., 2.]]),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomRotation,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:974: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.rotation.RandomRotation'>, target = 'tensorflow', init_args = (), init_kwargs = {'degrees': 45.0, 'p': 1.0}
call_args = (tensor([[1., 0., 0., 2.],
        [0., 0., 0., 0.],
        [0., 1., 2., 0.],
        [0., 0., 1., 2.]]),), call_kwargs = {}, deterministic_output = False, backend_compile = False
tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
args = (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>,), kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f368cf42c40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=Tru...=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True), v = None, buffers = None
args = (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>,), kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=Tru...=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True), v = None, buffers = None
args = (<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>,), kwargs = {}
first_arr = <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>, replace_v = False
replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True),)
kwargs = {'input': <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(4, 4), dtype=float32, numpy=
array([[1., 0., 0., 2.],
       [0., 0., 0., 0.],
       [0., 1., 2., 0.],
       [0., 0., 1., 2.]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...y([11.688477], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f368cec64d0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f368cf71bd0>
tensor = <function tensorflow_tensor_frnt at 0x7f369c12c040>
in_tensor = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([4, 4]), batch_shape = ivy.frontends.torch.Size([1, 1, 4, 4]), flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
in_tensor = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...y([11.688477], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...y([11.688477], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation(degrees=45.0, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[1., 0., 0., 2.],
         [0., 0., 0., 0.],
         [0., 1., 2., 0.],
         [0., 0., 1., 2.]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'degrees': <tf.Tensor: shape=...y([11.688477], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([1, 1, 4, 4])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....geometry.transform.affwarp import tensorflow__compute_tensor_center
        from ....geometry.transform.affwarp import tensorflow__compute_rotation_matrix
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        angles: typing.Any = tensorflow_to_frnt_(params["degrees"], input)
        center: typing.Any = tensorflow__compute_tensor_center(input)
        rotation_mat: typing.Any = tensorflow__compute_rotation_matrix(
>           angles, center.expand(tensorflow_shape_frnt_(angles)[0], -1)
        )
E       AttributeError: Exception encountered when calling tensorflow_RandomRotation.call().
E       
E       [1m'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'expand'[0m
E       
E       Arguments received by tensorflow_RandomRotation.call():
E         â€¢ input=tf.Tensor(shape=(4, 4), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/rotation.py:67: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomRotation
________________________________________________________________________________ test_RandomShear[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomShear(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomShear")
    
        init_args = ((-5., 2., 5., 10.),)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 3, 3),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomShear,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:994: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.shear.RandomShear'>, target = 'tensorflow', init_args = ((-5.0, 2.0, 5.0, 10.0),), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[0.6815, 0.6089, 0.9450],
          [0.6343, 0.3045, 0.5485],
          [0.8766, 0.0407, 0.3898]]]]),), call_kwargs = {}, deterministic_output = False, backend_compile = False
tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomShear(shear=(-5.0, 2.0, 5.0, 10.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.6814926 , 0.6089112 , 0.94500613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f369c299640, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomShear(shear=(-5.0, 2.0, 5.0, 10.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padd...613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomShear(shear=(-5.0, 2.0, 5.0, 10.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.6814926 , 0.6089112 , 0.94500613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomShear(shear=(-5.0, 2.0, 5.0, 10.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padd...613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomShear(shear=(-5.0, 2.0, 5.0, 10.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.6814926 , 0.6089112 , 0.94500613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.6814926 , 0.6089112 , 0.94500613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomShear(shear=(-5.0, 2.0, 5.0, 10.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.6814926 , 0.6089112 , 0.94500613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomShear(shear=(-5.0, 2.0, 5.0, 10.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.6814926 , 0.6089112 , 0.94500613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'center': <tf.Tensor: shape=(...array([1, 1, 3, 3])>, 'shear_x': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.9139972], dtype=float32)>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f36aa13e9e0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f369c2c9f30>
tensor = <function tensorflow_tensor_frnt at 0x7f368c9c1630>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.6814926 , 0.6089112 , 0.94500613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3])
flags = {'align_corners': False, 'padding_mode': <tensorflow_SamplePadding.ZEROS: 0>, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomShear(shear=(-5.0, 2.0, 5.0, 10.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.6814926 , 0.6089112 , 0.94500613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'center': <tf.Tensor: shape=(...array([1, 1, 3, 3])>, 'shear_x': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.9139972], dtype=float32)>, ...}
flags = {'align_corners': False, 'padding_mode': <tensorflow_SamplePadding.ZEROS: 0>, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomShear(shear=(-5.0, 2.0, 5.0, 10.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.6814926 , 0.6089112 , 0.94500613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'center': <tf.Tensor: shape=(...array([1, 1, 3, 3])>, 'shear_x': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.9139972], dtype=float32)>, ...}
flags = {'align_corners': False, 'padding_mode': <tensorflow_SamplePadding.ZEROS: 0>, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomShear(shear=(-5.0, 2.0, 5.0, 10.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, padding_mode=zeros, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3), dtype=float32, numpy=
array([[[[0.6814926 , 0.6089112 , 0.94500613],
         [0.6343364 , 0.3045469 , 0.54849285],
         [0.87656665, 0.04065645, 0.38982016]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'center': <tf.Tensor: shape=(...array([1, 1, 3, 3])>, 'shear_x': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-3.9139972], dtype=float32)>, ...}
flags = {'align_corners': False, 'padding_mode': <tensorflow_SamplePadding.ZEROS: 0>, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def compute_transformation(self, input, params, flags):
        from ....core._backend import as_tensor
        from ....geometry.transform.imgwarp import tensorflow_get_shear_matrix2d
        from ....geometry.conversions import tensorflow_deg2rad
    
>       return tensorflow_get_shear_matrix2d(
            as_tensor(params["center"], device=input.device, dtype=input.dtype),
            tensorflow_deg2rad(
                as_tensor(params["shear_x"], device=input.device, dtype=input.dtype)
            ),
            tensorflow_deg2rad(
                as_tensor(params["shear_y"], device=input.device, dtype=input.dtype)
            ),
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/shear.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

center = <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1., 1.]], dtype=float32)>, sx = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.06831214], dtype=float32)>
sy = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.10896832], dtype=float32)>

    def tensorflow_get_shear_matrix2d(center, sx=None, sy=None):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ....ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_split_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ..conversions import tensorflow_convert_affinematrix_to_homography
        from ...core._backend import tensor
        from ...core._backend import tan
        from ...core._backend import ones_like
        from ...core._backend import stack
    
        sx = (
            tensorflow_repeat_frnt_(tensor([0.0]), tensorflow_size_frnt_(center, 0))
            if sx is None
            else sx
        )
        sy = (
            tensorflow_repeat_frnt_(tensor([0.0]), tensorflow_size_frnt_(center, 0))
            if sy is None
            else sy
        )
        x, y = tensorflow_split_frnt(center, 1, dim=-1)
        x, y = tensorflow_view_frnt_(x, -1), tensorflow_view_frnt_(y, -1)
        sx_tan = tan(sx)
        sy_tan = tan(sy)
        ones = ones_like(sx)
        shear_mat = tensorflow_view_frnt_(
            stack(
                [
                    ones,
                    -sx_tan,
                    sx_tan * y,
                    -sy_tan,
                    ones + sx_tan * sy_tan,
                    sy_tan * (x - sx_tan * y),
                ],
                dim=-1,
            ),
            -1,
            2,
            3,
        )
>       shear_mat = tensorflow_convert_affinematrix_to_homography(shear_mat)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/imgwarp.py:392: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.        ,  0.0684186 , -0.0684186 ],
        [-0.10940168,  0.9925149 ,  0.11688679]]], dtype=float32)>

    def tensorflow_convert_affinematrix_to_homography(A):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        if not isinstance(A, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(A)}")
        if not (
            len(tensorflow_shape_frnt_(A)) == 3 and tensorflow_shape_frnt_(A)[-2:] == (2, 3)
        ):
            raise ValueError(
                f"Input matrix must be a Bx2x3 tensor. Got {tensorflow_shape_frnt_(A)}"
            )
>       return tensorflow__convert_affinematrix_to_homography_impl(A)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.        ,  0.0684186 , -0.0684186 ],
        [-0.10940168,  0.9925149 ,  0.11688679]]], dtype=float32)>

    def tensorflow__convert_affinematrix_to_homography_impl(A):
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import pad
    
>       H: typing.Any = pad(A, [0, 0, 0, 1], "constant", value=0.0)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=
array([[[ 1.        ,  0.0684186 , -0.0684186 ],
        [-0.10940168,  0.9925149 ,  0.11688679]]], dtype=float32)>
pad = ((0, 0), (0, 1), (0, 0)), mode = 'constant', value = 0.0

    def tensorflow_pad_frnt(input, pad, mode="constant", value=0):
        from .....backends.tensorflow.general import tensorflow_get_item
        from .....ivy.general import tensorflow_set_item_bknd
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.experimental.manipulation import tensorflow_pad
    
        if any([(pad_value < 0) for pad_value in pad]):
            pad = list(pad)
            slices = []
            for n in reversed(range(len(pad) // 2)):
                i = n * 2
                j = i + 1
                start = None
                stop = None
                if tensorflow_get_item(pad, i) < 0:
                    start = -tensorflow_get_item(pad, i)
                    pad = tensorflow_set_item_bknd(pad, i, 0)
                if tensorflow_get_item(pad, j) < 0:
                    stop = tensorflow_get_item(pad, j)
                    pad = tensorflow_set_item_bknd(pad, j, 0)
                slices.append(slice(start, stop))
            ndim = len(tensorflow_shape_frnt_(input))
            while len(slices) < ndim:
                slices.insert(0, slice(None))
            input = tensorflow_get_item(input, tuple(slices))
        value = 0 if value is None else value
        mode_dict = {
            "constant": "constant",
            "reflect": "reflect",
            "replicate": "edge",
            "circular": "wrap",
        }
        if mode not in mode_dict:
            raise ValueError(f"Unsupported padding mode: {mode}")
        pad = tensorflow__handle_padding_shape_frnt(
            pad, len(tensorflow_shape_frnt_(input)), mode
        )
        order = 0, 2, 3, 1
>       pad = tuple(pad[i] for i in order)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f368c9487f0>

>   pad = tuple(pad[i] for i in order)
E   IndexError: Exception encountered when calling tensorflow_RandomShear.call().
E   
E   [1mtuple index out of range[0m
E   
E   Arguments received by tensorflow_RandomShear.call():
E     â€¢ input=tf.Tensor(shape=(1, 1, 3, 3), dtype=float32)
E     â€¢ params=None
E     â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomShear
______________________________________________________________________________ test_RandomCutMixV2[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomCutMixV2(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomCutMixV2")
    
        input = torch.rand(2, 1, 3, 3)
        input[0] = torch.ones((1, 3, 3))
        label = torch.tensor([0, 1])
    
        init_args = ()
        init_kwargs = {"data_keys": ["input", "class"]}
        call_args = (input, label)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomCutMixV2,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1058: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.cutmix.RandomCutMixV2'>, target = 'tensorflow', init_args = (), init_kwargs = {'data_keys': ['input', 'class']}
call_args = (tensor([[[[1.0000e+00, 1.0000e+00, 1.0000e+00],
          [1.0000e+00, 1.0000e+00, 1.0000e+00],
          [1.0000e+00...1],
          [4.5822e-01, 6.8568e-01, 5.2208e-04],
          [9.8517e-01, 4.7785e-01, 7.5218e-01]]]]), tensor([0, 1]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
>       transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)

kornia/augmentation/test_augmentation.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.augmentation._2d.mix.cutmix.RandomCutMixV2'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: source code not available

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomCutMixV2
_______________________________________________________________________________ test_RandomJigsaw[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomJigsaw(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomJigsaw")
    
        init_args = ((4, 4),)
        init_kwargs = {}
        call_args = (torch.randn(8, 3, 256, 256),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomJigsaw,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1078: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.jigsaw.RandomJigsaw'>, target = 'tensorflow', init_args = ((4, 4),), init_kwargs = {}
call_args = (tensor([[[[ 0.0207, -1.0132, -0.5122,  ...,  0.5290,  0.7178,  1.0057],
          [-0.6230,  0.2497, -0.3830,  ...,  ...-0.3000,  ...,  1.2045,  0.6385,  0.1202],
          [-0.7157,  0.6969,  0.7241,  ..., -0.0549,  0.1928,  0.1268]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4))
args = (<tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 0.02073964, -1.0132091 , -0.51215076, ...,  0.52...    [-0.7156773 ,  0.6969489 ,  0.72410125, ..., -0.05494847,
           0.19277984,  0.12680477]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f368ca65a40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), <tf.Tensor: shape=(8, 3, ...     [-0.7156773 ,  0.6969489 ,  0.72410125, ..., -0.05494847,
           0.19277984,  0.12680477]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 0.02073964, -1.0132091 , -0.51215076, ...,  0.52...    [-0.7156773 ,  0.6969489 ,  0.72410125, ..., -0.05494847,
           0.19277984,  0.12680477]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), <tf.Tensor: shape=(8, 3, ...     [-0.7156773 ,  0.6969489 ,  0.72410125, ..., -0.05494847,
           0.19277984,  0.12680477]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)), v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 0.02073964, -1.0132091 , -0.51215076, ...,  0.52...    [-0.7156773 ,  0.6969489 ,  0.72410125, ..., -0.05494847,
           0.19277984,  0.12680477]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 0.02073964, -1.0132091 , -0.51215076, ...,  0.528...      [-0.7156773 ,  0.6969489 ,  0.72410125, ..., -0.05494847,
           0.19277984,  0.12680477]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomJigsaw(grid=(4, 4), p=0.5, p_batch=1.0, same_on_batch=False, grid=(4, 4)),)
kwargs = {'input': <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 0.02073964, -1.0132091 , -0.51215076, ....     [-0.7156773 ,  0.6969489 ,  0.72410125, ..., -0.05494847,
           0.19277984,  0.12680477]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 0.02073964, -1.0132091 , -0.51215076, ....     [-0.7156773 ,  0.6969489 ,  0.72410125, ..., -0.05494847,
           0.19277984,  0.12680477]]]], dtype=float32)>}

    def bind(self, /, *args, **kwargs):
        """Get a BoundArguments object, that maps the passed `args`
        and `kwargs` to the function's signature.  Raises `TypeError`
        if the passed arguments can not be bound.
        """
>       return self._bind(args, kwargs)

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 256, 256), dtype=float32, numpy=
array([[[[ 0.02073964, -1.0132091 , -0.51215076, ....     [-0.7156773 ,  0.6969489 ,  0.72410125, ..., -0.05494847,
           0.19277984,  0.12680477]]]], dtype=float32)>}

    def _bind(self, args, kwargs, *, partial=False):
        """Private method. Don't use directly."""
    
        arguments = {}
    
        parameters = iter(self.parameters.values())
        parameters_ex = ()
        arg_vals = iter(args)
    
        while True:
            # Let's iterate through the positional arguments and corresponding
            # parameters
            try:
                arg_val = next(arg_vals)
            except StopIteration:
                # No more positional arguments
                try:
                    param = next(parameters)
                except StopIteration:
                    # No more parameters. That's it. Just need to check that
                    # we have no `kwargs` after this while loop
                    break
                else:
                    if param.kind == _VAR_POSITIONAL:
                        # That's OK, just empty *args.  Let's start parsing
                        # kwargs
                        break
                    elif param.name in kwargs:
                        if param.kind == _POSITIONAL_ONLY:
                            msg = '{arg!r} parameter is positional only, ' \
                                  'but was passed as a keyword'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
                        parameters_ex = (param,)
                        break
                    elif (param.kind == _VAR_KEYWORD or
                                                param.default is not _empty):
                        # That's fine too - we have a default value for this
                        # parameter.  So, lets start parsing `kwargs`, starting
                        # with the current parameter
                        parameters_ex = (param,)
                        break
                    else:
                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,
                        # not in `kwargs`
                        if partial:
                            parameters_ex = (param,)
                            break
                        else:
                            msg = 'missing a required argument: {arg!r}'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
            else:
                # We have a positional argument to process
                try:
                    param = next(parameters)
                except StopIteration:
                    raise TypeError('too many positional arguments') from None
                else:
                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):
                        # Looks like we have no parameter for this positional
                        # argument
                        raise TypeError(
                            'too many positional arguments') from None
    
                    if param.kind == _VAR_POSITIONAL:
                        # We have an '*args'-like argument, let's fill it with
                        # all positional arguments we have left and move on to
                        # the next phase
                        values = [arg_val]
                        values.extend(arg_vals)
                        arguments[param.name] = tuple(values)
                        break
    
                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:
                        raise TypeError(
                            'multiple values for argument {arg!r}'.format(
                                arg=param.name)) from None
    
                    arguments[param.name] = arg_val
    
        # Now, we iterate through the remaining parameters to process
        # keyword arguments
        kwargs_param = None
        for param in itertools.chain(parameters_ex, parameters):
            if param.kind == _VAR_KEYWORD:
                # Memorize that we have a '**kwargs'-like parameter
                kwargs_param = param
                continue
    
            if param.kind == _VAR_POSITIONAL:
                # Named arguments don't refer to '*args'-like parameters.
                # We only arrive here if the positional arguments ended
                # before reaching the last parameter before *args.
                continue
    
            param_name = param.name
            try:
                arg_val = kwargs.pop(param_name)
            except KeyError:
                # We have no value for this parameter.  It's fine though,
                # if it has a default value, or it is an '*args'-like
                # parameter, left alone by the processing of positional
                # arguments.
                if (not partial and param.kind != _VAR_POSITIONAL and
                                                    param.default is _empty):
                    raise TypeError('missing a required argument: {arg!r}'. \
                                    format(arg=param_name)) from None
    
            else:
                if param.kind == _POSITIONAL_ONLY:
                    # This should never happen in case of a properly built
                    # Signature object (but let's have this check here
                    # to ensure correct behaviour just in case)
                    raise TypeError('{arg!r} parameter is positional only, '
                                    'but was passed as a keyword'. \
                                    format(arg=param.name))
    
                arguments[param_name] = arg_val
    
        if kwargs:
            if kwargs_param is not None:
                # Process our '**kwargs'-like parameter
                arguments[kwargs_param.name] = kwargs
            else:
>               raise TypeError(
                    'got an unexpected keyword argument {arg!r}'.format(
                        arg=next(iter(kwargs))))
E               TypeError: got an unexpected keyword argument 'input'

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3166: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomJigsaw
_______________________________________________________________________________ test_RandomMixUpV2[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomMixUpV2(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomMixUpV2")
    
        init_args = ()
        init_kwargs = {"data_keys": ["input", "class"]}
        call_args = (torch.rand(2, 1, 3, 3), torch.tensor([0, 1]))
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomMixUpV2,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1098: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.mixup.RandomMixUpV2'>, target = 'tensorflow', init_args = (), init_kwargs = {'data_keys': ['input', 'class']}
call_args = (tensor([[[[0.1724, 0.1529, 0.9817],
          [0.5692, 0.1985, 0.8853],
          [0.9207, 0.5330, 0.5372]]],


        [[[0.7551, 0.1111, 0.6869],
          [0.7820, 0.1574, 0.1866],
          [0.5223, 0.5556, 0.8823]]]]), tensor([0, 1]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.1724065 , 0.15287459, 0.9816701 ],
         [0.5692...   [0.5222986 , 0.55560416, 0.8823272 ]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55b3c1aaa380, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 1, 3, 3), d...   [0.5222986 , 0.55560416, 0.8823272 ]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.1724065 , 0.15287459, 0.9816701 ],
         [0.5692...   [0.5222986 , 0.55560416, 0.8823272 ]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 1, 3, 3), d...   [0.5222986 , 0.55560416, 0.8823272 ]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.1724065 , 0.15287459, 0.9816701 ],
         [0.5692...   [0.5222986 , 0.55560416, 0.8823272 ]]]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.1724065 , 0.15287459, 0.9816701 ],
         [0.56921...203 ],
         [0.78195375, 0.15739954, 0.1865797 ],
         [0.5222986 , 0.55560416, 0.8823272 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMixUpV2(lambda_val=None, p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.1724065 , 0.15287459, 0.9816701 ],
       ...986 , 0.55560416, 0.8823272 ]]]], dtype=float32)>, 'params': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 1])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.1724065 , 0.15287459, 0.9816701 ],
       ...03 ],
         [0.78195375, 0.15739954, 0.1865797 ],
         [0.5222986 , 0.55560416, 0.8823272 ]]]], dtype=float32)>}

    def bind(self, /, *args, **kwargs):
        """Get a BoundArguments object, that maps the passed `args`
        and `kwargs` to the function's signature.  Raises `TypeError`
        if the passed arguments can not be bound.
        """
>       return self._bind(args, kwargs)

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(2, 1, 3, 3), dtype=float32, numpy=
array([[[[0.1724065 , 0.15287459, 0.9816701 ],
       ...03 ],
         [0.78195375, 0.15739954, 0.1865797 ],
         [0.5222986 , 0.55560416, 0.8823272 ]]]], dtype=float32)>}

    def _bind(self, args, kwargs, *, partial=False):
        """Private method. Don't use directly."""
    
        arguments = {}
    
        parameters = iter(self.parameters.values())
        parameters_ex = ()
        arg_vals = iter(args)
    
        while True:
            # Let's iterate through the positional arguments and corresponding
            # parameters
            try:
                arg_val = next(arg_vals)
            except StopIteration:
                # No more positional arguments
                try:
                    param = next(parameters)
                except StopIteration:
                    # No more parameters. That's it. Just need to check that
                    # we have no `kwargs` after this while loop
                    break
                else:
                    if param.kind == _VAR_POSITIONAL:
                        # That's OK, just empty *args.  Let's start parsing
                        # kwargs
                        break
                    elif param.name in kwargs:
                        if param.kind == _POSITIONAL_ONLY:
                            msg = '{arg!r} parameter is positional only, ' \
                                  'but was passed as a keyword'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
                        parameters_ex = (param,)
                        break
                    elif (param.kind == _VAR_KEYWORD or
                                                param.default is not _empty):
                        # That's fine too - we have a default value for this
                        # parameter.  So, lets start parsing `kwargs`, starting
                        # with the current parameter
                        parameters_ex = (param,)
                        break
                    else:
                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,
                        # not in `kwargs`
                        if partial:
                            parameters_ex = (param,)
                            break
                        else:
                            msg = 'missing a required argument: {arg!r}'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
            else:
                # We have a positional argument to process
                try:
                    param = next(parameters)
                except StopIteration:
                    raise TypeError('too many positional arguments') from None
                else:
                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):
                        # Looks like we have no parameter for this positional
                        # argument
                        raise TypeError(
                            'too many positional arguments') from None
    
                    if param.kind == _VAR_POSITIONAL:
                        # We have an '*args'-like argument, let's fill it with
                        # all positional arguments we have left and move on to
                        # the next phase
                        values = [arg_val]
                        values.extend(arg_vals)
                        arguments[param.name] = tuple(values)
                        break
    
                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:
                        raise TypeError(
                            'multiple values for argument {arg!r}'.format(
                                arg=param.name)) from None
    
                    arguments[param.name] = arg_val
    
        # Now, we iterate through the remaining parameters to process
        # keyword arguments
        kwargs_param = None
        for param in itertools.chain(parameters_ex, parameters):
            if param.kind == _VAR_KEYWORD:
                # Memorize that we have a '**kwargs'-like parameter
                kwargs_param = param
                continue
    
            if param.kind == _VAR_POSITIONAL:
                # Named arguments don't refer to '*args'-like parameters.
                # We only arrive here if the positional arguments ended
                # before reaching the last parameter before *args.
                continue
    
            param_name = param.name
            try:
                arg_val = kwargs.pop(param_name)
            except KeyError:
                # We have no value for this parameter.  It's fine though,
                # if it has a default value, or it is an '*args'-like
                # parameter, left alone by the processing of positional
                # arguments.
                if (not partial and param.kind != _VAR_POSITIONAL and
                                                    param.default is _empty):
                    raise TypeError('missing a required argument: {arg!r}'. \
                                    format(arg=param_name)) from None
    
            else:
                if param.kind == _POSITIONAL_ONLY:
                    # This should never happen in case of a properly built
                    # Signature object (but let's have this check here
                    # to ensure correct behaviour just in case)
                    raise TypeError('{arg!r} parameter is positional only, '
                                    'but was passed as a keyword'. \
                                    format(arg=param.name))
    
                arguments[param_name] = arg_val
    
        if kwargs:
            if kwargs_param is not None:
                # Process our '**kwargs'-like parameter
                arguments[kwargs_param.name] = kwargs
            else:
>               raise TypeError(
                    'got an unexpected keyword argument {arg!r}'.format(
                        arg=next(iter(kwargs))))
E               TypeError: got an unexpected keyword argument 'input'

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3166: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomMixUpV2
_______________________________________________________________________________ test_RandomMosaic[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomMosaic(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomMosaic")
    
        init_args = ((300, 300),)
        init_kwargs = {"data_keys": ["input", "bbox_xyxy"]}
        call_args = (
            torch.randn(8, 3, 224, 224),
            torch.tensor([[
                [70, 5, 150, 100],
                [60, 180, 175, 220],
            ]]).repeat(8, 1, 1),
        )
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomMosaic,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.mosaic.RandomMosaic'>, target = 'tensorflow', init_args = ((300, 300),), init_kwargs = {'data_keys': ['input', 'bbox_xyxy']}
call_args = (tensor([[[[-0.1830, -0.2479, -0.0979,  ..., -1.3566, -2.1176, -0.4483],
          [ 0.1252, -0.0472,  0.3868,  ...,  ...[ 70,   5, 150, 100],
         [ 60, 180, 175, 220]],

        [[ 70,   5, 150, 100],
         [ 60, 180, 175, 220]]]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0, ..._size=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice)
args = (<tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[-0.18298922, -0.2479091 , -0.09792672, ..., -1.35... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f368ca66e40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0,... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0, ..._size=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice)
v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[-0.18298922, -0.2479091 , -0.09792672, ..., -1.35... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0,... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0, ..._size=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice)
v = None, buffers = None
args = (<tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[-0.18298922, -0.2479091 , -0.09792672, ..., -1.35... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[-0.18298922, -0.2479091 , -0.09792672, ..., -1.356...      [-1.5956074 ,  0.15499817, -0.4331766 , ..., -2.64613   ,
           1.7581006 ,  0.28645512]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMosaic(output_size=(300, 300), mosaic_grid=(2, 2), start_ratio_range=(0.3, 0.7), p=0.7, p_batch=1.0,...ize=(300, 300), min_bbox_size=0.0, padding_mode=constant, resample=bilinear, align_corners=True, cropping_mode=slice),)
kwargs = {'input': <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[-0.18298922, -0.2479091 , -0.09792672, .... [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]],

       [[ 70,   5, 150, 100],
        [ 60, 180, 175, 220]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[-0.18298922, -0.2479091 , -0.09792672, ....     [-1.5956074 ,  0.15499817, -0.4331766 , ..., -2.64613   ,
           1.7581006 ,  0.28645512]]]], dtype=float32)>}

    def bind(self, /, *args, **kwargs):
        """Get a BoundArguments object, that maps the passed `args`
        and `kwargs` to the function's signature.  Raises `TypeError`
        if the passed arguments can not be bound.
        """
>       return self._bind(args, kwargs)

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <Signature (*input, params=None, data_keys=None)>, args = ()
kwargs = {'input': <tf.Tensor: shape=(8, 3, 224, 224), dtype=float32, numpy=
array([[[[-0.18298922, -0.2479091 , -0.09792672, ....     [-1.5956074 ,  0.15499817, -0.4331766 , ..., -2.64613   ,
           1.7581006 ,  0.28645512]]]], dtype=float32)>}

    def _bind(self, args, kwargs, *, partial=False):
        """Private method. Don't use directly."""
    
        arguments = {}
    
        parameters = iter(self.parameters.values())
        parameters_ex = ()
        arg_vals = iter(args)
    
        while True:
            # Let's iterate through the positional arguments and corresponding
            # parameters
            try:
                arg_val = next(arg_vals)
            except StopIteration:
                # No more positional arguments
                try:
                    param = next(parameters)
                except StopIteration:
                    # No more parameters. That's it. Just need to check that
                    # we have no `kwargs` after this while loop
                    break
                else:
                    if param.kind == _VAR_POSITIONAL:
                        # That's OK, just empty *args.  Let's start parsing
                        # kwargs
                        break
                    elif param.name in kwargs:
                        if param.kind == _POSITIONAL_ONLY:
                            msg = '{arg!r} parameter is positional only, ' \
                                  'but was passed as a keyword'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
                        parameters_ex = (param,)
                        break
                    elif (param.kind == _VAR_KEYWORD or
                                                param.default is not _empty):
                        # That's fine too - we have a default value for this
                        # parameter.  So, lets start parsing `kwargs`, starting
                        # with the current parameter
                        parameters_ex = (param,)
                        break
                    else:
                        # No default, not VAR_KEYWORD, not VAR_POSITIONAL,
                        # not in `kwargs`
                        if partial:
                            parameters_ex = (param,)
                            break
                        else:
                            msg = 'missing a required argument: {arg!r}'
                            msg = msg.format(arg=param.name)
                            raise TypeError(msg) from None
            else:
                # We have a positional argument to process
                try:
                    param = next(parameters)
                except StopIteration:
                    raise TypeError('too many positional arguments') from None
                else:
                    if param.kind in (_VAR_KEYWORD, _KEYWORD_ONLY):
                        # Looks like we have no parameter for this positional
                        # argument
                        raise TypeError(
                            'too many positional arguments') from None
    
                    if param.kind == _VAR_POSITIONAL:
                        # We have an '*args'-like argument, let's fill it with
                        # all positional arguments we have left and move on to
                        # the next phase
                        values = [arg_val]
                        values.extend(arg_vals)
                        arguments[param.name] = tuple(values)
                        break
    
                    if param.name in kwargs and param.kind != _POSITIONAL_ONLY:
                        raise TypeError(
                            'multiple values for argument {arg!r}'.format(
                                arg=param.name)) from None
    
                    arguments[param.name] = arg_val
    
        # Now, we iterate through the remaining parameters to process
        # keyword arguments
        kwargs_param = None
        for param in itertools.chain(parameters_ex, parameters):
            if param.kind == _VAR_KEYWORD:
                # Memorize that we have a '**kwargs'-like parameter
                kwargs_param = param
                continue
    
            if param.kind == _VAR_POSITIONAL:
                # Named arguments don't refer to '*args'-like parameters.
                # We only arrive here if the positional arguments ended
                # before reaching the last parameter before *args.
                continue
    
            param_name = param.name
            try:
                arg_val = kwargs.pop(param_name)
            except KeyError:
                # We have no value for this parameter.  It's fine though,
                # if it has a default value, or it is an '*args'-like
                # parameter, left alone by the processing of positional
                # arguments.
                if (not partial and param.kind != _VAR_POSITIONAL and
                                                    param.default is _empty):
                    raise TypeError('missing a required argument: {arg!r}'. \
                                    format(arg=param_name)) from None
    
            else:
                if param.kind == _POSITIONAL_ONLY:
                    # This should never happen in case of a properly built
                    # Signature object (but let's have this check here
                    # to ensure correct behaviour just in case)
                    raise TypeError('{arg!r} parameter is positional only, '
                                    'but was passed as a keyword'. \
                                    format(arg=param.name))
    
                arguments[param_name] = arg_val
    
        if kwargs:
            if kwargs_param is not None:
                # Process our '**kwargs'-like parameter
                arguments[kwargs_param.name] = kwargs
            else:
>               raise TypeError(
                    'got an unexpected keyword argument {arg!r}'.format(
                        arg=next(iter(kwargs))))
E               TypeError: got an unexpected keyword argument 'input'

/opt/miniconda/envs/multienv/lib/python3.10/inspect.py:3166: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomMosaic
___________________________________________________________________________ test_RandomTransplantation[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomTransplantation(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomTransplantation")
    
        init_args = ()
        init_kwargs = {"p": 1.}
        call_args = (torch.randn(2, 3, 5, 5), torch.randint(0, 3, (2, 5, 5)))
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomTransplantation,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1144: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.mix.transplantation.RandomTransplantation'>, target = 'tensorflow', init_args = (), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[-1.1705,  1.1574, -0.1502, -1.0095, -1.6022],
          [-1.5677,  0.9013,  0.0881,  0.1934, -1.4528],
   ...1, 1, 2, 2],
         [2, 1, 1, 1, 2],
         [0, 1, 2, 1, 1],
         [0, 2, 0, 0, 2],
         [0, 0, 1, 2, 0]]]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.170468  ,  1.1574354 , -0.15015525, -1.0095383 ,
 ...1, 1, 1, 2, 2],
        [2, 1, 1, 1, 2],
        [0, 1, 2, 1, 1],
        [0, 2, 0, 0, 2],
        [0, 0, 1, 2, 0]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f368cbd2240, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=floa...1, 1, 1, 2, 2],
        [2, 1, 1, 1, 2],
        [0, 1, 2, 1, 1],
        [0, 2, 0, 0, 2],
        [0, 0, 1, 2, 0]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.170468  ,  1.1574354 , -0.15015525, -1.0095383 ,
 ...1, 1, 1, 2, 2],
        [2, 1, 1, 1, 2],
        [0, 1, 2, 1, 1],
        [0, 2, 0, 0, 2],
        [0, 0, 1, 2, 0]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=floa...1, 1, 1, 2, 2],
        [2, 1, 1, 1, 2],
        [0, 1, 2, 1, 1],
        [0, 2, 0, 0, 2],
        [0, 0, 1, 2, 0]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.170468  ,  1.1574354 , -0.15015525, -1.0095383 ,
 ...1, 1, 1, 2, 2],
        [2, 1, 1, 1, 2],
        [0, 1, 2, 1, 1],
        [0, 2, 0, 0, 2],
        [0, 0, 1, 2, 0]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.170468  ,  1.1574354 , -0.15015525, -1.0095383 ,
  ...  0.43476948],
         [-0.7786411 , -0.06321806, -1.3121114 , -1.3296562 ,
          -1.6573142 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.170468  ,  1.1574354 , -0.15015525, -1.00...1, 1, 1, 2, 2],
        [2, 1, 1, 1, 2],
        [0, 1, 2, 1, 1],
        [0, 2, 0, 0, 2],
        [0, 0, 1, 2, 0]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation(p=1.0, p_batch=1.0, same_on_batch=False)
params = <tf.Tensor: shape=(2, 5, 5), dtype=int64, numpy=
array([[[2, 1, 1, 2, 1],
        [0, 2, 1, 0, 1],
        [1, 2, 2, 2...[1, 1, 1, 2, 2],
        [2, 1, 1, 1, 2],
        [0, 1, 2, 1, 1],
        [0, 2, 0, 0, 2],
        [0, 0, 1, 2, 0]]])>
data_keys = None, input = ()
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[-1.170468  ,  1.1574354 , -0.15015525, -1.00... 0.43476948],
         [-0.7786411 , -0.06321806, -1.3121114 , -1.3296562 ,
          -1.6573142 ]]]], dtype=float32)>}
tensorflow_get_item = <function tensorflow_get_item at 0x7f368ccae050>, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f368cce5d80>
tensorflow_clone_frnt_ = <function tensorflow_clone_frnt_ at 0x7f368cce5090>, tensorflow__validate_input_dtype = <function tensorflow__validate_input_dtype at 0x7f368c24ba30>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f368ccf7b50>, keys = [<tensorflow_DataKey.IMAGE: 0>, <tensorflow_DataKey.MASK: 1>]

    def call(self, *input, params=None, data_keys=None, **kwargs):
        from ....constants import tensorflow_DataKey
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...utils.helpers import tensorflow__validate_input_dtype
        from .....ivy.functional.frontends.torch.tensor import (
            tensorflow_index_put_frnt_,
        )
    
        keys: typing.Any
        if data_keys is None:
            keys = self.data_keys
        else:
            keys = [tensorflow_DataKey.get(inp) for inp in data_keys]
        if params is None:
            mask: typing.Any = tensorflow_get_item(
                input, keys.index(tensorflow_DataKey.MASK)
            )
            self._params = self.forward_parameters(tensorflow_shape_frnt_(mask))
        else:
            self._params = params
>       if any(
            k not in self._params
            for k in ["acceptor_indices", "donor_indices", "selection"]
        ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:242: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f368cd07910>

    if any(
>       k not in self._params
        for k in ["acceptor_indices", "donor_indices", "selection"]
    ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(5, 5), dtype=int64, numpy=
array([[2, 1, 1, 2, 1],
       [0, 2, 1, 0, 1],
       [1, 2, 2, 2, 1],
       [2, 0, 2, 1, 0],
       [0, 0, 1, 1, 0]])>, 'acceptor_indices')
kwargs = {}, arg = 'acceptor_indices'

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       TypeError: Exception encountered when calling tensorflow_RandomTransplantation.call().
E       
E       [1mdata type 'acceptor_indices' not understood[0m
E       
E       Arguments received by tensorflow_RandomTransplantation.call():
E         â€¢ input=<class 'inspect._empty'>
E         â€¢ params=tf.Tensor(shape=(2, 5, 5), dtype=int64)
E         â€¢ data_keys=None
E         â€¢ kwargs={'input': 'tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)'}

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomTransplantation
_______________________________________________________________________________ test_CenterCrop3D[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_CenterCrop3D(target_framework, mode, backend_compile):
        print("kornia.augmentation.CenterCrop3D")
    
        init_args = (2,)
        init_kwargs = {"p": 1.}
        call_args = (torch.randn(1, 1, 2, 4, 6),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.CenterCrop3D,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1164: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._3d.geometric.center_crop.CenterCrop3D'>, target = 'tensorflow', init_args = (2,), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[[ 1.1705,  0.1722,  1.2092, -0.4275,  1.2249,  2.6308],
           [ 1.0623,  0.2027,  0.2196, -1.1999, -1... -1.1135, -1.9959, -2.5128,  1.1125,  0.4904],
           [ 0.5841,  1.0833, -0.8188, -0.6766,  1.4980,  2.0478]]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop3D(p=1.0, p_batch=1.0, same_on_batch=True, align_corners=True, resample=bilinear)
args = (<tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -0.4274710...       [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f368c2a0c40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_CenterCrop3D(p=1.0, p_batch=1.0, same_on_batch=True, align_corners=True, resample=bilinear), <tf.Tensor: s...        [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop3D(p=1.0, p_batch=1.0, same_on_batch=True, align_corners=True, resample=bilinear), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -0.4274710...       [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_CenterCrop3D(p=1.0, p_batch=1.0, same_on_batch=True, align_corners=True, resample=bilinear), <tf.Tensor: s...        [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop3D(p=1.0, p_batch=1.0, same_on_batch=True, align_corners=True, resample=bilinear), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -0.4274710...       [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -0.42747107...         [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_CenterCrop3D(p=1.0, p_batch=1.0, same_on_batch=True, align_corners=True, resample=bilinear),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -...        [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop3D(p=1.0, p_batch=1.0, same_on_batch=True, align_corners=True, resample=bilinear)
input = <tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -0.42747107...         [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...,
        [3, 2, 0],
        [2, 2, 0],
        [2, 1, 1],
        [3, 1, 1],
        [3, 2, 1],
        [2, 2, 1]]])>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f368cc9e9e0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36aa0951b0>
tensor = <function tensorflow_tensor_frnt at 0x7f368c20f640>
in_tensor = <tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -0.42747107...         [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 2, 4, 6]), batch_shape = ivy.frontends.torch.Size([1, 1, 2, 4, 6]), flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop3D(p=1.0, p_batch=1.0, same_on_batch=True, align_corners=True, resample=bilinear)
in_tensor = <tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -0.42747107...         [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...,
        [3, 2, 0],
        [2, 2, 0],
        [2, 1, 1],
        [3, 1, 1],
        [3, 2, 1],
        [2, 2, 1]]])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/base.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop3D(p=1.0, p_batch=1.0, same_on_batch=True, align_corners=True, resample=bilinear)
input = <tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -0.42747107...         [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...,
        [3, 2, 0],
        [2, 2, 0],
        [2, 1, 1],
        [3, 1, 1],
        [3, 2, 1],
        [2, 2, 1]]])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}
transform = <tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=
array([[[ 1.0000000e+00,  8.3266727e-17,  5.5511151e-17, -2.0000000...+00, -2.3111159e-32],
        [-5.5511151e-17, -5.5511151e-17,  5.5511151e-17,  1.0000000e+00]]],
      dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f368cc9e9e0>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f368cc9fd00>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f36aa209000>, tensorflow_get_item = <function tensorflow_get_item at 0x7f369fa12290>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f368c8df880>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f36aa20a950>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f36a4f1f880>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_CenterCrop3D(p=1.0, p_batch=1.0, same_on_batch=True, align_corners=True, resample=bilinear)
input = <tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -0.42747107...         [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...,
        [3, 2, 0],
        [2, 2, 0],
        [2, 1, 1],
        [3, 1, 1],
        [3, 2, 1],
        [2, 2, 1]]])>}
flags = {'align_corners': True, 'resample': <tensorflow_Resample.BILINEAR: 1>}
transform = <tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=
array([[[ 1.0000000e+00,  8.3266727e-17,  5.5511151e-17, -2.0000000...+00, -2.3111159e-32],
        [-5.5511151e-17, -5.5511151e-17,  5.5511151e-17,  1.0000000e+00]]],
      dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....geometry.transform.crop3d import tensorflow_crop_by_transform_mat3d
    
        transform = cast(tensorflow.Variable, transform)
>       return tensorflow_crop_by_transform_mat3d(
            input,
            transform,
            self.size,
            mode=flags["resample"].name.lower(),
            align_corners=flags["align_corners"],
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/geometric/center_crop.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -0.42747107...         [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>
transform = <tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=
array([[[ 1.0000000e+00,  8.3266727e-17,  5.5511151e-17, -2.0000000...+00, -2.3111159e-32],
        [-5.5511151e-17, -5.5511151e-17,  5.5511151e-17,  1.0000000e+00]]],
      dtype=float32)>
out_size = (2, 2, 2), mode = 'bilinear', padding_mode = 'zeros', align_corners = True

    def tensorflow_crop_by_transform_mat3d(
        tensor,
        transform,
        out_size,
        mode="bilinear",
        padding_mode="zeros",
        align_corners=True,
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .imgwarp import tensorflow_warp_affine3d
    
        dst_trans_src = tensorflow_expand_frnt_(
            transform, tensorflow_shape_frnt_(tensor)[0], -1, -1
        )
>       patches: typing.Any = tensorflow_warp_affine3d(
            tensor,
            dst_trans_src[:, :3, :],
            out_size,
            flags=mode,
            padding_mode=padding_mode,
            align_corners=align_corners,
        )

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/crop3d.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

src = <tf.Tensor: shape=(1, 1, 2, 4, 6), dtype=float32, numpy=
array([[[[[ 1.1705312 ,  0.1721829 ,  1.2092106 , -0.42747107...         [ 0.58410114,  1.08328   , -0.8187895 , -0.6765712 ,
            1.4979883 ,  2.0477512 ]]]]], dtype=float32)>
M = <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=
array([[[ 1.0000000e+00,  8.3266727e-17,  5.5511151e-17, -2.0000000...+00, -1.0000000e+00],
        [ 3.7007435e-17, -5.5511151e-17,  1.0000000e+00, -2.3111159e-32]]],
      dtype=float32)>
dsize = (2, 2, 2), flags = 'bilinear', padding_mode = 'zeros', align_corners = True

    def tensorflow_warp_affine3d(
        src, M, dsize, flags="bilinear", padding_mode="zeros", align_corners=True
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ..conversions import tensorflow_convert_affinematrix_to_homography3d
        from ..conversions import tensorflow_normalize_homography3d
        from ...utils.helpers import tensorflow__torch_inverse_cast
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_affine_grid_frnt,
        )
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_grid_sample_frnt,
        )
    
        if len(tensorflow_shape_frnt_(src)) != 5:
            raise AssertionError(tensorflow_shape_frnt_(src))
        if not (
            len(tensorflow_shape_frnt_(M)) == 3 and tensorflow_shape_frnt_(M)[-2:] == (3, 4)
        ):
            raise AssertionError(tensorflow_shape_frnt_(M))
        if len(dsize) != 3:
            raise AssertionError(dsize)
        B, C, D, H, W = tensorflow_size_frnt_(src)
        size_src: typing.Any = (D, H, W)
        size_out: typing.Any = dsize
>       M_4x4 = tensorflow_convert_affinematrix_to_homography3d(M)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/imgwarp.py:422: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=
array([[[ 1.0000000e+00,  8.3266727e-17,  5.5511151e-17, -2.0000000...+00, -1.0000000e+00],
        [ 3.7007435e-17, -5.5511151e-17,  1.0000000e+00, -2.3111159e-32]]],
      dtype=float32)>

    def tensorflow_convert_affinematrix_to_homography3d(A):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        if not isinstance(A, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(A)}")
        if not (
            len(tensorflow_shape_frnt_(A)) == 3 and tensorflow_shape_frnt_(A)[-2:] == (3, 4)
        ):
            raise ValueError(
                f"Input matrix must be a Bx3x4 tensor. Got {tensorflow_shape_frnt_(A)}"
            )
>       return tensorflow__convert_affinematrix_to_homography_impl(A)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=
array([[[ 1.0000000e+00,  8.3266727e-17,  5.5511151e-17, -2.0000000...+00, -1.0000000e+00],
        [ 3.7007435e-17, -5.5511151e-17,  1.0000000e+00, -2.3111159e-32]]],
      dtype=float32)>

    def tensorflow__convert_affinematrix_to_homography_impl(A):
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import pad
    
>       H: typing.Any = pad(A, [0, 0, 0, 1], "constant", value=0.0)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=
array([[[ 1.0000000e+00,  8.3266727e-17,  5.5511151e-17, -2.0000000...+00, -1.0000000e+00],
        [ 3.7007435e-17, -5.5511151e-17,  1.0000000e+00, -2.3111159e-32]]],
      dtype=float32)>
pad = ((0, 0), (0, 1), (0, 0)), mode = 'constant', value = 0.0

    def tensorflow_pad_frnt(input, pad, mode="constant", value=0):
        from .....backends.tensorflow.general import tensorflow_get_item
        from .....ivy.general import tensorflow_set_item_bknd
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.experimental.manipulation import tensorflow_pad
    
        if any([(pad_value < 0) for pad_value in pad]):
            pad = list(pad)
            slices = []
            for n in reversed(range(len(pad) // 2)):
                i = n * 2
                j = i + 1
                start = None
                stop = None
                if tensorflow_get_item(pad, i) < 0:
                    start = -tensorflow_get_item(pad, i)
                    pad = tensorflow_set_item_bknd(pad, i, 0)
                if tensorflow_get_item(pad, j) < 0:
                    stop = tensorflow_get_item(pad, j)
                    pad = tensorflow_set_item_bknd(pad, j, 0)
                slices.append(slice(start, stop))
            ndim = len(tensorflow_shape_frnt_(input))
            while len(slices) < ndim:
                slices.insert(0, slice(None))
            input = tensorflow_get_item(input, tuple(slices))
        value = 0 if value is None else value
        mode_dict = {
            "constant": "constant",
            "reflect": "reflect",
            "replicate": "edge",
            "circular": "wrap",
        }
        if mode not in mode_dict:
            raise ValueError(f"Unsupported padding mode: {mode}")
        pad = tensorflow__handle_padding_shape_frnt(
            pad, len(tensorflow_shape_frnt_(input)), mode
        )
        order = 0, 2, 3, 1
>       pad = tuple(pad[i] for i in order)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f36a41ab850>

>   pad = tuple(pad[i] for i in order)
E   IndexError: Exception encountered when calling tensorflow_CenterCrop3D.call().
E   
E   [1mtuple index out of range[0m
E   
E   Arguments received by tensorflow_CenterCrop3D.call():
E     â€¢ input=tf.Tensor(shape=(1, 1, 2, 4, 6), dtype=float32)
E     â€¢ params=None
E     â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.CenterCrop3D
______________________________________________________________________________ test_RandomAffine3D[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomAffine3D(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomAffine3D")
    
        init_args = ((15., 20., 20.),)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 3, 3, 3),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomAffine3D,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1184: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._3d.geometric.affine.RandomAffine3D'>, target = 'tensorflow', init_args = ((15.0, 20.0, 20.0),), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[[0.2393, 0.5286, 0.7996],
           [0.7568, 0.2669, 0.4487],
           [0.6465, 0.7700, 0.3156]],

    ...,

          [[0.4922, 0.8348, 0.6917],
           [0.3019, 0.9317, 0.7531],
           [0.3622, 0.7430, 0.8199]]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine3D(degrees=(15.0, 20.0, 20.0), shears=None, translate=None, scale=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.23931742, 0.5286037 , 0.79960567],
          [0...,
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f368c67ba40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomAffine3D(degrees=(15.0, 20.0, 20.0), shears=None, translate=None, scale=None, p=1.0, p_batch=1.0, sa...],
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine3D(degrees=(15.0, 20.0, 20.0), shears=None, translate=None, scale=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False), v = None
buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.23931742, 0.5286037 , 0.79960567],
          [0...,
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomAffine3D(degrees=(15.0, 20.0, 20.0), shears=None, translate=None, scale=None, p=1.0, p_batch=1.0, sa...],
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine3D(degrees=(15.0, 20.0, 20.0), shears=None, translate=None, scale=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False), v = None
buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.23931742, 0.5286037 , 0.79960567],
          [0...,
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.23931742, 0.5286037 , 0.79960567],
          [0.... ],
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomAffine3D(degrees=(15.0, 20.0, 20.0), shears=None, translate=None, scale=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.23931742, 0.5286037 , 0.79960567],
   ...],
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine3D(degrees=(15.0, 20.0, 20.0), shears=None, translate=None, scale=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.23931742, 0.5286037 , 0.79960567],
          [0.... ],
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>
params = {'angles': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-6.562972 , 14.45015  , -1.9256363]], dtype=float32)... 1.]], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 1, 3, 3, 3])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f368c218670>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f369c3adf30>
tensor = <function tensorflow_tensor_frnt at 0x7f368cad6cb0>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.23931742, 0.5286037 , 0.79960567],
          [0.... ],
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 3, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3, 3]), flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine3D(degrees=(15.0, 20.0, 20.0), shears=None, translate=None, scale=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.23931742, 0.5286037 , 0.79960567],
          [0.... ],
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>
params = {'angles': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-6.562972 , 14.45015  , -1.9256363]], dtype=float32)... 1.]], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 1, 3, 3, 3])>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/base.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine3D(degrees=(15.0, 20.0, 20.0), shears=None, translate=None, scale=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.23931742, 0.5286037 , 0.79960567],
          [0.... ],
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>
params = {'angles': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-6.562972 , 14.45015  , -1.9256363]], dtype=float32)... 1.]], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 1, 3, 3, 3])>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/base.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomAffine3D(degrees=(15.0, 20.0, 20.0), shears=None, translate=None, scale=None, p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.23931742, 0.5286037 , 0.79960567],
          [0.... ],
          [0.3019067 , 0.9316663 , 0.75306   ],
          [0.36222595, 0.7430475 , 0.8198767 ]]]]], dtype=float32)>
params = {'angles': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-6.562972 , 14.45015  , -1.9256363]], dtype=float32)... 1.]], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 1, 3, 3, 3])>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....geometry.transform.imgwarp import tensorflow_get_affine_matrix3d
        from ....geometry.conversions import tensorflow_deg2rad
    
        transform: typing.Any = tensorflow_to_frnt_(
>           tensorflow_get_affine_matrix3d(
                params["translations"],
                params["center"],
                params["scale"],
                params["angles"],
                tensorflow_deg2rad(params["sxy"]),
                tensorflow_deg2rad(params["sxz"]),
                tensorflow_deg2rad(params["syx"]),
                tensorflow_deg2rad(params["syz"]),
                tensorflow_deg2rad(params["szx"]),
                tensorflow_deg2rad(params["szy"]),
            ),
            input,
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/geometric/affine.py:67: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

translations = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0., 0., 0.]], dtype=float32)>, center = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1., 1., 1.]], dtype=float32)>
scale = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1., 1., 1.]], dtype=float32)>
angles = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-6.562972 , 14.45015  , -1.9256363]], dtype=float32)>, sxy = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>
sxz = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, syx = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>
syz = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, szx = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>
szy = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>

    def tensorflow_get_affine_matrix3d(
        translations,
        center,
        scale,
        angles,
        sxy=None,
        sxz=None,
        syx=None,
        syz=None,
        szx=None,
        szy=None,
    ):
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..conversions import tensorflow_convert_affinematrix_to_homography3d
    
>       transform: typing.Any = tensorflow_get_projective_transform(center, -angles, scale)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/imgwarp.py:411: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

center = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1., 1., 1.]], dtype=float32)>
angles = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[  6.562972 , -14.45015  ,   1.9256363]], dtype=float32)>
scales = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1., 1., 1.]], dtype=float32)>

    def tensorflow_get_projective_transform(center, angles, scales):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..conversions import tensorflow_deg2rad
        from ..conversions import tensorflow_axis_angle_to_rotation_matrix
        from ...utils.misc import tensorflow_eye_like
        from ....ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...utils.helpers import tensorflow__torch_inverse_cast
        from ....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_zeros_like_frnt,
        )
        from ..conversions import tensorflow_convert_affinematrix_to_homography3d
    
        if not (
            len(tensorflow_shape_frnt_(center)) == 2
            and tensorflow_shape_frnt_(center)[-1] == 3
        ):
            raise AssertionError(tensorflow_shape_frnt_(center))
        if not (
            len(tensorflow_shape_frnt_(angles)) == 2
            and tensorflow_shape_frnt_(angles)[-1] == 3
        ):
            raise AssertionError(tensorflow_shape_frnt_(angles))
        if center.device != angles.device:
            raise AssertionError(center.device, angles.device)
        if center.dtype != angles.dtype:
            raise AssertionError(center.dtype, angles.dtype)
        axis_angle_rad: typing.Any = tensorflow_deg2rad(angles)
        rmat: typing.Any = tensorflow_axis_angle_to_rotation_matrix(axis_angle_rad)
        scaling_matrix: typing.Any = tensorflow_eye_like(3, rmat)
        scaling_matrix = scaling_matrix * tensorflow_unsqueeze_frnt_(scales, dim=1)
        rmat = rmat @ tensorflow_to_frnt_(scaling_matrix, rmat)
        from_origin_mat = tensorflow_eye_like(4, rmat, shared_memory=False)
        from_origin_mat = tensorflow_set_item_bknd(
            from_origin_mat,
            (..., slice(None, 3, None), -1),
            from_origin_mat[..., :3, -1] + center,
        )
        to_origin_mat = tensorflow_clone_frnt_(from_origin_mat)
        to_origin_mat = tensorflow__torch_inverse_cast(from_origin_mat)
        proj_mat = tensorflow_projection_from_Rt(
            rmat, tensorflow_zeros_like_frnt(center)[..., None]
        )
>       proj_mat = tensorflow_convert_affinematrix_to_homography3d(proj_mat)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/imgwarp.py:620: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=
array([[[ 0.96784157, -0.04752491, -0.24702951,  0.        ],
     ...92076, -0.11727513,  0.        ],
        [ 0.25085428,  0.10885384,  0.96188486,  0.        ]]],
      dtype=float32)>

    def tensorflow_convert_affinematrix_to_homography3d(A):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        if not isinstance(A, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(A)}")
        if not (
            len(tensorflow_shape_frnt_(A)) == 3 and tensorflow_shape_frnt_(A)[-2:] == (3, 4)
        ):
            raise ValueError(
                f"Input matrix must be a Bx3x4 tensor. Got {tensorflow_shape_frnt_(A)}"
            )
>       return tensorflow__convert_affinematrix_to_homography_impl(A)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=
array([[[ 0.96784157, -0.04752491, -0.24702951,  0.        ],
     ...92076, -0.11727513,  0.        ],
        [ 0.25085428,  0.10885384,  0.96188486,  0.        ]]],
      dtype=float32)>

    def tensorflow__convert_affinematrix_to_homography_impl(A):
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import pad
    
>       H: typing.Any = pad(A, [0, 0, 0, 1], "constant", value=0.0)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=
array([[[ 0.96784157, -0.04752491, -0.24702951,  0.        ],
     ...92076, -0.11727513,  0.        ],
        [ 0.25085428,  0.10885384,  0.96188486,  0.        ]]],
      dtype=float32)>
pad = ((0, 0), (0, 1), (0, 0)), mode = 'constant', value = 0.0

    def tensorflow_pad_frnt(input, pad, mode="constant", value=0):
        from .....backends.tensorflow.general import tensorflow_get_item
        from .....ivy.general import tensorflow_set_item_bknd
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.experimental.manipulation import tensorflow_pad
    
        if any([(pad_value < 0) for pad_value in pad]):
            pad = list(pad)
            slices = []
            for n in reversed(range(len(pad) // 2)):
                i = n * 2
                j = i + 1
                start = None
                stop = None
                if tensorflow_get_item(pad, i) < 0:
                    start = -tensorflow_get_item(pad, i)
                    pad = tensorflow_set_item_bknd(pad, i, 0)
                if tensorflow_get_item(pad, j) < 0:
                    stop = tensorflow_get_item(pad, j)
                    pad = tensorflow_set_item_bknd(pad, j, 0)
                slices.append(slice(start, stop))
            ndim = len(tensorflow_shape_frnt_(input))
            while len(slices) < ndim:
                slices.insert(0, slice(None))
            input = tensorflow_get_item(input, tuple(slices))
        value = 0 if value is None else value
        mode_dict = {
            "constant": "constant",
            "reflect": "reflect",
            "replicate": "edge",
            "circular": "wrap",
        }
        if mode not in mode_dict:
            raise ValueError(f"Unsupported padding mode: {mode}")
        pad = tensorflow__handle_padding_shape_frnt(
            pad, len(tensorflow_shape_frnt_(input)), mode
        )
        order = 0, 2, 3, 1
>       pad = tuple(pad[i] for i in order)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f369c807d00>

>   pad = tuple(pad[i] for i in order)
E   IndexError: Exception encountered when calling tensorflow_RandomAffine3D.call().
E   
E   [1mtuple index out of range[0m
E   
E   Arguments received by tensorflow_RandomAffine3D.call():
E     â€¢ input=tf.Tensor(shape=(1, 1, 3, 3, 3), dtype=float32)
E     â€¢ params=None
E     â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomAffine3D
_______________________________________________________________________________ test_RandomCrop3D[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomCrop3D(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomCrop3D")
    
        init_args = ((2, 2, 2),)
        init_kwargs = {"p": 1.}
        call_args = (torch.randn(1, 1, 3, 3, 3),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomCrop3D,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1204: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._3d.geometric.crop.RandomCrop3D'>, target = 'tensorflow', init_args = ((2, 2, 2),), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[[ 4.2671e-02, -1.5965e+00,  1.0471e+00],
           [-1.6812e-01, -1.2218e+00, -1.4049e+00],
           [-...877e-02],
           [ 1.6344e+00, -4.8288e-01, -2.0012e+00],
           [ 4.8195e-01, -1.9491e+00, -1.2521e+00]]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop3D(crop_size=(2, 2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2, 2), padding=None, pad_if_needed=False, padding_mode=constant, fill=0, resample=bilinear, align_corners=True)
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
...4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f368cbd3840, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomCrop3D(crop_size=(2, 2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2, 2), padding=None, p...-4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop3D(crop_size=(2, 2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2, 2), padding=None, pad_if_needed=False, padding_mode=constant, fill=0, resample=bilinear, align_corners=True)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
...4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomCrop3D(crop_size=(2, 2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2, 2), padding=None, p...-4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop3D(crop_size=(2, 2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2, 2), padding=None, pad_if_needed=False, padding_mode=constant, fill=0, resample=bilinear, align_corners=True)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
...4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
 ... -4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomCrop3D(crop_size=(2, 2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2, 2), padding=None, pad_if_needed=False, padding_mode=constant, fill=0, resample=bilinear, align_corners=True),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.04714...-4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop3D(crop_size=(2, 2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2, 2), padding=None, pad_if_needed=False, padding_mode=constant, fill=0, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
 ... -4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>
params = None, kwargs = {}

    def call(self, input, params=None, **kwargs):
        input = self.precrop_padding(input)
>       return super().call(input, params)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/geometric/crop.py:168: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop3D(crop_size=(2, 2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2, 2), padding=None, pad_if_needed=False, padding_mode=constant, fill=0, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
 ... -4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...1., 2., 0.],
        [1., 1., 1.],
        [2., 1., 1.],
        [2., 2., 1.],
        [1., 2., 1.]]], dtype=float32)>}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f368c3f1d80>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3684de4d30>
tensor = <function tensorflow_tensor_frnt at 0x7f3684fdaa70>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
 ... -4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 3, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3, 3])
flags = {'align_corners': True, 'fill': 0, 'pad_if_needed': False, 'padding': None, ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop3D(crop_size=(2, 2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2, 2), padding=None, pad_if_needed=False, padding_mode=constant, fill=0, resample=bilinear, align_corners=True)
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
 ... -4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...1., 2., 0.],
        [1., 1., 1.],
        [2., 1., 1.],
        [2., 2., 1.],
        [1., 2., 1.]]], dtype=float32)>}
flags = {'align_corners': True, 'fill': 0, 'pad_if_needed': False, 'padding': None, ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/base.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop3D(crop_size=(2, 2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2, 2), padding=None, pad_if_needed=False, padding_mode=constant, fill=0, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
 ... -4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...1., 2., 0.],
        [1., 1., 1.],
        [2., 1., 1.],
        [2., 2., 1.],
        [1., 2., 1.]]], dtype=float32)>}
flags = {'align_corners': True, 'fill': 0, 'pad_if_needed': False, 'padding': None, ...}
transform = <tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=
array([[[ 1.000000e+00, -7.401487e-17,  5.551115e-17, -1.000000e+00...0000e+00,  0.000000e+00],
        [-5.551115e-17, -5.551115e-17,  5.551115e-17,  1.000000e+00]]],
      dtype=float32)>
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f368c3f1d80>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f368c3f1870>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f368c58c820>, tensorflow_get_item = <function tensorflow_get_item at 0x7f3684d052d0>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f3684d58790>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f368c58c9d0>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f368c58cb80>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomCrop3D(crop_size=(2, 2, 2), p=1.0, p_batch=1.0, same_on_batch=False, size=(2, 2, 2), padding=None, pad_if_needed=False, padding_mode=constant, fill=0, resample=bilinear, align_corners=True)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
 ... -4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'dst': <tf.Tensor: shape=(1, ...1., 2., 0.],
        [1., 1., 1.],
        [2., 1., 1.],
        [2., 2., 1.],
        [1., 2., 1.]]], dtype=float32)>}
flags = {'align_corners': True, 'fill': 0, 'pad_if_needed': False, 'padding': None, ...}
transform = <tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=
array([[[ 1.000000e+00, -7.401487e-17,  5.551115e-17, -1.000000e+00...0000e+00,  0.000000e+00],
        [-5.551115e-17, -5.551115e-17,  5.551115e-17,  1.000000e+00]]],
      dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from ....geometry.transform.crop3d import tensorflow_crop_by_transform_mat3d
    
        if not isinstance(transform, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(
                f"Expected the transform to be a Tensor. Gotcha {type(transform)}"
            )
>       return tensorflow_crop_by_transform_mat3d(
            input,
            transform,
            flags["size"],
            mode=flags["resample"].name.lower(),
            align_corners=flags["align_corners"],
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/geometric/crop.py:158: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
 ... -4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>
transform = <tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=
array([[[ 1.000000e+00, -7.401487e-17,  5.551115e-17, -1.000000e+00...0000e+00,  0.000000e+00],
        [-5.551115e-17, -5.551115e-17,  5.551115e-17,  1.000000e+00]]],
      dtype=float32)>
out_size = (2, 2, 2), mode = 'bilinear', padding_mode = 'zeros', align_corners = True

    def tensorflow_crop_by_transform_mat3d(
        tensor,
        transform,
        out_size,
        mode="bilinear",
        padding_mode="zeros",
        align_corners=True,
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .imgwarp import tensorflow_warp_affine3d
    
        dst_trans_src = tensorflow_expand_frnt_(
            transform, tensorflow_shape_frnt_(tensor)[0], -1, -1
        )
>       patches: typing.Any = tensorflow_warp_affine3d(
            tensor,
            dst_trans_src[:, :3, :],
            out_size,
            flags=mode,
            padding_mode=padding_mode,
            align_corners=align_corners,
        )

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/crop3d.py:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

src = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[ 4.2671368e-02, -1.5965255e+00,  1.0471448e+00],
 ... -4.8287839e-01, -2.0011749e+00],
          [ 4.8194900e-01, -1.9491383e+00, -1.2520510e+00]]]]],
      dtype=float32)>
M = <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=
array([[[ 1.000000e+00, -7.401487e-17,  5.551115e-17, -1.000000e+00...0000e+00, -1.000000e+00],
        [ 1.110223e-16, -1.110223e-16,  1.000000e+00,  0.000000e+00]]],
      dtype=float32)>
dsize = (2, 2, 2), flags = 'bilinear', padding_mode = 'zeros', align_corners = True

    def tensorflow_warp_affine3d(
        src, M, dsize, flags="bilinear", padding_mode="zeros", align_corners=True
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ..conversions import tensorflow_convert_affinematrix_to_homography3d
        from ..conversions import tensorflow_normalize_homography3d
        from ...utils.helpers import tensorflow__torch_inverse_cast
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_affine_grid_frnt,
        )
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_grid_sample_frnt,
        )
    
        if len(tensorflow_shape_frnt_(src)) != 5:
            raise AssertionError(tensorflow_shape_frnt_(src))
        if not (
            len(tensorflow_shape_frnt_(M)) == 3 and tensorflow_shape_frnt_(M)[-2:] == (3, 4)
        ):
            raise AssertionError(tensorflow_shape_frnt_(M))
        if len(dsize) != 3:
            raise AssertionError(dsize)
        B, C, D, H, W = tensorflow_size_frnt_(src)
        size_src: typing.Any = (D, H, W)
        size_out: typing.Any = dsize
>       M_4x4 = tensorflow_convert_affinematrix_to_homography3d(M)

Translated_Outputs/tensorflow_outputs/kornia/geometry/transform/imgwarp.py:545: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=
array([[[ 1.000000e+00, -7.401487e-17,  5.551115e-17, -1.000000e+00...0000e+00, -1.000000e+00],
        [ 1.110223e-16, -1.110223e-16,  1.000000e+00,  0.000000e+00]]],
      dtype=float32)>

    def tensorflow_convert_affinematrix_to_homography3d(A):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        if not isinstance(A, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(A)}")
        if not (
            len(tensorflow_shape_frnt_(A)) == 3 and tensorflow_shape_frnt_(A)[-2:] == (3, 4)
        ):
            raise ValueError(
                f"Input matrix must be a Bx3x4 tensor. Got {tensorflow_shape_frnt_(A)}"
            )
>       return tensorflow__convert_affinematrix_to_homography_impl(A)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

A = <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=
array([[[ 1.000000e+00, -7.401487e-17,  5.551115e-17, -1.000000e+00...0000e+00, -1.000000e+00],
        [ 1.110223e-16, -1.110223e-16,  1.000000e+00,  0.000000e+00]]],
      dtype=float32)>

    def tensorflow__convert_affinematrix_to_homography_impl(A):
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import pad
    
>       H: typing.Any = pad(A, [0, 0, 0, 1], "constant", value=0.0)

Translated_Outputs/tensorflow_outputs/kornia/geometry/conversions.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 4), dtype=float32, numpy=
array([[[ 1.000000e+00, -7.401487e-17,  5.551115e-17, -1.000000e+00...0000e+00, -1.000000e+00],
        [ 1.110223e-16, -1.110223e-16,  1.000000e+00,  0.000000e+00]]],
      dtype=float32)>
pad = ((0, 0), (0, 1), (0, 0)), mode = 'constant', value = 0.0

    def tensorflow_pad_frnt(input, pad, mode="constant", value=0):
        from .....backends.tensorflow.general import tensorflow_get_item
        from .....ivy.general import tensorflow_set_item_bknd
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.experimental.manipulation import tensorflow_pad
    
        if any([(pad_value < 0) for pad_value in pad]):
            pad = list(pad)
            slices = []
            for n in reversed(range(len(pad) // 2)):
                i = n * 2
                j = i + 1
                start = None
                stop = None
                if tensorflow_get_item(pad, i) < 0:
                    start = -tensorflow_get_item(pad, i)
                    pad = tensorflow_set_item_bknd(pad, i, 0)
                if tensorflow_get_item(pad, j) < 0:
                    stop = tensorflow_get_item(pad, j)
                    pad = tensorflow_set_item_bknd(pad, j, 0)
                slices.append(slice(start, stop))
            ndim = len(tensorflow_shape_frnt_(input))
            while len(slices) < ndim:
                slices.insert(0, slice(None))
            input = tensorflow_get_item(input, tuple(slices))
        value = 0 if value is None else value
        mode_dict = {
            "constant": "constant",
            "reflect": "reflect",
            "replicate": "edge",
            "circular": "wrap",
        }
        if mode not in mode_dict:
            raise ValueError(f"Unsupported padding mode: {mode}")
        pad = tensorflow__handle_padding_shape_frnt(
            pad, len(tensorflow_shape_frnt_(input)), mode
        )
        order = 0, 2, 3, 1
>       pad = tuple(pad[i] for i in order)

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f369fd7fbb0>

>   pad = tuple(pad[i] for i in order)
E   IndexError: Exception encountered when calling tensorflow_RandomCrop3D.call().
E   
E   [1mtuple index out of range[0m
E   
E   Arguments received by tensorflow_RandomCrop3D.call():
E     â€¢ input=tf.Tensor(shape=(1, 1, 3, 3, 3), dtype=float32)
E     â€¢ params=None
E     â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/vision_functions.py:743: IndexError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomCrop3D
_____________________________________________________________________________ test_RandomRotation3D[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomRotation3D(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomRotation3D")
    
        init_args = ((15., 20., 20.),)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 3, 3, 3),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomRotation3D,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1264: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._3d.geometric.rotation.RandomRotation3D'>, target = 'tensorflow', init_args = ((15.0, 20.0, 20.0),), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[[0.9015, 0.4550, 0.5629],
           [0.5536, 0.4517, 0.7548],
           [0.6145, 0.1684, 0.1931]],

    ...,

          [[0.3564, 0.4128, 0.9040],
           [0.6434, 0.5994, 0.4835],
           [0.8465, 0.5568, 0.2725]]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.90150434, 0.45502192, 0.56291175],
          [0...,
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3684f29040, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, a...],
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.90150434, 0.45502192, 0.56291175],
          [0...,
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, a...],
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.90150434, 0.45502192, 0.56291175],
          [0...,
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.90150434, 0.45502192, 0.56291175],
          [0....6],
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.90150434, 0.45502192, 0.56291175],
   ...],
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.90150434, 0.45502192, 0.56291175],
          [0....6],
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...521894], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.3487854], dtype=float32)>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f368cdb7ac0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3684d94b80>
tensor = <function tensorflow_tensor_frnt at 0x7f368c138160>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.90150434, 0.45502192, 0.56291175],
          [0....6],
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 3, 3, 3]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 3, 3]), flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
in_tensor = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.90150434, 0.45502192, 0.56291175],
          [0....6],
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...521894], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.3487854], dtype=float32)>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/base.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.90150434, 0.45502192, 0.56291175],
          [0....6],
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...521894], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.3487854], dtype=float32)>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/base.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomRotation3D(degrees=(15.0, 20.0, 20.0), p=1.0, p_batch=1.0, same_on_batch=False, resample=bilinear, align_corners=False)
input = <tf.Tensor: shape=(1, 1, 3, 3, 3), dtype=float32, numpy=
array([[[[[0.90150434, 0.45502192, 0.56291175],
          [0....6],
          [0.6434027 , 0.59941834, 0.48347586],
          [0.84645534, 0.55679256, 0.27251947]]]]], dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, 'forward_input_shape': <tf.Te...521894], dtype=float32)>, 'roll': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.3487854], dtype=float32)>, ...}
flags = {'align_corners': False, 'resample': <tensorflow_Resample.BILINEAR: 1>}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ....geometry.transform.affwarp import tensorflow__compute_tensor_center3d
        from ....geometry.transform.affwarp import tensorflow__compute_rotation_matrix3d
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
        yaw: typing.Any = tensorflow_to_frnt_(params["yaw"], input)
        pitch: typing.Any = tensorflow_to_frnt_(params["pitch"], input)
        roll: typing.Any = tensorflow_to_frnt_(params["roll"], input)
        center: typing.Any = tensorflow__compute_tensor_center3d(input)
        rotation_mat: typing.Any = tensorflow__compute_rotation_matrix3d(
>           yaw, pitch, roll, center.expand(tensorflow_shape_frnt_(yaw)[0], -1)
        )
E       AttributeError: Exception encountered when calling tensorflow_RandomRotation3D.call().
E       
E       [1m'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'expand'[0m
E       
E       Arguments received by tensorflow_RandomRotation3D.call():
E         â€¢ input=tf.Tensor(shape=(1, 1, 3, 3, 3), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/geometric/rotation.py:65: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomRotation3D
____________________________________________________________________________ test_RandomMotionBlur3D[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomMotionBlur3D(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomMotionBlur3D")
    
        init_args = (3, 35., 0.5)
        init_kwargs = {"p": 1.}
        call_args = (torch.rand(1, 1, 3, 5, 5),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomMotionBlur3D,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1324: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._3d.intensity.motion_blur.RandomMotionBlur3D'>, target = 'tensorflow', init_args = (3, 35.0, 0.5), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[[0.2770, 0.6011, 0.2895, 0.6004, 0.0375],
           [0.5034, 0.4688, 0.4082, 0.3789, 0.7081],
           ....6312],
           [0.9877, 0.5487, 0.7716, 0.3844, 0.1075],
           [0.9441, 0.6241, 0.9466, 0.4223, 0.1130]]]]]),)
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur3D(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[0.27701324, 0.60113746, 0.28953236, 0.60043997, 0...3887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3684b7da40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur3D(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, bord...43887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur3D(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[0.27701324, 0.60113746, 0.28953236, 0.60043997, 0...3887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur3D(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, bord...43887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur3D(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[0.27701324, 0.60113746, 0.28953236, 0.60043997, 0...3887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[0.27701324, 0.60113746, 0.28953236, 0.60043997, 0....843887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomMotionBlur3D(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest),)
kwargs = {'input': <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[0.27701324, 0.60113746, 0.28953236, 0.60...43887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur3D(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
input = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[0.27701324, 0.60113746, 0.28953236, 0.60043997, 0....843887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>
params = {'angle_factor': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 17.781933,  14.024586, -14.323618]], dtype=fl...5844], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 1, 3, 5, 5])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3684a4fc70>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3684e38700>
tensor = <function tensorflow_tensor_frnt at 0x7f3684d5ae60>
in_tensor = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[0.27701324, 0.60113746, 0.28953236, 0.60043997, 0....843887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([1, 1, 3, 5, 5]), batch_shape = ivy.frontends.torch.Size([1, 1, 3, 5, 5])
flags = {'border_type': <tensorflow_BorderType.CONSTANT: 0>, 'resample': <tensorflow_Resample.NEAREST: 0>}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur3D(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
in_tensor = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[0.27701324, 0.60113746, 0.28953236, 0.60043997, 0....843887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>
params = {'angle_factor': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 17.781933,  14.024586, -14.323618]], dtype=fl...5844], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 1, 3, 5, 5])>, ...}
flags = {'border_type': <tensorflow_BorderType.CONSTANT: 0>, 'resample': <tensorflow_Resample.NEAREST: 0>}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
        trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)
>       output = self.transform_inputs(in_tensor, params, flags, trans_matrix)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/base.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur3D(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
input = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[0.27701324, 0.60113746, 0.28953236, 0.60043997, 0....843887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>
params = {'angle_factor': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 17.781933,  14.024586, -14.323618]], dtype=fl...5844], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 1, 3, 5, 5])>, ...}
flags = {'border_type': <tensorflow_BorderType.CONSTANT: 0>, 'resample': <tensorflow_Resample.NEAREST: 0>}
transform = <tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=
array([[[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]]], dtype=float32)>, kwargs = {}
tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3684a4fc70>, tensorflow_all_frnt_ = <function tensorflow_all_frnt_ at 0x7f3684a4f760>
tensorflow_any_frnt_ = <function tensorflow_any_frnt_ at 0x7f3684a4cf70>, tensorflow_get_item = <function tensorflow_get_item at 0x7f3684d05120>
tensorflow_is_autocast_enabled = <function tensorflow_is_autocast_enabled at 0x7f3684e21a20>, tensorflow_type_frnt_ = <function tensorflow_type_frnt_ at 0x7f3684a4ca60>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f3684a4c8b0>

    def transform_inputs(self, input, params, flags, transform=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ..utils.helpers import tensorflow_is_autocast_enabled
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
        from .utils.helpers import tensorflow__transform_output_shape
    
        params, flags = self._process_kwargs_to_params_and_flags(
            self._params if params is None else params, flags, **kwargs
        )
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        ori_shape = tensorflow_shape_frnt_(input)
        in_tensor = self.transform_tensor(input)
        self.validate_tensor(in_tensor)
        if tensorflow_all_frnt_(to_apply):
>           output = self.apply_transform(in_tensor, params, flags, transform=transform)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:607: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomMotionBlur3D(kernel_size=3, angle=35.0, direction=0.5, p=1.0, p_batch=1.0, same_on_batch=False, border_type=constant, resample=nearest)
input = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[0.27701324, 0.60113746, 0.28953236, 0.60043997, 0....843887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>
params = {'angle_factor': <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 17.781933,  14.024586, -14.323618]], dtype=fl...5844], dtype=float32)>, 'forward_input_shape': <tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 1, 3, 5, 5])>, ...}
flags = {'border_type': <tensorflow_BorderType.CONSTANT: 0>, 'resample': <tensorflow_Resample.NEAREST: 0>}
transform = <tf.Tensor: shape=(1, 4, 4), dtype=float32, numpy=
array([[[1., 0., 0., 0.],
        [0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 0., 0., 1.]]], dtype=float32)>

    def apply_transform(self, input, params, flags, transform=None):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_unique_frnt_
        from ....filters.motion import tensorflow_motion_blur3d
    
        kernel_size = int(
            tensorflow_item_frnt_(tensorflow_unique_frnt_(params["ksize_factor"]))
        )
        angle = params["angle_factor"]
        direction = params["direction_factor"]
>       return tensorflow_motion_blur3d(
            input,
            kernel_size,
            angle,
            direction,
            self.flags["border_type"].name.lower(),
            self.flags["resample"].name.lower(),
        )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_3d/intensity/motion_blur.py:68: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 1, 3, 5, 5), dtype=float32, numpy=
array([[[[[0.27701324, 0.60113746, 0.28953236, 0.60043997, 0....843887 , 0.10751182],
          [0.9441485 , 0.6240612 , 0.9466124 , 0.42227072, 0.1129666 ]]]]],
      dtype=float32)>
kernel_size = 3, angle = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 17.781933,  14.024586, -14.323618]], dtype=float32)>
direction = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.02605844], dtype=float32)>, border_type = 'constant', mode = 'nearest'

    def tensorflow_motion_blur3d(
        input, kernel_size, angle, direction, border_type="constant", mode="nearest"
    ):
        from .kernels_geometry import tensorflow_get_motion_kernel3d
        from .filter import tensorflow_filter3d
    
>       kernel = tensorflow_get_motion_kernel3d(kernel_size, angle, direction, mode)

Translated_Outputs/tensorflow_outputs/kornia/filters/motion.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kernel_size = 3, angle = <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[ 17.781933,  14.024586, -14.323618]], dtype=float32)>
direction = <tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.5130292], dtype=float32)>, mode = 'nearest'

    def tensorflow_get_motion_kernel3d(kernel_size, angle, direction=0.0, mode="nearest"):
        from ..utils.helpers import tensorflow__extract_device_dtype
        from .kernels import tensorflow__unpack_3d_ks
        from .kernels import tensorflow__check_kernel_size
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_clamp_frnt
        from ..geometry.transform.affwarp import tensorflow_rotate3d
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ..core._backend import tensor
        from ..core._backend import zeros
        from ..core._backend import stack
        from ..core._backend import pad
    
        device, dtype = tensorflow__extract_device_dtype(
            [
                angle
                if isinstance(angle, (tensorflow.Tensor, tensorflow.Variable))
                else None,
                direction
                if isinstance(direction, (tensorflow.Tensor, tensorflow.Variable))
                else None,
            ]
        )
        kernel_tuple = tensorflow__unpack_3d_ks(kernel_size)
        tensorflow__check_kernel_size(kernel_size, 2)
        if not isinstance(angle, (tensorflow.Tensor, tensorflow.Variable)):
            angle = tensor([angle], device=device, dtype=dtype)
        if tensorflow_dim_frnt_(angle) == 1:
            angle = angle[None]
        tensorflow_KORNIA_CHECK_SHAPE(angle, ["B", "3"])
        if not isinstance(direction, (tensorflow.Tensor, tensorflow.Variable)):
            direction = tensor([direction], device=device, dtype=dtype)
        if tensorflow_dim_frnt_(direction) == 0:
            direction = direction[None]
        tensorflow_KORNIA_CHECK_SHAPE(direction, ["B"])
        tensorflow_KORNIA_CHECK(
            tensorflow_size_frnt_(direction, 0) == tensorflow_size_frnt_(angle, 0),
            f"direction and angle must have the same batch size. Got {tensorflow_shape_frnt_(direction)} and {tensorflow_shape_frnt_(angle)}.",
        )
        direction = (tensorflow_clamp_frnt(direction, -1.0, 1.0) + 1.0) / 2.0
        kernel = zeros(
            (tensorflow_size_frnt_(direction, 0), *kernel_tuple), device=device, dtype=dtype
        )
        ag__result_list_0 = []
        for i in range(kernel_size):
            res = direction + (1 - 2 * direction) / (kernel_size - 1) * i
            ag__result_list_0.append(res)
        k = stack(ag__result_list_0, -1)
        kernel = pad(
            k[:, None, None],
            [
                0,
                0,
                kernel_size // 2,
                kernel_size // 2,
                kernel_size // 2,
                kernel_size // 2,
                0,
                0,
            ],
        )
        expected_shape = tuple([tensorflow_size_frnt_(direction, 0), *kernel_tuple])
>       tensorflow_KORNIA_CHECK(
            tensorflow_shape_frnt_(kernel) == expected_shape,
            f"Kernel shape should be {expected_shape}. Gotcha {tensorflow_shape_frnt_(kernel)}",
        )

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels_geometry.py:157: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

condition = False, msg = 'Kernel shape should be (1, 3, 3, 3). Gotcha ivy.frontends.torch.Size([1, 3, 1, 5])', raises = True

    def tensorflow_KORNIA_CHECK(condition, msg=None, raises=True):
        if not condition:
            if raises:
>               raise Exception(f"{condition} not true.\n{msg}")
E               Exception: Exception encountered when calling tensorflow_RandomMotionBlur3D.call().
E               
E               [1mFalse not true.
E               Kernel shape should be (1, 3, 3, 3). Gotcha ivy.frontends.torch.Size([1, 3, 1, 5])[0m
E               
E               Arguments received by tensorflow_RandomMotionBlur3D.call():
E                 â€¢ input=tf.Tensor(shape=(1, 1, 3, 5, 5), dtype=float32)
E                 â€¢ params=None
E                 â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/core/check.py:74: Exception
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomMotionBlur3D
__________________________________________________________________________ test_RandomTransplantation3D[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_RandomTransplantation3D(target_framework, mode, backend_compile):
        print("kornia.augmentation.RandomTransplantation3D")
    
        init_args = ()
        init_kwargs = {"p": 1.}
        call_args = (torch.randn(2, 3, 5, 5), torch.randint(0, 3, (2, 5, 5)))
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.RandomTransplantation3D,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=False,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1344: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._3d.mix.transplantation.RandomTransplantation3D'>, target = 'tensorflow', init_args = (), init_kwargs = {'p': 1.0}
call_args = (tensor([[[[ 0.0084, -0.0694, -2.3739,  1.6707,  0.0182],
          [-0.6084, -1.4338,  0.0683,  1.4086, -0.0405],
   ...2, 1, 2, 2],
         [2, 0, 2, 1, 2],
         [2, 0, 1, 1, 1],
         [1, 1, 0, 1, 0],
         [2, 2, 0, 0, 2]]]))
call_kwargs = {}, deterministic_output = False, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False)
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.00835101, -0.06937687, -2.3738792 ,  1.6706849 ,
 ...2, 2, 1, 2, 2],
        [2, 0, 2, 1, 2],
        [2, 0, 1, 1, 1],
        [1, 1, 0, 1, 0],
        [2, 2, 0, 0, 2]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f368493a640, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=fl...2, 2, 1, 2, 2],
        [2, 0, 2, 1, 2],
        [2, 0, 1, 1, 1],
        [1, 1, 0, 1, 0],
        [2, 2, 0, 0, 2]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.00835101, -0.06937687, -2.3738792 ,  1.6706849 ,
 ...2, 2, 1, 2, 2],
        [2, 0, 2, 1, 2],
        [2, 0, 1, 1, 1],
        [1, 1, 0, 1, 0],
        [2, 2, 0, 0, 2]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), <tf.Tensor: shape=(2, 3, 5, 5), dtype=fl...2, 2, 1, 2, 2],
        [2, 0, 2, 1, 2],
        [2, 0, 1, 1, 1],
        [1, 1, 0, 1, 0],
        [2, 2, 0, 0, 2]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.00835101, -0.06937687, -2.3738792 ,  1.6706849 ,
 ...2, 2, 1, 2, 2],
        [2, 0, 2, 1, 2],
        [2, 0, 1, 1, 1],
        [1, 1, 0, 1, 0],
        [2, 2, 0, 0, 2]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.00835101, -0.06937687, -2.3738792 ,  1.6706849 ,
  ...  1.3074199 ],
         [ 0.05080854,  0.51300275,  0.31940436,  0.12165203,
          -1.5053022 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (*input, params=None, data_keys=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False),)
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.00835101, -0.06937687, -2.3738792 ,  1.67...2, 2, 1, 2, 2],
        [2, 0, 2, 1, 2],
        [2, 0, 1, 1, 1],
        [1, 1, 0, 1, 0],
        [2, 2, 0, 0, 2]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_RandomTransplantation3D(p=1.0, p_batch=1.0, same_on_batch=False)
params = <tf.Tensor: shape=(2, 5, 5), dtype=int64, numpy=
array([[[0, 1, 2, 2, 1],
        [2, 1, 1, 0, 0],
        [1, 0, 1, 2...[2, 2, 1, 2, 2],
        [2, 0, 2, 1, 2],
        [2, 0, 1, 1, 1],
        [1, 1, 0, 1, 0],
        [2, 2, 0, 0, 2]]])>
data_keys = None, input = ()
kwargs = {'input': <tf.Tensor: shape=(2, 3, 5, 5), dtype=float32, numpy=
array([[[[ 0.00835101, -0.06937687, -2.3738792 ,  1.67... 1.3074199 ],
         [ 0.05080854,  0.51300275,  0.31940436,  0.12165203,
          -1.5053022 ]]]], dtype=float32)>}
tensorflow_get_item = <function tensorflow_get_item at 0x7f369fbc70a0>, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f368c7bc430>
tensorflow_clone_frnt_ = <function tensorflow_clone_frnt_ at 0x7f368c7bee60>, tensorflow__validate_input_dtype = <function tensorflow__validate_input_dtype at 0x7f36848e2a70>
tensorflow_index_put_frnt_ = <function tensorflow_index_put_frnt_ at 0x7f3684ddbac0>, keys = [<tensorflow_DataKey.IMAGE: 0>, <tensorflow_DataKey.MASK: 1>]

    def call(self, *input, params=None, data_keys=None, **kwargs):
        from ....constants import tensorflow_DataKey
        from .....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from .....ivy.functional.frontends.torch.tensor import tensorflow_clone_frnt_
        from ...utils.helpers import tensorflow__validate_input_dtype
        from .....ivy.functional.frontends.torch.tensor import (
            tensorflow_index_put_frnt_,
        )
    
        keys: typing.Any
        if data_keys is None:
            keys = self.data_keys
        else:
            keys = [tensorflow_DataKey.get(inp) for inp in data_keys]
        if params is None:
            mask: typing.Any = tensorflow_get_item(
                input, keys.index(tensorflow_DataKey.MASK)
            )
            self._params = self.forward_parameters(tensorflow_shape_frnt_(mask))
        else:
            self._params = params
>       if any(
            k not in self._params
            for k in ["acceptor_indices", "donor_indices", "selection"]
        ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:242: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <tuple_iterator object at 0x7f368c170bb0>

    if any(
>       k not in self._params
        for k in ["acceptor_indices", "donor_indices", "selection"]
    ):

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/mix/transplantation.py:243: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(5, 5), dtype=int64, numpy=
array([[0, 1, 2, 2, 1],
       [2, 1, 1, 0, 0],
       [1, 0, 1, 2, 1],
       [2, 1, 1, 2, 0],
       [1, 1, 2, 1, 0]])>, 'acceptor_indices')
kwargs = {}, arg = 'acceptor_indices'

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       TypeError: Exception encountered when calling tensorflow_RandomTransplantation3D.call().
E       
E       [1mdata type 'acceptor_indices' not understood[0m
E       
E       Arguments received by tensorflow_RandomTransplantation3D.call():
E         â€¢ input=<class 'inspect._empty'>
E         â€¢ params=tf.Tensor(shape=(2, 5, 5), dtype=int64)
E         â€¢ data_keys=None
E         â€¢ kwargs={'input': 'tf.Tensor(shape=(2, 3, 5, 5), dtype=float32)'}

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.RandomTransplantation3D
______________________________________________________________________________ test_LongestMaxSize[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LongestMaxSize(target_framework, mode, backend_compile):
        print("kornia.augmentation.LongestMaxSize")
    
        init_args = (100,)
        init_kwargs = {}
        call_args = (torch.rand(10, 3, 200, 200),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.LongestMaxSize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=True,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1404: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resize.LongestMaxSize'>, target = 'tensorflow', init_args = (100,), init_kwargs = {}
call_args = (tensor([[[[7.5409e-03, 3.6874e-01, 7.7567e-01,  ..., 6.4153e-01,
           6.5882e-01, 6.6837e-02],
          [8.858... 3.3341e-01],
          [9.8929e-02, 6.7702e-01, 4.0360e-01,  ..., 1.8864e-01,
           9.6386e-01, 4.7271e-01]]]]),)
call_kwargs = {}, deterministic_output = True, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
args = (<tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[7.54088163e-03, 3.68740916e-01, 7.75665939e-01, ...7020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3684625640, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bili...77020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[7.54088163e-03, 3.68740916e-01, 7.75665939e-01, ...7020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bili...77020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[7.54088163e-03, 3.68740916e-01, 7.75665939e-01, ...7020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[7.54088163e-03, 3.68740916e-01, 7.75665939e-01, .....77020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False),)
kwargs = {'input': <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[7.54088163e-03, 3.68740916e-01, 7.75665...77020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[7.54088163e-03, 3.68740916e-01, 7.75665939e-01, .....77020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3684b4fbe0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3684b26440>
tensor = <function tensorflow_tensor_frnt at 0x7f3684b717e0>
in_tensor = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[7.54088163e-03, 3.68740916e-01, 7.75665939e-01, .....77020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([10, 3, 200, 200]), batch_shape = ivy.frontends.torch.Size([10, 3, 200, 200])
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
in_tensor = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[7.54088163e-03, 3.68740916e-01, 7.75665939e-01, .....77020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[7.54088163e-03, 3.68740916e-01, 7.75665939e-01, .....77020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LongestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=long, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 200, 200), dtype=float32, numpy=
array([[[[7.54088163e-03, 3.68740916e-01, 7.75665939e-01, .....77020013e-01, 4.03599858e-01, ...,
          1.88637197e-01, 9.63859379e-01, 4.72711921e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...     [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200],
       [200, 200]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'long', ...}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_as_tensor_frnt,
        )
        from ....geometry.transform.imgwarp import tensorflow_get_perspective_transform
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
    
>       if params["output_size"] == tensorflow_shape_frnt_(input)[-2:]:
E       ValueError: Exception encountered when calling tensorflow_LongestMaxSize.call().
E       
E       [1mThe truth value of an array with more than one element is ambiguous. Use a.any() or a.all()[0m
E       
E       Arguments received by tensorflow_LongestMaxSize.call():
E         â€¢ input=tf.Tensor(shape=(10, 3, 200, 200), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/resize.py:67: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.LongestMaxSize
__________________________________________________________________________________ test_Resize[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_Resize(target_framework, mode, backend_compile):
        print("kornia.augmentation.Resize")
    
        init_args = ((100, 100),)
        init_kwargs = {}
        call_args = (torch.rand(10, 3, 50, 50),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.Resize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=True,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1424: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resize.Resize'>, target = 'tensorflow', init_args = ((100, 100),), init_kwargs = {}
call_args = (tensor([[[[4.2814e-01, 7.7092e-01, 8.3562e-02,  ..., 5.1288e-01,
           8.3706e-01, 7.7352e-01],
          [7.330... 1.1698e-01],
          [7.6932e-01, 5.5396e-01, 5.6322e-01,  ..., 4.2486e-01,
           5.8830e-01, 6.8693e-01]]]]),)
call_kwargs = {}, deterministic_output = True, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.28140640e-01, 7.70919681e-01, 8.35618377e-02, .....3963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x55b3c2599d80, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resamp...53963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.28140640e-01, 7.70919681e-01, 8.35618377e-02, .....3963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resamp...53963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.28140640e-01, 7.70919681e-01, 8.35618377e-02, .....3963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.28140640e-01, 7.70919681e-01, 8.35618377e-02, .......53963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False),)
kwargs = {'input': <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.28140640e-01, 7.70919681e-01, 8.3561837...53963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.28140640e-01, 7.70919681e-01, 8.35618377e-02, .......53963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3684b4fbe0>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f3684b26440>
tensor = <function tensorflow_tensor_frnt at 0x7f3684b717e0>
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.28140640e-01, 7.70919681e-01, 8.35618377e-02, .......53963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([10, 3, 50, 50]), batch_shape = ivy.frontends.torch.Size([10, 3, 50, 50])
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.28140640e-01, 7.70919681e-01, 8.35618377e-02, .......53963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.28140640e-01, 7.70919681e-01, 8.35618377e-02, .......53963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Resize(output_size=(100, 100), p=1.0, p_batch=1.0, same_on_batch=True, size=(100, 100), side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[4.28140640e-01, 7.70919681e-01, 8.35618377e-02, .......53963184e-01, 5.63216746e-01, ...,
          4.24855411e-01, 5.88297427e-01, 6.86925590e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_as_tensor_frnt,
        )
        from ....geometry.transform.imgwarp import tensorflow_get_perspective_transform
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
    
>       if params["output_size"] == tensorflow_shape_frnt_(input)[-2:]:
E       ValueError: Exception encountered when calling tensorflow_Resize.call().
E       
E       [1mThe truth value of an array with more than one element is ambiguous. Use a.any() or a.all()[0m
E       
E       Arguments received by tensorflow_Resize.call():
E         â€¢ input=tf.Tensor(shape=(10, 3, 50, 50), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/resize.py:67: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.Resize
______________________________________________________________________________ test_SmallestMaxSize[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SmallestMaxSize(target_framework, mode, backend_compile):
        print("kornia.augmentation.SmallestMaxSize")
    
        init_args = (100,)
        init_kwargs = {}
        call_args = (torch.rand(10, 3, 50, 50),)
        call_kwargs = {}
    
>       _test_augmentation_class(
            kornia.augmentation.SmallestMaxSize,
            target_framework,
            init_args,
            init_kwargs,
            call_args,
            call_kwargs,
            deterministic_output=True,
            backend_compile=backend_compile,
        )

kornia/augmentation/test_augmentation.py:1444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

augmentation_cls = <class 'kornia.augmentation._2d.geometric.resize.SmallestMaxSize'>, target = 'tensorflow', init_args = (100,), init_kwargs = {}
call_args = (tensor([[[[7.9359e-01, 6.5751e-01, 7.7289e-01,  ..., 9.8440e-01,
           8.3070e-01, 3.1106e-01],
          [3.158... 6.7720e-02],
          [6.7399e-01, 6.1044e-01, 8.3670e-01,  ..., 5.4618e-01,
           7.9427e-01, 8.2516e-01]]]]),)
call_kwargs = {}, deterministic_output = True, backend_compile = False, tolerance = 0.001

    def _test_augmentation_class(
        augmentation_cls,
        target,
        init_args=(),
        init_kwargs={},
        call_args=(),
        call_kwargs={},
        deterministic_output=True,
        backend_compile=False,
        tolerance=1e-3,
    ):
        if backend_compile:
            pytest.skip()
    
        transpiled_cls = ivy.transpile(augmentation_cls, source="torch", target=target)
    
        torch_aug = augmentation_cls(*init_args, **init_kwargs)
        transpiled_init_args = _nest_torch_tensor_to_new_framework(init_args, target)
        transpiled_init_kwargs = _nest_torch_tensor_to_new_framework(init_kwargs, target)
        transpiled_aug = transpiled_cls(*transpiled_init_args, **transpiled_init_kwargs)
    
        # assert dir(torch_aug) == dir(transpiled_aug), f"attributes/methods of transpiled object do not align with the original - orig: {dir(torch_aug)} != transpiled: {dir(transpiled_aug)}"
    
        torch_out = torch_aug(*call_args, **call_kwargs)
        transpiled_call_args = _nest_torch_tensor_to_new_framework(call_args, target)
        transpiled_call_kwargs = _nest_torch_tensor_to_new_framework(call_kwargs, target)
>       transpiled_out = transpiled_aug(*transpiled_call_args, **transpiled_call_kwargs)

kornia/augmentation/test_augmentation.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[7.93594182e-01, 6.57507598e-01, 7.72890270e-01, .....0435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f3684627a40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...kexec', code_context=['        return self._inner_hookexec(hook_name, methods, kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bi...10435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[7.93594182e-01, 6.57507598e-01, 7.72890270e-01, .....0435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bi...10435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False), v = None, buffers = None
args = (<tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[7.93594182e-01, 6.57507598e-01, 7.72890270e-01, .....0435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[7.93594182e-01, 6.57507598e-01, 7.72890270e-01, .......10435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input, params=None, **kwargs)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False),)
kwargs = {'input': <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[7.93594182e-01, 6.57507598e-01, 7.7289027...10435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[7.93594182e-01, 6.57507598e-01, 7.72890270e-01, .......10435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
kwargs = {}, tensorflow_shape_frnt_ = <function tensorflow_shape_frnt_ at 0x7f3684695f30>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f36848db130>
tensor = <function tensorflow_tensor_frnt at 0x7f3684969d80>
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[7.93594182e-01, 6.57507598e-01, 7.72890270e-01, .......10435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>
input_shape = ivy.frontends.torch.Size([10, 3, 50, 50]), batch_shape = ivy.frontends.torch.Size([10, 3, 50, 50])
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def call(self, input, params=None, **kwargs):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..core._backend import tensor
    
        in_tensor = self.__unpack_input__(input)
        input_shape = tensorflow_shape_frnt_(in_tensor)
        in_tensor = self.transform_tensor(in_tensor)
        batch_shape = tensorflow_shape_frnt_(in_tensor)
        if params is None:
            params = self.forward_parameters(batch_shape)
        if "batch_prob" not in params:
            params = tensorflow_set_item_bknd(
                params, "batch_prob", tensor([True] * batch_shape[0])
            )
        params, flags = self._process_kwargs_to_params_and_flags(
            params, self.flags, **kwargs
        )
>       output = self.apply_func(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/base.py:235: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
in_tensor = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[7.93594182e-01, 6.57507598e-01, 7.72890270e-01, .......10435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def apply_func(self, in_tensor, params, flags=None):
        if flags is None:
            flags = self.flags
>       trans_matrix = self.generate_transformation_matrix(in_tensor, params, flags)

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[7.93594182e-01, 6.57507598e-01, 7.72890270e-01, .......10435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def generate_transformation_matrix(self, input, params, flags):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_all_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...utils.helpers import tensorflow_is_autocast_enabled
        from ....ivy.functional.frontends.torch.tensor import tensorflow_type_frnt_
        from ....ivy.functional.frontends.torch.tensor import tensorflow_index_put_frnt_
    
        batch_prob = params["batch_prob"]
        to_apply = batch_prob > 0.5
        in_tensor = self.transform_tensor(input)
        if not tensorflow_any_frnt_(to_apply):
            trans_matrix = self.identity_matrix(in_tensor)
        elif tensorflow_all_frnt_(to_apply):
>           trans_matrix = self.compute_transformation(
                in_tensor, params=params, flags=flags
            )

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/base.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SmallestMaxSize(output_size=100, p=1.0, p_batch=1.0, same_on_batch=True, size=100, side=short, resample=bilinear, align_corners=True, antialias=False)
input = <tf.Tensor: shape=(10, 3, 50, 50), dtype=float32, numpy=
array([[[[7.93594182e-01, 6.57507598e-01, 7.72890270e-01, .......10435665e-01, 8.36695433e-01, ...,
          5.46179950e-01, 7.94269264e-01, 8.25157225e-01]]]],
      dtype=float32)>
params = {'batch_prob': <tf.Tensor: shape=(10,), dtype=float32, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=flo...[50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50],
       [50, 50]])>, ...}
flags = {'align_corners': True, 'antialias': False, 'resample': <tensorflow_Resample.BILINEAR: 1>, 'side': 'short', ...}

    def compute_transformation(self, input, params, flags):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....utils.misc import tensorflow_eye_like
        from .....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_as_tensor_frnt,
        )
        from ....geometry.transform.imgwarp import tensorflow_get_perspective_transform
        from .....ivy.functional.frontends.torch.tensor import tensorflow_expand_frnt_
    
>       if params["output_size"] == tensorflow_shape_frnt_(input)[-2:]:
E       ValueError: Exception encountered when calling tensorflow_SmallestMaxSize.call().
E       
E       [1mThe truth value of an array with more than one element is ambiguous. Use a.any() or a.all()[0m
E       
E       Arguments received by tensorflow_SmallestMaxSize.call():
E         â€¢ input=tf.Tensor(shape=(10, 3, 50, 50), dtype=float32)
E         â€¢ params=None
E         â€¢ kwargs=<class 'inspect._empty'>

Translated_Outputs/tensorflow_outputs/kornia/augmentation/_2d/geometric/resize.py:67: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.augmentation.SmallestMaxSize
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/augmentation/test_augmentation.py::test_RandomBoxBlur[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorfl...
FAILED kornia/augmentation/test_augmentation.py::test_RandomClahe[tensorflow-s2s-False] - IndexError: Exception encountered when calling tensorflow_RandomClahe.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomGaussianBlur[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling ten...
FAILED kornia/augmentation/test_augmentation.py::test_RandomJPEG[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_...
FAILED kornia/augmentation/test_augmentation.py::test_RandomLinearCornerIllumination[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_LinearCornerIlluminationGe...
FAILED kornia/augmentation/test_augmentation.py::test_RandomMotionBlur[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_RandomMotionBlur.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomRain[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_...
FAILED kornia/augmentation/test_augmentation.py::test_RandomSaltAndPepperNoise[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_RandomSaltAndPepperNoise.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomSharpness[tensorflow-s2s-False] - NameError: Exception encountered when calling tensorflow_RandomSharpness.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomSnow[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_...
FAILED kornia/augmentation/test_augmentation.py::test_RandomSolarize[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorf...
FAILED kornia/augmentation/test_augmentation.py::test_CenterCrop[tensorflow-s2s-False] - IndexError: Exception encountered when calling tensorflow_CenterCrop.call().
FAILED kornia/augmentation/test_augmentation.py::test_PadTo[tensorflow-s2s-False] - AssertionError: numpy array shapes are not all close
FAILED kornia/augmentation/test_augmentation.py::test_RandomAffine[tensorflow-s2s-False] - IndexError: Exception encountered when calling tensorflow_RandomAffine.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomCrop[tensorflow-s2s-False] - IndexError: Exception encountered when calling tensorflow_RandomCrop.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomPerspective[tensorflow-s2s-False] - IndexError: Exception encountered when calling tensorflow_RandomPerspective.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomResizedCrop[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tens...
FAILED kornia/augmentation/test_augmentation.py::test_RandomRotation[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_RandomRotation.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomShear[tensorflow-s2s-False] - IndexError: Exception encountered when calling tensorflow_RandomShear.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomCutMixV2[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: source code not available
FAILED kornia/augmentation/test_augmentation.py::test_RandomJigsaw[tensorflow-s2s-False] - TypeError: got an unexpected keyword argument 'input'
FAILED kornia/augmentation/test_augmentation.py::test_RandomMixUpV2[tensorflow-s2s-False] - TypeError: got an unexpected keyword argument 'input'
FAILED kornia/augmentation/test_augmentation.py::test_RandomMosaic[tensorflow-s2s-False] - TypeError: got an unexpected keyword argument 'input'
FAILED kornia/augmentation/test_augmentation.py::test_RandomTransplantation[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_RandomTransplantation.call().
FAILED kornia/augmentation/test_augmentation.py::test_CenterCrop3D[tensorflow-s2s-False] - IndexError: Exception encountered when calling tensorflow_CenterCrop3D.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomAffine3D[tensorflow-s2s-False] - IndexError: Exception encountered when calling tensorflow_RandomAffine3D.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomCrop3D[tensorflow-s2s-False] - IndexError: Exception encountered when calling tensorflow_RandomCrop3D.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomRotation3D[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_RandomRotation3D.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomMotionBlur3D[tensorflow-s2s-False] - Exception: Exception encountered when calling tensorflow_RandomMotionBlur3D.call().
FAILED kornia/augmentation/test_augmentation.py::test_RandomTransplantation3D[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_RandomTransplantation3D.call().
FAILED kornia/augmentation/test_augmentation.py::test_LongestMaxSize[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_LongestMaxSize.call().
FAILED kornia/augmentation/test_augmentation.py::test_Resize[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_Resize.call().
FAILED kornia/augmentation/test_augmentation.py::test_SmallestMaxSize[tensorflow-s2s-False] - ValueError: Exception encountered when calling tensorflow_SmallestMaxSize.call().
============================================================================== 33 failed, 35 passed in 1692.22s (0:28:12) ==============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 8 items

kornia/geometry/test_linalg.py ........                                                                                                                                                          [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 8 passed in 120.34s (0:02:00) =====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 35 items

kornia/test_losses.py .................FF..F.............                                                                                                                                        [100%]

=============================================================================================== FAILURES ===============================================================================================
______________________________________________________________________________ test_HausdorffERLoss[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HausdorffERLoss(target_framework, mode, backend_compile):
        print("kornia.losses.HausdorffERLoss")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHausdorffERLoss = ivy.transpile(kornia.losses.HausdorffERLoss, source="torch", target=target_framework)
    
        torch_loss_fn = kornia.losses.HausdorffERLoss()
        transpiled_loss_fn = TranspiledHausdorffERLoss()
    
        torch_args = (
            torch.randn(5, 3, 20, 20),
            (torch.rand(5, 1, 20, 20) * 2).long(),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_res = torch_loss_fn(*torch_args)
>       transpiled_res = transpiled_loss_fn(*transpiled_args)

kornia/test_losses.py:446: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
args = (<tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 0.5920321 ,  1.1724997 , -1.5012025 , ..., -0.3015...        ...,
         [0, 0, 1, ..., 0, 0, 1],
         [1, 0, 1, ..., 0, 0, 0],
         [0, 0, 1, ..., 1, 0, 0]]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f4abf24ca40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss(), <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 0.5920321 ,  1.17249...        ...,
         [0, 0, 1, ..., 0, 0, 1],
         [1, 0, 1, ..., 0, 0, 0],
         [0, 0, 1, ..., 1, 0, 0]]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 0.5920321 ,  1.1724997 , -1.5012025 , ..., -0.3015...        ...,
         [0, 0, 1, ..., 0, 0, 1],
         [1, 0, 1, ..., 0, 0, 0],
         [0, 0, 1, ..., 1, 0, 0]]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss(), <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 0.5920321 ,  1.17249...        ...,
         [0, 0, 1, ..., 0, 0, 1],
         [1, 0, 1, ..., 0, 0, 0],
         [0, 0, 1, ..., 1, 0, 0]]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 0.5920321 ,  1.1724997 , -1.5012025 , ..., -0.3015...        ...,
         [0, 0, 1, ..., 0, 0, 1],
         [1, 0, 1, ..., 0, 0, 0],
         [0, 0, 1, ..., 1, 0, 0]]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 0.5920321 ,  1.1724997 , -1.5012025 , ..., -0.30157...      [-1.1175686 ,  0.17457291,  0.44908786, ...,  1.1753882 ,
          -0.18965758,  0.90584457]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (pred, target)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss(),)
kwargs = {'pred': <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 0.5920321 ,  1.1724997 , -1.5012025 , ...,...        ...,
         [0, 0, 1, ..., 0, 0, 1],
         [1, 0, 1, ..., 0, 0, 0],
         [0, 0, 1, ..., 1, 0, 0]]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
pred = <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 0.5920321 ,  1.1724997 , -1.5012025 , ..., -0.30157...      [-1.1175686 ,  0.17457291,  0.44908786, ...,  1.1753882 ,
          -0.18965758,  0.90584457]]]], dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20), dtype=int64, numpy=
array([[[[1, 0, 0, ..., 0, 1, 1],
         [0, 1, 0, ..., 0, 0, ...         ...,
         [0, 0, 1, ..., 0, 0, 1],
         [1, 0, 1, ..., 0, 0, 0],
         [0, 0, 1, ..., 1, 0, 0]]]])>

    def call(self, pred, target):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
    
        if tensorflow_dim_frnt_(pred) != 4:
            raise ValueError(
                f"Only 2D images supported. Got {tensorflow_dim_frnt_(pred)}."
            )
        if not (
            tensorflow_max_frnt_(target) < tensorflow_size_frnt_(pred, 1)
            and tensorflow_min_frnt_(target) >= 0
            and target.dtype == tf.int64
        ):
            raise ValueError(
                f"Expect long type target value in range (0, {tensorflow_size_frnt_(pred, 1)}). ({tensorflow_min_frnt_(target)}, {tensorflow_max_frnt_(target)})"
            )
>       return super().call(pred, target)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:537: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
pred = <tf.Tensor: shape=(5, 3, 20, 20), dtype=float32, numpy=
array([[[[ 0.5920321 ,  1.1724997 , -1.5012025 , ..., -0.30157...      [-1.1175686 ,  0.17457291,  0.44908786, ...,  1.1753882 ,
          -0.18965758,  0.90584457]]]], dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20), dtype=int64, numpy=
array([[[[1, 0, 0, ..., 0, 1, 1],
         [0, 1, 0, ..., 0, 0, ...         ...,
         [0, 0, 1, ..., 0, 0, 1],
         [1, 0, 1, ..., 0, 0, 0],
         [0, 0, 1, ..., 1, 0, 0]]]])>

    def call(self, pred, target):
        from ..core._backend import where
        from ..core._backend import stack
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
    
        if not (
            tensorflow_shape_frnt_(pred)[2:] == tensorflow_shape_frnt_(target)[2:]
            and tensorflow_size_frnt_(pred, 0) == tensorflow_size_frnt_(target, 0)
            and tensorflow_size_frnt_(target, 1) == 1
        ):
            raise ValueError(
                f"Prediction and target need to be of same size, and target should not be one-hot.Got {tensorflow_shape_frnt_(pred)} and {tensorflow_shape_frnt_(target)}."
            )
        if tensorflow_size_frnt_(pred, 1) < tensorflow_item_frnt_(
            tensorflow_max_frnt_(target)
        ):
            raise ValueError("Invalid target value.")
        out = stack(
>           [
                self.perform_erosion(
                    tensorflow_get_item(
                        pred, (slice(None, None, None), slice(i, i + 1, None))
                    ),
                    where(
                        target == i,
                        tensor(1, device=target.device, dtype=target.dtype),
                        tensor(0, device=target.device, dtype=target.dtype),
                    ),
                )
                for i in range(tensorflow_size_frnt_(pred, 1))
            ]
        )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <range_iterator object at 0x7f4abf2a3fc0>

        [
>           self.perform_erosion(
                tensorflow_get_item(
                    pred, (slice(None, None, None), slice(i, i + 1, None))
                ),
                where(
                    target == i,
                    tensor(1, device=target.device, dtype=target.dtype),
                    tensor(0, device=target.device, dtype=target.dtype),
                ),
            )
            for i in range(tensorflow_size_frnt_(pred, 1))
        ]
    )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss()
pred = <tf.Tensor: shape=(5, 1, 20, 20), dtype=float32, numpy=
array([[[[ 0.5920321 ,  1.1724997 , -1.5012025 , ..., -0.30157...      [ 2.001943  ,  0.76590586,  0.48381037, ...,  0.62904894,
           0.89066845, -0.8066055 ]]]], dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20), dtype=int64, numpy=
array([[[[0, 1, 1, ..., 1, 0, 0],
         [1, 0, 1, ..., 1, 1, ...         ...,
         [1, 1, 0, ..., 1, 1, 0],
         [0, 1, 0, ..., 1, 1, 1],
         [1, 1, 0, ..., 0, 1, 1]]]])>

    def perform_erosion(self, pred, target):
        from ..core._backend import as_tensor
        from ..core._backend import zeros_like
        from ..core._backend import where
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_ones_like_v_0p4p0_and_above_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
    
        bound = (pred - target) ** 2
        kernel = as_tensor(self.kernel, device=pred.device, dtype=pred.dtype)
        eroded = zeros_like(bound, device=pred.device, dtype=pred.dtype)
        mask = tensorflow_ones_like_v_0p4p0_and_above_frnt(
            bound, device=pred.device, dtype=tf.bool
        )
        padding = (tensorflow_size_frnt_(kernel, -1) - 1) // 2
        for k in range(self.k):
>           dilation = self.conv(bound, weight=kernel, padding=padding, groups=1)
E           TypeError: Exception encountered when calling tensorflow_HausdorffERLoss.call().
E           
E           [1mtensorflow_conv2d_frnt() got multiple values for argument 'weight'[0m
E           
E           Arguments received by tensorflow_HausdorffERLoss.call():
E             â€¢ pred=tf.Tensor(shape=(5, 3, 20, 20), dtype=float32)
E             â€¢ target=tf.Tensor(shape=(5, 1, 20, 20), dtype=int64)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:82: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.losses.HausdorffERLoss
_____________________________________________________________________________ test_HausdorffERLoss3D[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HausdorffERLoss3D(target_framework, mode, backend_compile):
        print("kornia.losses.HausdorffERLoss3D")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHausdorffERLoss3D = ivy.transpile(kornia.losses.HausdorffERLoss3D, source="torch", target=target_framework)
    
        torch_loss_fn = kornia.losses.HausdorffERLoss3D()
        transpiled_loss_fn = TranspiledHausdorffERLoss3D()
    
        torch_args = (
            torch.randn(5, 3, 20, 20, 20),
            (torch.rand(5, 1, 20, 20, 20) * 2).long(),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_res = torch_loss_fn(*torch_args)
>       transpiled_res = transpiled_loss_fn(*transpiled_args)

kornia/test_losses.py:471: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
args = (<tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[-1.13642228e+00, -4.63399112e-01,  1.93841660e...    ...,
          [0, 0, 1, ..., 0, 0, 1],
          [1, 0, 0, ..., 0, 0, 0],
          [0, 0, 0, ..., 0, 0, 1]]]]])>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f4abe26a2b0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss3D(), <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[-1.13642228e+0...    ...,
          [0, 0, 1, ..., 0, 0, 1],
          [1, 0, 0, ..., 0, 0, 0],
          [0, 0, 0, ..., 0, 0, 1]]]]])>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[-1.13642228e+00, -4.63399112e-01,  1.93841660e...    ...,
          [0, 0, 1, ..., 0, 0, 1],
          [1, 0, 0, ..., 0, 0, 0],
          [0, 0, 0, ..., 0, 0, 1]]]]])>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss3D(), <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[-1.13642228e+0...    ...,
          [0, 0, 1, ..., 0, 0, 1],
          [1, 0, 0, ..., 0, 0, 0],
          [0, 0, 0, ..., 0, 0, 1]]]]])>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D(), v = None, buffers = None
args = (<tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[-1.13642228e+00, -4.63399112e-01,  1.93841660e...    ...,
          [0, 0, 1, ..., 0, 0, 1],
          [1, 0, 0, ..., 0, 0, 0],
          [0, 0, 0, ..., 0, 0, 1]]]]])>)
kwargs = {}
first_arr = <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[-1.13642228e+00, -4.63399112e-01,  1.93841660e+...824e-03, -7.92331994e-01, ...,
            9.97335613e-01, -3.36668342e-01, -8.92423630e-01]]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (pred, target)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HausdorffERLoss3D(),)
kwargs = {'pred': <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[-1.13642228e+00, -4.63399112e-01,  1.9...    ...,
          [0, 0, 1, ..., 0, 0, 1],
          [1, 0, 0, ..., 0, 0, 0],
          [0, 0, 0, ..., 0, 0, 1]]]]])>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
pred = <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[-1.13642228e+00, -4.63399112e-01,  1.93841660e+...824e-03, -7.92331994e-01, ...,
            9.97335613e-01, -3.36668342e-01, -8.92423630e-01]]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=int64, numpy=
array([[[[[0, 1, 0, ..., 0, 0, 1],
          [1, 0, 1, ..., ...     ...,
          [0, 0, 1, ..., 0, 0, 1],
          [1, 0, 0, ..., 0, 0, 0],
          [0, 0, 0, ..., 0, 0, 1]]]]])>

    def call(self, pred, target):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_dim_frnt_
    
        if tensorflow_dim_frnt_(pred) != 5:
            raise ValueError(
                f"Only 3D images supported. Got {tensorflow_dim_frnt_(pred)}."
            )
>       return super().call(pred, target)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:564: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
pred = <tf.Tensor: shape=(5, 3, 20, 20, 20), dtype=float32, numpy=
array([[[[[-1.13642228e+00, -4.63399112e-01,  1.93841660e+...824e-03, -7.92331994e-01, ...,
            9.97335613e-01, -3.36668342e-01, -8.92423630e-01]]]]],
      dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=int64, numpy=
array([[[[[0, 1, 0, ..., 0, 0, 1],
          [1, 0, 1, ..., ...     ...,
          [0, 0, 1, ..., 0, 0, 1],
          [1, 0, 0, ..., 0, 0, 0],
          [0, 0, 0, ..., 0, 0, 1]]]]])>

    def call(self, pred, target):
        from ..core._backend import where
        from ..core._backend import stack
        from ..core._backend import tensor
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_max_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
    
        if not (
            tensorflow_shape_frnt_(pred)[2:] == tensorflow_shape_frnt_(target)[2:]
            and tensorflow_size_frnt_(pred, 0) == tensorflow_size_frnt_(target, 0)
            and tensorflow_size_frnt_(target, 1) == 1
        ):
            raise ValueError(
                f"Prediction and target need to be of same size, and target should not be one-hot.Got {tensorflow_shape_frnt_(pred)} and {tensorflow_shape_frnt_(target)}."
            )
        if tensorflow_size_frnt_(pred, 1) < tensorflow_item_frnt_(
            tensorflow_max_frnt_(target)
        ):
            raise ValueError("Invalid target value.")
        out = stack(
>           [
                self.perform_erosion(
                    tensorflow_get_item(
                        pred, (slice(None, None, None), slice(i, i + 1, None))
                    ),
                    where(
                        target == i,
                        tensor(1, device=target.device, dtype=target.dtype),
                        tensor(0, device=target.device, dtype=target.dtype),
                    ),
                )
                for i in range(tensorflow_size_frnt_(pred, 1))
            ]
        )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:125: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <range_iterator object at 0x7f4abdf57030>

        [
>           self.perform_erosion(
                tensorflow_get_item(
                    pred, (slice(None, None, None), slice(i, i + 1, None))
                ),
                where(
                    target == i,
                    tensor(1, device=target.device, dtype=target.dtype),
                    tensor(0, device=target.device, dtype=target.dtype),
                ),
            )
            for i in range(tensorflow_size_frnt_(pred, 1))
        ]
    )

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HausdorffERLoss3D()
pred = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=float32, numpy=
array([[[[[-1.1364223 , -0.4633991 ,  1.9384166 , ...,  0....    [-0.23729703, -1.4231896 , -0.643859  , ..., -0.79408574,
           -0.3027152 ,  0.13002212]]]]], dtype=float32)>
target = <tf.Tensor: shape=(5, 1, 20, 20, 20), dtype=int64, numpy=
array([[[[[1, 0, 1, ..., 1, 1, 0],
          [0, 1, 0, ..., ...     ...,
          [1, 1, 0, ..., 1, 1, 0],
          [0, 1, 1, ..., 1, 1, 1],
          [1, 1, 1, ..., 1, 1, 0]]]]])>

    def perform_erosion(self, pred, target):
        from ..core._backend import as_tensor
        from ..core._backend import zeros_like
        from ..core._backend import where
        from ...ivy.functional.frontends.torch.creation_ops import (
            tensorflow_ones_like_v_0p4p0_and_above_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_any_frnt_
    
        bound = (pred - target) ** 2
        kernel = as_tensor(self.kernel, device=pred.device, dtype=pred.dtype)
        eroded = zeros_like(bound, device=pred.device, dtype=pred.dtype)
        mask = tensorflow_ones_like_v_0p4p0_and_above_frnt(
            bound, device=pred.device, dtype=tf.bool
        )
        padding = (tensorflow_size_frnt_(kernel, -1) - 1) // 2
        for k in range(self.k):
>           dilation = self.conv(bound, weight=kernel, padding=padding, groups=1)
E           TypeError: Exception encountered when calling tensorflow_HausdorffERLoss3D.call().
E           
E           [1mtensorflow_conv3d_frnt() got multiple values for argument 'weight'[0m
E           
E           Arguments received by tensorflow_HausdorffERLoss3D.call():
E             â€¢ pred=tf.Tensor(shape=(5, 3, 20, 20, 20), dtype=float32)
E             â€¢ target=tf.Tensor(shape=(5, 1, 20, 20, 20), dtype=int64)

Translated_Outputs/tensorflow_outputs/kornia/losses/hausdorff.py:86: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.losses.HausdorffERLoss3D
________________________________________________________________________________ test_MS_SSIMLoss[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_MS_SSIMLoss(target_framework, mode, backend_compile):
        print("kornia.losses.MS_SSIMLoss")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledMS_SSIMLoss = ivy.transpile(kornia.losses.MS_SSIMLoss, source="torch", target=target_framework)
    
        torch_loss_fn = kornia.losses.MS_SSIMLoss()
        transpiled_loss_fn = TranspiledMS_SSIMLoss()
    
        torch_args = (
            torch.rand(1, 3, 5, 5),
            torch.rand(1, 3, 5, 5),
        )
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
    
        torch_res = torch_loss_fn(*torch_args)
>       transpiled_res = transpiled_loss_fn(*transpiled_args)

kornia/test_losses.py:546: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss()
args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.44236583, 0.44737428, 0.790...26976782, 0.7325508 ],
         [0.81275696, 0.47515577, 0.98288137, 0.6136984 , 0.6123721 ]]]],
      dtype=float32)>)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x5638403acba0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MS_SSIMLoss(), <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.4...26976782, 0.7325508 ],
         [0.81275696, 0.47515577, 0.98288137, 0.6136984 , 0.6123721 ]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.44236583, 0.44737428, 0.790...26976782, 0.7325508 ],
         [0.81275696, 0.47515577, 0.98288137, 0.6136984 , 0.6123721 ]]]],
      dtype=float32)>)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MS_SSIMLoss(), <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.4...26976782, 0.7325508 ],
         [0.81275696, 0.47515577, 0.98288137, 0.6136984 , 0.6123721 ]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.44236583, 0.44737428, 0.790...26976782, 0.7325508 ],
         [0.81275696, 0.47515577, 0.98288137, 0.6136984 , 0.6123721 ]]]],
      dtype=float32)>)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.44236583, 0.44737428, 0.7902....6421601 , 0.76124465],
         [0.9921301 , 0.4747575 , 0.94826883, 0.7730004 , 0.3672161 ]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (img1, img2)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_MS_SSIMLoss(),)
kwargs = {'img1': <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.44236583, 0.4473742...26976782, 0.7325508 ],
         [0.81275696, 0.47515577, 0.98288137, 0.6136984 , 0.6123721 ]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_MS_SSIMLoss()
img1 = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.44236583, 0.44737428, 0.7902....6421601 , 0.76124465],
         [0.9921301 , 0.4747575 , 0.94826883, 0.7730004 , 0.3672161 ]]]],
      dtype=float32)>
img2 = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.7387863 , 0.79796404, 0.55800444, 0.29656273, 0.1722....26976782, 0.7325508 ],
         [0.81275696, 0.47515577, 0.98288137, 0.6136984 , 0.6123721 ]]]],
      dtype=float32)>

    def call(self, img1, img2):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.convolution_functions import (
            tensorflow_conv2d_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_prod_frnt_
        from ...ivy.functional.frontends.torch.nn.functional.loss_functions import (
            tensorflow_l1_loss_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_mean_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_mean_frnt
        from ...ivy.functional.frontends.torch.reduction_ops import tensorflow_sum_frnt
    
        if not isinstance(img1, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a torch.Tensor. Got {type(img1)}")
        if not isinstance(img2, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Output type is not a torch.Tensor. Got {type(img2)}")
        if not len(tensorflow_shape_frnt_(img1)) == len(tensorflow_shape_frnt_(img2)):
            raise ValueError(
                f"Input shapes should be same. Got {type(img1)} and {type(img2)}."
            )
        g_masks: typing.Any = []
        CH: typing.Any = tensorflow_shape_frnt_(img1)[-3]
>       mux = tensorflow_conv2d_frnt(img1, g_masks, groups=CH, padding=self.pad)

Translated_Outputs/tensorflow_outputs/kornia/losses/ms_ssim.py:131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.44236583, 0.44737428, 0.7902....6421601 , 0.76124465],
         [0.9921301 , 0.4747575 , 0.94826883, 0.7730004 , 0.3672161 ]]]],
      dtype=float32)>
weight = [], bias = None, stride = 1, padding = 16, dilation = 1, groups = 3

    def tensorflow_conv2d_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
>       return tensorflow__conv_frnt(
            input,
            weight,
            bias=bias,
            stride=stride,
            padding=padding,
            dilation=dilation,
            groups=groups,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:59: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.44236583, 0.44737428, 0.7902....6421601 , 0.76124465],
         [0.9921301 , 0.4747575 , 0.94826883, 0.7730004 , 0.3672161 ]]]],
      dtype=float32)>
weight = [], bias = None, stride = 1, padding = [(16, 16), (16, 16)], dilation = 1, groups = 3

    def tensorflow__conv_frnt(
        input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1
    ):
        from ...tensor import tensorflow_shape_frnt_
        from .....backends.tensorflow.layers import tensorflow_conv_general_dilated
    
        dims = len(tensorflow_shape_frnt_(input)) - 2
        if isinstance(padding, (str,)):
            padding = padding.upper()
        elif isinstance(padding, (int,)):
            padding = [*[(padding, padding) for _ in range(dims)]]
        else:
            padding = [*[(p, p) for p in padding]]
>       ret = tensorflow_conv_general_dilated(
            input,
            weight,
            stride,
            padding,
            dims=dims,
            data_format="channel_first",
            filter_format="channel_first",
            dilations=dilation,
            feature_group_count=groups,
            bias=bias,
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/convolution_functions.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [<tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.44236583, 0.44737428, 0.790...      dtype=float32)>, <tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)>, 1, [(16, 16), (16, 16)]]
kwargs = {'bias': None, 'data_format': 'channel_first', 'dilations': 1, 'dims': 2, ...}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f4abe6e89d0>
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f4abe7500d0>, tensorflow_asarray = <function tensorflow_asarray at 0x7f4abdc01b40>
tensorflow_get_item = <function tensorflow_get_item at 0x7f4abe35af80>, num_args = 4
type_hints = mappingproxy(OrderedDict([('x', <Parameter "x: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops...."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['x', 'filters', 'strides', 'padding', 'dims', 'data_format', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable], typing.Union[tenso...le[int, int, int]], typing.Union[str, int, typing.Sequence[typing.Tuple[int, int]]], <class 'int'>, <class 'str'>, ...]
device = '/job:localhost/replica:0/task:0/device:CPU:0', i = 3

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
        from .functional.backends.tensorflow.general import tensorflow_get_item
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = <tf.Tensor: shape=(1, 3, 5, 5), dtype=float32, numpy=
array([[[[0.85511816, 0.08780694, 0.44236583, 0.44737428, 0.7902....6421601 , 0.76124465],
         [0.9921301 , 0.4747575 , 0.94826883, 0.7730004 , 0.3672161 ]]]],
      dtype=float32)>
filters = <tf.Tensor: shape=(0,), dtype=float32, numpy=array([], dtype=float32)>, strides = 1, padding = [(16, 16), (16, 16)]

    @tensorflow_handle_array_like_without_promotion
    def tensorflow_conv_general_dilated(
        x: Union[tensorflow.Tensor, tensorflow.Variable],
        filters: Union[tensorflow.Tensor, tensorflow.Variable],
        strides: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]],
        padding: Union[str, int, Sequence[Tuple[int, int]]],
        /,
        *,
        dims: int = 2,
        data_format: str = "channel_last",
        filter_format: str = "channel_last",
        feature_group_count: int = 1,
        x_dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        dilations: Union[int, Tuple[int], Tuple[int, int], Tuple[int, int, int]] = 1,
        bias: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from .device import tensorflow_dev
        from ...ivy.layers import tensorflow__get_x_data_format_bknd
    
        if filter_format == "channel_first":
>           filters = tensorflow.transpose(filters, (*range(2, dims + 2), 1, 0))
E           tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_MS_SSIMLoss.call().
E           
E           [1m{{function_node __wrapped__Transpose_device_/job:localhost/replica:0/task:0/device:CPU:0}} transpose expects a vector of size 1. But input(1) is a vector of size 4 [Op:Transpose][0m
E           
E           Arguments received by tensorflow_MS_SSIMLoss.call():
E             â€¢ img1=tf.Tensor(shape=(1, 3, 5, 5), dtype=float32)
E             â€¢ img2=tf.Tensor(shape=(1, 3, 5, 5), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/layers.py:155: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.losses.MS_SSIMLoss
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
2024-09-12 00:23:05.448660: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at transpose_op.cc:142 : INVALID_ARGUMENT: transpose expects a vector of size 1. But input(1) is a vector of size 4
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_losses.py::test_HausdorffERLoss[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_HausdorffERLoss.call().
FAILED kornia/test_losses.py::test_HausdorffERLoss3D[tensorflow-s2s-False] - TypeError: Exception encountered when calling tensorflow_HausdorffERLoss3D.call().
FAILED kornia/test_losses.py::test_MS_SSIMLoss[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_MS_SSIMLoss.call().
=============================================================================== 3 failed, 32 passed in 520.36s (0:08:40) ===============================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 19 items

kornia/geometry/test_camera.py ...................                                                                                                                                               [100%]

--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
==================================================================================== 19 passed in 189.07s (0:03:09) ====================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 6 items

kornia/geometry/test_depth.py ...FF.                                                                                                                                                             [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________ test_unproject_meshgrid[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_unproject_meshgrid(target_framework, mode, backend_compile):
        trace_args = (
            4,
            4,
            torch.eye(3),
        )
        trace_kwargs = {'normalize_points': False, 'device': 'cpu', 'dtype': torch.float32}
        test_args = (
            5,
            5,
            torch.eye(3),
        )
        test_kwargs = {'normalize_points': False, 'device': 'cpu', 'dtype': torch.float32}
>       _test_function(
            kornia.geometry.depth.unproject_meshgrid,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_depth.py:96: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unproject_meshgrid at 0x7f2b67a7ea70>, trace_args = (4, 4, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
trace_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, test_args = (5, 5, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
test_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function unproject_meshgrid at 0x7f2b67a7ea70>, trace_args = (4, 4, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
trace_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, test_args = (5, 5, tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]))
test_kwargs = {'device': 'cpu', 'dtype': torch.float32, 'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

height = 4, width = 4, camera_matrix = <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[1., 0., 0.],
       [0., 1., 0.],
       [0., 0., 1.]], dtype=float32)>, normalize_points = False
device = 'cpu', dtype = torch.float32

    def tensorflow_unproject_meshgrid(
        height, width, camera_matrix, normalize_points=False, device=None, dtype=None
    ):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ..utils.grid import tensorflow_create_meshgrid
        from .conversions import tensorflow_normalize_points_with_intrinsics
        from .conversions import tensorflow_convert_points_to_homogeneous
        from ..core._backend import normalize
    
        tensorflow_KORNIA_CHECK_SHAPE(camera_matrix, ["3", "3"])
        points_uv: typing.Any = tensorflow_squeeze_frnt_(
>           tensorflow_create_meshgrid(
                height, width, normalized_coordinates=False, device=device, dtype=dtype
            )
        )

Translated_Outputs/tensorflow_outputs/kornia/geometry/depth.py:44: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

height = 4, width = 4, normalized_coordinates = False, device = 'cpu', dtype = torch.float32

    def tensorflow_create_meshgrid(
        height, width, normalized_coordinates=True, device=None, dtype=None
    ):
        from ..core._backend import stack
        from ...ivy.functional.frontends.torch.creation_ops import tensorflow_linspace_frnt
        from ._compat import tensorflow_torch_meshgrid
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
    
>       xs: typing.Any = tensorflow_linspace_frnt(
            0, width - 1, width, device=device, dtype=dtype
        )

Translated_Outputs/tensorflow_outputs/kornia/utils/grid.py:39: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

start = 0, end = 3, steps = 4

    def tensorflow_linspace_frnt(
        start,
        end,
        steps,
        *,
        out=None,
        dtype=None,
        device=None,
        layout=None,
        requires_grad=False,
    ):
        from .dtype import tensorflow_get_default_dtype_frnt
        from ...backends.tensorflow.creation import tensorflow_linspace
    
        dtype = tensorflow_get_default_dtype_frnt() if dtype is None else dtype
>       return tensorflow_linspace(
            start, end, num=steps, dtype=dtype, device=device, out=out
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/creation_ops.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

dtype = torch.float32, args = (0, 3), kwargs = {'device': 'cpu', 'num': 4, 'out': None}, tensorflow_default_dtype_bknd = <function tensorflow_default_dtype_bknd at 0x7f2b608301f0>
tensorflow_exists_bknd = <function tensorflow_exists_bknd at 0x7f2b60830b80>, arr = None

    @functools.wraps(fn)
    def _infer_dtype(*args, dtype=None, **kwargs):
        from .functional.ivy.data_type import tensorflow_default_dtype_bknd
        from .functional.ivy.general import tensorflow_exists_bknd
    
        arr = (
            None
            if tensorflow_exists_bknd(dtype)
            else tensorflow__get_first_array(*args, **kwargs)
        )
        dtype = tensorflow_default_dtype_bknd(dtype=dtype, item=arr, as_native=True)
>       return fn(*args, dtype=dtype, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = [0, 3], kwargs = {'device': 'cpu', 'dtype': torch.float32, 'num': 4, 'out': None}, tensorflow_get_item = <function tensorflow_get_item at 0x7f2b60865360>
tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f2b60830a60>, tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f2b60830dc0>
tensorflow_asarray = <function tensorflow_asarray at 0x7f2b60832830>, num_args = 2
type_hints = mappingproxy(OrderedDict([('start', <Parameter "start: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.pyt..."out: Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, NoneType] = None">)]))
parameters = ['start', 'stop', 'num', 'axis', 'endpoint', 'dtype', ...]
annotations = [typing.Union[tensorflow.python.framework.tensor.Tensor, tensorflow.python.ops.variables.Variable, float], typing.Unio..., float], <class 'int'>, typing.Optional[int], <class 'bool'>, <class 'tensorflow.python.framework.dtypes.DType'>, ...]
device = None, i = 1

    @functools.wraps(fn)
    def _handle_array_like_without_promotion(*args, **kwargs):
        from .functional.backends.tensorflow.general import tensorflow_get_item
        from .functional.ivy.general import tensorflow_is_array_bknd
        from .functional.ivy.general import tensorflow_set_item_bknd
        from .functional.backends.tensorflow.creation import tensorflow_asarray
    
        args = list(args)
        num_args = len(args)
        try:
            type_hints = inspect.signature(fn).parameters
        except (TypeError, ValueError):
            return fn(*args, **kwargs)
        parameters = list(type_hints.keys())
        annotations = [param.annotation for param in type_hints.values()]
        device = tensorflow__get_preferred_device(args, kwargs)
        for i, (annotation, parameter, arg) in enumerate(
            zip(annotations, parameters, args)
        ):
            annotation_str = str(annotation)
            if (
                ("rray" in annotation_str or "Tensor" in annotation_str)
                and parameter != "out"
                and all(
                    sq not in annotation_str
                    for sq in ["Sequence", "List", "Tuple", "float", "int", "bool"]
                )
            ):
                if i < num_args:
                    if arg is None or tensorflow__check_in_nested_sequence(
                        arg, value=Ellipsis, _type=slice
                    ):
                        continue
                    if not tensorflow_is_array_bknd(arg):
                        args = tensorflow_set_item_bknd(
                            args, i, tensorflow_asarray(arg, device=device)
                        )
                elif parameters in kwargs:
                    kwarg = tensorflow_get_item(kwargs, parameter)
                    if not tensorflow_is_array_bknd(kwarg):
                        kwargs = tensorflow_set_item_bknd(
                            kwargs, parameter, tensorflow_asarray(kwarg, device=device)
                        )
>       return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/func_wrapper.py:136: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

start = 0, stop = 3, num = 4

    @tensorflow_infer_dtype
    @tensorflow_handle_array_like_without_promotion
    def tensorflow_linspace(
        start: Union[tensorflow.Tensor, tensorflow.Variable, float],
        stop: Union[tensorflow.Tensor, tensorflow.Variable, float],
        /,
        num: int,
        *,
        axis: Optional[int] = None,
        endpoint: bool = True,
        dtype: tensorflow.DType,
        device: Optional[str] = None,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        if axis is None:
            axis = -1
>       start = tensorflow.cast(tensorflow.constant(start), dtype=dtype)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/creation.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(), dtype=int32, numpy=0>,), kwargs = {'dtype': torch.float32}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

type_value = torch.float32

    @tf_export("dtypes.as_dtype", "as_dtype")
    def as_dtype(type_value):
      """Converts the given `type_value` to a `tf.DType`.
    
      Inputs can be existing `tf.DType` objects, a [`DataType`
      enum](https://www.tensorflow.org/code/tensorflow/core/framework/types.proto),
      a string type name, or a
      [`numpy.dtype`](https://numpy.org/doc/stable/reference/generated/numpy.dtype.html).
    
      Examples:
      >>> tf.as_dtype(2)  # Enum value for float64.
      tf.float64
    
      >>> tf.as_dtype('float')
      tf.float32
    
      >>> tf.as_dtype(np.int32)
      tf.int32
    
      Note: `DType` values are interned (i.e. a single instance of each dtype is
      stored in a map). When passed a new `DType` object, `as_dtype` always returns
      the interned value.
    
      Args:
        type_value: A value that can be converted to a `tf.DType` object.
    
      Returns:
        A `DType` corresponding to `type_value`.
    
      Raises:
        TypeError: If `type_value` cannot be converted to a `DType`.
      """
      if isinstance(type_value, DType):
        if type_value._handle_data is None:  # pylint:disable=protected-access
          return _INTERN_TABLE[type_value.as_datatype_enum]
        else:
          return type_value
    
      if isinstance(type_value, np.dtype):
        try:
          return _NP_TO_TF[type_value.type]
        except KeyError:
          pass
    
      try:
        return _ANY_TO_TF[type_value]
      except (KeyError, TypeError):
        # TypeError indicates that type_value is not hashable.
        pass
    
      if hasattr(type_value, "dtype"):
        try:
          return _NP_TO_TF[np.dtype(type_value.dtype).type]
        except (KeyError, TypeError):
          pass
    
      if isinstance(type_value, _dtypes.DType):
        return _INTERN_TABLE[type_value.as_datatype_enum]
    
>     raise TypeError(f"Cannot convert the argument `type_value`: {type_value!r} "
                      "to a TensorFlow DType.")
E     TypeError: Cannot convert the argument `type_value`: torch.float32 to a TensorFlow DType.

/opt/fw/tensorflow/tensorflow/python/framework/dtypes.py:852: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.depth.unproject_meshgrid
_____________________________________________________________________________ test_depth_to_normals[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_depth_to_normals(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 1, 4, 4),
            torch.eye(3)[None],
        )
        trace_kwargs = {'normalize_points': False}
        test_args = (
            torch.rand(1, 1, 5, 5),
            torch.eye(3)[None],
        )
        test_kwargs = {'normalize_points': False}
>       _test_function(
            kornia.geometry.depth.depth_to_normals,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/geometry/test_depth.py:120: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function depth_to_normals at 0x7f2b67a7eb90>
trace_args = (tensor([[[[0.6325, 0.5110, 0.7933, 0.4413],
          [0.0590, 0.7811, 0.3546, 0.4630],
          [0.8674, 0.8299, 0....          [0.7379, 0.9214, 0.5677, 0.4558]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
trace_kwargs = {'normalize_points': False}
test_args = (tensor([[[[0.1478, 0.6425, 0.0782, 0.6797, 0.4224],
          [0.8491, 0.6372, 0.5766, 0.3685, 0.3179],
          [0....  [0.0186, 0.8931, 0.4696, 0.8522, 0.4008]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
test_kwargs = {'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function depth_to_normals at 0x7f2b67a7eb90>
trace_args = (tensor([[[[0.6325, 0.5110, 0.7933, 0.4413],
          [0.0590, 0.7811, 0.3546, 0.4630],
          [0.8674, 0.8299, 0....          [0.7379, 0.9214, 0.5677, 0.4558]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
trace_kwargs = {'normalize_points': False}
test_args = (tensor([[[[0.1478, 0.6425, 0.0782, 0.6797, 0.4224],
          [0.8491, 0.6372, 0.5766, 0.3685, 0.3179],
          [0....  [0.0186, 0.8931, 0.4696, 0.8522, 0.4008]]]]), tensor([[[1., 0., 0.],
         [0., 1., 0.],
         [0., 0., 1.]]]))
test_kwargs = {'normalize_points': False}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

depth = <tf.Tensor: shape=(1, 1, 4, 4), dtype=float32, numpy=
array([[[[0.6324527 , 0.51097816, 0.793313  , 0.4412508 ],
     ....8299177 , 0.92642707, 0.4443673 ],
         [0.7379483 , 0.9214427 , 0.56772435, 0.45575553]]]],
      dtype=float32)>
camera_matrix = <tf.Tensor: shape=(1, 3, 3), dtype=float32, numpy=
array([[[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]]], dtype=float32)>, normalize_points = False

    def tensorflow_depth_to_normals(depth, camera_matrix, normalize_points=False):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ..filters.sobel import tensorflow_sobel
        from ...ivy.functional.frontends.torch.miscellaneous_ops import (
            tensorflow_cross_frnt,
        )
        from ..core._backend import normalize
    
        if not isinstance(depth, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input depht type is not a Tensor. Got {type(depth)}.")
        if not (
            len(tensorflow_shape_frnt_(depth)) == 4
            and tensorflow_shape_frnt_(depth)[-3] == 1
        ):
            raise ValueError(
                f"Input depth musth have a shape (B, 1, H, W). Got: {tensorflow_shape_frnt_(depth)}"
            )
        if not isinstance(camera_matrix, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(
                f"Input camera_matrix type is not a Tensor. Got {type(camera_matrix)}."
            )
        if not (
            len(tensorflow_shape_frnt_(camera_matrix)) == 3
            and tensorflow_shape_frnt_(camera_matrix)[-2:] == (3, 3)
        ):
            raise ValueError(
                f"Input camera_matrix must have a shape (B, 3, 3). Got: {tensorflow_shape_frnt_(camera_matrix)}."
            )
        xyz: typing.Any = tensorflow_depth_to_3d(depth, camera_matrix, normalize_points)
>       gradients: typing.Any = tensorflow_sobel.spatial_gradient(xyz)
E       AttributeError: 'function' object has no attribute 'spatial_gradient'

Translated_Outputs/tensorflow_outputs/kornia/geometry/depth.py:153: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.geometry.depth.depth_to_normals
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/geometry/depth.py:205: DeprecationWarning: Since kornia 0.7.0 the `depth_to_3d` is deprecated in favor of `depth_to_3d_v2`. This function will be replaced with the `depth_to_3d_v2` behaviour, where the that does not require the creation of a meshgrid. The return shape can be not backward compatible between these implementations.
  xyz: Tensor = depth_to_3d(depth, camera_matrix, normalize_points)  # Bx3xHxW
/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/kornia/geometry/depth.py:152: DeprecationWarning: Since kornia 0.7.0 the `tensorflow_depth_to_3d` is deprecated in favor of `depth_to_3d_v2`. This function will be replaced with the `depth_to_3d_v2` behaviour, where the that does not require the creation of a meshgrid. The return shape can be not backward compatible between these implementations.
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/geometry/test_depth.py::test_unproject_meshgrid[tensorflow-s2s-False] - TypeError: Cannot convert the argument `type_value`: torch.float32 to a TensorFlow DType.
FAILED kornia/geometry/test_depth.py::test_depth_to_normals[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'spatial_gradient'
=============================================================================== 2 failed, 4 passed in 167.00s (0:02:47) ================================================================================

========================================================================================= test session starts ==========================================================================================
platform linux -- Python 3.10.0, pytest-8.3.3, pluggy-1.5.0
rootdir: /ivy/ivy-integration-tests
plugins: anyio-4.4.0, hypothesis-6.98.10, metadata-3.1.1, json-report-1.5.0
collected 75 items

kornia/test_feature.py .....F......F.............F...FFF.FF.F......FFFFFFFFF.FFFFF..FFFF.FFFF.....                                                                                               [100%]

=============================================================================================== FAILURES ===============================================================================================
____________________________________________________________________________ test_get_laf_descriptors[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_get_laf_descriptors(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 1, 32, 32),
            torch.rand(1, 3, 2, 2),
            kornia.feature.HardNet8(pretrained=False),
        )
        trace_kwargs = {'patch_size': 32, 'grayscale_descriptor': True}
        test_args = (
            torch.rand(5, 1, 32, 32),
            torch.rand(5, 3, 2, 2),
            kornia.feature.HardNet8(pretrained=False),
        )
        test_kwargs = {'patch_size': 32, 'grayscale_descriptor': True}
>       _test_function(
            kornia.feature.get_laf_descriptors,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_feature.py:140: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function get_laf_descriptors at 0x7f0054edff40>
trace_args = (tensor([[[[0.1323, 0.4215, 0.0440,  ..., 0.6737, 0.2551, 0.9242],
          [0.1614, 0.7849, 0.4030,  ..., 0.1113, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
trace_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}
test_args = (tensor([[[[0.0600, 0.5072, 0.5549,  ..., 0.0654, 0.3079, 0.6926],
          [0.5148, 0.0340, 0.7607,  ..., 0.8487, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
test_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function get_laf_descriptors at 0x7f0054edff40>
trace_args = (tensor([[[[0.1323, 0.4215, 0.0440,  ..., 0.6737, 0.2551, 0.9242],
          [0.1614, 0.7849, 0.4030,  ..., 0.1113, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
trace_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}
test_args = (tensor([[[[0.0600, 0.5072, 0.5549,  ..., 0.0654, 0.3079, 0.6926],
          [0.5148, 0.0340, 0.7607,  ..., 0.8487, 0....=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
))
test_kwargs = {'grayscale_descriptor': True, 'patch_size': 32}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
>       orig_out = fn(*trace_args, **trace_kwargs)

helpers.py:253: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

img = tensor([[[[0.1323, 0.4215, 0.0440,  ..., 0.6737, 0.2551, 0.9242],
          [0.1614, 0.7849, 0.4030,  ..., 0.1113, 0.3...24, 0.2408, 0.6056,  ..., 0.0092, 0.6691, 0.9791],
          [0.5498, 0.6737, 0.5923,  ..., 0.0515, 0.0959, 0.2889]]]])
lafs = tensor([[[[0.2352, 0.2189],
          [0.6346, 0.3185]],

         [[0.5345, 0.2441],
          [0.0663, 0.7361]],

         [[0.2473, 0.1259],
          [0.5380, 0.0606]]]])
patch_descriptor = HardNet8(
  (features): Sequential(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=Fal...e=(1, 1), bias=False)
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
  )
)
patch_size = 32, grayscale_descriptor = True

    def get_laf_descriptors(
        img: Tensor, lafs: Tensor, patch_descriptor: Module, patch_size: int = 32, grayscale_descriptor: bool = True
    ) -> Tensor:
        r"""Function to get local descriptors, corresponding to LAFs (keypoints).
    
        Args:
            img: image features with shape :math:`(B,C,H,W)`.
            lafs: local affine frames :math:`(B,N,2,3)`.
            patch_descriptor: patch descriptor module, e.g. :class:`~kornia.feature.SIFTDescriptor`
                or :class:`~kornia.feature.HardNet`.
            patch_size: patch size in pixels, which descriptor expects.
            grayscale_descriptor: True if ``patch_descriptor`` expects single-channel image.
    
        Returns:
            Local descriptors of shape :math:`(B,N,D)` where :math:`D` is descriptor size.
        """
>       KORNIA_CHECK_LAF(lafs)

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/feature/integrated.py:46: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

laf = tensor([[[[0.2352, 0.2189],
          [0.6346, 0.3185]],

         [[0.5345, 0.2441],
          [0.0663, 0.7361]],

         [[0.2473, 0.1259],
          [0.5380, 0.0606]]]]), raises = True

    def KORNIA_CHECK_LAF(laf: Tensor, raises: bool = True) -> bool:
        """Check whether a Local Affine Frame (laf) has a valid shape.
    
        Args:
            laf: local affine frame tensor to evaluate.
            raises: bool indicating whether an exception should be raised upon failure.
    
        Raises:
            Exception: if the input laf does not have a shape :math:`(B,N,2,3)` and raises is True.
    
        Example:
            >>> lafs = torch.rand(2, 10, 2, 3)
            >>> KORNIA_CHECK_LAF(lafs)
            True
        """
>       return KORNIA_CHECK_SHAPE(laf, ["B", "N", "2", "3"], raises)

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/core/check.py:429: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = tensor([[[[0.2352, 0.2189],
          [0.6346, 0.3185]],

         [[0.5345, 0.2441],
          [0.0663, 0.7361]],

         [[0.2473, 0.1259],
          [0.5380, 0.0606]]]])
shape = ['B', 'N', '2', '3'], raises = True

    def KORNIA_CHECK_SHAPE(x: Tensor, shape: list[str], raises: bool = True) -> bool:
        """Check whether a tensor has a specified shape.
    
        The shape can be specified with a implicit or explicit list of strings.
        The guard also check whether the variable is a type `Tensor`.
    
        Args:
            x: the tensor to evaluate.
            shape: a list with strings with the expected shape.
            raises: bool indicating whether an exception should be raised upon failure.
    
        Raises:
            Exception: if the input tensor is has not the expected shape and raises is True.
    
        Example:
            >>> x = torch.rand(2, 3, 4, 4)
            >>> KORNIA_CHECK_SHAPE(x, ["B", "C", "H", "W"])  # implicit
            True
    
            >>> x = torch.rand(2, 3, 4, 4)
            >>> KORNIA_CHECK_SHAPE(x, ["2", "3", "H", "W"])  # explicit
            True
        """
        if "*" == shape[0]:
            shape_to_check = shape[1:]
            x_shape_to_check = x.shape[-len(shape) + 1 :]
        elif "*" == shape[-1]:
            shape_to_check = shape[:-1]
            x_shape_to_check = x.shape[: len(shape) - 1]
        else:
            shape_to_check = shape
            x_shape_to_check = x.shape
    
        if len(x_shape_to_check) != len(shape_to_check):
            if raises:
                raise TypeError(f"{x} shape must be [{shape}]. Got {x.shape}")
            else:
                return False
    
        for i in range(len(x_shape_to_check)):
            # The voodoo below is because torchscript does not like
            # that dim can be both int and str
            dim_: str = shape_to_check[i]
            if not dim_.isnumeric():
                continue
            dim = int(dim_)
            if x_shape_to_check[i] != dim:
                if raises:
>                   raise TypeError(f"{x} shape must be [{shape}]. Got {x.shape}")
E                   TypeError: tensor([[[[0.2352, 0.2189],
E                             [0.6346, 0.3185]],
E                   
E                            [[0.5345, 0.2441],
E                             [0.0663, 0.7361]],
E                   
E                            [[0.2473, 0.1259],
E                             [0.5380, 0.0606]]]]) shape must be [['B', 'N', '2', '3']]. Got torch.Size([1, 3, 2, 2])

/opt/miniconda/envs/multienv/lib/python3.10/site-packages/kornia/core/check.py:80: TypeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.integrated.get_laf_descriptors
_______________________________________________________________________ test_extract_patches_from_pyramid[tensorflow-s2s-False] ________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_extract_patches_from_pyramid(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 3, 32, 32),
            torch.rand(1, 5, 2, 3),
        )
        trace_kwargs = {'PS': 32}
        test_args = (
            torch.rand(1, 3, 64, 64),  # TODO: changing the batch size of these causes the trace_graph test to fail
            torch.rand(1, 5, 2, 3),
        )
        test_kwargs = {'PS': 16}
>       _test_function(
            kornia.feature.extract_patches_from_pyramid,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_feature.py:317: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function extract_patches_from_pyramid at 0x7f0054e6aa70>
trace_args = (tensor([[[[0.8158, 0.7970, 0.5079,  ..., 0.7139, 0.1498, 0.0307],
          [0.6229, 0.7976, 0.7277,  ..., 0.2333, 0....933],
          [0.6877, 0.8527, 0.1262]],

         [[0.5741, 0.2852, 0.8743],
          [0.1915, 0.2321, 0.9128]]]]))
trace_kwargs = {'PS': 32}
test_args = (tensor([[[[0.1847, 0.0042, 0.3931,  ..., 0.3311, 0.2591, 0.4594],
          [0.6858, 0.4141, 0.4045,  ..., 0.0780, 0....643],
          [0.8416, 0.8834, 0.8578]],

         [[0.4078, 0.5079, 0.4272],
          [0.2734, 0.3585, 0.2653]]]]))
test_kwargs = {'PS': 16}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function extract_patches_from_pyramid at 0x7f0054e6aa70>
trace_args = (tensor([[[[0.8158, 0.7970, 0.5079,  ..., 0.7139, 0.1498, 0.0307],
          [0.6229, 0.7976, 0.7277,  ..., 0.2333, 0....933],
          [0.6877, 0.8527, 0.1262]],

         [[0.5741, 0.2852, 0.8743],
          [0.1915, 0.2321, 0.9128]]]]))
trace_kwargs = {'PS': 32}
test_args = (tensor([[[[0.1847, 0.0042, 0.3931,  ..., 0.3311, 0.2591, 0.4594],
          [0.6858, 0.4141, 0.4045,  ..., 0.0780, 0....643],
          [0.8416, 0.8834, 0.8578]],

         [[0.4078, 0.5079, 0.4272],
          [0.2734, 0.3585, 0.2653]]]]))
test_kwargs = {'PS': 16}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
        graph_out = translated_fn(*graph_args, **graph_kwargs)
    
        if deterministic:
>           _to_numpy_and_allclose(orig_out, graph_out, tolerance=tolerance)

helpers.py:259: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = tensor([[[[[0.8158, 0.8158, 0.8158,  ..., 0.7654, 0.7550, 0.7446],
           [0.8158, 0.8158, 0.8158,  ..., 0.7589, 0..., 0.4565, 0.4427,  ..., 0.3138, 0.3411, 0.3688],
           [0.4630, 0.4492, 0.4356,  ..., 0.3295, 0.3571, 0.3851]]]]])
transpiled_x = <tf.Tensor: shape=(1, 5, 3, 32, 32), dtype=float32, numpy=
array([[[[[0.8157782, 0.8157782, 0.8157782, ..., 0.       ,...   ],
          [0.       , 0.       , 0.       , ..., 0.       , 0.       ,
           0.       ]]]]], dtype=float32)>
tolerance = 0.001

    def _to_numpy_and_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:100: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[[0.8157782 , 0.8157782 , 0.8157782 , ..., 0.7653868 ,
           0.75500274, 0.7446185 ],
          [0.81577...
          [0.46304423, 0.44922793, 0.4356398 , ..., 0.3294935 ,
           0.3571009 , 0.3850616 ]]]]], dtype=float32)
y = array([[[[[0.8157782, 0.8157782, 0.8157782, ..., 0.       , 0.       ,
           0.       ],
          [0.       , 0....    ],
          [0.       , 0.       , 0.       , ..., 0.       , 0.       ,
           0.       ]]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.laf.extract_patches_from_pyramid
____________________________________________________________________________ test_laf_is_inside_image[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_laf_is_inside_image(target_framework, mode, backend_compile):
        trace_args = (
            torch.rand(1, 5, 2, 3),
            torch.rand(1, 1, 32, 32),
        )
        trace_kwargs = {'border': 0}
        test_args = (
            torch.rand(2, 10, 2, 3),
            torch.rand(2, 1, 64, 64),
        )
        test_kwargs = {'border': 1}
>       _test_function(
            kornia.feature.laf_is_inside_image,
            trace_args,
            trace_kwargs,
            test_args,
            test_kwargs,
            target_framework,
            backend_compile,
            tolerance=1e-3,
            mode=mode,
        )

kornia/test_feature.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function laf_is_inside_image at 0x7f0054e6ab00>
trace_args = (tensor([[[[0.3367, 0.8003, 0.7110],
          [0.9875, 0.9419, 0.0070]],

         [[0.4645, 0.5193, 0.0300],
       ...9, 0.3729, 0.1458,  ..., 0.2556, 0.9776, 0.7983],
          [0.6844, 0.7354, 0.7803,  ..., 0.7168, 0.4957, 0.0898]]]]))
trace_kwargs = {'border': 0}
test_args = (tensor([[[[0.6612, 0.8231, 0.4464],
          [0.7031, 0.5987, 0.7290]],

         [[0.2764, 0.4744, 0.2469],
       ...0, 0.0651, 0.1874,  ..., 0.8303, 0.2979, 0.5687],
          [0.4365, 0.7809, 0.9847,  ..., 0.7925, 0.2361, 0.1556]]]]))
test_kwargs = {'border': 1}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, mode = 's2s', skip = False, deterministic = True

    def _test_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        mode="transpile",
        skip=False,
        deterministic=True,
    ):
        # print out the full function module/name, so it will appear in the test_report.json
        print(f"{fn.__module__}.{fn.__name__}")
    
        if skip and mode != "s2s":
            # any skipped due to DCF issues should still work with ivy.source_to_source
            pytest.skip()
    
        if mode == "s2s":
>           _test_source_to_source_function(
                fn,
                trace_args,
                trace_kwargs,
                test_args,
                test_kwargs,
                target,
                backend_compile,
                tolerance=tolerance,
                deterministic=deterministic,

helpers.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

fn = <function laf_is_inside_image at 0x7f0054e6ab00>
trace_args = (tensor([[[[0.3367, 0.8003, 0.7110],
          [0.9875, 0.9419, 0.0070]],

         [[0.4645, 0.5193, 0.0300],
       ...9, 0.3729, 0.1458,  ..., 0.2556, 0.9776, 0.7983],
          [0.6844, 0.7354, 0.7803,  ..., 0.7168, 0.4957, 0.0898]]]]))
trace_kwargs = {'border': 0}
test_args = (tensor([[[[0.6612, 0.8231, 0.4464],
          [0.7031, 0.5987, 0.7290]],

         [[0.2764, 0.4744, 0.2469],
       ...0, 0.0651, 0.1874,  ..., 0.8303, 0.2979, 0.5687],
          [0.4365, 0.7809, 0.9847,  ..., 0.7925, 0.2361, 0.1556]]]]))
test_kwargs = {'border': 1}, target = 'tensorflow', backend_compile = False, tolerance = 0.001, deterministic = True

    def _test_source_to_source_function(
        fn,
        trace_args,
        trace_kwargs,
        test_args,
        test_kwargs,
        target,
        backend_compile,
        tolerance=1e-3,
        deterministic=True,
    ):
        if backend_compile and target == "numpy":
            pytest.skip()
    
        translated_fn = ivy.source_to_source(fn, source="torch", target=target)
    
        if backend_compile:
            try:
                fn = torch.compile(fn)
                fn(*trace_args, **trace_kwargs)
                orig_compilable = True
            except:
                orig_compilable = False
    
            # only test with backend compilation if the original function was compilable in torch
            if orig_compilable:
                translated_fn = _backend_compile(translated_fn, target)
    
        # test it works with the trace_args as input
        orig_out = fn(*trace_args, **trace_kwargs)
        graph_args = _nest_torch_tensor_to_new_framework(trace_args, target)
        graph_kwargs = _nest_torch_tensor_to_new_framework(trace_kwargs, target)
>       graph_out = translated_fn(*graph_args, **graph_kwargs)

helpers.py:256: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

laf = <tf.Tensor: shape=(1, 5, 2, 3), dtype=float32, numpy=
array([[[[0.33670074, 0.8003421 , 0.7109976 ],
         [0.98748...46]],

        [[0.29027623, 0.24802965, 0.37763953],
         [0.79946804, 0.4140513 , 0.9651664 ]]]], dtype=float32)>
images = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[0.67416847, 0.7239858 , 0.8376539 , ..., 0.2850259 ,...],
         [0.6843512 , 0.7353921 , 0.7803289 , ..., 0.71677446,
          0.49568248, 0.08977616]]]], dtype=float32)>
border = 0

    def tensorflow_laf_is_inside_image(laf, images, border=0):
        from ..core.check import tensorflow_KORNIA_CHECK_LAF
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_min_frnt_
    
        tensorflow_KORNIA_CHECK_LAF(laf)
        _, _, h, w = tensorflow_size_frnt_(images)
        pts = tensorflow_laf_to_boundary_points(laf, 12)
        good_lafs_mask = (
>           (pts[..., 0] >= border)
            * (pts[..., 0] <= w - border)
            * (pts[..., 1] >= border)
            * (pts[..., 1] <= h - border)
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/laf.py:523: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 12), dtype=bool, numpy=
array([[[ True,  True,  True,  True,  True,  True, False, False,  Tr...rue,  True],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True]]])>)
kwargs = {}
arg = <tf.Tensor: shape=(1, 5, 12), dtype=bool, numpy=
array([[[ True,  True,  True,  True,  True,  True,  True,  True,  Tru...True,  True],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True]]])>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 5, 12), dtype=bool, numpy=
array([[[ True,  True,  True,  True,  True,  True, False, False,  Tr...rue,  True],
        [ True,  True,  True,  True,  True,  True,  True,  True,  True,
          True,  True,  True]]])>)
kwargs = {}

    def error_handler(*args, **kwargs):
      try:
        if not is_traceback_filtering_enabled():
          return fn(*args, **kwargs)
      except NameError:
        # In some very rare cases,
        # `is_traceback_filtering_enabled` (from the outer scope) may not be
        # accessible from inside this function
        return fn(*args, **kwargs)
    
      filtered_tb = None
      try:
        return fn(*args, **kwargs)
      except Exception as e:
        filtered_tb = _process_traceback_frames(e.__traceback__)
>       raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/tensorflow/python/util/traceback_utils.py:153: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

e = _NotOkStatusException(), name = None

    def raise_from_not_ok_status(e, name) -> NoReturn:
      e.message += (" name: " + str(name if name is not None else ""))
>     raise core._status_to_exception(e) from None  # pylint: disable=protected-access
E     tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of bool is not in the list of allowed values: bfloat16, half, float, double, uint8, int8, uint16, int16, int32, uint32, uint64, int64, complex64, complex128
E     	; NodeDef: {{node Mul}}; Op<name=Mul; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_UINT8, DT_INT8, DT_UINT16, DT_INT16, DT_INT32, DT_UINT32, DT_UINT64, DT_INT64, DT_COMPLEX64, DT_COMPLEX128]; is_commutative=true> [Op:Mul] name:

/opt/fw/tensorflow/tensorflow/python/framework/ops.py:5983: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.laf.laf_is_inside_image
____________________________________________________________________________ test_DenseSIFTDescriptor[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DenseSIFTDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.DenseSIFTDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDenseSIFTDescriptor = ivy.transpile(kornia.feature.DenseSIFTDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(2, 1, 200, 300)
        torch_out = kornia.feature.DenseSIFTDescriptor()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDenseSIFTDescriptor()(transpiled_x)

kornia/test_feature.py:736: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2)
args = (<tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.3118208 , 0.03191394, 0.97444963, ..., 0.842147...
         [0.85345346, 0.6320972 , 0.12276614, ..., 0.73208255,
          0.15004092, 0.9627395 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f004dac0950, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipv...,
         [0.85345346, 0.6320972 , 0.12276614, ..., 0.73208255,
          0.15004092, 0.9627395 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.3118208 , 0.03191394, 0.97444963, ..., 0.842147...
         [0.85345346, 0.6320972 , 0.12276614, ..., 0.73208255,
          0.15004092, 0.9627395 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipv...,
         [0.85345346, 0.6320972 , 0.12276614, ..., 0.73208255,
          0.15004092, 0.9627395 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.3118208 , 0.03191394, 0.97444963, ..., 0.842147...
         [0.85345346, 0.6320972 , 0.12276614, ..., 0.73208255,
          0.15004092, 0.9627395 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.3118208 , 0.03191394, 0.97444963, ..., 0.8421476...],
         [0.85345346, 0.6320972 , 0.12276614, ..., 0.73208255,
          0.15004092, 0.9627395 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2),)
kwargs = {'input': <tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.3118208 , 0.03191394, 0.97444963, ...,...,
         [0.85345346, 0.6320972 , 0.12276614, ..., 0.73208255,
          0.15004092, 0.9627395 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DenseSIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, spatial_bin_size=4, rootsift=True, stride=1, clipval=0.2)
input = <tf.Tensor: shape=(2, 1, 200, 300), dtype=float32, numpy=
array([[[[0.3118208 , 0.03191394, 0.97444963, ..., 0.8421476...],
         [0.85345346, 0.6320972 , 0.12276614, ..., 0.73208255,
          0.15004092, 0.9627395 ]]]], dtype=float32)>

    def call(self, input):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ..filters.sobel import tensorflow_spatial_gradient
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_atan2_frnt,
        )
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_floor_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_clamp__frnt_
        from ..constants import pi
        from ..core._backend import concatenate
        from ..core._backend import normalize
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "1", "H", "W"])
        B, CH, W, H = tensorflow_size_frnt_(input)
>       self.bin_pooling_kernel = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.bin_pooling_kernel, input.dtype), input.device
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/siftdesc.py:169: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (KerasConv2D(), '/job:localhost/replica:0/task:0/device:CPU:0'), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f004e18b370>, array_like = KerasConv2D()
pattern = '_bknd_|_bknd|_frnt_|_frnt', fn_name = 'to'

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
            return fn(*args, **kwargs)
        else:
            pattern = "_bknd_|_bknd|_frnt_|_frnt"
            fn_name = extract_function_name(re.sub(pattern, "", fn.__name__))
            try:
                new_fn = getattr(array_like, fn_name)
                if not callable(new_fn):
                    return new_fn
                return new_fn(*args[1:], **kwargs)
            except AttributeError:
>               return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:195: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = KerasConv2D(), args = ('/job:localhost/replica:0/task:0/device:CPU:0',), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f004e18b370>
tensorflow_dev = <function tensorflow_dev at 0x7f004d54c700>, tensorflow_dtype = <function tensorflow_dtype at 0x7f004d54c5e0>
tensorflow_check_elem_in_list = <function tensorflow_check_elem_in_list at 0x7f004dd5c430>, tensorflow_as_ivy_dev = <function tensorflow_as_ivy_dev at 0x7f004d54c790>
tensorflow_asarray = <function tensorflow_asarray at 0x7f004e0f6710>, _all_ivy_dtypes_str = ('int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', ...)
device = '/job:localhost/replica:0/task:0/device:CPU:0', dtype = None, arg = '/job:localhost/replica:0/task:0/device:CPU:0'

    @tensorflow_handle_methods
    def tensorflow_to_frnt_(tensor, *args, **kwargs):
        from ...ivy.general import tensorflow_is_array_bknd
        from ...backends.tensorflow.device import tensorflow_dev
        from ...backends.tensorflow.data_type import tensorflow_dtype
        from ....utils.assertions import tensorflow_check_elem_in_list
        from ...backends.tensorflow.device import tensorflow_as_ivy_dev
        from ...backends.tensorflow.creation import tensorflow_asarray
        from ....__init__ import _all_ivy_dtypes_str
    
        device = None
        dtype = None
        for arg in args:
            if hasattr(arg, "ivy_array") or tensorflow_is_array_bknd(arg):
                device = tensorflow_dev(arg)
                dtype = tensorflow_dtype(arg)
            elif (
                isinstance(arg, (tf.DType,))
                or isinstance(arg, (str,))
                and hasattr(arg, "as_native_dtype")
                or arg in _all_ivy_dtypes_str
            ):
                dtype = arg
            elif isinstance(arg, (str, str, str)):
                if isinstance(arg, (str,)) and not isinstance(arg, (str, str)):
                    tensorflow_check_elem_in_list(
                        arg,
                        [
                            "cpu",
                            "cuda",
                            "mps",
                            "xpu",
                            "mkldnn",
                            "opengl",
                            "opencl",
                            "ideep",
                            "hip",
                            "ve",
                            "ort",
                            "mlc",
                            "xla",
                            "lazy",
                            "vulkan",
                            "meta",
                            "hpu",
                        ],
                    )
                device = arg
        if "device" in kwargs:
            device = kwargs["device"]
        if "dtype" in kwargs:
            dtype = kwargs["dtype"]
        if (dtype is None or tensor.dtype == dtype) and (
>           device is None or tensor.device == tensorflow_as_ivy_dev(device)
        ):

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D(), name = 'device'

    def __getattribute__(self, name):
        built = object.__getattribute__(self, "__dict__").get("built", False)
        if built:
            attr_map = {"weight": "kernel", "out_channels": "filters"}
        else:
            attr_map = {
                "out_channels": "filters",
            }
    
        new_name = attr_map[name] if name in attr_map else name
>       return super().__getattribute__(new_name)
E       AttributeError: Exception encountered when calling tensorflow_DenseSIFTDescriptor.call().
E       
E       [1m'KerasConv2D' object has no attribute 'device'[0m
E       
E       Arguments received by tensorflow_DenseSIFTDescriptor.call():
E         â€¢ input=tf.Tensor(shape=(2, 1, 200, 300), dtype=float32)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:465: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.DenseSIFTDescriptor
______________________________________________________________________________ test_SIFTDescriptor[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SIFTDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.SIFTDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSIFTDescriptor = ivy.transpile(kornia.feature.SIFTDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(23, 1, 41, 41)
        torch_out = kornia.feature.SIFTDescriptor()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSIFTDescriptor()(transpiled_x)

kornia/test_feature.py:753: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2)
args = (<tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[4.84474301e-02, 2.14360297e-01, 7.23243952e-01, .....0486085e-01, 5.27064621e-01, ...,
          1.94222569e-01, 2.71370709e-01, 8.06314230e-01]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f004dac1d40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), <tf.Tensor:...40486085e-01, 5.27064621e-01, ...,
          1.94222569e-01, 2.71370709e-01, 8.06314230e-01]]]],
      dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[4.84474301e-02, 2.14360297e-01, 7.23243952e-01, .....0486085e-01, 5.27064621e-01, ...,
          1.94222569e-01, 2.71370709e-01, 8.06314230e-01]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), <tf.Tensor:...40486085e-01, 5.27064621e-01, ...,
          1.94222569e-01, 2.71370709e-01, 8.06314230e-01]]]],
      dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2), v = None, buffers = None
args = (<tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[4.84474301e-02, 2.14360297e-01, 7.23243952e-01, .....0486085e-01, 5.27064621e-01, ...,
          1.94222569e-01, 2.71370709e-01, 8.06314230e-01]]]],
      dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[4.84474301e-02, 2.14360297e-01, 7.23243952e-01, .......40486085e-01, 5.27064621e-01, ...,
          1.94222569e-01, 2.71370709e-01, 8.06314230e-01]]]],
      dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2),)
kwargs = {'input': <tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[4.84474301e-02, 2.14360297e-01, 7.2324395...40486085e-01, 5.27064621e-01, ...,
          1.94222569e-01, 2.71370709e-01, 8.06314230e-01]]]],
      dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SIFTDescriptor(num_ang_bins=8, num_spatial_bins=4, patch_size=41, rootsift=True, clipval=0.2)
input = <tf.Tensor: shape=(23, 1, 41, 41), dtype=float32, numpy=
array([[[[4.84474301e-02, 2.14360297e-01, 7.23243952e-01, .......40486085e-01, 5.27064621e-01, ...,
          1.94222569e-01, 2.71370709e-01, 8.06314230e-01]]]],
      dtype=float32)>

    def call(self, input):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ..filters.sobel import tensorflow_spatial_gradient
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_sqrt_frnt
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_atan2_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_type_as_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_expand_as_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_floor_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import (
            tensorflow_clamp_frnt,
        )
        from ..constants import pi
        from ..core._backend import concatenate
        from ..core._backend import normalize
    
        tensorflow_KORNIA_CHECK_SHAPE(
            input, ["B", "1", f"{self.patch_size}", f"{self.patch_size}"]
        )
        B: typing.Any = tensorflow_shape_frnt_(input)[0]
>       self.pk = tensorflow_to_frnt_(
            tensorflow_to_frnt_(self.pk, input.dtype), input.device
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/siftdesc.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (KerasConv2D(), '/job:localhost/replica:0/task:0/device:CPU:0'), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f004d5adfc0>, array_like = KerasConv2D()
pattern = '_bknd_|_bknd|_frnt_|_frnt', fn_name = 'to'

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
            return fn(*args, **kwargs)
        else:
            pattern = "_bknd_|_bknd|_frnt_|_frnt"
            fn_name = extract_function_name(re.sub(pattern, "", fn.__name__))
            try:
                new_fn = getattr(array_like, fn_name)
                if not callable(new_fn):
                    return new_fn
                return new_fn(*args[1:], **kwargs)
            except AttributeError:
>               return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:195: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensor = KerasConv2D(), args = ('/job:localhost/replica:0/task:0/device:CPU:0',), kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f004d5adfc0>
tensorflow_dev = <function tensorflow_dev at 0x7f004d5872e0>, tensorflow_dtype = <function tensorflow_dtype at 0x7f004d5871c0>
tensorflow_check_elem_in_list = <function tensorflow_check_elem_in_list at 0x7f004e08ab00>, tensorflow_as_ivy_dev = <function tensorflow_as_ivy_dev at 0x7f004d587370>
tensorflow_asarray = <function tensorflow_asarray at 0x7f004d585360>, _all_ivy_dtypes_str = ('int8', 'int16', 'int32', 'int64', 'uint8', 'uint16', ...)
device = '/job:localhost/replica:0/task:0/device:CPU:0', dtype = None, arg = '/job:localhost/replica:0/task:0/device:CPU:0'

    @tensorflow_handle_methods
    def tensorflow_to_frnt_(tensor, *args, **kwargs):
        from ...ivy.general import tensorflow_is_array_bknd
        from ...backends.tensorflow.device import tensorflow_dev
        from ...backends.tensorflow.data_type import tensorflow_dtype
        from ....utils.assertions import tensorflow_check_elem_in_list
        from ...backends.tensorflow.device import tensorflow_as_ivy_dev
        from ...backends.tensorflow.creation import tensorflow_asarray
        from ....__init__ import _all_ivy_dtypes_str
    
        device = None
        dtype = None
        for arg in args:
            if hasattr(arg, "ivy_array") or tensorflow_is_array_bknd(arg):
                device = tensorflow_dev(arg)
                dtype = tensorflow_dtype(arg)
            elif (
                isinstance(arg, (tf.DType,))
                or isinstance(arg, (str,))
                and hasattr(arg, "as_native_dtype")
                or arg in _all_ivy_dtypes_str
            ):
                dtype = arg
            elif isinstance(arg, (str, str, str)):
                if isinstance(arg, (str,)) and not isinstance(arg, (str, str)):
                    tensorflow_check_elem_in_list(
                        arg,
                        [
                            "cpu",
                            "cuda",
                            "mps",
                            "xpu",
                            "mkldnn",
                            "opengl",
                            "opencl",
                            "ideep",
                            "hip",
                            "ve",
                            "ort",
                            "mlc",
                            "xla",
                            "lazy",
                            "vulkan",
                            "meta",
                            "hpu",
                        ],
                    )
                device = arg
        if "device" in kwargs:
            device = kwargs["device"]
        if "dtype" in kwargs:
            dtype = kwargs["dtype"]
        if (dtype is None or tensor.dtype == dtype) and (
>           device is None or tensor.device == tensorflow_as_ivy_dev(device)
        ):

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/tensor.py:197: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasConv2D(), name = 'device'

    def __getattribute__(self, name):
        built = object.__getattribute__(self, "__dict__").get("built", False)
        if built:
            attr_map = {"weight": "kernel", "out_channels": "filters"}
        else:
            attr_map = {
                "out_channels": "filters",
            }
    
        new_name = attr_map[name] if name in attr_map else name
>       return super().__getattribute__(new_name)
E       AttributeError: Exception encountered when calling tensorflow_SIFTDescriptor.call().
E       
E       [1m'KerasConv2D' object has no attribute 'device'[0m
E       
E       Arguments received by tensorflow_SIFTDescriptor.call():
E         â€¢ input=tf.Tensor(shape=(23, 1, 41, 41), dtype=float32)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:465: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SIFTDescriptor
_______________________________________________________________________________ test_MKDDescriptor[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_MKDDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.MKDDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledMKDDescriptor = ivy.transpile(kornia.feature.MKDDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(23, 1, 32, 32)
        torch_out = kornia.feature.MKDDescriptor()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledMKDDescriptor()(transpiled_x)

kornia/test_feature.py:770: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_MKDDescriptor' object has no attribute 'output_dims'") raised in repr()] tensorflow_MKDDescriptor object at 0x7f004ed0d1b0>, patch_size = 32
kernel_type = 'concat', whitening = 'pcawt', training_set = 'liberty', output_dims = 128

    def __init__(
        self,
        patch_size=32,
        kernel_type="concat",
        whitening="pcawt",
        training_set="liberty",
        output_dims=128,
    ):
        from ..filters.gaussian import tensorflow_GaussianBlur2d
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ..utils.helpers import tensorflow_map_location_to_cpu
    
        self.super___init__(
            patch_size=patch_size,
            kernel_type=kernel_type,
            whitening=whitening,
            training_set=training_set,
            output_dims=output_dims,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size: typing.Any = patch_size
        self.kernel_type: typing.Any = kernel_type
        self.whitening: typing.Any = whitening
        self.training_set: typing.Any = training_set
        self.sigma = 1.4 * (patch_size / 64)
        self.smoothing = tensorflow_GaussianBlur2d(
            (5, 5), (self.sigma, self.sigma), "replicate"
        )
        self.gradients = tensorflow_MKDGradients()
        polar_s: typing.Any = "polar"
        cart_s: typing.Any = "cart"
        self.parametrizations = (
            [polar_s, cart_s] if self.kernel_type == "concat" else [self.kernel_type]
        )
        self.odims: typing.Any = 0
        relative_orientations = {polar_s: True, cart_s: False}
        self.feats = {}
        for parametrization in self.parametrizations:
            gradient_embedding = tensorflow_EmbedGradients(
                patch_size=patch_size,
                relative=tensorflow_get_item(relative_orientations, parametrization),
            )
>           spatial_encoding = tensorflow_ExplicitSpacialEncoding(
                kernel_type=parametrization,
                fmap_size=patch_size,
                in_dims=gradient_embedding.kernel.d,
            )

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:2227: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_ExplicitSpacialEncoding' object has no attribute 'out_dims'") raised in repr()] tensorflow_ExplicitSpacialEncoding object at 0x7f004cceac20>, args = ()
kwargs = {'fmap_size': 32, 'in_dims': 7, 'kernel_type': 'polar'}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_ExplicitSpacialEncoding' object has no attribute 'out_dims'") raised in repr()] tensorflow_ExplicitSpacialEncoding object at 0x7f004cceac20>, kernel_type = 'polar'
fmap_size = 32, in_dims = 7, do_gmask = True, do_l2 = True

    @tensorflow_store_config_info
    def __init__(
        self, kernel_type="polar", fmap_size=32, in_dims=7, do_gmask=True, do_l2=True
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        self.super___init__(
            kernel_type=kernel_type,
            fmap_size=fmap_size,
            in_dims=in_dims,
            do_gmask=do_gmask,
            do_l2=do_l2,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if kernel_type not in ["polar", "cart"]:
            raise NotImplementedError(
                f"{kernel_type} is not valid, use polar or cart)."
            )
        self.kernel_type = kernel_type
        self.fmap_size = fmap_size
        self.in_dims = in_dims
        self.do_gmask = do_gmask
        self.do_l2 = do_l2
        self.grid = tensorflow_get_grid_dict(fmap_size)
        self.gmask = None
>       emb = tensorflow_spatial_kernel_embedding(self.kernel_type, self.grid)

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:1322: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kernel_type = 'polar'
grids = {'x': <tf.Tensor: shape=(32, 32), dtype=float32, numpy=
array([[-1.        , -0.9354839 , -0.87096775, ...,  0.8709677...,
       [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
         0.81871915,  0.7853981 ]], dtype=float32)>}

    def tensorflow_spatial_kernel_embedding(kernel_type, grids):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_float_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_squeeze_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_index_select_frnt_
        from ..constants import pi
    
        factors = {"phi": 1.0, "rho": pi / sqrt2, "x": pi / 2, "y": pi / 2}
        if kernel_type == "cart":
            coeffs_ = "xy"
            params_ = ["x", "y"]
        elif kernel_type == "polar":
            coeffs_ = "rhophi"
            params_ = ["phi", "rho"]
        keys = list(grids.keys())
        patch_size = tensorflow_shape_frnt_(tensorflow_get_item(grids, keys[0]))[-1]
        grids_normed = {k: (v * tensorflow_get_item(factors, k)) for k, v in grids.items()}
        grids_normed = {
            k: tensorflow_float_frnt_(
                tensorflow_unsqueeze_frnt_(tensorflow_unsqueeze_frnt_(v, 0), 0)
            )
            for k, v in grids_normed.items()
        }
        vm_a = tensorflow_VonMisesKernel(
            patch_size=patch_size, coeffs=tensorflow_get_item(COEFFS, coeffs_)
        )
        vm_b = tensorflow_VonMisesKernel(
            patch_size=patch_size, coeffs=tensorflow_get_item(COEFFS, coeffs_)
        )
        emb_a = tensorflow_squeeze_frnt_(
>           vm_a(tensorflow_get_item(grids_normed, params_[0]))
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:1281: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234])
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.8542...    [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d16c13a30, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...n='test_MKDDescriptor', code_context=['    transpiled_out = TranspiledMKDDescriptor()(transpiled_x)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), <tf.Tensor: shape=(1, ...     [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.8542...    [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), <tf.Tensor: shape=(1, ...     [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.8542...    [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.85425...      [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]),)
kwargs = {'x': <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0...     [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234])
x = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[-2.3561945 , -2.3228736 , -2.287338  , ..., -0.85425...      [ 2.3561945 ,  2.3228736 ,  2.287338  , ...,  0.8542547 ,
           0.81871915,  0.7853981 ]]]], dtype=float32)>

    def call(self, x):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_cat_frnt,
        )
        from ..core._backend import cos
        from ..core._backend import sin
    
        if not isinstance(x, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Input type is not a Tensor. Got {type(x)}")
        if not len(tensorflow_shape_frnt_(x)) == 4 or tensorflow_shape_frnt_(x)[1] != 1:
            raise ValueError(
                f"Invalid input shape, we expect Bx1xHxW. Got: {tensorflow_shape_frnt_(x)}"
            )
        if not isinstance(self.emb0, (tensorflow.Tensor, tensorflow.Variable)):
            raise TypeError(f"Emb0 type is not a Tensor. Got {type(x)}")
        emb0 = tensorflow_repeat_frnt_(
            tensorflow_to_frnt_(self.emb0, x), tensorflow_size_frnt_(x, 0), 1, 1, 1
        )
        frange = tensorflow_to_frnt_(self.frange, x) * x
        emb1 = cos(frange)
        emb2 = sin(frange)
        embedding = tensorflow_cat_frnt([emb0, emb1, emb2], dim=1)
>       embedding = self.pt_weights * embedding

Translated_Outputs/tensorflow_outputs/kornia/feature/mkd.py:514: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_VonMisesKernel(patch_size=32, n=2, d=5, coeffs=[0.14343168 0.268285   0.21979234]), name = 'pt_weights'

    @tf.autograph.experimental.do_not_convert
    def __getattr__(self, name):
        if name == "v":
            if not super().__getattribute__("_v") and not getattr(  # noqa: E501
                self, "_built", False
            ):
                return self._build_and_return_v(
                    *self._args, dynamic_backend=self._dynamic_backend, **self._kwargs
                )
    
        _dict = super().__getattribute__("__dict__")
        if name in _dict:
            return _dict[name]
    
        elif "_v" in _dict and name in _dict["_v"]:
            return _dict["_v"][name]
    
>       return super().__getattribute__(name)
E       AttributeError: Exception encountered when calling tensorflow_VonMisesKernel.call().
E       
E       [1m'tensorflow_VonMisesKernel' object has no attribute 'pt_weights'[0m
E       
E       Arguments received by tensorflow_VonMisesKernel.call():
E         â€¢ x=tf.Tensor(shape=(1, 1, 32, 32), dtype=float32)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:998: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.MKDDescriptor
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/manyids2/mkd_pytorch/raw/master/mkd_pytorch/mkd-concat-64.pth" to /root/.cache/torch/hub/checkpoints/mkd-concat-64.pth

  0%|          | 0.00/1.31M [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.31M/1.31M [00:00<00:00, 145MB/s]
_________________________________________________________________________________ test_HardNet8[tensorflow-s2s-False] __________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HardNet8(target_framework, mode, backend_compile):
        print("kornia.feature.HardNet8")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledHardNet8 = ivy.transpile(kornia.feature.HardNet8, source="torch", target=target_framework)
    
        x = torch.rand(16, 1, 32, 32)
        torch_out = kornia.feature.HardNet8()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledHardNet8()(transpiled_x)

kornia/test_feature.py:804: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.8038789 , 0.10424453, 0.35386807, ..., 0.6511917...
         [0.46515608, 0.85620433, 0.6462326 , ..., 0.86205703,
          0.2926476 , 0.8153571 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d145ca2d0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2):...,
         [0.46515608, 0.85620433, 0.6462326 , ..., 0.86205703,
          0.2926476 , 0.8153571 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.8038789 , 0.10424453, 0.35386807, ..., 0.6511917...
         [0.46515608, 0.85620433, 0.6462326 , ..., 0.86205703,
          0.2926476 , 0.8153571 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2):...,
         [0.46515608, 0.85620433, 0.6462326 , ..., 0.86205703,
          0.2926476 , 0.8153571 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.8038789 , 0.10424453, 0.35386807, ..., 0.6511917...
         [0.46515608, 0.85620433, 0.6462326 , ..., 0.86205703,
          0.2926476 , 0.8153571 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2):...    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
),)
kwargs = {'input': <tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.8038789 , 0.10424453, 0.35386807, ..., ...,
         [0.46515608, 0.85620433, 0.6462326 , ..., 0.86205703,
          0.2926476 , 0.8153571 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet8(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): ...)
    (20): tensorflow_ReLU()
    (21): tensorflow_Dropout()
    (22): KerasConv2D()
    (23): KerasBatchNorm2D()
  )
)
input = <tf.Tensor: shape=(16, 1, 32, 32), dtype=float32, numpy=
array([[[[0.8038789 , 0.10424453, 0.35386807, ..., 0.6511917 ...],
         [0.46515608, 0.85620433, 0.6462326 , ..., 0.86205703,
          0.2926476 , 0.8153571 ]]]], dtype=float32)>

    def call(self, input):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.nn.functional.non_linear_activation_functions import (
            tensorflow_normalize_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.blas_and_lapack_ops import (
            tensorflow_mm_frnt,
        )
    
        tensorflow_KORNIA_CHECK_SHAPE(input, ["B", "1", "32", "32"])
        x_norm: typing.Any = self._normalize_input(input)
        x_features: typing.Any = self.features(x_norm)
        mean: typing.Any = []
        components: typing.Any = []
        x_prePCA = tensorflow_normalize_frnt(
            tensorflow_view_frnt_(x_features, tensorflow_size_frnt_(x_features, 0), -1)
        )
>       pca = tensorflow_mm_frnt(x_prePCA - mean, components)

Translated_Outputs/tensorflow_outputs/kornia/feature/hardnet.py:868: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(16, 512), dtype=float32, numpy=
array([[-0.01274675,  0.02516742, -0.00576432, ..., -0.09751675,
 ...     [-0.01714501,  0.02099643, -0.00109645, ..., -0.07411349,
        -0.04836481,  0.00989416]], dtype=float32)>, [])
kwargs = {}, arg = []

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_HardNet8.call().
E       
E       [1m{{function_node __wrapped__Sub_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [16,512] vs. [0] [Op:Sub] name: [0m
E       
E       Arguments received by tensorflow_HardNet8.call():
E         â€¢ input=tf.Tensor(shape=(16, 1, 32, 32), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.HardNet8
___________________________________________________________________________________ test_HyNet[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_HyNet(target_framework, mode, backend_compile):
        print("kornia.feature.HyNet")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledHyNet = ivy.transpile(kornia.feature.HyNet, source="torch", target=target_framework)

kornia/test_feature.py:815: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.feature.hynet.HyNet'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_mixed_handler = local_response_norm.partial_mixed_handler = lambda x, size, **kwargs: size % 2 != 0

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.HyNet
__________________________________________________________________________________ test_SOSNet[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SOSNet(target_framework, mode, backend_compile):
        print("kornia.feature.SOSNet")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledSOSNet = ivy.transpile(kornia.feature.SOSNet, source="torch", target=target_framework)

kornia/test_feature.py:849: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.feature.sosnet.SOSNet'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_mixed_handler = local_response_norm.partial_mixed_handler = lambda x, size, **kwargs: size % 2 != 0

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SOSNet
__________________________________________________________________________ test_MultiResolutionDetector[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_MultiResolutionDetector(target_framework, mode, backend_compile):
        print("kornia.feature.MultiResolutionDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledMultiResolutionDetector = ivy.transpile(kornia.feature.MultiResolutionDetector, source="torch", target=target_framework)
        TranspiledKeyNet = ivy.transpile(kornia.feature.KeyNet, source="torch", target=target_framework)
    
        model = kornia.feature.KeyNet()
        transpiled_model = TranspiledKeyNet()
    
        x = torch.rand(1, 1, 32, 32) * 10.
        torch_out = kornia.feature.MultiResolutionDetector(model)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_out = TranspiledMultiResolutionDetector(transpiled_model)(transpiled_x)
    
>       _to_numpy_and_shape_allclose(torch_out, transpiled_out)

kornia/test_feature.py:980: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([[[[15.6444,  0.0000, 11.3778],
          [-0.0000, 15.6444, 11.3778]],

         [[15.6444,  0.0000, 12.8000]....0000, 20.6222],
          [-0.0000, 15.6444, 12.0889]]]]), tensor([[0.0130, 0.0090, 0.0029]], grad_fn=<CatBackward0>))
transpiled_x = (<tf.Tensor: shape=(1, 3, 2, 3), dtype=float32, numpy=
array([[[[15.644444,  0.      , 19.2     ],
         [-0.      ...loat32)>, <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.5369913 , 0.48147172, 0.1360265 ]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_shape_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_shape_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([[[[15.644444,  0.      , 11.377778],
         [-0.      , 15.644444, 11.377778]],

        [[15.644444,  0.   ...    [-0.      , 15.644444, 12.088889]]]], dtype=float32), array([[0.01304023, 0.00902967, 0.00292502]], dtype=float32))
y = (array([[[[15.644444,  0.      , 19.2     ],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.   ...    [-0.      , 22.      , 15.      ]]]], dtype=float32), array([[0.5369913 , 0.48147172, 0.1360265 ]], dtype=float32))
tolerance = 0.001

    def _check_shape_allclose(x, y, tolerance=1e-3):
        """
        Checks that all array shapes are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x.shape, y.shape, atol=tolerance), "numpy array shapes are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7f00442d5940>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([[[[15.644444,  0.      , 11.377778],
         [-0.      , 15.644444, 11.377778]],

        [[15.644444,  0.    ...17.066668]],

        [[15.644444,  0.      , 20.622223],
         [-0.      , 15.644444, 12.088889]]]], dtype=float32)
y = array([[[[15.644444,  0.      , 19.2     ],
         [-0.      , 15.644444, 10.666667]],

        [[15.644444,  0.    ...18.48889 ]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 15.      ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"
E           AssertionError: numpy array values are not all close

helpers.py:22: AssertionError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.MultiResolutionDetector
____________________________________________________________________________ test_ScaleSpaceDetector[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_ScaleSpaceDetector(target_framework, mode, backend_compile):
        print("kornia.feature.ScaleSpaceDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledScaleSpaceDetector = ivy.transpile(kornia.feature.ScaleSpaceDetector, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 32, 32) * 10.
        torch_out = kornia.feature.ScaleSpaceDetector()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledScaleSpaceDetector()(transpiled_x)

kornia/test_feature.py:995: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[3.6274827 , 2.7212489 , 5.278713  , ..., 7.944663  ...
         [5.451088  , 1.7086279 , 7.698758  , ..., 6.882837  ,
          7.5458508 , 8.134245  ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d16cb6a30, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...,
         [5.451088  , 1.7086279 , 7.698758  , ..., 6.882837  ,
          7.5458508 , 8.134245  ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[3.6274827 , 2.7212489 , 5.278713  , ..., 7.944663  ...
         [5.451088  , 1.7086279 , 7.698758  , ..., 6.882837  ,
          7.5458508 , 8.134245  ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...,
         [5.451088  , 1.7086279 , 7.698758  , ..., 6.882837  ,
          7.5458508 , 8.134245  ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[3.6274827 , 2.7212489 , 5.278713  , ..., 7.944663  ...
         [5.451088  , 1.7086279 , 7.698758  , ..., 6.882837  ,
          7.5458508 , 8.134245  ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[3.6274827 , 2.7212489 , 5.278713  , ..., 7.944663  ,...],
         [5.451088  , 1.7086279 , 7.698758  , ..., 6.882837  ,
          7.5458508 , 8.134245  ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (img, mask=None)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma...es=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF()),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[3.6274827 , 2.7212489 , 5.278713  , ..., 7.9...,
         [5.451088  , 1.7086279 , 7.698758  , ..., 6.882837  ,
          7.5458508 , 8.134245  ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
img = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[3.6274827 , 2.7212489 , 5.278713  , ..., 7.944663  ,...],
         [5.451088  , 1.7086279 , 7.698758  , ..., 6.882837  ,
          7.5458508 , 8.134245  ]]]], dtype=float32)>
mask = None

    def call(self, img, mask=None):
>       responses, lafs = self.detect(img, self.num_features, mask)

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:274: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ScaleSpaceDetector(num_features=500, mr_size=6.0, scale_pyr=tensorflow_ScalePyramid(n_levels=3, init_sigma=...ates=False, eps=1e-08, strict_maxima_bonus=0.0, output_value=True), ori=tensorflow_PassLAF(), aff=tensorflow_PassLAF())
img = <tf.Tensor: shape=(1, 1, 32, 32), dtype=float32, numpy=
array([[[[3.6274827 , 2.7212489 , 5.278713  , ..., 7.944663  ,...],
         [5.451088  , 1.7086279 , 7.698758  , ..., 6.882837  ,
          7.5458508 , 8.134245  ]]]], dtype=float32)>
num_feats = 500, mask = None

    def detect(self, img, num_feats, mask=None):
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_reshape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_permute_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_unsqueeze_frnt_
        from ...ivy.functional.frontends.torch.comparison_ops import (
            tensorflow_topk_frnt,
        )
        from ...ivy.functional.frontends.torch.indexing_slicing_joining_mutating_ops import (
            tensorflow_gather_frnt,
        )
        from ...ivy.functional.frontends.torch.tensor import tensorflow_repeat_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_item_frnt_
        from .laf import tensorflow_laf_is_inside_image
        from ..core._backend import eye
        from ..core._backend import concatenate
    
        dev: typing.Any = img.device
        dtype: typing.Any = img.dtype
        sigmas: typing.Any
        sp, sigmas, _ = self.scale_pyr(img)
        all_responses: typing.Any = []
        all_lafs: typing.Any = []
        px_size = 0.5 if self.scale_pyr.double_image else 1.0
        for oct_idx, octave in enumerate(sp):
            sigmas_oct = tensorflow_get_item(sigmas, oct_idx)
            B, CH, L, H, W = tensorflow_size_frnt_(octave)
            if self.scale_space_response:
                oct_resp = self.resp(octave, tensorflow_view_frnt_(sigmas_oct, -1))
            else:
                oct_resp = tensorflow_view_frnt_(
                    self.resp(
                        tensorflow_reshape_frnt_(
                            tensorflow_permute_frnt_(octave, 0, 2, 1, 3, 4),
                            B * L,
                            CH,
                            H,
                            W,
                        ),
                        tensorflow_view_frnt_(sigmas_oct, -1),
                    ),
                    B,
                    L,
                    CH,
                    H,
                    W,
                )
                oct_resp = tensorflow_permute_frnt_(oct_resp, 0, 2, 1, 3, 4)
                if (
                    isinstance(
                        self.scale_pyr.extra_levels,
                        (tensorflow.Tensor, tensorflow.Variable),
                    )
                    and self.scale_pyr.extra_levels % 2 != 0
                ):
                    oct_resp = oct_resp[:, :, :-1]
            if mask is not None:
                oct_mask: typing.Any = tensorflow__create_octave_mask(
                    mask, tensorflow_shape_frnt_(oct_resp)
                )
                oct_resp = oct_mask * oct_resp
            coord_max: typing.Any
            response_max: typing.Any
            coord_max, response_max = self.nms(oct_resp)
            if self.minima_are_also_good:
                coord_min, response_min = self.nms(-oct_resp)
                take_min_mask = tensorflow_to_frnt_(
                    response_min > response_max, response_max.dtype
                )
                response_max = (
                    response_min * take_min_mask + (1 - take_min_mask) * response_max
                )
                coord_max = (
                    coord_min * tensorflow_unsqueeze_frnt_(take_min_mask, 2)
                    + (1 - tensorflow_unsqueeze_frnt_(take_min_mask, 2)) * coord_max
                )
            responses_flatten = tensorflow_view_frnt_(
                response_max, tensorflow_size_frnt_(response_max, 0), -1
            )
            max_coords_flatten = tensorflow_permute_frnt_(
                tensorflow_view_frnt_(
                    coord_max, tensorflow_size_frnt_(response_max, 0), 3, -1
                ),
                0,
                2,
                1,
            )
            if tensorflow_size_frnt_(responses_flatten, 1) > num_feats:
                resp_flat_best, idxs = tensorflow_topk_frnt(
                    responses_flatten, k=num_feats, dim=1
                )
                max_coords_best = tensorflow_gather_frnt(
                    max_coords_flatten,
                    1,
                    tensorflow_repeat_frnt_(
                        tensorflow_unsqueeze_frnt_(idxs, -1), 1, 1, 3
                    ),
                )
            else:
                resp_flat_best = responses_flatten
                max_coords_best = max_coords_flatten
            B, N = tensorflow_size_frnt_(resp_flat_best)
            if isinstance(
                self.scale_pyr.n_levels, (tensorflow.Tensor, tensorflow.Variable)
            ):
                num_levels = int(tensorflow_item_frnt_(self.scale_pyr.n_levels))
            elif isinstance(self.scale_pyr.n_levels, (int,)):
                num_levels = self.scale_pyr.n_levels
            else:
                raise TypeError(
                    f"Expected the scale pyramid module to have `n_levels` as a Tensor or int.Gotcha {type(self.scale_pyr.n_levels)}"
                )
>           max_coords_best = tensorflow__scale_index_to_scale(
                max_coords_best, sigmas_oct, num_levels
            )

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:238: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

max_coords = <tf.Tensor: shape=(1, 500, 3), dtype=float32, numpy=
array([[[ 0.50293493, 30.507263  , 17.03619   ],
        [ 0.9793... ],
        [ 4.0034976 , 29.000568  ,  0.504493  ],
        [ 3.9980514 , 29.002863  ,  1.9921972 ]]], dtype=float32)>
sigmas = <tf.Tensor: shape=(1, 6), dtype=float32, numpy=
array([[1.6      , 2.0158737, 2.5398417, 3.2      , 4.0317473, 5.0796833]],
      dtype=float32)>, num_levels = 3

    def tensorflow__scale_index_to_scale(max_coords, sigmas, num_levels):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_view_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_contiguous_frnt_
        from ...ivy.functional.frontends.torch.pointwise_ops import tensorflow_pow_frnt
        from ..core._backend import concatenate
    
        B, N, _ = tensorflow_shape_frnt_(max_coords)
        scale_coords = tensorflow_view_frnt_(
            tensorflow_contiguous_frnt_(max_coords[:, :, 0]), -1, 1, 1, 1
        )
        out = concatenate(
            [
                sigmas[0, 0]
                * tensorflow_view_frnt_(
>                   tensorflow_pow_frnt(2.0, scale_coords / float(num_levels)), B, N, 1
                ),
                max_coords[:, :, 1:],
            ],
            2,
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/scale_space_detector.py:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (2.0, <tf.Tensor: shape=(500, 1, 1, 1), dtype=float32, numpy=
array([[[[0.16764498]]],


       [[[0.3264465 ]]],


  ...   [[[0.65978247]]],


       [[[0.34197855]]],


       [[[1.3344992 ]]],


       [[[1.3326838 ]]]], dtype=float32)>)
kwargs = {}, tensorflow_is_array_bknd = <function tensorflow_is_array_bknd at 0x7f004d436320>, array_like = 2.0, pattern = '_bknd_|_bknd|_frnt_|_frnt', fn_name = 'pow'

    @functools.wraps(fn)
    def wrapper(*args, **kwargs):
        from ..functional.ivy.general import tensorflow_is_array_bknd
    
        array_like = args[0]
        if isinstance(array_like, (list, tuple)):
            array_like = array_like[0]
        if tensorflow_is_array_bknd(array_like):
            return fn(*args, **kwargs)
        else:
            pattern = "_bknd_|_bknd|_frnt_|_frnt"
            fn_name = extract_function_name(re.sub(pattern, "", fn.__name__))
            try:
                new_fn = getattr(array_like, fn_name)
                if not callable(new_fn):
                    return new_fn
                return new_fn(*args[1:], **kwargs)
            except AttributeError:
>               return fn(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:195: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = 2.0
exponent = <tf.Tensor: shape=(500, 1, 1, 1), dtype=float32, numpy=
array([[[[0.16764498]]],


       [[[0.3264465 ]]],


       [...    [[[0.65978247]]],


       [[[0.34197855]]],


       [[[1.3344992 ]]],


       [[[1.3326838 ]]]], dtype=float32)>

    @tensorflow_handle_methods
    def tensorflow_pow_frnt(input, exponent, *, out=None):
        from ...ivy.general import tensorflow_is_array_bknd
        from ...backends.tensorflow.creation import tensorflow_asarray
        from .creation_ops import tensorflow_as_tensor_frnt
        from ...ivy.data_type import tensorflow_is_int_dtype_bknd
        from ...backends.tensorflow.elementwise import tensorflow_pow
        from ...backends.tensorflow.utility import tensorflow_any
        from ...backends.tensorflow.searching import tensorflow_where
        from ...backends.tensorflow.elementwise import tensorflow_bitwise_and
        from ....data_classes.array.data_type import tensorflow_astype_bknd_
    
        if not tensorflow_is_array_bknd(exponent):
            if (
                any(dtype in str(input.dtype) for dtype in ["int8", "int16"])
                and isinstance(exponent, (int,))
                or "float16" in str(input.dtype)
                and isinstance(exponent, (float,))
            ):
                exponent = tensorflow_asarray(exponent, dtype=input.dtype)
            else:
                exponent = tensorflow_as_tensor_frnt(exponent)
>       ret_dtype = input.dtype
E       AttributeError: Exception encountered when calling tensorflow_ScaleSpaceDetector.call().
E       
E       [1m'float' object has no attribute 'dtype'[0m
E       
E       Arguments received by tensorflow_ScaleSpaceDetector.call():
E         â€¢ img=tf.Tensor(shape=(1, 1, 32, 32), dtype=float32)
E         â€¢ mask=None

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/pointwise_ops.py:130: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.ScaleSpaceDetector
______________________________________________________________________________ test_KeyNetDetector[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KeyNetDetector(target_framework, mode, backend_compile):
        print("kornia.feature.KeyNetDetector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetDetector = ivy.transpile(kornia.feature.KeyNetDetector, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 32, 32)
        torch_out = kornia.feature.KeyNetDetector()(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_out = TranspiledKeyNetDetector()(transpiled_x)
    
>       _to_numpy_and_shape_allclose(torch_out, transpiled_out)

kornia/test_feature.py:1014: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

torch_x = (tensor([], size=(1, 0, 2, 3)), tensor([], size=(1, 0), grad_fn=<CatBackward0>))
transpiled_x = (<tf.Tensor: shape=(1, 2, 2, 3), dtype=float32, numpy=
array([[[[15.644444,  0.      , 12.8     ],
         [-0.      ...]]], dtype=float32)>, <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.03995349, 0.02691804]], dtype=float32)>)
tolerance = 0.001

    def _to_numpy_and_shape_allclose(torch_x, transpiled_x, tolerance=1e-3):
        orig_data = _nest_array_to_numpy(torch_x)
        transpiled_data = _nest_array_to_numpy(transpiled_x)
>       _check_shape_allclose(orig_data, transpiled_data, tolerance=tolerance)

helpers.py:106: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = (array([], shape=(1, 0, 2, 3), dtype=float32), array([], shape=(1, 0), dtype=float32))
y = (array([[[[15.644444,  0.      , 12.8     ],
         [-0.      , 15.644444, 15.644444]],

        [[22.      ,  0.   ...    ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32), array([[0.03995349, 0.02691804]], dtype=float32))
tolerance = 0.001

    def _check_shape_allclose(x, y, tolerance=1e-3):
        """
        Checks that all array shapes are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
            assert np.allclose(x.shape, y.shape, atol=tolerance), "numpy array shapes are not all close"
            return
    
        if isinstance(x, (list, set, tuple)):
>           all([
                _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
            ])

helpers.py:56: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

.0 = <zip object at 0x7f004d8514c0>

    all([
>       _check_allclose(element_x, element_y, tolerance=tolerance) for element_x, element_y in zip(x, y)
    ])

helpers.py:57: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([], shape=(1, 0, 2, 3), dtype=float32)
y = array([[[[15.644444,  0.      , 12.8     ],
         [-0.      , 15.644444, 15.644444]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
tolerance = 0.001

    def _check_allclose(x, y, tolerance=1e-3):
        """
        Checks that all values are close. Any arrays must already be in numpy format, rather than native framework.
        """
    
        if isinstance(x, np.ndarray):
>           assert np.allclose(x, y, atol=tolerance), "numpy array values are not all close"

helpers.py:22: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([], shape=(1, 0, 2, 3), dtype=float32)
b = array([[[[15.644444,  0.      , 12.8     ],
         [-0.      , 15.644444, 15.644444]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
rtol = 1e-05, atol = 0.001, equal_nan = False

    @array_function_dispatch(_allclose_dispatcher)
    def allclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
        """
        Returns True if two arrays are element-wise equal within a tolerance.
    
        The tolerance values are positive, typically very small numbers.  The
        relative difference (`rtol` * abs(`b`)) and the absolute difference
        `atol` are added together to compare against the absolute difference
        between `a` and `b`.
    
        NaNs are treated as equal if they are in the same place and if
        ``equal_nan=True``.  Infs are treated as equal if they are in the same
        place and of the same sign in both arrays.
    
        Parameters
        ----------
        a, b : array_like
            Input arrays to compare.
        rtol : float
            The relative tolerance parameter (see Notes).
        atol : float
            The absolute tolerance parameter (see Notes).
        equal_nan : bool
            Whether to compare NaN's as equal.  If True, NaN's in `a` will be
            considered equal to NaN's in `b` in the output array.
    
            .. versionadded:: 1.10.0
    
        Returns
        -------
        allclose : bool
            Returns True if the two arrays are equal within the given
            tolerance; False otherwise.
    
        See Also
        --------
        isclose, all, any, equal
    
        Notes
        -----
        If the following equation is element-wise True, then allclose returns
        True.
    
         absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
    
        The above equation is not symmetric in `a` and `b`, so that
        ``allclose(a, b)`` might be different from ``allclose(b, a)`` in
        some rare cases.
    
        The comparison of `a` and `b` uses standard broadcasting, which
        means that `a` and `b` need not have the same shape in order for
        ``allclose(a, b)`` to evaluate to True.  The same is true for
        `equal` but not `array_equal`.
    
        `allclose` is not defined for non-numeric data types.
        `bool` is considered a numeric data-type for this purpose.
    
        Examples
        --------
        >>> np.allclose([1e10,1e-7], [1.00001e10,1e-8])
        False
        >>> np.allclose([1e10,1e-8], [1.00001e10,1e-9])
        True
        >>> np.allclose([1e10,1e-8], [1.0001e10,1e-9])
        False
        >>> np.allclose([1.0, np.nan], [1.0, np.nan])
        False
        >>> np.allclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
        True
    
        """
>       res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))

/opt/fw/mxnet/numpy/core/numeric.py:2241: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = array([], shape=(1, 0, 2, 3), dtype=float32)
b = array([[[[15.644444,  0.      , 12.8     ],
         [-0.      , 15.644444, 15.644444]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
rtol = 1e-05, atol = 0.001, equal_nan = False

    @array_function_dispatch(_isclose_dispatcher)
    def isclose(a, b, rtol=1.e-5, atol=1.e-8, equal_nan=False):
        """
        Returns a boolean array where two arrays are element-wise equal within a
        tolerance.
    
        The tolerance values are positive, typically very small numbers.  The
        relative difference (`rtol` * abs(`b`)) and the absolute difference
        `atol` are added together to compare against the absolute difference
        between `a` and `b`.
    
        .. warning:: The default `atol` is not appropriate for comparing numbers
                     that are much smaller than one (see Notes).
    
        Parameters
        ----------
        a, b : array_like
            Input arrays to compare.
        rtol : float
            The relative tolerance parameter (see Notes).
        atol : float
            The absolute tolerance parameter (see Notes).
        equal_nan : bool
            Whether to compare NaN's as equal.  If True, NaN's in `a` will be
            considered equal to NaN's in `b` in the output array.
    
        Returns
        -------
        y : array_like
            Returns a boolean array of where `a` and `b` are equal within the
            given tolerance. If both `a` and `b` are scalars, returns a single
            boolean value.
    
        See Also
        --------
        allclose
        math.isclose
    
        Notes
        -----
        .. versionadded:: 1.7.0
    
        For finite values, isclose uses the following equation to test whether
        two floating point values are equivalent.
    
         absolute(`a` - `b`) <= (`atol` + `rtol` * absolute(`b`))
    
        Unlike the built-in `math.isclose`, the above equation is not symmetric
        in `a` and `b` -- it assumes `b` is the reference value -- so that
        `isclose(a, b)` might be different from `isclose(b, a)`. Furthermore,
        the default value of atol is not zero, and is used to determine what
        small values should be considered close to zero. The default value is
        appropriate for expected values of order unity: if the expected values
        are significantly smaller than one, it can result in false positives.
        `atol` should be carefully selected for the use case at hand. A zero value
        for `atol` will result in `False` if either `a` or `b` is zero.
    
        `isclose` is not defined for non-numeric data types.
        `bool` is considered a numeric data-type for this purpose.
    
        Examples
        --------
        >>> np.isclose([1e10,1e-7], [1.00001e10,1e-8])
        array([ True, False])
        >>> np.isclose([1e10,1e-8], [1.00001e10,1e-9])
        array([ True, True])
        >>> np.isclose([1e10,1e-8], [1.0001e10,1e-9])
        array([False,  True])
        >>> np.isclose([1.0, np.nan], [1.0, np.nan])
        array([ True, False])
        >>> np.isclose([1.0, np.nan], [1.0, np.nan], equal_nan=True)
        array([ True, True])
        >>> np.isclose([1e-8, 1e-7], [0.0, 0.0])
        array([ True, False])
        >>> np.isclose([1e-100, 1e-7], [0.0, 0.0], atol=0.0)
        array([False, False])
        >>> np.isclose([1e-10, 1e-10], [1e-20, 0.0])
        array([ True,  True])
        >>> np.isclose([1e-10, 1e-10], [1e-20, 0.999999e-10], atol=0.0)
        array([False,  True])
        """
        def within_tol(x, y, atol, rtol):
            with errstate(invalid='ignore'), _no_nep50_warning():
                return less_equal(abs(x-y), atol + rtol * abs(y))
    
        x = asanyarray(a)
        y = asanyarray(b)
    
        # Make sure y is an inexact type to avoid bad behavior on abs(MIN_INT).
        # This will cause casting of x later. Also, make sure to allow subclasses
        # (e.g., for numpy.ma).
        # NOTE: We explicitly allow timedelta, which used to work. This could
        #       possibly be deprecated. See also gh-18286.
        #       timedelta works if `atol` is an integer or also a timedelta.
        #       Although, the default tolerances are unlikely to be useful
        if y.dtype.kind != "m":
            dt = multiarray.result_type(y, 1.)
            y = asanyarray(y, dtype=dt)
    
        xfin = isfinite(x)
        yfin = isfinite(y)
        if all(xfin) and all(yfin):
>           return within_tol(x, y, atol, rtol)

/opt/fw/mxnet/numpy/core/numeric.py:2351: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = array([], shape=(1, 0, 2, 3), dtype=float32)
y = array([[[[15.644444,  0.      , 12.8     ],
         [-0.      , 15.644444, 15.644444]],

        [[22.      ,  0.      , 15.      ],
         [-0.      , 22.      , 16.      ]]]], dtype=float32)
atol = 0.001, rtol = 1e-05

    def within_tol(x, y, atol, rtol):
        with errstate(invalid='ignore'), _no_nep50_warning():
>           return less_equal(abs(x-y), atol + rtol * abs(y))
E           ValueError: operands could not be broadcast together with shapes (1,0,2,3) (1,2,2,3)

/opt/fw/mxnet/numpy/core/numeric.py:2332: ValueError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.KeyNetDetector
_______________________________________________________________________________ test_LAFDescriptor[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LAFDescriptor(target_framework, mode, backend_compile):
        print("kornia.feature.LAFDescriptor")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLAFDescriptor = ivy.transpile(kornia.feature.LAFDescriptor, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 64, 64)
        lafs = torch.rand(1, 2, 2, 3)
        torch_out = kornia.feature.LAFDescriptor()(x, lafs)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
        transpiled_lafs = _nest_torch_tensor_to_new_framework(lafs, target_framework)
>       transpiled_out = TranspiledLAFDescriptor()(transpiled_x, transpiled_lafs)

kornia/test_feature.py:1031: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFDescriptor' object has no attribute 'descriptor'") raised in repr()] tensorflow_LAFDescriptor object at 0x7f004da5e2c0>, patch_descriptor_module = None
patch_size = 32, grayscale_descriptor = True

    def __init__(
        self, patch_descriptor_module=None, patch_size=32, grayscale_descriptor=True
    ):
        from .hardnet import tensorflow_HardNet
    
        self.super___init__(
            patch_descriptor_module=patch_descriptor_module,
            patch_size=patch_size,
            grayscale_descriptor=grayscale_descriptor,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if patch_descriptor_module is None:
>           patch_descriptor_module = tensorflow_HardNet(True)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): t...)
    (17): tensorflow_ReLU()
    (18): tensorflow_Dropout()
    (19): KerasConv2D()
    (20): KerasBatchNorm2D()
  )
)
pretrained = True

    def __init__(self, pretrained=False):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=128,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.3),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=8,
                strides=1,
                padding=0,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
        )
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["liberty_aug"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/hardnet.py:211: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LAFDescriptor
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /root/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth

  0%|          | 0.00/5.10M [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.10M/5.10M [00:00<00:00, 252MB/s]
___________________________________________________________________________________ test_SOLD2[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SOLD2(target_framework, mode, backend_compile):
        print("kornia.feature.SOLD2")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSOLD2 = ivy.transpile(kornia.feature.SOLD2, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 512, 512)
        torch_out = kornia.feature.SOLD2(pretrained=False)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSOLD2(pretrained=False)(transpiled_x)

kornia/test_feature.py:1048: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d13c67dc0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tens...,
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tens...,
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (img)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tens...   (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0...,
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (net): tenso...     (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
  (line_matcher): tensorflow_WunschLineMatcher()
)
img = <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.9258258...],
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>

    def call(self, img):
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from .sold2_detector import tensorflow_prob_to_junctions
        from .sold2_detector import tensorflow_line_map_to_segments
    
        tensorflow_KORNIA_CHECK_SHAPE(img, ["B", "1", "H", "W"])
        outputs = {}
>       net_outputs = self.model(img)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2.py:90: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2Net(
  (backbone_net): tensorflow_HourglassBackbone(
    (net): tensorflow_HourglassNet(
      (conv1)...rflow_SuperpointDescriptor(
    (relu): tensorflow_ReLU()
    (convPa): KerasConv2D()
    (convPb): KerasConv2D()
  )
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d16f1d110, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2Net(
  (backbone_net): tensorflow_HourglassBackbone(
    (net): tensorflow_HourglassNet(
      (conv1)...rflow_SuperpointDescriptor(
    (relu): tensorflow_ReLU()
    (convPa): KerasConv2D()
    (convPb): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2Net(
  (backbone_net): tensorflow_HourglassBackbone(
    (net): tensorflow_HourglassNet(
      (conv1)...rflow_SuperpointDescriptor(
    (relu): tensorflow_ReLU()
    (convPa): KerasConv2D()
    (convPb): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input_images)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2Net(
  (backbone_net): tensorflow_HourglassBackbone(
    (net): tensorflow_HourglassNet(
      (conv1)...rflow_SuperpointDescriptor(
    (relu): tensorflow_ReLU()
    (convPa): KerasConv2D()
    (convPb): KerasConv2D()
  )
)
input_images = <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.9258258...],
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>

    def call(self, input_images):
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
>       features = self.backbone_net(input_images)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:3613: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassBackbone(
  (net): tensorflow_HourglassNet(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D...ow_ModuleList(
      (0): KerasConv2D()
    )
    (score_): tensorflow_ModuleList(
      (0): KerasConv2D()
    )
  )
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f002885c3e0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassBackbone(
  (net): tensorflow_HourglassNet(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D...ow_ModuleList(
      (0): KerasConv2D()
    )
    (score_): tensorflow_ModuleList(
      (0): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassBackbone(
  (net): tensorflow_HourglassNet(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D...ow_ModuleList(
      (0): KerasConv2D()
    )
    (score_): tensorflow_ModuleList(
      (0): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input_images)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassBackbone(
  (net): tensorflow_HourglassNet(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D...ow_ModuleList(
      (0): KerasConv2D()
    )
    (score_): tensorflow_ModuleList(
      (0): KerasConv2D()
    )
  )
)
input_images = <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.9258258...],
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>

    def call(self, input_images):
>       return self.net(input_images)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassNet(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (relu): tensorflow_ReLU()
  (layer1): ...fc_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
  (score_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f002885e2b0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassNet(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (relu): tensorflow_ReLU()
  (layer1): ...fc_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
  (score_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassNet(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (relu): tensorflow_ReLU()
  (layer1): ...fc_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
  (score_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.73375833, 0.48237592, 0.8047618 , ..., 0.925825...
         [0.2651317 , 0.15939796, 0.9142911 , ..., 0.11197907,
          0.15688002, 0.2984572 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassNet(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (relu): tensorflow_ReLU()
  (layer1): ...fc_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
  (score_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
)
x = <tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.0829666 ,  0.13285583,  0.13763897, ...,  0.1...      [-0.04849613, -0.06723122, -0.0419379 , ..., -0.02220678,
          -0.03734604, -0.04212679]]]], dtype=float32)>

    def call(self, x):
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
        out = []
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.layer1(x)
        x = self.maxpool(x)
        x = self.layer2(x)
        x = self.layer3(x)
        for i in range(self.num_stacks):
>           y = tensorflow_get_item(self.hg, i)(x)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:1913: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Hourglass(
  (hg): tensorflow_ModuleList(
    (0): tensorflow_ModuleList(
      (0-3): 4 x tensorflow_Seque...: KerasBatchNorm2D()
          (conv3): KerasConv2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
args = (<tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.0829666 ,  0.13285583,  0.13763897, ...,  0....    [-0.04849613, -0.06723122, -0.0419379 , ..., -0.02220678,
          -0.03734604, -0.04212679]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d141ce950, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Hourglass(
  (hg): tensorflow_ModuleList(
    (0): tensorflow_ModuleList(
      (0-3): 4 x tensorflow_Seque...: KerasBatchNorm2D()
          (conv3): KerasConv2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.0829666 ,  0.13285583,  0.13763897, ...,  0....    [-0.04849613, -0.06723122, -0.0419379 , ..., -0.02220678,
          -0.03734604, -0.04212679]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Hourglass(
  (hg): tensorflow_ModuleList(
    (0): tensorflow_ModuleList(
      (0-3): 4 x tensorflow_Seque...: KerasBatchNorm2D()
          (conv3): KerasConv2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.0829666 ,  0.13285583,  0.13763897, ...,  0....    [-0.04849613, -0.06723122, -0.0419379 , ..., -0.02220678,
          -0.03734604, -0.04212679]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Hourglass(
  (hg): tensorflow_ModuleList(
    (0): tensorflow_ModuleList(
      (0-3): 4 x tensorflow_Seque...: KerasBatchNorm2D()
          (conv3): KerasConv2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
x = <tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.0829666 ,  0.13285583,  0.13763897, ...,  0.1...      [-0.04849613, -0.06723122, -0.0419379 , ..., -0.02220678,
          -0.03734604, -0.04212679]]]], dtype=float32)>

    def call(self, x):
>       return self._hour_glass_forward(self.depth, x)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:1378: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Hourglass(
  (hg): tensorflow_ModuleList(
    (0): tensorflow_ModuleList(
      (0-3): 4 x tensorflow_Seque...: KerasBatchNorm2D()
          (conv3): KerasConv2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
n = 4
x = <tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.0829666 ,  0.13285583,  0.13763897, ...,  0.1...      [-0.04849613, -0.06723122, -0.0419379 , ..., -0.02220678,
          -0.03734604, -0.04212679]]]], dtype=float32)>

    def _hour_glass_forward(self, n, x):
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.nn.functional.pooling_functions import (
            tensorflow_max_pool2d_frnt,
        )
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_interpolate_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        up1 = tensorflow_get_item(self.hg, n - 1)[0](x)
        low1 = tensorflow_max_pool2d_frnt(x, 2, stride=2)
>       low1 = tensorflow_get_item(self.hg, n - 1)[1](low1)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:1367: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck2D(
    (bn1): KerasBatchNorm2D()
    (conv1): KerasConv2D()
    (b...    (conv2): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (relu): tensorflow_ReLU()
  )
)
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.12495781,  0.18529658,  0.18954003, ...,  0.1...    [ 0.01952579,  0.01255256,  0.02353241, ...,  0.04558689,
           0.03191005,  0.02479785]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d15a298d0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ls.py', lineno=117, function='error_handler', code_context=['            return fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck2D(
    (bn1): KerasBatchNorm2D()
    (conv1): KerasConv2D()
    (b...    (conv2): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (relu): tensorflow_ReLU()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.12495781,  0.18529658,  0.18954003, ...,  0.1...    [ 0.01952579,  0.01255256,  0.02353241, ...,  0.04558689,
           0.03191005,  0.02479785]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck2D(
    (bn1): KerasBatchNorm2D()
    (conv1): KerasConv2D()
    (b...    (conv2): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (relu): tensorflow_ReLU()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.12495781,  0.18529658,  0.18954003, ...,  0.1...    [ 0.01952579,  0.01255256,  0.02353241, ...,  0.04558689,
           0.03191005,  0.02479785]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck2D(
    (bn1): KerasBatchNorm2D()
    (conv1): KerasConv2D()
    (b...    (conv2): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (relu): tensorflow_ReLU()
  )
)
input = <tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.12495781,  0.18529658,  0.18954003, ...,  0.17...      [ 0.01952579,  0.01255256,  0.02353241, ...,  0.04558689,
           0.03191005,  0.02479785]]]], dtype=float32)>

    def call(self, input):
        for i, module in enumerate(self):
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck2D(
  (bn1): KerasBatchNorm2D()
  (conv1): KerasConv2D()
  (bn2): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn3): KerasBatchNorm2D()
  (conv3): KerasConv2D()
  (relu): tensorflow_ReLU()
)
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.12495781,  0.18529658,  0.18954003, ...,  0.1...    [ 0.01952579,  0.01255256,  0.02353241, ...,  0.04558689,
           0.03191005,  0.02479785]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d17d4d390, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck2D(
  (bn1): KerasBatchNorm2D()
  (conv1): KerasConv2D()
  (bn2): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn3): KerasBatchNorm2D()
  (conv3): KerasConv2D()
  (relu): tensorflow_ReLU()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.12495781,  0.18529658,  0.18954003, ...,  0.1...    [ 0.01952579,  0.01255256,  0.02353241, ...,  0.04558689,
           0.03191005,  0.02479785]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck2D(
  (bn1): KerasBatchNorm2D()
  (conv1): KerasConv2D()
  (bn2): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn3): KerasBatchNorm2D()
  (conv3): KerasConv2D()
  (relu): tensorflow_ReLU()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.12495781,  0.18529658,  0.18954003, ...,  0.1...    [ 0.01952579,  0.01255256,  0.02353241, ...,  0.04558689,
           0.03191005,  0.02479785]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck2D(
  (bn1): KerasBatchNorm2D()
  (conv1): KerasConv2D()
  (bn2): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn3): KerasBatchNorm2D()
  (conv3): KerasConv2D()
  (relu): tensorflow_ReLU()
)
x = <tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.12495781,  0.18529658,  0.18954003, ...,  0.17...      [ 0.01952579,  0.01255256,  0.02353241, ...,  0.04558689,
           0.03191005,  0.02479785]]]], dtype=float32)>

    def call(self, x):
        residual = x
        out = self.bn1(x)
        out = self.relu(out)
        out = self.conv1(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn3(out)
        out = self.relu(out)
        out = self.conv3(out)
        if self.downsample is not None:
            residual = self.downsample(x)
>       out = out + residual

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:952: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 256, 64, 128), dtype=float32, numpy=
array([[[[-0.04031453, -0.04865555, -0.05614839, ..., -0.0...     [ 0.01952579,  0.01255256,  0.02353241, ...,  0.04558689,
           0.03191005,  0.02479785]]]], dtype=float32)>)
kwargs = {}
arg = <tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.12495781,  0.18529658,  0.18954003, ...,  0.17...      [ 0.01952579,  0.01255256,  0.02353241, ...,  0.04558689,
           0.03191005,  0.02479785]]]], dtype=float32)>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_Bottleneck2D.call().
E       
E       [1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1,256,64,128] vs. [1,128,64,128] [Op:AddV2] name: [0m
E       
E       Arguments received by tensorflow_Bottleneck2D.call():
E         â€¢ x=tf.Tensor(shape=(1, 128, 64, 128), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SOLD2
_______________________________________________________________________________ test_LocalFeature[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LocalFeature(target_framework, mode, backend_compile):
        print("kornia.feature.LocalFeature")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetDetector = ivy.transpile(
            kornia.feature.KeyNetDetector, source="torch", target=target_framework
        )
        TranspiledLAFDescriptor = ivy.transpile(
            kornia.feature.LAFDescriptor, source="torch", target=target_framework
        )
        TranspiledLocalFeature = ivy.transpile(
            kornia.feature.LocalFeature, source="torch", target=target_framework
        )
    
        torch_detector = kornia.feature.KeyNetDetector()
        torch_descriptor = kornia.feature.LAFDescriptor()
        transpiled_detector = TranspiledKeyNetDetector()
>       transpiled_descriptor = TranspiledLAFDescriptor()

kornia/test_feature.py:1072: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFDescriptor' object has no attribute 'descriptor'") raised in repr()] tensorflow_LAFDescriptor object at 0x7f004db82560>, patch_descriptor_module = None
patch_size = 32, grayscale_descriptor = True

    def __init__(
        self, patch_descriptor_module=None, patch_size=32, grayscale_descriptor=True
    ):
        from .hardnet import tensorflow_HardNet
    
        self.super___init__(
            patch_descriptor_module=patch_descriptor_module,
            patch_size=patch_size,
            grayscale_descriptor=grayscale_descriptor,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if patch_descriptor_module is None:
>           patch_descriptor_module = tensorflow_HardNet(True)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HardNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): t...)
    (17): tensorflow_ReLU()
    (18): tensorflow_Dropout()
    (19): KerasConv2D()
    (20): KerasBatchNorm2D()
  )
)
pretrained = True

    def __init__(self, pretrained=False):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=128,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.3),
            KerasConv2D(
                in_channels=128,
                filters=128,
                kernel_size=8,
                strides=1,
                padding=0,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=128,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
        )
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["liberty_aug"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/hardnet.py:211: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LocalFeature
______________________________________________________________________________ test_SOLD2_detector[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SOLD2_detector(target_framework, mode, backend_compile):
        print("kornia.feature.SOLD2_detector")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSOLD2Detector = ivy.transpile(kornia.feature.SOLD2_detector, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 512, 512)
        torch_out = kornia.feature.SOLD2_detector(pretrained=False)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSOLD2Detector(pretrained=False)(transpiled_x)

kornia/test_feature.py:1095: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f00441374d0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (n...,
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (n...,
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (img)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (n...ntDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
),)
kwargs = {'img': <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0...,
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2_detector(
  (model): tensorflow_SOLD2Net(
    (backbone_net): tensorflow_HourglassBackbone(
      (ne...ointDescriptor(
      (relu): tensorflow_ReLU()
      (convPa): KerasConv2D()
      (convPb): KerasConv2D()
    )
  )
)
img = <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.0867365...],
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>

    def call(self, img):
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ...core.check import tensorflow_KORNIA_CHECK_SHAPE
    
        tensorflow_KORNIA_CHECK_SHAPE(img, ["B", "1", "H", "W"])
        outputs = {}
>       net_outputs = self.model(img)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/sold2_detector.py:105: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2Net(
  (backbone_net): tensorflow_HourglassBackbone(
    (net): tensorflow_HourglassNet(
      (conv1)...rflow_SuperpointDescriptor(
    (relu): tensorflow_ReLU()
    (convPa): KerasConv2D()
    (convPb): KerasConv2D()
  )
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d199ff730, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2Net(
  (backbone_net): tensorflow_HourglassBackbone(
    (net): tensorflow_HourglassNet(
      (conv1)...rflow_SuperpointDescriptor(
    (relu): tensorflow_ReLU()
    (convPa): KerasConv2D()
    (convPb): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2Net(
  (backbone_net): tensorflow_HourglassBackbone(
    (net): tensorflow_HourglassNet(
      (conv1)...rflow_SuperpointDescriptor(
    (relu): tensorflow_ReLU()
    (convPa): KerasConv2D()
    (convPb): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input_images)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SOLD2Net(
  (backbone_net): tensorflow_HourglassBackbone(
    (net): tensorflow_HourglassNet(
      (conv1)...rflow_SuperpointDescriptor(
    (relu): tensorflow_ReLU()
    (convPa): KerasConv2D()
    (convPb): KerasConv2D()
  )
)
input_images = <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.0867365...],
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>

    def call(self, input_images):
        from ....ivy.functional.ivy.general import tensorflow_set_item_bknd
    
>       features = self.backbone_net(input_images)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:3613: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassBackbone(
  (net): tensorflow_HourglassNet(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D...ow_ModuleList(
      (0): KerasConv2D()
    )
    (score_): tensorflow_ModuleList(
      (0): KerasConv2D()
    )
  )
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f002885c5b0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassBackbone(
  (net): tensorflow_HourglassNet(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D...ow_ModuleList(
      (0): KerasConv2D()
    )
    (score_): tensorflow_ModuleList(
      (0): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassBackbone(
  (net): tensorflow_HourglassNet(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D...ow_ModuleList(
      (0): KerasConv2D()
    )
    (score_): tensorflow_ModuleList(
      (0): KerasConv2D()
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input_images)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassBackbone(
  (net): tensorflow_HourglassNet(
    (conv1): KerasConv2D()
    (bn1): KerasBatchNorm2D...ow_ModuleList(
      (0): KerasConv2D()
    )
    (score_): tensorflow_ModuleList(
      (0): KerasConv2D()
    )
  )
)
input_images = <tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.0867365...],
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>

    def call(self, input_images):
>       return self.net(input_images)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:75: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassNet(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (relu): tensorflow_ReLU()
  (layer1): ...fc_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
  (score_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
)
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f002885d600, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassNet(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (relu): tensorflow_ReLU()
  (layer1): ...fc_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
  (score_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassNet(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (relu): tensorflow_ReLU()
  (layer1): ...fc_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
  (score_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 1, 512, 512), dtype=float32, numpy=
array([[[[0.27231234, 0.90957177, 0.01029617, ..., 0.086736...
         [0.06576842, 0.48748803, 0.9287142 , ..., 0.36641318,
          0.53660667, 0.13418472]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_HourglassNet(
  (conv1): KerasConv2D()
  (bn1): KerasBatchNorm2D()
  (relu): tensorflow_ReLU()
  (layer1): ...fc_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
  (score_): tensorflow_ModuleList(
    (0): KerasConv2D()
  )
)
x = <tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.06719553,  0.09918805,  0.0690713 , ...,  0.0...      [ 0.04446239,  0.06263806,  0.04918936, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>

    def call(self, x):
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
        out = []
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.layer1(x)
        x = self.maxpool(x)
        x = self.layer2(x)
        x = self.layer3(x)
        for i in range(self.num_stacks):
>           y = tensorflow_get_item(self.hg, i)(x)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:1913: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Hourglass(
  (hg): tensorflow_ModuleList(
    (0): tensorflow_ModuleList(
      (0-3): 4 x tensorflow_Seque...: KerasBatchNorm2D()
          (conv3): KerasConv2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
args = (<tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.06719553,  0.09918805,  0.0690713 , ...,  0....    [ 0.04446239,  0.06263806,  0.04918936, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d172cf520, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Hourglass(
  (hg): tensorflow_ModuleList(
    (0): tensorflow_ModuleList(
      (0-3): 4 x tensorflow_Seque...: KerasBatchNorm2D()
          (conv3): KerasConv2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.06719553,  0.09918805,  0.0690713 , ...,  0....    [ 0.04446239,  0.06263806,  0.04918936, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Hourglass(
  (hg): tensorflow_ModuleList(
    (0): tensorflow_ModuleList(
      (0-3): 4 x tensorflow_Seque...: KerasBatchNorm2D()
          (conv3): KerasConv2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.06719553,  0.09918805,  0.0690713 , ...,  0....    [ 0.04446239,  0.06263806,  0.04918936, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Hourglass(
  (hg): tensorflow_ModuleList(
    (0): tensorflow_ModuleList(
      (0-3): 4 x tensorflow_Seque...: KerasBatchNorm2D()
          (conv3): KerasConv2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
x = <tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.06719553,  0.09918805,  0.0690713 , ...,  0.0...      [ 0.04446239,  0.06263806,  0.04918936, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>

    def call(self, x):
>       return self._hour_glass_forward(self.depth, x)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:1378: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Hourglass(
  (hg): tensorflow_ModuleList(
    (0): tensorflow_ModuleList(
      (0-3): 4 x tensorflow_Seque...: KerasBatchNorm2D()
          (conv3): KerasConv2D()
          (relu): tensorflow_ReLU()
        )
      )
    )
  )
)
n = 4
x = <tf.Tensor: shape=(1, 256, 128, 128), dtype=float32, numpy=
array([[[[ 0.06719553,  0.09918805,  0.0690713 , ...,  0.0...      [ 0.04446239,  0.06263806,  0.04918936, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>

    def _hour_glass_forward(self, n, x):
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.nn.functional.pooling_functions import (
            tensorflow_max_pool2d_frnt,
        )
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_interpolate_frnt,
        )
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
    
        up1 = tensorflow_get_item(self.hg, n - 1)[0](x)
        low1 = tensorflow_max_pool2d_frnt(x, 2, stride=2)
>       low1 = tensorflow_get_item(self.hg, n - 1)[1](low1)

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:1367: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck2D(
    (bn1): KerasBatchNorm2D()
    (conv1): KerasConv2D()
    (b...    (conv2): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (relu): tensorflow_ReLU()
  )
)
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.08529165,  0.09918805,  0.09158185, ...,  0.0...    [ 0.04446239,  0.06263806,  0.05330084, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d13412810, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ls.py', lineno=117, function='error_handler', code_context=['            return fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck2D(
    (bn1): KerasBatchNorm2D()
    (conv1): KerasConv2D()
    (b...    (conv2): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (relu): tensorflow_ReLU()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.08529165,  0.09918805,  0.09158185, ...,  0.0...    [ 0.04446239,  0.06263806,  0.05330084, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck2D(
    (bn1): KerasBatchNorm2D()
    (conv1): KerasConv2D()
    (b...    (conv2): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (relu): tensorflow_ReLU()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.08529165,  0.09918805,  0.09158185, ...,  0.0...    [ 0.04446239,  0.06263806,  0.05330084, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Sequential(
  (0): tensorflow_Bottleneck2D(
    (bn1): KerasBatchNorm2D()
    (conv1): KerasConv2D()
    (b...    (conv2): KerasConv2D()
    (bn3): KerasBatchNorm2D()
    (conv3): KerasConv2D()
    (relu): tensorflow_ReLU()
  )
)
input = <tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.08529165,  0.09918805,  0.09158185, ...,  0.08...      [ 0.04446239,  0.06263806,  0.05330084, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>

    def call(self, input):
        for i, module in enumerate(self):
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck2D(
  (bn1): KerasBatchNorm2D()
  (conv1): KerasConv2D()
  (bn2): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn3): KerasBatchNorm2D()
  (conv3): KerasConv2D()
  (relu): tensorflow_ReLU()
)
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.08529165,  0.09918805,  0.09158185, ...,  0.0...    [ 0.04446239,  0.06263806,  0.05330084, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d163a24c0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck2D(
  (bn1): KerasBatchNorm2D()
  (conv1): KerasConv2D()
  (bn2): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn3): KerasBatchNorm2D()
  (conv3): KerasConv2D()
  (relu): tensorflow_ReLU()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.08529165,  0.09918805,  0.09158185, ...,  0.0...    [ 0.04446239,  0.06263806,  0.05330084, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck2D(
  (bn1): KerasBatchNorm2D()
  (conv1): KerasConv2D()
  (bn2): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn3): KerasBatchNorm2D()
  (conv3): KerasConv2D()
  (relu): tensorflow_ReLU()
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.08529165,  0.09918805,  0.09158185, ...,  0.0...    [ 0.04446239,  0.06263806,  0.05330084, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (x)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Bottleneck2D(
  (bn1): KerasBatchNorm2D()
  (conv1): KerasConv2D()
  (bn2): KerasBatchNorm2D()
  (conv2): KerasConv2D()
  (bn3): KerasBatchNorm2D()
  (conv3): KerasConv2D()
  (relu): tensorflow_ReLU()
)
x = <tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.08529165,  0.09918805,  0.09158185, ...,  0.08...      [ 0.04446239,  0.06263806,  0.05330084, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>

    def call(self, x):
        residual = x
        out = self.bn1(x)
        out = self.relu(out)
        out = self.conv1(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn3(out)
        out = self.relu(out)
        out = self.conv3(out)
        if self.downsample is not None:
            residual = self.downsample(x)
>       out = out + residual

Translated_Outputs/tensorflow_outputs/kornia/feature/sold2/backbones.py:952: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<tf.Tensor: shape=(1, 256, 64, 128), dtype=float32, numpy=
array([[[[ 0.00554424,  0.00443872,  0.01073102, ...,  0.0...     [ 0.04446239,  0.06263806,  0.05330084, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>)
kwargs = {}
arg = <tf.Tensor: shape=(1, 128, 64, 128), dtype=float32, numpy=
array([[[[ 0.08529165,  0.09918805,  0.09158185, ...,  0.08...      [ 0.04446239,  0.06263806,  0.05330084, ...,  0.07951131,
           0.03993317,  0.07303785]]]], dtype=float32)>

    def rep_method(*args, **kwargs):
        for arg in args:
            if ivy.is_ivy_array(arg):
                return NotImplemented
>       return func(*args, **kwargs)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_Bottleneck2D.call().
E       
E       [1m{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [1,256,64,128] vs. [1,128,64,128] [Op:AddV2] name: [0m
E       
E       Arguments received by tensorflow_Bottleneck2D.call():
E         â€¢ x=tf.Tensor(shape=(1, 128, 64, 128), dtype=float32)

../ivy/ivy/functional/backends/tensorflow/__init__.py:40: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SOLD2_detector
__________________________________________________________________________________ test_DeDoDe[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DeDoDe(target_framework, mode, backend_compile):
        print("kornia.feature.DeDoDe")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDeDoDe = ivy.transpile(kornia.feature.DeDoDe, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 256, 256)
        torch_out = kornia.feature.DeDoDe(amp_dtype=torch.float32)(x)
    
        ivy.set_backend(target_framework)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDeDoDe(amp_dtype=ivy.as_native_dtype("float32"))(transpiled_x)

kornia/test_feature.py:1114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DeDoDe(), detector_model = 'L', descriptor_model = 'G', amp_dtype = tf.float32

    def __init__(self, detector_model="L", descriptor_model="G", amp_dtype=tf.float16):
        from .dedode_models import tensorflow_get_detector
        from .dedode_models import tensorflow_get_descriptor
        from ...enhance.normalize import tensorflow_normalize
        from ....ivy.functional.frontends.torch.creation_ops import (
            tensorflow_tensor_frnt,
        )
    
        self.super___init__(
            detector_model=detector_model,
            descriptor_model=descriptor_model,
            amp_dtype=amp_dtype,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.detector: typing.Any = tensorflow_get_detector(detector_model, amp_dtype)

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/dedode.py:68: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kind = 'L', amp_dtype = tf.float32

    def tensorflow_get_detector(kind="L", amp_dtype=tf.float16):
        if kind == "L":
>           return tensorflow_dedode_detector_L(amp_dtype)

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/dedode_models.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

amp_dtype = tf.float32

    def tensorflow_dedode_detector_L(amp_dtype=tf.float16):
        from ....torch.nn.modules.container import tensorflow_ModuleDict
        from .decoder import tensorflow_ConvRefiner
        from .encoder import tensorflow_VGG19
        from .decoder import tensorflow_Decoder
        from .detector import tensorflow_DeDoDeDetector
    
        NUM_PROTOTYPES = 1
        residual = True
        hidden_blocks = 8
        amp = True
        conv_refiner = tensorflow_ModuleDict(
            {
>               "8": tensorflow_ConvRefiner(
                    512,
                    512,
                    256 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
                "4": tensorflow_ConvRefiner(
                    256 + 256,
                    256,
                    128 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
                "2": tensorflow_ConvRefiner(
                    128 + 128,
                    128,
                    64 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
                "1": tensorflow_ConvRefiner(
                    64 + 64,
                    64,
                    1 + NUM_PROTOTYPES,
                    hidden_blocks=hidden_blocks,
                    residual=residual,
                    amp=amp,
                    amp_dtype=amp_dtype,
                ),
            }
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/dedode_models.py:43: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvRefiner(), args = (512, 512, 257), kwargs = {'amp': True, 'amp_dtype': tf.float32, 'hidden_blocks': 8, 'residual': True}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvRefiner(), in_dim = 512, hidden_dim = 512, out_dim = 257, dw = True, kernel_size = 5, hidden_blocks = 8, amp = True, residual = True, amp_dtype = tf.float32

    @tensorflow_store_config_info
    def __init__(
        self,
        in_dim=6,
        hidden_dim=16,
        out_dim=2,
        dw=True,
        kernel_size=5,
        hidden_blocks=5,
        amp=True,
        residual=False,
        amp_dtype=tf.float16,
    ):
        from ....torch.nn.modules.container import tensorflow_Sequential
        from ....tensorflow__stateful_layers import KerasConv2D
    
        self.super___init__(
            in_dim=in_dim,
            hidden_dim=hidden_dim,
            out_dim=out_dim,
            dw=dw,
            kernel_size=kernel_size,
            hidden_blocks=hidden_blocks,
            amp=amp,
            residual=residual,
            amp_dtype=amp_dtype,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.block1 = self.create_block(in_dim, hidden_dim, dw=False, kernel_size=1)

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/decoder.py:466: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ConvRefiner(), in_dim = 512, out_dim = 512, dw = False, kernel_size = 1, bias = True
norm_type = <class 'Translated_Outputs.tensorflow_outputs.tensorflow__stateful_layers.KerasBatchNorm2D'>

    def create_block(
        self,
        in_dim,
        out_dim,
        dw=True,
        kernel_size=5,
        bias=True,
        norm_type=KerasBatchNorm2D,
    ):
        from ....torch.nn.modules.container import tensorflow_Sequential
        from ....torch.nn.modules.activation import tensorflow_ReLU
        from ....tensorflow__stateful_layers import KerasConv2D
        from ....tensorflow__stateful_layers import resolve_convolution
        from ....tensorflow__stateful_layers import KerasBatchNorm2D
    
        num_groups = 1 if not dw else in_dim
        if dw:
            if out_dim % in_dim != 0:
                raise Exception("outdim must be divisible by indim for depthwise")
        conv1 = resolve_convolution(
            in_channels=in_dim,
            filters=out_dim,
            kernel_size=kernel_size,
            strides=1,
            padding=kernel_size // 2,
            use_bias=bias,
            dilation_rate=1,
            groups=num_groups,
            padding_mode="zeros",
            data_format="channels_last",
        )
        norm = (
>           norm_type(out_dim)
            if norm_type is KerasBatchNorm2D
            else norm_type(num_channels=out_dim)
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/dedode/decoder.py:524: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = KerasBatchNorm2D(), args = (512,), kwargs = {}

    def __init__(self, *args, **kwargs):
        self._previous_frame_info = None
    
        # pytorch layer attributes
>       self.num_features = kwargs.pop("num_features")
E       KeyError: 'num_features'

Translated_Outputs/tensorflow_outputs/tensorflow__stateful_layers.py:585: KeyError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.DeDoDe
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_pretrain.pth" to /root/.cache/torch/hub/checkpoints/dinov2_vitl14_pretrain.pth

  0%|          | 0.00/1.13G [00:00<?, ?B/s]
  1%|â–         | 16.1M/1.13G [00:00<00:07, 168MB/s]
  3%|â–Ž         | 38.1M/1.13G [00:00<00:05, 203MB/s]
  5%|â–Œ         | 59.8M/1.13G [00:00<00:05, 213MB/s]
  7%|â–‹         | 83.0M/1.13G [00:00<00:05, 225MB/s]
 10%|â–‰         | 111M/1.13G [00:00<00:04, 250MB/s] 
 12%|â–ˆâ–        | 137M/1.13G [00:00<00:04, 258MB/s]
 14%|â–ˆâ–        | 162M/1.13G [00:00<00:04, 246MB/s]
 17%|â–ˆâ–‹        | 192M/1.13G [00:00<00:03, 268MB/s]
 19%|â–ˆâ–‰        | 220M/1.13G [00:00<00:03, 275MB/s]
 21%|â–ˆâ–ˆ        | 247M/1.13G [00:01<00:03, 274MB/s]
 23%|â–ˆâ–ˆâ–Ž       | 273M/1.13G [00:01<00:03, 271MB/s]
 26%|â–ˆâ–ˆâ–Œ       | 299M/1.13G [00:01<00:03, 252MB/s]
 28%|â–ˆâ–ˆâ–Š       | 325M/1.13G [00:01<00:03, 257MB/s]
 30%|â–ˆâ–ˆâ–ˆ       | 350M/1.13G [00:01<00:03, 257MB/s]
 32%|â–ˆâ–ˆâ–ˆâ–      | 376M/1.13G [00:01<00:03, 261MB/s]
 35%|â–ˆâ–ˆâ–ˆâ–      | 401M/1.13G [00:01<00:03, 257MB/s]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 426M/1.13G [00:01<00:03, 228MB/s]
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 448M/1.13G [00:01<00:03, 224MB/s]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 470M/1.13G [00:02<00:03, 208MB/s]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 494M/1.13G [00:02<00:03, 219MB/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 515M/1.13G [00:02<00:03, 221MB/s]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 540M/1.13G [00:02<00:02, 233MB/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 564M/1.13G [00:02<00:02, 237MB/s]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 590M/1.13G [00:02<00:02, 248MB/s]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 614M/1.13G [00:02<00:02, 243MB/s]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 642M/1.13G [00:02<00:02, 256MB/s]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 670M/1.13G [00:02<00:01, 268MB/s]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 697M/1.13G [00:02<00:01, 272MB/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 725M/1.13G [00:03<00:01, 279MB/s]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 754M/1.13G [00:03<00:01, 286MB/s]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 781M/1.13G [00:03<00:01, 282MB/s]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 813M/1.13G [00:03<00:01, 296MB/s]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 841M/1.13G [00:03<00:01, 281MB/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 869M/1.13G [00:03<00:01, 283MB/s]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 896M/1.13G [00:03<00:00, 279MB/s]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 923M/1.13G [00:03<00:00, 281MB/s]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 952M/1.13G [00:03<00:00, 288MB/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 984M/1.13G [00:03<00:00, 299MB/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 0.99G/1.13G [00:04<00:00, 312MB/s]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1.02G/1.13G [00:04<00:00, 300MB/s]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 1.05G/1.13G [00:04<00:00, 298MB/s]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1.08G/1.13G [00:04<00:00, 295MB/s]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1.11G/1.13G [00:04<00:00, 306MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.13G/1.13G [00:04<00:00, 265MB/s]
___________________________________________________________________________________ test_DISK[tensorflow-s2s-False] ____________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_DISK(target_framework, mode, backend_compile):
        print("kornia.feature.DISK")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledDISK = ivy.transpile(kornia.feature.DISK, source="torch", target=target_framework)
    
        x = torch.rand(1, 3, 256, 256)
        torch_out = kornia.feature.DISK()(x)[0]
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledDISK()(transpiled_x)

kornia/test_feature.py:1131: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.6158247 , 0.76258427, 0.48678058, ..., 0.316871...
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f002885f130, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ode_context=['        return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDow...,
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>)
kwargs = {}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.6158247 , 0.76258427, 0.48678058, ..., 0.316871...
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDow...,
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>)
kwargs = {'buffers': None, 'v': None}

    def wrapper(*args, **kwargs):
      with ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):
>       return func(*args, **kwargs)

/opt/fw/tensorflow/tensorflow/python/autograph/impl/api.py:643: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.6158247 , 0.76258427, 0.48678058, ..., 0.316871...
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>,)
kwargs = {}
first_arr = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.6158247 , 0.76258427, 0.48678058, ..., 0.3168719...],
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>
replace_v = False, replace_buffers = False, call_signature = <Signature (images, n=None, window_size=5, score_threshold=0.0, pad_if_not_divisible=False)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Model, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Model, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Model, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:1438: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDow... tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
),)
kwargs = {'images': <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.6158247 , 0.76258427, 0.48678058, ......,
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>}

    @wraps(fn)
    def error_handler(*args, **kwargs):
        if not is_traceback_filtering_enabled():
            return fn(*args, **kwargs)
    
        filtered_tb = None
        try:
            return fn(*args, **kwargs)
        except Exception as e:
            filtered_tb = _process_traceback_frames(e.__traceback__)
            # To get the full stack trace, call:
            # `keras.config.disable_traceback_filtering()`
>           raise e.with_traceback(filtered_tb) from None

/opt/fw/tensorflow/keras/src/utils/traceback_utils.py:122: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
images = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.6158247 , 0.76258427, 0.48678058, ..., 0.3168719...],
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>
n = None, window_size = 5, score_threshold = 0.0, pad_if_not_divisible = False

    def call(
        self,
        images,
        n=None,
        window_size=5,
        score_threshold=0.0,
        pad_if_not_divisible=False,
    ):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ....ivy.functional.frontends.torch.nn.functional.vision_functions import (
            tensorflow_pad_frnt,
        )
        from .detector import tensorflow_heatmap_to_keypoints
    
        B = tensorflow_shape_frnt_(images)[0]
        if pad_if_not_divisible:
            h, w = (
                tensorflow_shape_frnt_(images)[2:][0],
                tensorflow_shape_frnt_(images)[2:][1],
            )
            pd_h = 16 - h % 16 if h % 16 > 0 else 0
            pd_w = 16 - w % 16 if w % 16 > 0 else 0
            images = tensorflow_pad_frnt(images, (0, pd_w, 0, pd_h), value=0.0)
>       heatmaps, descriptors = self.heatmap_and_dense_descriptors(images)

Translated_Outputs/tensorflow_outputs/kornia/feature/disk/disk.py:97: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_DISK(
  (unet): tensorflow_Unet(
    (path_down): tensorflow_ModuleList(
      (0): tensorflow_ThinUnetDown...): tensorflow_PReLU()
          (2): tensorflow_Sequential()
          (3): KerasConv2D()
        )
      )
    )
  )
)
images = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.6158247 , 0.76258427, 0.48678058, ..., 0.3168719...],
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>

    def heatmap_and_dense_descriptors(self, images):
        from ....ivy.functional.frontends.torch.tensor import tensorflow_shape_frnt_
        from ....ivy.functional.backends.tensorflow.general import tensorflow_get_item
    
>       unet_output = self.unet(images)

Translated_Outputs/tensorflow_outputs/kornia/feature/disk/disk.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.6158247 , 0.76258427, 0.48678058, ..., 0.316871...
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f0028b183e0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...ls.py', lineno=117, function='error_handler', code_context=['            return fn(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.6158247 , 0.76258427, 0.48678058, ..., 0.316871...
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.6158247 , 0.76258427, 0.48678058, ..., 0.316871...
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (inp)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Unet(
  (path_down): tensorflow_ModuleList(
    (0): tensorflow_ThinUnetDownBlock(
      (0): tensorflow_Se...d()
        (1): tensorflow_PReLU()
        (2): tensorflow_Sequential()
        (3): KerasConv2D()
      )
    )
  )
)
inp = <tf.Tensor: shape=(1, 3, 256, 256), dtype=float32, numpy=
array([[[[0.6158247 , 0.76258427, 0.48678058, ..., 0.3168719...],
         [0.30227917, 0.04109645, 0.49693435, ..., 0.3046708 ,
          0.19709766, 0.3033738 ]]]], dtype=float32)>

    def call(self, inp):
        from .....ivy.functional.frontends.torch.tensor import tensorflow_size_frnt_
    
        if tensorflow_size_frnt_(inp, 1) != self.in_features:
            fmt = "Expected {} feature channels in input, got {}"
            msg = fmt.format(self.in_features, tensorflow_size_frnt_(inp, 1))
            raise ValueError(msg)
        input_size_divisor = 2 ** len(self.up)
        if (
            tensorflow_size_frnt_(inp, 2) % input_size_divisor != 0
            or tensorflow_size_frnt_(inp, 3) % input_size_divisor != 0
        ):
            raise ValueError(
                f"Input image shape must be divisible by {input_size_divisor} (got {tensorflow_size_frnt_(inp)}). This is not inherent to DISK, but to the U-Net architecture used in pretrained models. Please pad if necessary."
            )
        features = [inp]
        for layer in self.path_down:
>           features.append(layer(features[-1]))

Translated_Outputs/tensorflow_outputs/kornia/feature/disk/_unets/unet.py:91: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_TrivialDownsample()
  (1): tensorflow_Conv(
    (0): tensorflow_InstanceNorm2d()
    (1): tensorflow_PReLU()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
args = (<tf.Tensor: shape=(1, 16, 256, 256), dtype=float32, numpy=
array([[[[-0.06769268,  0.04299971, -0.06261031, ...,  0.1...    [ 0.06088565,  0.15352976, -0.04454881, ..., -0.33716542,
          -0.0647005 , -0.3231163 ]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f0028b19d40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_TrivialDownsample()
  (1): tensorflow_Conv(
    (0): tensorflow_InstanceNorm2d()
    (1): tensorflow_PReLU()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 16, 256, 256), dtype=float32, numpy=
array([[[[-0.06769268,  0.04299971, -0.06261031, ...,  0.1...    [ 0.06088565,  0.15352976, -0.04454881, ..., -0.33716542,
          -0.0647005 , -0.3231163 ]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_TrivialDownsample()
  (1): tensorflow_Conv(
    (0): tensorflow_InstanceNorm2d()
    (1): tensorflow_PReLU()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
v = None, buffers = None
args = (<tf.Tensor: shape=(1, 16, 256, 256), dtype=float32, numpy=
array([[[[-0.06769268,  0.04299971, -0.06261031, ...,  0.1...    [ 0.06088565,  0.15352976, -0.04454881, ..., -0.33716542,
          -0.0647005 , -0.3231163 ]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_ThinUnetDownBlock(
  (0): tensorflow_TrivialDownsample()
  (1): tensorflow_Conv(
    (0): tensorflow_InstanceNorm2d()
    (1): tensorflow_PReLU()
    (2): tensorflow_Sequential()
    (3): KerasConv2D()
  )
)
input = <tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[ 0.01406393,  0.14293404, -0.03812317, ...,  0.013...      [ 0.1430209 ,  0.02851307, -0.10367332, ..., -0.17331323,
          -0.17559534, -0.23682098]]]], dtype=float32)>

    def call(self, input):
        for i, module in enumerate(self):
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_InstanceNorm2d()
  (1): tensorflow_PReLU()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
)
args = (<tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[ 0.01406393,  0.14293404, -0.03812317, ...,  0.01...    [ 0.1430209 ,  0.02851307, -0.10367332, ..., -0.17331323,
          -0.17559534, -0.23682098]]]], dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x559d171657f0, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_InstanceNorm2d()
  (1): tensorflow_PReLU()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[ 0.01406393,  0.14293404, -0.03812317, ...,  0.01...    [ 0.1430209 ,  0.02851307, -0.10367332, ..., -0.17331323,
          -0.17559534, -0.23682098]]]], dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_InstanceNorm2d()
  (1): tensorflow_PReLU()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[ 0.01406393,  0.14293404, -0.03812317, ...,  0.01...    [ 0.1430209 ,  0.02851307, -0.10367332, ..., -0.17331323,
          -0.17559534, -0.23682098]]]], dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Conv(
  (0): tensorflow_InstanceNorm2d()
  (1): tensorflow_PReLU()
  (2): tensorflow_Sequential()
  (3): KerasConv2D()
)
input = <tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[-1.62880957e+00,  1.68498755e-01, -2.35664558e+00,...52843e-01, -1.39350724e+00, ...,
          -2.22703838e+00, -2.25435328e+00, -2.98717284e+00]]]],
      dtype=float32)>

    def call(self, input):
        for i, module in enumerate(self):
>           input = module(input)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/container.py:196: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PReLU()
args = (<tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[-1.62880957e+00,  1.68498755e-01, -2.35664558e+00...843e-01, -1.39350724e+00, ...,
          -2.22703838e+00, -2.25435328e+00, -2.98717284e+00]]]],
      dtype=float32)>,)
kwargs = {}
stack = [FrameInfo(frame=<frame at 0x7f0028b1ba40, file '/ivy/ivy-integration-tests/Translated_Outputs/tensorflow_outputs/tens...function='__call__', code_context=['                    outputs = super().__call__(*args, **kwargs)\n'], index=0), ...]

    @functools.wraps(fn)
    def frame_info_wrapper(self, *args, **kwargs):
        if self._previous_frame_info is None:
            # store the info about the calling frame.
            stack = inspect.stack()
            self._previous_frame_info = stack[1]
>       res = fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:89: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PReLU(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[-1.62880957e+00,  1.68498755e-01, -2.35664558e+00...843e-01, -1.39350724e+00, ...,
          -2.22703838e+00, -2.25435328e+00, -2.98717284e+00]]]],
      dtype=float32)>,)
kwargs = {}

    @store_frame_info
    @tf.autograph.experimental.do_not_convert
    def __call__(
        self,
        *args,
        v=None,
        buffers=None,
        **kwargs,
    ):
        # TODO: Temp workaround to avoid `call`` from being transformed by AutoGraph
        if not hasattr(self.__class__.call, "autograph_info__"):
            setattr(self.__class__.call, "autograph_info__", True)
>       ret = self._call(*args, v=v, buffers=buffers, **kwargs)

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:978: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PReLU(), v = None, buffers = None
args = (<tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[-1.62880957e+00,  1.68498755e-01, -2.35664558e+00...843e-01, -1.39350724e+00, ...,
          -2.22703838e+00, -2.25435328e+00, -2.98717284e+00]]]],
      dtype=float32)>,)
kwargs = {}, replace_v = False, replace_buffers = False, call_signature = <Signature (input)>

    @tf.autograph.experimental.do_not_convert
    def _call(self, *args, v=None, buffers=None, **kwargs):
        if not self._built or not self.built:
            if not self._built:
                first_arr = self._get_first_array(*args, **kwargs)
                self.build(
                    *args,
                    **kwargs,
                    from_call=True,
                    dtype=first_arr.dtype if first_arr is not None else tf.float32,
                )
    
            if not self.built:
                # Don't use `keras` build method
                if os.environ.get("USE_KERAS_BUILD", "False").lower() == "false":
                    self.inputs = tf.nest.flatten(args)
                else:
                    input_shapes = self._get_input_shapes(*args)
                    if len(input_shapes) == 0:
                        input_shapes = tf.TensorShape(None)
                    elif len(input_shapes) == 1:
                        input_shapes = input_shapes[0]
    
                super(Layer, self).build(tf.TensorShape(None))  # noqa: UP008
    
        # If `v` was provided, replace with the module's v
        replace_v = False
        if v is not None:
            v_orig = self.v
            self._v = v
            replace_v = True
    
        # If `buffers` were provided, replace with the module's buffers
        replace_buffers = False
        if buffers is not None:
            buffers_orig = self.buffers
            self._buffers = buffers
            replace_buffers = True
    
        if replace_v or replace_buffers:
            # Call the forward pass
            ret = super(Layer, self).__call__(*args, **kwargs)  # noqa: UP008
            # Replace v, buffers if needed
            self._v = v_orig if replace_v else self._v
            self._buffers = buffers_orig if replace_buffers else self._buffers
            return ret
        elif hasattr(self.__call__, "wrapped"):
            return self.__call__(*args, **kwargs)
    
        # Get the signature of the call method
        call_signature = inspect.signature(self.call)
    
        # Convert all positional arguments to keyword arguments based on the signature
        new_kwargs = {}
        for idx, (param_name, param) in enumerate(call_signature.parameters.items()):
            if idx < len(args):
                new_kwargs[param_name] = args[idx]
    
        # Merge the existing kwargs
        new_kwargs.update(kwargs)
>       return super(Layer, self).__call__(**new_kwargs)  # noqa: UP008

Translated_Outputs/tensorflow_outputs/tensorflow__stateful.py:746: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PReLU(), args = ()
kwargs = {'input': <tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[-1.62880957e+00,  1.68498755e-01, -2.356...2843e-01, -1.39350724e+00, ...,
          -2.22703838e+00, -2.25435328e+00, -2.98717284e+00]]]],
      dtype=float32)>}
tensorflow_set_item_bknd = <function tensorflow_set_item_bknd at 0x7f000d2512d0>, DATA_FORMAT = 'channels_first'
fn_args_and_kwargs = {'input': <tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[-1.62880957e+00,  1.68498755e-01, -2.356...2843e-01, -1.39350724e+00, ...,
          -2.22703838e+00, -2.25435328e+00, -2.98717284e+00]]]],
      dtype=float32)>}
conv_block_start = <function tensorflow_handle_transpose_in_input_and_output.<locals>.transpose_wrapper.<locals>.<lambda> at 0x7f000d2825f0>, next_call_in_seq = None, conv_block_continued = None

    @functools.wraps(fn)
    def transpose_wrapper(self, *args, **kwargs):
        from ..functional.ivy.general import tensorflow_set_item_bknd
    
        DATA_FORMAT = os.environ.get("DATA_FORMAT", "channels_first")
        kwargs_call = {
            key: val
            for key, val in kwargs.items()
            if key not in dict(original_signature.parameters)
        }
        fn_args_and_kwargs = {
            key: val for key, val in kwargs.items() if key not in kwargs_call
        }
        fn_args_and_kwargs.update(dict(zip(fn.__code__.co_varnames[1:], args)))
        conv_block_start = lambda f: any(
            substr in f.__qualname__
            for substr in CONV_FUNCS
            + NORM_FUNCS
            + POOL_FUNCS
            + KERAS_CONV_FUNCS
            + KERAS_NORM_FUNCS
            + KERAS_POOL_FUNCS
        )
        next_call_in_seq = tensorflow_get_next_func(self)
        name_of_next_call = (
            next_call_in_seq.__class__.__name__
            if hasattr(next_call_in_seq, "__class__")
            else ""
        )
        conv_block_continued = next_call_in_seq and any(
            substr in name_of_next_call for substr in CONV_BLOCK_FNS
        )
        if DATA_FORMAT == "channels_first" and conv_block_start(self.__class__):
            input = fn_args_and_kwargs["input"]
            if len(input.shape) > 4:
                transpose = tensorflow_TransposeType.CONV3D
            elif len(input.shape) > 3:
                transpose = tensorflow_TransposeType.CONV2D
            elif len(input.shape) > 2:
                transpose = tensorflow_TransposeType.CONV1D
            else:
                transpose = tensorflow_TransposeType.NO_TRANSPOSE
            fn_args_and_kwargs = tensorflow_set_item_bknd(
                fn_args_and_kwargs,
                "input",
                tensorflow_apply_transpose(input, transpose=transpose, pt_to_tf=True),
            )
            DATA_FORMAT = "channels_last"
            os.environ = tensorflow_set_item_bknd(
                os.environ, "DATA_FORMAT", DATA_FORMAT
            )
>       res = fn(self, **fn_args_and_kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:391: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PReLU()
input = <tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[-1.62880957e+00,  1.68498755e-01, -2.35664558e+00,...52843e-01, -1.39350724e+00, ...,
          -2.22703838e+00, -2.25435328e+00, -2.98717284e+00]]]],
      dtype=float32)>

    @tensorflow_handle_transpose_in_input_and_output
    def call(self, input):
        from ....ivy.functional.frontends.torch.nn.functional.non_linear_activation_functions import (
            tensorflow_prelu_frnt,
        )
    
>       return tensorflow_prelu_frnt(input, self.weight)

Translated_Outputs/tensorflow_outputs/torch/nn/modules/activation.py:1220: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = <tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[-1.62880957e+00,  1.68498755e-01, -2.35664558e+00,...52843e-01, -1.39350724e+00, ...,
          -2.22703838e+00, -2.25435328e+00, -2.98717284e+00]]]],
      dtype=float32)>
weight = <tf.Variable 'Variable:0' shape=(16,) dtype=float32, numpy=
array([0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
       0.25, 0.25, 0.25, 0.25, 0.25], dtype=float32)>

    def tensorflow_prelu_frnt(input, weight):
        from .....backends.tensorflow.elementwise import tensorflow_add
        from .....backends.tensorflow.elementwise import tensorflow_maximum
        from .....backends.tensorflow.elementwise import tensorflow_multiply
        from .....backends.tensorflow.elementwise import tensorflow_minimum
    
        return tensorflow_add(
            tensorflow_maximum(0, input),
>           tensorflow_multiply(weight, tensorflow_minimum(0, input)),
        )

Translated_Outputs/tensorflow_outputs/ivy/functional/frontends/torch/nn/functional/non_linear_activation_functions.py:61: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x1 = <tf.Variable 'Variable:0' shape=(16,) dtype=float32, numpy=
array([0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25,
       0.25, 0.25, 0.25, 0.25, 0.25], dtype=float32)>
x2 = <tf.Tensor: shape=(1, 8, 128, 256), dtype=float32, numpy=
array([[[[-1.6288096 ,  0.        , -2.3566456 , ..., -1.642...      [ 0.        ,  0.        , -1.3935072 , ..., -2.2270384 ,
          -2.2543533 , -2.9871728 ]]]], dtype=float32)>

    def tensorflow_multiply(
        x1: Union[float, tensorflow.Tensor, tensorflow.Variable],
        x2: Union[float, tensorflow.Tensor, tensorflow.Variable],
        /,
        *,
        out: Optional[Union[tensorflow.Tensor, tensorflow.Variable]] = None,
    ):
        from ...ivy.data_type import tensorflow_default_dtype_bknd
        from ...ivy.general import tensorflow_is_array_bknd
        from .creation import tensorflow_asarray
    
        oirg_x1 = x1
        oirg_x2 = x2
        try:
            dtype = (
                x1.dtype
                if hasattr(x1, "dtype")
                else x2.dtype
                if hasattr(x2, "dtype")
                else tensorflow_default_dtype_bknd()
            )
            if not tensorflow_is_array_bknd(x1):
                x1 = tensorflow_asarray(x1, dtype=dtype)
            if not tensorflow_is_array_bknd(x2):
                x2 = tensorflow_asarray(x2, dtype=dtype)
        except:
            x1 = oirg_x1
            x2 = oirg_x2
>       return tensorflow.math.multiply(x1, x2)
E       tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PReLU.call().
E       
E       [1m{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:CPU:0}} Incompatible shapes: [16] vs. [1,8,128,256] [Op:Mul] name: [0m
E       
E       Arguments received by tensorflow_PReLU.call():
E         â€¢ input=tf.Tensor(shape=(1, 8, 128, 256), dtype=float32)

Translated_Outputs/tensorflow_outputs/ivy/functional/backends/tensorflow/elementwise.py:441: InvalidArgumentError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.DISK
________________________________________________________________________________ test_SIFTFeature[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SIFTFeature(target_framework, mode, backend_compile):
        print("kornia.feature.SIFTFeature")
    
        if backend_compile:
            pytest.skip()
    
        import os
        flag = os.environ.get("APPLY_TRANSPOSE_OPTIMIZATION")
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = "false"
    
        TranspiledSIFTFeature = ivy.transpile(kornia.feature.SIFTFeature, source="torch", target=target_framework)
    
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = flag
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.SIFTFeature(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSIFTFeature(num_features=5000)(transpiled_x)

kornia/test_feature.py:1177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_SIFTFeature object at 0x7f0044c1a230>, num_features = 5000, upright = False, rootsift = True
device = 'cpu', config = {'nms_size': 15, 'pyramid_levels': 4, 's_mult': 22.0, 'scale_factor_levels': 1.4142135623730951, ...}

    def __init__(
        self,
        num_features=8000,
        upright=False,
        rootsift=True,
        device=tensorflow_device_frnt("cpu"),
        config=tensorflow_get_default_detector_config(),
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .scale_space_detector import tensorflow_MultiResolutionDetector
        from .responses import tensorflow_BlobDoGSingle
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .siftdesc import tensorflow_SIFTDescriptor
    
        patch_size: typing.Any = 41
        detector = tensorflow_to_frnt_(
            tensorflow_MultiResolutionDetector(
                tensorflow_BlobDoGSingle(1.0, 1.6),
                num_features,
                config,
                ori_module=tensorflow_PassLAF()
                if upright
>               else tensorflow_LAFOrienter(19),
                aff_module=tensorflow_PassLAF(),
            ),
            device,
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:851: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFOrienter' object has no attribute 'angle_detector'") raised in repr()] tensorflow_LAFOrienter object at 0x7f004cd20910>, args = (19,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFOrienter' object has no attribute 'angle_detector'") raised in repr()] tensorflow_LAFOrienter object at 0x7f004cd20910>, patch_size = 19, num_angular_bins = 36
angle_detector = None

    @tensorflow_store_config_info
    def __init__(self, patch_size=32, num_angular_bins=36, angle_detector=None):
        self.super___init__(
            patch_size=patch_size,
            num_angular_bins=num_angular_bins,
            angle_detector=angle_detector,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.num_ang_bins = num_angular_bins
        self.angle_detector: typing.Any
        if angle_detector is None:
>           self.angle_detector = tensorflow_PatchDominantGradientOrientation(
                self.patch_size, self.num_ang_bins
            )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:883: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08), args = (19, 36), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08), patch_size = 19, num_angular_bins = 36, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, patch_size=32, num_angular_bins=36, eps=1e-08):
        from ..filters.sobel import tensorflow_SpatialGradient
        from ...torch.nn.modules.conv import tensorflow_Conv1d
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..filters.kernels import tensorflow_get_gaussian_discrete_kernel1d
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            num_angular_bins=num_angular_bins,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.num_ang_bins = num_angular_bins
        self.gradient = tensorflow_SpatialGradient("sobel", 1)
        self.eps = eps
        self.angular_smooth = tensorflow_Conv1d(
            1, 1, kernel_size=5, padding=2, bias=False, padding_mode="circular"
        )
        self.angular_smooth.weight = tensorflow_set_item_bknd(
            self.angular_smooth.weight,
            slice(None, None, None),
>           tensorflow_get_gaussian_discrete_kernel1d(5, 1.6),
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:428: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kernel_size = 5, sigma = 1.6, force_even = False

    def tensorflow_get_gaussian_discrete_kernel1d(
        kernel_size, sigma, force_even=False, *, device=None, dtype=None
    ):
        tensorflow__check_kernel_size(kernel_size, allow_even=force_even)
>       return tensorflow_gaussian_discrete(kernel_size, sigma, device=device, dtype=dtype)

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:346: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

window_size = 5, sigma = <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.6]], dtype=float32)>

    def tensorflow_gaussian_discrete(window_size, sigma, *, device=None, dtype=None):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_exp_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ..core._backend import tensor
        from ..core._backend import concatenate
    
        if isinstance(sigma, (float,)):
            sigma = tensor([[sigma]], device=device, dtype=dtype)
        tensorflow_KORNIA_CHECK_SHAPE(sigma, ["B", "1"])
        sigma2 = sigma * sigma
        tail = int(window_size // 2) + 1
        bessels = [
>           ivy__modified_bessel_0(sigma2),
            tensorflow__modified_bessel_1(sigma2),
            *(tensorflow__modified_bessel_i(k, sigma2) for k in range(2, tail)),
        ]
E       NameError: name 'ivy__modified_bessel_0' is not defined

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:209: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SIFTFeature
___________________________________________________________________________ test_SIFTFeatureScaleSpace[tensorflow-s2s-False] ___________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_SIFTFeatureScaleSpace(target_framework, mode, backend_compile):
        print("kornia.feature.SIFTFeatureScaleSpace")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledSIFTFeatureScaleSpace = ivy.transpile(kornia.feature.SIFTFeatureScaleSpace, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.SIFTFeatureScaleSpace(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledSIFTFeatureScaleSpace(num_features=5000)(transpiled_x)

kornia/test_feature.py:1194: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_SIFTFeatureScaleSpace object at 0x7f004ef1cd00>, num_features = 5000, upright = False
rootsift = True, device = 'cpu'

    def __init__(
        self,
        num_features=8000,
        upright=False,
        rootsift=True,
        device=tensorflow_device_frnt("cpu"),
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .scale_space_detector import tensorflow_ScaleSpaceDetector
        from .responses import tensorflow_BlobDoG
        from ..geometry.subpix.spatial_soft_argmax import tensorflow_ConvQuadInterp3d
        from ..geometry.transform.pyramid import tensorflow_ScalePyramid
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .siftdesc import tensorflow_SIFTDescriptor
    
        patch_size: typing.Any = 41
        detector = tensorflow_to_frnt_(
            tensorflow_ScaleSpaceDetector(
                num_features,
                resp_module=tensorflow_BlobDoG(),
                nms_module=tensorflow_ConvQuadInterp3d(10),
                scale_pyr_module=tensorflow_ScalePyramid(3, 1.6, 32, double_image=True),
                ori_module=tensorflow_PassLAF()
                if upright
>               else tensorflow_LAFOrienter(19),
                scale_space_response=True,
                minima_are_also_good=True,
                mr_size=6.0,
            ),
            device,
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:893: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFOrienter' object has no attribute 'angle_detector'") raised in repr()] tensorflow_LAFOrienter object at 0x7f000cc04790>, args = (19,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFOrienter' object has no attribute 'angle_detector'") raised in repr()] tensorflow_LAFOrienter object at 0x7f000cc04790>, patch_size = 19, num_angular_bins = 36
angle_detector = None

    @tensorflow_store_config_info
    def __init__(self, patch_size=32, num_angular_bins=36, angle_detector=None):
        self.super___init__(
            patch_size=patch_size,
            num_angular_bins=num_angular_bins,
            angle_detector=angle_detector,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.num_ang_bins = num_angular_bins
        self.angle_detector: typing.Any
        if angle_detector is None:
>           self.angle_detector = tensorflow_PatchDominantGradientOrientation(
                self.patch_size, self.num_ang_bins
            )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:883: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08), args = (19, 36), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08), patch_size = 19, num_angular_bins = 36, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, patch_size=32, num_angular_bins=36, eps=1e-08):
        from ..filters.sobel import tensorflow_SpatialGradient
        from ...torch.nn.modules.conv import tensorflow_Conv1d
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..filters.kernels import tensorflow_get_gaussian_discrete_kernel1d
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            num_angular_bins=num_angular_bins,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.num_ang_bins = num_angular_bins
        self.gradient = tensorflow_SpatialGradient("sobel", 1)
        self.eps = eps
        self.angular_smooth = tensorflow_Conv1d(
            1, 1, kernel_size=5, padding=2, bias=False, padding_mode="circular"
        )
        self.angular_smooth.weight = tensorflow_set_item_bknd(
            self.angular_smooth.weight,
            slice(None, None, None),
>           tensorflow_get_gaussian_discrete_kernel1d(5, 1.6),
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:428: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kernel_size = 5, sigma = 1.6, force_even = False

    def tensorflow_get_gaussian_discrete_kernel1d(
        kernel_size, sigma, force_even=False, *, device=None, dtype=None
    ):
        tensorflow__check_kernel_size(kernel_size, allow_even=force_even)
>       return tensorflow_gaussian_discrete(kernel_size, sigma, device=device, dtype=dtype)

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:346: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

window_size = 5, sigma = <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.6]], dtype=float32)>

    def tensorflow_gaussian_discrete(window_size, sigma, *, device=None, dtype=None):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_exp_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ..core._backend import tensor
        from ..core._backend import concatenate
    
        if isinstance(sigma, (float,)):
            sigma = tensor([[sigma]], device=device, dtype=dtype)
        tensorflow_KORNIA_CHECK_SHAPE(sigma, ["B", "1"])
        sigma2 = sigma * sigma
        tail = int(window_size // 2) + 1
        bessels = [
>           ivy__modified_bessel_0(sigma2),
            tensorflow__modified_bessel_1(sigma2),
            *(tensorflow__modified_bessel_i(k, sigma2) for k in range(2, tail)),
        ]
E       NameError: name 'ivy__modified_bessel_0' is not defined

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:209: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.SIFTFeatureScaleSpace
_____________________________________________________________________________ test_GFTTAffNetHardNet[tensorflow-s2s-False] _____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_GFTTAffNetHardNet(target_framework, mode, backend_compile):
        print("kornia.feature.GFTTAffNetHardNet")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledGFTTAffNetHardNet = ivy.transpile(kornia.feature.GFTTAffNetHardNet, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.GFTTAffNetHardNet(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledGFTTAffNetHardNet(num_features=5000)(transpiled_x)

kornia/test_feature.py:1211: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_GFTTAffNetHardNet object at 0x7f003cfeca90>, num_features = 5000, upright = False, device = 'cpu'
config = {'nms_size': 15, 'pyramid_levels': 4, 's_mult': 22.0, 'scale_factor_levels': 1.4142135623730951, ...}

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        config=tensorflow_get_default_detector_config(),
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .scale_space_detector import tensorflow_MultiResolutionDetector
        from .responses import tensorflow_CornerGFTT
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .affine_shape import tensorflow_LAFAffNetShapeEstimator
    
        detector = tensorflow_to_frnt_(
            tensorflow_MultiResolutionDetector(
                tensorflow_CornerGFTT(),
                num_features,
                config,
                ori_module=tensorflow_PassLAF()
                if upright
>               else tensorflow_LAFOrienter(19),
                aff_module=tensorflow_LAFAffNetShapeEstimator(True).eval(),
            ),
            device,
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:933: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFOrienter' object has no attribute 'angle_detector'") raised in repr()] tensorflow_LAFOrienter object at 0x7f004ed73bb0>, args = (19,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFOrienter' object has no attribute 'angle_detector'") raised in repr()] tensorflow_LAFOrienter object at 0x7f004ed73bb0>, patch_size = 19, num_angular_bins = 36
angle_detector = None

    @tensorflow_store_config_info
    def __init__(self, patch_size=32, num_angular_bins=36, angle_detector=None):
        self.super___init__(
            patch_size=patch_size,
            num_angular_bins=num_angular_bins,
            angle_detector=angle_detector,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.num_ang_bins = num_angular_bins
        self.angle_detector: typing.Any
        if angle_detector is None:
>           self.angle_detector = tensorflow_PatchDominantGradientOrientation(
                self.patch_size, self.num_ang_bins
            )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:883: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08), args = (19, 36), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08), patch_size = 19, num_angular_bins = 36, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, patch_size=32, num_angular_bins=36, eps=1e-08):
        from ..filters.sobel import tensorflow_SpatialGradient
        from ...torch.nn.modules.conv import tensorflow_Conv1d
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..filters.kernels import tensorflow_get_gaussian_discrete_kernel1d
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            num_angular_bins=num_angular_bins,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.num_ang_bins = num_angular_bins
        self.gradient = tensorflow_SpatialGradient("sobel", 1)
        self.eps = eps
        self.angular_smooth = tensorflow_Conv1d(
            1, 1, kernel_size=5, padding=2, bias=False, padding_mode="circular"
        )
        self.angular_smooth.weight = tensorflow_set_item_bknd(
            self.angular_smooth.weight,
            slice(None, None, None),
>           tensorflow_get_gaussian_discrete_kernel1d(5, 1.6),
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:428: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kernel_size = 5, sigma = 1.6, force_even = False

    def tensorflow_get_gaussian_discrete_kernel1d(
        kernel_size, sigma, force_even=False, *, device=None, dtype=None
    ):
        tensorflow__check_kernel_size(kernel_size, allow_even=force_even)
>       return tensorflow_gaussian_discrete(kernel_size, sigma, device=device, dtype=dtype)

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:346: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

window_size = 5, sigma = <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.6]], dtype=float32)>

    def tensorflow_gaussian_discrete(window_size, sigma, *, device=None, dtype=None):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_exp_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ..core._backend import tensor
        from ..core._backend import concatenate
    
        if isinstance(sigma, (float,)):
            sigma = tensor([[sigma]], device=device, dtype=dtype)
        tensorflow_KORNIA_CHECK_SHAPE(sigma, ["B", "1"])
        sigma2 = sigma * sigma
        tail = int(window_size // 2) + 1
        bessels = [
>           ivy__modified_bessel_0(sigma2),
            tensorflow__modified_bessel_1(sigma2),
            *(tensorflow__modified_bessel_i(k, sigma2) for k in range(2, tail)),
        ]
E       NameError: name 'ivy__modified_bessel_0' is not defined

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:209: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.GFTTAffNetHardNet
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/ducha-aiki/affnet/raw/master/pretrained/AffNet.pth" to /root/.cache/torch/hub/checkpoints/AffNet.pth

  0%|          | 0.00/332k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 332k/332k [00:00<00:00, 12.2MB/s]
____________________________________________________________________________ test_KeyNetAffNetHardNet[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KeyNetAffNetHardNet(target_framework, mode, backend_compile):
        print("kornia.feature.KeyNetAffNetHardNet")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetAffNetHardNet = ivy.transpile(kornia.feature.KeyNetAffNetHardNet, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.KeyNetAffNetHardNet(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledKeyNetAffNetHardNet(num_features=5000)(transpiled_x)

kornia/test_feature.py:1228: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_KeyNetAffNetHardNet object at 0x7f0044dc3ac0>, num_features = 5000, upright = False, device = 'cpu'
scale_laf = 1.0

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        scale_laf=1.0,
    ):
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .orientation import tensorflow_OriNet
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .keynet import tensorflow_KeyNetDetector
        from .affine_shape import tensorflow_LAFAffNetShapeEstimator
    
        ori_module = (
            tensorflow_PassLAF()
            if upright
>           else tensorflow_LAFOrienter(angle_detector=tensorflow_OriNet(True))
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:963: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
pretrained = True, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, pretrained=False, eps=1e-08):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ...torch.nn.modules.activation import tensorflow_Tanh
        from ...torch.nn.modules.pooling import tensorflow_AdaptiveAvgPool2d
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=32,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.25),
            KerasConv2D(
                in_channels=64,
                filters=2,
                kernel_size=8,
                strides=1,
                padding=1,
                use_bias=True,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            tensorflow_Tanh(),
            tensorflow_AdaptiveAvgPool2d(1),
        )
        self.eps = eps
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["orinet"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:1040: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.KeyNetAffNetHardNet
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/ducha-aiki/affnet/raw/master/pretrained/OriNet.pth" to /root/.cache/torch/hub/checkpoints/OriNet.pth

  0%|          | 0.00/316k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 316k/316k [00:00<00:00, 64.7MB/s]
Downloading: "https://github.com/axelBarroso/Key.Net-Pytorch/raw/main/model/weights/keynet_pytorch.pth" to /root/.cache/torch/hub/checkpoints/keynet_pytorch.pth

  0%|          | 0.00/78.0k [00:00<?, ?B/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78.0k/78.0k [00:00<00:00, 18.9MB/s]
_______________________________________________________________________________ test_KeyNetHardNet[tensorflow-s2s-False] _______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_KeyNetHardNet(target_framework, mode, backend_compile):
        print("kornia.feature.KeyNetHardNet")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledKeyNetHardNet = ivy.transpile(kornia.feature.KeyNetHardNet, source="torch", target=target_framework)
    
        x = torch.rand(1, 1, 256, 256)
        torch_out = kornia.feature.KeyNetHardNet(num_features=5000)(x)
    
        transpiled_x = _nest_torch_tensor_to_new_framework(x, target_framework)
>       transpiled_out = TranspiledKeyNetHardNet(num_features=5000)(transpiled_x)

kornia/test_feature.py:1245: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_KeyNetHardNet object at 0x7f004e2b3fa0>, num_features = 5000, upright = False, device = 'cpu'
scale_laf = 1.0

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        scale_laf=1.0,
    ):
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .orientation import tensorflow_OriNet
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .keynet import tensorflow_KeyNetDetector
    
        ori_module = (
            tensorflow_PassLAF()
            if upright
>           else tensorflow_LAFOrienter(angle_detector=tensorflow_OriNet(True))
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:962: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_OriNet(
  (features): tensorflow_Sequential(
    (0): KerasConv2D()
    (1): KerasBatchNorm2D()
    (2): te...tensorflow_Dropout()
    (19): KerasConv2D()
    (20): tensorflow_Tanh()
    (21): tensorflow_AdaptiveAvgPool2d()
  )
)
pretrained = True, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, pretrained=False, eps=1e-08):
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.activation import tensorflow_ReLU
        from ...torch.nn.modules.dropout import tensorflow_Dropout
        from ...torch.nn.modules.activation import tensorflow_Tanh
        from ...torch.nn.modules.pooling import tensorflow_AdaptiveAvgPool2d
        from ..utils.helpers import tensorflow_map_location_to_cpu
        from ...tensorflow__stateful_layers import KerasConv2D
        from ...tensorflow__stateful_layers import KerasBatchNorm2D
    
        self.super___init__(
            pretrained=pretrained,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.features = tensorflow_Sequential(
            KerasConv2D(
                in_channels=1,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=16,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=16,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=16,
                filters=32,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=32,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=32,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=32,
                filters=64,
                kernel_size=3,
                strides=2,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            KerasConv2D(
                in_channels=64,
                filters=64,
                kernel_size=3,
                strides=1,
                padding=1,
                use_bias=False,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            KerasBatchNorm2D(
                num_features=64,
                momentum=0.1,
                epsilon=1e-05,
                center=False,
                scale=False,
                axis=-1,
                track_running_stats=True,
            ),
            tensorflow_ReLU(),
            tensorflow_Dropout(0.25),
            KerasConv2D(
                in_channels=64,
                filters=2,
                kernel_size=8,
                strides=1,
                padding=1,
                use_bias=True,
                dilation_rate=1,
                groups=1,
                padding_mode="zeros",
                data_format="channels_last",
            ),
            tensorflow_Tanh(),
            tensorflow_AdaptiveAvgPool2d(1),
        )
        self.eps = eps
        if pretrained:
>           pretrained_dict = torch.hub.load_state_dict_from_url(
                urls["orinet"], map_location=tensorflow_map_location_to_cpu
            )
E           NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:1040: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.KeyNetHardNet
____________________________________________________________________________ test_LocalFeatureMatcher[tensorflow-s2s-False] ____________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LocalFeatureMatcher(target_framework, mode, backend_compile):
        print("kornia.feature.LocalFeatureMatcher")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledGFTTAffNetHardNet = ivy.transpile(kornia.feature.GFTTAffNetHardNet, source="torch", target=target_framework)
        TranspiledDescriptorMatcher = ivy.transpile(kornia.feature.DescriptorMatcher, source="torch", target=target_framework)
        TranspiledLocalFeatureMatcher = ivy.transpile(kornia.feature.LocalFeatureMatcher, source="torch", target=target_framework)
    
        data = {
            "image0": torch.rand(1, 1, 320, 200),
            "image1": torch.rand(1, 1, 128, 128),
        }
        torch_local_feature = kornia.feature.GFTTAffNetHardNet(10)
        torch_matcher = kornia.feature.DescriptorMatcher('snn', 0.8)
        torch_out = kornia.feature.LocalFeatureMatcher(torch_local_feature, torch_matcher)(data)
    
>       transpiled_local_feature = TranspiledGFTTAffNetHardNet(10)

kornia/test_feature.py:1309: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'NoneType' object has no attribute 'items'") raised in repr()] tensorflow_GFTTAffNetHardNet object at 0x7f004e575270>, num_features = 10, upright = False, device = 'cpu'
config = {'nms_size': 15, 'pyramid_levels': 4, 's_mult': 22.0, 'scale_factor_levels': 1.4142135623730951, ...}

    def __init__(
        self,
        num_features=8000,
        upright=False,
        device=tensorflow_device_frnt("cpu"),
        config=tensorflow_get_default_detector_config(),
    ):
        from ...ivy.functional.frontends.torch.tensor import tensorflow_to_frnt_
        from .scale_space_detector import tensorflow_MultiResolutionDetector
        from .responses import tensorflow_CornerGFTT
        from .orientation import tensorflow_PassLAF
        from .orientation import tensorflow_LAFOrienter
        from .affine_shape import tensorflow_LAFAffNetShapeEstimator
    
        detector = tensorflow_to_frnt_(
            tensorflow_MultiResolutionDetector(
                tensorflow_CornerGFTT(),
                num_features,
                config,
                ori_module=tensorflow_PassLAF()
                if upright
>               else tensorflow_LAFOrienter(19),
                aff_module=tensorflow_LAFAffNetShapeEstimator(True).eval(),
            ),
            device,
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:934: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFOrienter' object has no attribute 'angle_detector'") raised in repr()] tensorflow_LAFOrienter object at 0x7f000db7f3d0>, args = (19,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFOrienter' object has no attribute 'angle_detector'") raised in repr()] tensorflow_LAFOrienter object at 0x7f000db7f3d0>, patch_size = 19, num_angular_bins = 36
angle_detector = None

    @tensorflow_store_config_info
    def __init__(self, patch_size=32, num_angular_bins=36, angle_detector=None):
        self.super___init__(
            patch_size=patch_size,
            num_angular_bins=num_angular_bins,
            angle_detector=angle_detector,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.num_ang_bins = num_angular_bins
        self.angle_detector: typing.Any
        if angle_detector is None:
>           self.angle_detector = tensorflow_PatchDominantGradientOrientation(
                self.patch_size, self.num_ang_bins
            )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:1442: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08), args = (19, 36), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=19, num_ang_bins=36, eps=1e-08), patch_size = 19, num_angular_bins = 36, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, patch_size=32, num_angular_bins=36, eps=1e-08):
        from ..filters.sobel import tensorflow_SpatialGradient
        from ...torch.nn.modules.conv import tensorflow_Conv1d
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..filters.kernels import tensorflow_get_gaussian_discrete_kernel1d
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            num_angular_bins=num_angular_bins,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.num_ang_bins = num_angular_bins
        self.gradient = tensorflow_SpatialGradient("sobel", 1)
        self.eps = eps
        self.angular_smooth = tensorflow_Conv1d(
            1, 1, kernel_size=5, padding=2, bias=False, padding_mode="circular"
        )
        self.angular_smooth.weight = tensorflow_set_item_bknd(
            self.angular_smooth.weight,
            slice(None, None, None),
>           tensorflow_get_gaussian_discrete_kernel1d(5, 1.6),
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:430: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kernel_size = 5, sigma = 1.6, force_even = False

    def tensorflow_get_gaussian_discrete_kernel1d(
        kernel_size, sigma, force_even=False, *, device=None, dtype=None
    ):
        tensorflow__check_kernel_size(kernel_size, allow_even=force_even)
>       return tensorflow_gaussian_discrete(kernel_size, sigma, device=device, dtype=dtype)

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:346: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

window_size = 5, sigma = <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.6]], dtype=float32)>

    def tensorflow_gaussian_discrete(window_size, sigma, *, device=None, dtype=None):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_exp_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ..core._backend import tensor
        from ..core._backend import concatenate
    
        if isinstance(sigma, (float,)):
            sigma = tensor([[sigma]], device=device, dtype=dtype)
        tensorflow_KORNIA_CHECK_SHAPE(sigma, ["B", "1"])
        sigma2 = sigma * sigma
        tail = int(window_size // 2) + 1
        bessels = [
>           ivy__modified_bessel_0(sigma2),
            tensorflow__modified_bessel_1(sigma2),
            *(tensorflow__modified_bessel_i(k, sigma2) for k in range(2, tail)),
        ]
E       NameError: name 'ivy__modified_bessel_0' is not defined

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:209: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LocalFeatureMatcher
_____________________________________________________________________________ test_LightGlueMatcher[tensorflow-s2s-False] ______________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LightGlueMatcher(target_framework, mode, backend_compile):
        print("kornia.feature.LightGlueMatcher")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLightGlueMatcher = ivy.transpile(kornia.feature.LightGlueMatcher, source="torch", target=target_framework)
    
        torch_args = (
            torch.rand(2, 128),
            torch.rand(5, 128),
            torch.rand(1, 2, 2, 3),
            torch.rand(1, 5, 2, 3),
        )
        torch_out = kornia.feature.LightGlueMatcher('disk')(*torch_args)
    
        transpiled_args = _nest_torch_tensor_to_new_framework(torch_args, target_framework)
>       transpiled_out = TranspiledLightGlueMatcher('disk')(*transpiled_args)

kornia/test_feature.py:1334: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlueMatcher(), feature_name = 'disk', params = {}

    def __init__(self, feature_name="disk", params={}):
        from .lightglue import tensorflow_LightGlue
    
        feature_name_: typing.Any = feature_name.lower()
        super().__init__(feature_name_)
        self.feature_name = feature_name_
        self.params = params
>       self.matcher = tensorflow_LightGlue(self.feature_name, **params)

Translated_Outputs/tensorflow_outputs/kornia/feature/integrated.py:1493: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): KerasDense()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), args = ('disk',), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): KerasDense()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), features = 'disk', conf_ = {}
tensorflow_KORNIA_CHECK = <function tensorflow_KORNIA_CHECK at 0x7f000db476d0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f0044b823b0>
tensorflow_Identity = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.linear.tensorflow_Identity'>
ModuleList = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.container.tensorflow_ModuleList'>
KerasDense = <class 'Translated_Outputs.tensorflow_outputs.tensorflow__stateful_layers.KerasDense'>
conf = namespace(name='lightglue', input_dim=128, descriptor_dim=256, add_scale_ori=False, add_laf=False, scale_coef=1.0, n_l...=4, flash=True, mp=False, depth_confidence=0.95, width_confidence=0.99, filter_threshold=0.1, weights='disk_lightglue')
k = 'input_dim'

    @tensorflow_store_config_info
    def __init__(self, features="superpoint", **conf_):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...torch.nn.modules.linear import tensorflow_Identity
        from ..core._backend import ModuleList
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            features=features,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.conf = conf = SimpleNamespace(**{**self.default_conf, **conf_})
        if features is not None:
            tensorflow_KORNIA_CHECK(
                features in list(self.features.keys()), "Features keys are wrong"
            )
            for k, v in tensorflow_get_item(self.features, features).items():
                setattr(conf, k, v)
        tensorflow_KORNIA_CHECK(not (self.conf.add_scale_ori and self.conf.add_laf))
        if conf.input_dim != conf.descriptor_dim:
            self.input_proj = KerasDense(
                in_features=conf.input_dim, units=conf.descriptor_dim, use_bias=True
            )
        else:
            self.input_proj = tensorflow_Identity()
        head_dim = conf.descriptor_dim // conf.num_heads
        self.posenc = tensorflow_LearnableFourierPositionalEncoding(
            2 + 2 * conf.add_scale_ori + 4 * conf.add_laf, head_dim, head_dim
        )
        h, n, d = conf.num_heads, conf.n_layers, conf.descriptor_dim
        ag__result_list_0 = []
        for _ in range(n):
>           res = tensorflow_TransformerLayer(d, h, conf.flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:3173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @tensorflow_store_config_info
    def __init__(self, *args, **kwargs):
        self.super___init__(
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.self_attn = tensorflow_SelfBlock(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:2224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), embed_dim = 256, num_heads = 4, flash = True, bias = True

    @tensorflow_store_config_info
    def __init__(self, embed_dim, num_heads, flash=False, bias=True):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.normalization import tensorflow_LayerNorm
        from ...torch.nn.modules.activation import tensorflow_GELU
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            embed_dim,
            num_heads,
            flash=flash,
            bias=bias,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        tensorflow_KORNIA_CHECK(
            self.embed_dim % num_heads == 0,
            "Embed dimension should be dividable by num_heads",
        )
        self.head_dim = self.embed_dim // num_heads
        self.Wqkv = KerasDense(
            in_features=embed_dim, units=3 * embed_dim, use_bias=bias
        )
>       self.inner_attn = tensorflow_Attention(flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:1362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), allow_flash = True

    @tensorflow_store_config_info
    def __init__(self, allow_flash):
        from ...torch.backends.cuda.__init__ import tensorflow_enable_flash_sdp
    
        self.super___init__(
            allow_flash,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if allow_flash and not FLASH_AVAILABLE:
            warnings.warn(
                "FlashAttention is not available. For optimal speed, consider installing torch >= 2.0 or flash-attn.",
                stacklevel=2,
            )
        self.enable_flash = allow_flash and FLASH_AVAILABLE
>       self.has_sdp = hasattr(torch.nn.functional, "scaled_dot_product_attention")
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:911: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LightGlueMatcher
Loaded LightGlue model
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/cvg/LightGlue/releases/download/v0.1_arxiv/disk_lightglue.pth" to /root/.cache/torch/hub/checkpoints/disk_lightglue_v0-1_arxiv-pth

  0%|          | 0.00/45.4M [00:00<?, ?B/s]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 22.2M/45.4M [00:00<00:00, 232MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45.4M/45.4M [00:00<00:00, 263MB/s]
_________________________________________________________________________________ test_LightGlue[tensorflow-s2s-False] _________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LightGlue(target_framework, mode, backend_compile):
        print("kornia.feature.LightGlue")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLightGlue = ivy.transpile(kornia.feature.LightGlue, source="torch", target=target_framework)
    
        data = {
            "image0": {
                "keypoints": torch.rand(1, 100, 2),
                "descriptors": torch.rand(1, 100, 256),
                "image_size": torch.tensor([[640, 480]]),
            },
            "image1": {
                "keypoints": torch.rand(1, 120, 2),
                "descriptors": torch.rand(1, 120, 256),
                "image_size": torch.tensor([[640, 480]]),
            }
        }
        torch_out = kornia.feature.LightGlue(features='superpoint')(data)
    
        transpiled_data = _nest_torch_tensor_to_new_framework(data, target_framework)
>       transpiled_out = TranspiledLightGlue(features='superpoint')(transpiled_data)

kornia/test_feature.py:1362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): tensorflow_Identity()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), args = ()
kwargs = {'features': 'superpoint'}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_LightGlue(
  (input_proj): tensorflow_Identity()
  (posenc): tensorflow_LearnableFourierPositionalEncoding(
    (Wr): KerasDense()
  )
), features = 'superpoint', conf_ = {}
tensorflow_KORNIA_CHECK = <function tensorflow_KORNIA_CHECK at 0x7f000db476d0>, tensorflow_get_item = <function tensorflow_get_item at 0x7f0044b823b0>
tensorflow_Identity = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.linear.tensorflow_Identity'>
ModuleList = <class 'Translated_Outputs.tensorflow_outputs.torch.nn.modules.container.tensorflow_ModuleList'>
KerasDense = <class 'Translated_Outputs.tensorflow_outputs.tensorflow__stateful_layers.KerasDense'>
conf = namespace(name='lightglue', input_dim=256, descriptor_dim=256, add_scale_ori=False, add_laf=False, scale_coef=1.0, n_l...ash=True, mp=False, depth_confidence=0.95, width_confidence=0.99, filter_threshold=0.1, weights='superpoint_lightglue')
k = 'input_dim'

    @tensorflow_store_config_info
    def __init__(self, features="superpoint", **conf_):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...ivy.functional.backends.tensorflow.general import tensorflow_get_item
        from ...torch.nn.modules.linear import tensorflow_Identity
        from ..core._backend import ModuleList
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            features=features,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.conf = conf = SimpleNamespace(**{**self.default_conf, **conf_})
        if features is not None:
            tensorflow_KORNIA_CHECK(
                features in list(self.features.keys()), "Features keys are wrong"
            )
            for k, v in tensorflow_get_item(self.features, features).items():
                setattr(conf, k, v)
        tensorflow_KORNIA_CHECK(not (self.conf.add_scale_ori and self.conf.add_laf))
        if conf.input_dim != conf.descriptor_dim:
            self.input_proj = KerasDense(
                in_features=conf.input_dim, units=conf.descriptor_dim, use_bias=True
            )
        else:
            self.input_proj = tensorflow_Identity()
        head_dim = conf.descriptor_dim // conf.num_heads
        self.posenc = tensorflow_LearnableFourierPositionalEncoding(
            2 + 2 * conf.add_scale_ori + 4 * conf.add_laf, head_dim, head_dim
        )
        h, n, d = conf.num_heads, conf.n_layers, conf.descriptor_dim
        ag__result_list_0 = []
        for _ in range(n):
>           res = tensorflow_TransformerLayer(d, h, conf.flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:3173: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_TransformerLayer(), args = (256, 4, True), kwargs = {}

    @tensorflow_store_config_info
    def __init__(self, *args, **kwargs):
        self.super___init__(
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
>       self.self_attn = tensorflow_SelfBlock(*args, **kwargs)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:2224: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), args = (256, 4, True), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_SelfBlock(
  (Wqkv): KerasDense()
), embed_dim = 256, num_heads = 4, flash = True, bias = True

    @tensorflow_store_config_info
    def __init__(self, embed_dim, num_heads, flash=False, bias=True):
        from ..core.check import tensorflow_KORNIA_CHECK
        from ...torch.nn.modules.container import tensorflow_Sequential
        from ...torch.nn.modules.normalization import tensorflow_LayerNorm
        from ...torch.nn.modules.activation import tensorflow_GELU
        from ...tensorflow__stateful_layers import KerasDense
    
        self.super___init__(
            embed_dim,
            num_heads,
            flash=flash,
            bias=bias,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        tensorflow_KORNIA_CHECK(
            self.embed_dim % num_heads == 0,
            "Embed dimension should be dividable by num_heads",
        )
        self.head_dim = self.embed_dim // num_heads
        self.Wqkv = KerasDense(
            in_features=embed_dim, units=3 * embed_dim, use_bias=bias
        )
>       self.inner_attn = tensorflow_Attention(flash)

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:1362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), args = (True,), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_Attention(), allow_flash = True

    @tensorflow_store_config_info
    def __init__(self, allow_flash):
        from ...torch.backends.cuda.__init__ import tensorflow_enable_flash_sdp
    
        self.super___init__(
            allow_flash,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        if allow_flash and not FLASH_AVAILABLE:
            warnings.warn(
                "FlashAttention is not available. For optimal speed, consider installing torch >= 2.0 or flash-attn.",
                stacklevel=2,
            )
        self.enable_flash = allow_flash and FLASH_AVAILABLE
>       self.has_sdp = hasattr(torch.nn.functional, "scaled_dot_product_attention")
E       NameError: name 'torch' is not defined

Translated_Outputs/tensorflow_outputs/kornia/feature/lightglue.py:911: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LightGlue
Loaded LightGlue model
----------------------------------------------------------------------------------------- Captured stderr call -----------------------------------------------------------------------------------------
Downloading: "https://github.com/cvg/LightGlue/releases/download/v0.1_arxiv/superpoint_lightglue.pth" to /root/.cache/torch/hub/checkpoints/superpoint_lightglue_v0-1_arxiv-pth

  0%|          | 0.00/45.3M [00:00<?, ?B/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 20.1M/45.3M [00:00<00:00, 192MB/s]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 38.5M/45.3M [00:00<00:00, 187MB/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45.3M/45.3M [00:00<00:00, 185MB/s]
___________________________________________________________________________________ test_LoFTR[tensorflow-s2s-False] ___________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LoFTR(target_framework, mode, backend_compile):
        print("kornia.feature.LoFTR")
    
        if backend_compile:
            pytest.skip()
    
>       TranspiledLoFTR = ivy.transpile(kornia.feature.LoFTR, source="torch", target=target_framework)

kornia/test_feature.py:1373: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

object = <class 'kornia.feature.loftr.loftr.LoFTR'>, source = 'torch', target = 'tensorflow', profiling = False, reuse_existing = True

    def transpile(
        object,
        source: str = "torch",
        target: str = "tensorflow",
        profiling: bool = False,
        reuse_existing: bool = True,
    ):
        """Converts a given object (class/function) from one framework to another.
    
        This function performs source-to-source translation of a given object from the source framework
        to the target framework.
    
        The object can be translated between two frameworks or between the Ivy IR as well
        e.g. (source="torch_frontend", target="ivy") or (source="torch_frontend", target="tensorflow") etc.
    
        Args:
        ----
            object: The object (class/function) to be translated.
            source (str, optional): The source framework. Defaults to 'torch'.
            target (str, optional): The target framework. Defaults to 'tensorflow'.
            profiling: Whether to add performance profiling.
            reuse_existing (bool, optional): If True, the function will check if `object`
                                             already exists in the translated directory and reuse it.
                                             If False, it will re-translate `object`,
                                             even if it already exists in the directory, and overwrite
                                             the old implementation. Defaults to 'True'.
    
        Returns:
        -------
        The translated object.
        """
        from ._compiler import transpile as _transpile
    
>       return _transpile(
            object=object,
            source=source,
            target=target,
            profiling=profiling,
            reuse_existing=reuse_existing,
        )

../ivy/ivy/compiler/compiler.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, line 77)

IXC.pyx:226: IvyException
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LoFTR
_________________________________________________________________________ test_PatchAffineShapeEstimator[tensorflow-s2s-False] _________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_PatchAffineShapeEstimator(target_framework, mode, backend_compile):
        print("kornia.feature.PatchAffineShapeEstimator")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledPatchAffineShapeEstimator = ivy.transpile(kornia.feature.PatchAffineShapeEstimator, source="torch", target=target_framework)
    
        patch = torch.rand(1, 1, 19, 19)
        torch_out = kornia.feature.PatchAffineShapeEstimator()(patch)
    
        transpiled_patch = _nest_torch_tensor_to_new_framework(patch, target_framework)
>       transpiled_out = TranspiledPatchAffineShapeEstimator()(transpiled_patch)

kornia/test_feature.py:1415: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_PatchAffineShapeEstimator' object has no attribute 'eps'") raised in repr()] tensorflow_PatchAffineShapeEstimator object at 0x7f004d22b310>, patch_size = 19
eps = 1e-10

    def __init__(self, patch_size=19, eps=1e-10):
        from ..filters.sobel import tensorflow_sobel
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size: typing.Any = patch_size
>       self.gradient: typing.Any = tensorflow_sobel.SpatialGradient("sobel", 1)
E       AttributeError: 'function' object has no attribute 'SpatialGradient'

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:53: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.PatchAffineShapeEstimator
__________________________________________________________________________ test_LAFAffineShapeEstimator[tensorflow-s2s-False] __________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LAFAffineShapeEstimator(target_framework, mode, backend_compile):
        print("kornia.feature.LAFAffineShapeEstimator")
    
        if backend_compile:
            pytest.skip()
    
        TranspiledLAFAffineShapeEstimator = ivy.transpile(kornia.feature.LAFAffineShapeEstimator, source="torch", target=target_framework)
    
        laf = torch.rand(1, 2, 2, 3)
        img = torch.rand(1, 1, 32, 32)
        torch_out = kornia.feature.LAFAffineShapeEstimator()(laf, img)
    
        transpiled_laf = _nest_torch_tensor_to_new_framework(laf, target_framework)
        transpiled_img = _nest_torch_tensor_to_new_framework(img, target_framework)
>       transpiled_out = TranspiledLAFAffineShapeEstimator()(transpiled_laf, transpiled_img)

kornia/test_feature.py:1434: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFAffineShapeEstimator' object has no attribute 'affine_shape_detector'") raised in repr()] tensorflow_LAFAffineShapeEstimator object at 0x7f0028450fa0>
patch_size = 32, affine_shape_detector = None, preserve_orientation = True

    def __init__(
        self, patch_size=32, affine_shape_detector=None, preserve_orientation=True
    ):
        self.super___init__(
            patch_size=patch_size,
            affine_shape_detector=affine_shape_detector,
            preserve_orientation=preserve_orientation,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.affine_shape_detector = (
            affine_shape_detector
>           or tensorflow_PatchAffineShapeEstimator(self.patch_size)
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:484: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_PatchAffineShapeEstimator' object has no attribute 'eps'") raised in repr()] tensorflow_PatchAffineShapeEstimator object at 0x7f004dffb910>, patch_size = 32
eps = 1e-10

    def __init__(self, patch_size=19, eps=1e-10):
        from ..filters.sobel import tensorflow_sobel
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size: typing.Any = patch_size
>       self.gradient: typing.Any = tensorflow_sobel.SpatialGradient("sobel", 1)
E       AttributeError: 'function' object has no attribute 'SpatialGradient'

Translated_Outputs/tensorflow_outputs/kornia/feature/affine_shape.py:53: AttributeError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LAFAffineShapeEstimator
________________________________________________________________________________ test_LAFOrienter[tensorflow-s2s-False] ________________________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_LAFOrienter(target_framework, mode, backend_compile):
        print("kornia.feature.LAFOrienter")
    
        if backend_compile:
            pytest.skip()
    
        import os
        flag = os.environ.get("APPLY_TRANSPOSE_OPTIMIZATION")
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = "false"
    
        TranspiledLAFOrienter = ivy.transpile(kornia.feature.LAFOrienter, source="torch", target=target_framework)
    
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = flag
    
        laf = torch.rand(1, 2, 2, 3)
        img = torch.rand(1, 1, 32, 32)
        torch_out = kornia.feature.LAFOrienter()(laf, img)
    
        transpiled_laf = _nest_torch_tensor_to_new_framework(laf, target_framework)
        transpiled_img = _nest_torch_tensor_to_new_framework(img, target_framework)
>       transpiled_out = TranspiledLAFOrienter()(transpiled_laf, transpiled_img)

kornia/test_feature.py:1459: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFOrienter' object has no attribute 'angle_detector'") raised in repr()] tensorflow_LAFOrienter object at 0x7f0028630a00>, args = (), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <[AttributeError("'tensorflow_LAFOrienter' object has no attribute 'angle_detector'") raised in repr()] tensorflow_LAFOrienter object at 0x7f0028630a00>, patch_size = 32, num_angular_bins = 36
angle_detector = None

    @tensorflow_store_config_info
    def __init__(self, patch_size=32, num_angular_bins=36, angle_detector=None):
        self.super___init__(
            patch_size=patch_size,
            num_angular_bins=num_angular_bins,
            angle_detector=angle_detector,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.num_ang_bins = num_angular_bins
        self.angle_detector: typing.Any
        if angle_detector is None:
>           self.angle_detector = tensorflow_PatchDominantGradientOrientation(
                self.patch_size, self.num_ang_bins
            )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:1442: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08), args = (32, 36), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08), patch_size = 32, num_angular_bins = 36, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, patch_size=32, num_angular_bins=36, eps=1e-08):
        from ..filters.sobel import tensorflow_SpatialGradient
        from ...torch.nn.modules.conv import tensorflow_Conv1d
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..filters.kernels import tensorflow_get_gaussian_discrete_kernel1d
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            num_angular_bins=num_angular_bins,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.num_ang_bins = num_angular_bins
        self.gradient = tensorflow_SpatialGradient("sobel", 1)
        self.eps = eps
        self.angular_smooth = tensorflow_Conv1d(
            1, 1, kernel_size=5, padding=2, bias=False, padding_mode="circular"
        )
        self.angular_smooth.weight = tensorflow_set_item_bknd(
            self.angular_smooth.weight,
            slice(None, None, None),
>           tensorflow_get_gaussian_discrete_kernel1d(5, 1.6),
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:430: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kernel_size = 5, sigma = 1.6, force_even = False

    def tensorflow_get_gaussian_discrete_kernel1d(
        kernel_size, sigma, force_even=False, *, device=None, dtype=None
    ):
        tensorflow__check_kernel_size(kernel_size, allow_even=force_even)
>       return tensorflow_gaussian_discrete(kernel_size, sigma, device=device, dtype=dtype)

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:346: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

window_size = 5, sigma = <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.6]], dtype=float32)>

    def tensorflow_gaussian_discrete(window_size, sigma, *, device=None, dtype=None):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_exp_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ..core._backend import tensor
        from ..core._backend import concatenate
    
        if isinstance(sigma, (float,)):
            sigma = tensor([[sigma]], device=device, dtype=dtype)
        tensorflow_KORNIA_CHECK_SHAPE(sigma, ["B", "1"])
        sigma2 = sigma * sigma
        tail = int(window_size // 2) + 1
        bessels = [
>           ivy__modified_bessel_0(sigma2),
            tensorflow__modified_bessel_1(sigma2),
            *(tensorflow__modified_bessel_i(k, sigma2) for k in range(2, tail)),
        ]
E       NameError: name 'ivy__modified_bessel_0' is not defined

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:209: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.LAFOrienter
_____________________________________________________________________ test_PatchDominantGradientOrientation[tensorflow-s2s-False] ______________________________________________________________________

target_framework = 'tensorflow', mode = 's2s', backend_compile = False

    def test_PatchDominantGradientOrientation(target_framework, mode, backend_compile):
        print("kornia.feature.PatchDominantGradientOrientation")
    
        if backend_compile:
            pytest.skip()
    
        import os
        flag = os.environ.get("APPLY_TRANSPOSE_OPTIMIZATION")
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = "false"
    
        TranspiledPatchDominantGradientOrientation = ivy.transpile(kornia.feature.PatchDominantGradientOrientation, source="torch", target=target_framework)
    
        os.environ["APPLY_TRANSPOSE_OPTIMIZATION"] = flag
    
        patch = torch.rand(10, 1, 32, 32)
        torch_out = kornia.feature.PatchDominantGradientOrientation()(patch)
    
        transpiled_patch = _nest_torch_tensor_to_new_framework(patch, target_framework)
>       transpiled_out = TranspiledPatchDominantGradientOrientation()(transpiled_patch)

kornia/test_feature.py:1482: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08), args = (), kwargs = {}

    @functools.wraps(fn)
    def wrapper(self, *args, **kwargs):
>       fn(self, *args, **kwargs)

Translated_Outputs/tensorflow_outputs/ivy/utils/decorator_utils.py:232: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = tensorflow_PatchDominantGradientOrientation(patch_size=32, num_ang_bins=36, eps=1e-08), patch_size = 32, num_angular_bins = 36, eps = 1e-08

    @tensorflow_store_config_info
    def __init__(self, patch_size=32, num_angular_bins=36, eps=1e-08):
        from ..filters.sobel import tensorflow_SpatialGradient
        from ...torch.nn.modules.conv import tensorflow_Conv1d
        from ...ivy.functional.ivy.general import tensorflow_set_item_bknd
        from ..filters.kernels import tensorflow_get_gaussian_discrete_kernel1d
        from ..filters.kernels import tensorflow_get_gaussian_kernel2d
    
        self.super___init__(
            patch_size=patch_size,
            num_angular_bins=num_angular_bins,
            eps=eps,
            v=getattr(self, "_v", None),
            buffers=getattr(self, "_buffers", None),
            module_dict=getattr(self, "_module_dict", None),
        )
        self.patch_size = patch_size
        self.num_ang_bins = num_angular_bins
        self.gradient = tensorflow_SpatialGradient("sobel", 1)
        self.eps = eps
        self.angular_smooth = tensorflow_Conv1d(
            1, 1, kernel_size=5, padding=2, bias=False, padding_mode="circular"
        )
        self.angular_smooth.weight = tensorflow_set_item_bknd(
            self.angular_smooth.weight,
            slice(None, None, None),
>           tensorflow_get_gaussian_discrete_kernel1d(5, 1.6),
        )

Translated_Outputs/tensorflow_outputs/kornia/feature/orientation.py:430: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

kernel_size = 5, sigma = 1.6, force_even = False

    def tensorflow_get_gaussian_discrete_kernel1d(
        kernel_size, sigma, force_even=False, *, device=None, dtype=None
    ):
        tensorflow__check_kernel_size(kernel_size, allow_even=force_even)
>       return tensorflow_gaussian_discrete(kernel_size, sigma, device=device, dtype=dtype)

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:346: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

window_size = 5, sigma = <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.6]], dtype=float32)>

    def tensorflow_gaussian_discrete(window_size, sigma, *, device=None, dtype=None):
        from ..core.check import tensorflow_KORNIA_CHECK_SHAPE
        from ...ivy.functional.frontends.torch.tensor import tensorflow_exp_frnt_
        from ...ivy.functional.frontends.torch.tensor import tensorflow_sum_frnt_
        from ..core._backend import tensor
        from ..core._backend import concatenate
    
        if isinstance(sigma, (float,)):
            sigma = tensor([[sigma]], device=device, dtype=dtype)
        tensorflow_KORNIA_CHECK_SHAPE(sigma, ["B", "1"])
        sigma2 = sigma * sigma
        tail = int(window_size // 2) + 1
        bessels = [
>           ivy__modified_bessel_0(sigma2),
            tensorflow__modified_bessel_1(sigma2),
            *(tensorflow__modified_bessel_i(k, sigma2) for k in range(2, tail)),
        ]
E       NameError: name 'ivy__modified_bessel_0' is not defined

Translated_Outputs/tensorflow_outputs/kornia/filters/kernels.py:209: NameError
----------------------------------------------------------------------------------------- Captured stdout call -----------------------------------------------------------------------------------------
kornia.feature.PatchDominantGradientOrientation
--------------------------------------------------------------------------------------------- JSON report ----------------------------------------------------------------------------------------------
report saved to: test_report.json
======================================================================================= short test summary info ========================================================================================
FAILED kornia/test_feature.py::test_get_laf_descriptors[tensorflow-s2s-False] - TypeError: tensor([[[[0.2352, 0.2189],
FAILED kornia/test_feature.py::test_extract_patches_from_pyramid[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_feature.py::test_laf_is_inside_image[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of bool is not in the list of allowe...
FAILED kornia/test_feature.py::test_DenseSIFTDescriptor[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_DenseSIFTDescriptor.call().
FAILED kornia/test_feature.py::test_SIFTDescriptor[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_SIFTDescriptor.call().
FAILED kornia/test_feature.py::test_MKDDescriptor[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_VonMisesKernel.call().
FAILED kornia/test_feature.py::test_HardNet8[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_HardNet8.call().
FAILED kornia/test_feature.py::test_HyNet[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_mix...
FAILED kornia/test_feature.py::test_SOSNet[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: multiple targets found for assignment inspect.unwrap(tensorflow_local_response_norm).partial_mi...
FAILED kornia/test_feature.py::test_MultiResolutionDetector[tensorflow-s2s-False] - AssertionError: numpy array values are not all close
FAILED kornia/test_feature.py::test_ScaleSpaceDetector[tensorflow-s2s-False] - AttributeError: Exception encountered when calling tensorflow_ScaleSpaceDetector.call().
FAILED kornia/test_feature.py::test_KeyNetDetector[tensorflow-s2s-False] - ValueError: operands could not be broadcast together with shapes (1,0,2,3) (1,2,2,3)
FAILED kornia/test_feature.py::test_LAFDescriptor[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_SOLD2[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_Bottleneck2D.call().
FAILED kornia/test_feature.py::test_LocalFeature[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_SOLD2_detector[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_Bottleneck2D.c...
FAILED kornia/test_feature.py::test_DeDoDe[tensorflow-s2s-False] - KeyError: 'num_features'
FAILED kornia/test_feature.py::test_DISK[tensorflow-s2s-False] - tensorflow.python.framework.errors_impl.InvalidArgumentError: Exception encountered when calling tensorflow_PReLU.call().
FAILED kornia/test_feature.py::test_SIFTFeature[tensorflow-s2s-False] - NameError: name 'ivy__modified_bessel_0' is not defined
FAILED kornia/test_feature.py::test_SIFTFeatureScaleSpace[tensorflow-s2s-False] - NameError: name 'ivy__modified_bessel_0' is not defined
FAILED kornia/test_feature.py::test_GFTTAffNetHardNet[tensorflow-s2s-False] - NameError: name 'ivy__modified_bessel_0' is not defined
FAILED kornia/test_feature.py::test_KeyNetAffNetHardNet[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_KeyNetHardNet[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_LocalFeatureMatcher[tensorflow-s2s-False] - NameError: name 'ivy__modified_bessel_0' is not defined
FAILED kornia/test_feature.py::test_LightGlueMatcher[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_LightGlue[tensorflow-s2s-False] - NameError: name 'torch' is not defined
FAILED kornia/test_feature.py::test_LoFTR[tensorflow-s2s-False] - ivy.utils.exceptions.IvyException: Syntax Error: expected an indented block after 'try' statement on line 76 (<string>, line 77)
FAILED kornia/test_feature.py::test_PatchAffineShapeEstimator[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'SpatialGradient'
FAILED kornia/test_feature.py::test_LAFAffineShapeEstimator[tensorflow-s2s-False] - AttributeError: 'function' object has no attribute 'SpatialGradient'
FAILED kornia/test_feature.py::test_LAFOrienter[tensorflow-s2s-False] - NameError: name 'ivy__modified_bessel_0' is not defined
FAILED kornia/test_feature.py::test_PatchDominantGradientOrientation[tensorflow-s2s-False] - NameError: name 'ivy__modified_bessel_0' is not defined
============================================================================== 31 failed, 44 passed in 3444.23s (0:57:24) ==============================================================================

